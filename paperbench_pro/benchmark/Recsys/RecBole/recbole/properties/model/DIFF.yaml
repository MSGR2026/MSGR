# DIFF Model Configuration
# Paper: "DIFF: Dual Side-Information Filtering and Fusion for Sequential Recommendation" (SIGIR'25)

# Model Architecture
n_layers: 2                          # Number of transformer layers
n_heads: 2                           # Number of attention heads
hidden_size: 256                     # Hidden dimension
attribute_hidden_size: [256, 256]    # Hidden dimensions for each feature type
inner_size: 256                      # Feed-forward inner dimension
hidden_dropout_prob: 0.5             # Dropout probability
attn_dropout_prob: 0.5               # Attention dropout probability
hidden_act: 'gelu'                   # Activation function
layer_norm_eps: 1e-12                # Layer normalization epsilon
initializer_range: 0.02              # Weight initialization range

# Feature Configuration
selected_features: ['categories','brand']    # Feature fields to use (can add 'brand', etc.)
pooling_mode: 'sum'                  # Pooling mode: 'mean', 'max', 'sum', 'gate', 'raw'

# DIFF Specific Parameters
fusion_type: 'gate'                  # Fusion type: 'sum', 'concat', 'gate'
c: 5                                 # Frequency filter cutoff
alpha: 0.5                           # Balance between dual branches
align: True                          # Use alignment loss
lambda: 0.1                          # Alignment loss weight
lamdas: [0.1, 0.1]                   # Loss weights for different components
attribute_predictor: 'linear'        # 'linear' or 'MLP'

# Training Configuration
loss_type: 'CE'                      # Loss type: 'BPR' or 'CE'
train_neg_sample_args: ~             # Disable negative sampling for CE loss
weight_sharing: 'not'
temp: 1

