{
  "id": "LightTS_2022",
  "paper_title": "Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures",
  "alias": "LightTS",
  "year": 2022,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "LightTS proposes a fast MLP-based architecture for multivariate time series forecasting using light sampling-oriented structures. The core innovations are: (1) Two sampling strategies (continuous sampling and interval sampling) that transform time series into non-overlapping sub-sequences to capture both short-term local patterns and long-term global patterns without eliminating tokens, and (2) Information Exchange Blocks (IEBlock) with a bottleneck design that efficiently exchange information along temporal and channel dimensions through weight-shared MLPs, enabling the model to handle very long sequences efficiently while maintaining high accuracy.",
  "introduction": "# 1 INTRODUCTION\n\nMultivariate time series, as one of fundamental real-word data types, consists of more than one time-dependent variable, and, more importantly, each variable depends not only on its past values but also has some dependency on other variables. Multivariate time series forecasting has become a critical application task in various domains, including economics, traffic, energy, and healthcare. The vital modeling challenge lies in capturing 1) sophisticated temporal patterns (both short-term local patterns and long-term global patterns) of each variable as well as 2) complex inter-dependency among different variables. Due to the ability of deep learning to model complex patterns, especially its successful applications in computer vision and natural language processing tasks, there have been significantly growing research interests to apply deep neural networks into multivariate time series forecasting [15] rather than traditional methods (such as ARIMA [3], Holt-Winters [5], etc).\n\nParticularly, with increasing computing power and the development of neural network architectures, many recent studies turned their eyes upon the realms of RNNs, GNNs, and Transformers. For instance, LSTM [8] and TPR-LSTM [17] used the hybrids of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and the Attention Mechanism [22] to capture the long/short-term dependencies. Informer [27] and Autoformer [23] further explored the potential of Transformer in capturing very long-range dependency of time series. MTGNN [24], StemGNN [4], and IGMTF [25] used Graph Neural Networks (GNNs) to explicitly model the dependencies among variables. While all these proposed complex deep learning models have demonstrated promising performance under specific scenarios, the introduced sophisticated\n\nneural network architectures usually indicate computationally expensive training and inference process, especially when facing the time series with long input length and many multiple correlated variables. Moreover, complex neural network architectures are always data-hungry due to the large number of parameters [7, 27], which may cause the trained models not robust when the amount of available training data is limited. Motivated by the challenges above, we naturally raise a question, is it necessary to apply complex and computationally expensive models to achieve state-of-the-art performance in multivariate time series forecasting?\n\nIn this paper, we explore the possibility of using simple and lightweight neural network architectures, i.e., merely using simple multi-layer perceptron (MLP) structure, for accurate multivariate time series forecasting. Specifically, relying on a critical observation that down-sampling time series often preserves the majority of its information [11] due to the trending, seasonal and irregular characteristics of time series, we thus propose LightTS, a light deep learning architecture constructed exclusively based on simple MLP-based structures. The key idea of LightTS is to apply an MLP-based structure on top of two delicate down-sampling strategies, including interval sampling and continuous sampling, inspired by crucial characteristics of multivariate time series. To be more concrete, continuous sampling focuses on capturing the short-term local patterns, while interval sampling focuses on capturing the long-term dependencies. On top of the sampling strategies, we propose an MLP-based architecture that can exchange information among different down-sampled sub-sequences and time steps. In such a way, the model can adaptively select useful information for forecasting from both local and global patterns. Furthermore, since each time the model only needs to deal with a fraction of the input sequence after down-sampling, our model is highly efficient in handling time series with very long input length.\n\nThe contributions of this paper are summarized as follows:\n\n- We propose LightTS, a simple architecture that is highly efficient and accurate in multivariate time series forecasting tasks. To the best of our knowledge, this is the first work that demonstrates the great potential of the MLP-based structures in multivariate time series forecasting.  \n- We propose continuous and interval sampling according to the special property of time series. The sampling methods help capture long/short-term patterns effectively and enable greater efficiency for long input sequences.  \n- Experimental results show that LightTS is competent in both short sequence and long sequence forecasting tasks. LightTS outperforms the state-of-the-art methods on 5 out of 8 widely-used benchmark datasets. In addition, LightTS is highly efficient. LightTS uses less than  $5\\%$  FLOPS on the largest benchmark dataset compared with previous SOTA methods. Moreover, LightTS is robust and has a much smaller variance in forecasting accuracy than previous SOTA methods in long sequence forecasting tasks.\n\nWe hope that our results can inspire further research beyond the realms of RNNs, GNNs, and Transformers in time series forecasting. We organize the rest of the paper as follows. We start with a survey of related work in Section 2. We formally define our problem in 3.1, and we present our model LightTS in Section 3.2. We present\n\nexperimental results in Section 4. We provide discussion in Section 5. We conclude our paper in Section 6.\n",
  "method": "# 2 RELATED WORK\n\nTime series forecasting methods can generally be classified into statistical and deep-learning-based methods.\n\n# 2.1 Statistical Methods\n\nStatistical methods for time series forecasting have been studied for a long time. Traditional methods include auto-regression (AR), moving average (MA), and auto-regressive moving average (ARMA). The auto-regressive integrated moving average model (ARIMA) [3] extends the ARMA model by incorporating the notion of integration. The vector autoregressive model (VAR) [3] is an extension of the AR model and captures the linear dependencies among different time series. Gaussian Process (GP) [6] is a Bayesian approach that models the distribution of multivariate time series over continuous functions. Statistical methods are popular for their simplicity and interpretability. However, those approaches require strong assumptions (such as the stationary assumption) and do not scale well to large multivariate time series data.\n\n# 2.2 Deep-learning-based Methods\n\nRecently, deep-learning-based methods have become increasingly popular in time series forecasting. LSTM [8] and TPR-LSTM [17] both employ the convolutional neural networks, the recurrent neural networks, and the attention mechanism [22] to model short-term local dependencies among different time series and long-term temporal dependencies. MTGNN [24], StemGNN [4], and IGMTF [25] leverage the graph neural networks to explicitly model the correlations among different time series. SCINet [11] downsamples the time series and uses convolutional filters to extract features and interact information. VARMLP [26] is a hybrid model that combines the AR model and an MLP with a single hidden layer, which is substantially different from LightTS we propose.\n\nLong sequence time series forecasting (LSTF) has received increasing attention in the forecasting community. LSTF is important for various real-world applications (e.g., energy consumption planning). RNN-based models such as LSTM [8] have limited prediction capacity in LSTF [27]. Informer [27] improves the vanilla Transformer [22] in time complexity and memory usage for LSTF tasks. Autoformer [23] uses decomposition and an auto-correlation mechanism to achieve better results. We show through experiments that LightTS is more efficient and effective for long sequence time series forecasting.\n\nProposing a pure MLP-based model for multivariate time series forecasting is partly motivated by N-BEATS [15]. N-BEATS is the first pure deep-learning-based method for univariate time series forecasting that achieves SOTA in the M4 competition[2] [13, 15]. In the ablation study of N-BEATS, where the backward residual connection is disabled, the architecture becomes a pure MLP-based structure. The architecture shows comparable results to N-BEATS\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/9dade731dbab3c5afb61943ee44f2a5cce1562e08eba3fca2a664070c93a4382.jpg)  \nFigure 1: The overview of LightTS. In Part I, the model captures the short/long-term dependencies and extract features of each time series. In Part II, the model learns the interdependencies among different time series and make predictions.\n\nand the winning solution of the M4 competition<sup>3</sup>. This finding supports the fact that, with careful design, MLP-based structures are powerful in capturing the historical patterns of time series.\n\nRecently, several MLP-based architectures have been proposed for computer vision, such as MLP-Mixer [20], gMLP [10], and ResMLP [21]. Such architectures leverage information exchange over channels and spatial tokens. Compared with those architectures, our model is different since we consider the information exchange both in the original input and down-sampled sub-sequences.\n\n# 3 OUR MODEL: LIGHTTS\n\n# 3.1 Problem Definition\n\nWe first formulate the problem of multivariate time series forecasting. At timestamp  $t$ , given a look-back window of fixed length  $T$ , we have a series of observations  $X_{t} = \\{x_{t - T + 1},\\dots,x_{t - 1},x_{t}|x_{i}\\in \\mathbb{R}^{N}\\}$  where  $N$  is the number of time series (i.e., variables in a multivariate sample). Given a forecasting horizon  $L$ , our goal is to predict either the values on multiple timestamps  $\\{x_{t + 1},\\dots,x_{t + L}\\}$ , which is called multi-step forecasting; or the value of  $x_{t + L}$ , which is called single-step forecasting. Long sequence time series forecasting usually has  $L$  much larger than one hundred [27].\n\n# 3.2 Architecture Overview\n\nWe present the overall architecture of LightTS in Figure 1. Recall that there are two major challenges in multivariate time series forecasting: (1) capture the short-term local patterns and long-term global patterns; (2) capture the interdependencies among different time series variables. LightTS also consists of two parts that correspond to the two challenges. In the first part, we treat different time series (i.e., input variables) independently without considering their interdependencies. This part aims to capture the short/long-term dependencies and extract the corresponding features of each time\n\nseries (the first challenge). In the second part, we concatenate all the time series and learn the correlations among different input variables (the second challenge).\n\nThe key components of these two parts are two sampling methods called continuous sampling and interval sampling, and three Information Exchange Block (IEBlock), where we describe the details in the following subsections.\n\n# 3.3 Continuous Sampling and Interval Sampling\n\nWe first present the sampling strategies used in LightTS. Compared with the other sequential data such as natural language and audio data, time series is a special sequence in that down-sampling the time series often preserves the majority of its information [11]. Nevertheless, the naive down-sampling method (such as the uniform sampling) could lead to information loss. Motivated by this, we design continuous and interval sampling, which helps the model capture both the local and global temporal patterns without eliminating any tokens.\n\nOur sampling methods transform each time series of length  $T$  to several non-overlapping sub-sequence with length  $C$ . For continuous sampling, each time we consecutively select  $C$  tokens as one sub-sequence. Thus, for a input sequence  $X_{t} \\in \\mathbb{R}^{T}$ , we down-sample it to  $\\frac{T}{C}$  sub-sequences and obtain a 2D matrix  $X_{t}^{\\text{con}} \\in \\mathbb{R}^{C \\times \\frac{T}{C}}$ , where the  $j$ -th column is\n\n$$\nX _ {t} ^ {\\text {c o n}} _ {. j} = \\left\\{x _ {t - T + (j - 1) \\cdot C + 1}, x _ {t - T + (j - 1) \\cdot C + 2}, \\dots , x _ {t - T + j \\cdot C} \\right\\} \\tag {1}\n$$\n\nFor interval sampling, each time we sample  $C$  tokens with a fixed time interval. Similar to continuous sampling, the  $j$ -th column of the down-sample matrix for interval sampling  $X_{t}^{int}$  is\n\n$$\nX _ {t} ^ {\\text {i n t}} \\cdot_ {j} = \\left\\{x _ {t - T + j}, x _ {t - T + j + \\lfloor \\frac {T}{C} \\rfloor}, x _ {t - T + j + 2 \\cdot \\lfloor \\frac {T}{C} \\rfloor}, \\dots , x _ {t - T + j + (C - 1) \\cdot \\lfloor \\frac {T}{C} \\rfloor} \\right\\} \\tag {2}\n$$\n\nThe proposed sampling methods help the models focus on specific temporal patterns. For example, for continuous sampling, we subsample the time series into continuous pieces without long-range\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/e3d83cdcec09eb65ea790f1ea1aa5581d3ef259121dcb40c18176fa9e884ef89.jpg)  \nFigure 2: The overview of IEBlock and the bottleneck design.\n\ninformation, thus the model would pay more attention to the short-term local patterns. For interval sampling, we put aside local details, and the model would focus on long-term global patterns. Such design makes the model more effective and efficient to train since we only need to handle a fraction of the input sequence, especially when the input sequence is very long. We emphasize that unlike the naive down-sampling methods, we do not eliminate any tokens here. Instead, we keep all the original tokens and transform them into several non-overlapping sub-sequences. The following section presents an MLP-based architecture to learn the useful features from the down-sampled sub-sequences.\n\n# 3.4 Information Exchange Block\n\nInformation Exchange Block (IEBlock) is the basic building block we design for LightTS. In short, an IIEBlock takes a  $2D$  matrix of shape  $H \\times W$ , where  $H$  is the temporal dimension and  $W$  is the channel dimension. The goal of an IIEBlock is to leverage the information exchange along different dimensions and outputs another feature map of shape  $F \\times W$  ( $F$  is the hyperparameter that corresponds to the output feature dimension). The obtained matrix can be regarded as the extracted features of the input matrix. IIEblocks are the key components of LightTS and are used in both the down-sampling part and the prediction part.\n\nWe present the architecture of IEBlock in Figure 2. We use  $Z = (z_{ij})_{H \\times W}$  to denote the input 2D matrix,  $z_{i} = (z_{1i}, z_{2i}, \\dots, z_{Hi})^{T}$  to denote the  $i$ -th column and  $z_{j} = (z_{j1}, z_{j2}, \\dots, z_{jW})$  to denote the  $j$ -th row. We first apply an MLP of  $\\mathbb{R}^H \\to \\mathbb{R}^{F'}$  on each column, where  $F' \\ll F$ . We refer to such operation as the temporal projection:\n\n$$\nz _ {\\cdot , i} ^ {t} = \\mathrm {M L P} (z _ {\\cdot , i}), \\forall i = 1, 2, \\dots , W\n$$\n\nThe temporal projection extracts features along the temporal dimension. We use weight sharing for all columns in the temporal projection for efficiency. Next, we apply an MLP of  $\\mathbb{R}^W\\to \\mathbb{R}^W$  on each row, which we refer to as the channel projection:\n\n$$\nz _ {j \\cdot} ^ {\\boldsymbol {c}} = \\mathrm {M L P} (z _ {j \\cdot} ^ {\\boldsymbol {t}}), \\forall j = 1, 2, \\dots , F ^ {'}.\n$$\n\nThe channel projection exchanges information among different channels but keeps the input shape unchanged. We also use weight sharing for all rows in the channel projection. Finally, we apply another MLP of  $\\mathbb{R}^{F^{\\prime}}\\rightarrow \\mathbb{R}^{F}$  on each column to map the feature dimension from  $F^{\\prime}$  to  $F$ , which we refer to such operation as the output projection:\n\n$$\nz _ {. i} ^ {o} = \\mathrm {M L P} (z _ {. i} ^ {c}), \\forall i = 1, 2, \\dots , W\n$$\n\nWe call such an architecture the \"bottleneck architecture\", where the middle layer feature dimension is far less than the output layer feature dimension.\n\nNote that we use IEBlocks in both the down-sampling part and prediction part. In different parts,  $H$  and  $W$  could have different meanings. For example, in the sampling part,  $H$  corresponds to the length of sub-sequence  $C$ , and  $W$  corresponds to the number of sub-sequences  $\\frac{T}{C}$ . In the prediction part,  $H$  corresponds to the feature dimension extracted in the first part, and  $W$  corresponds to the number of time series variables  $N$ .\n\nIn an IIEblock, we repeatedly apply the channel projection on each time step. When the input time series is long, such an operation leads to expensive computational costs. Therefore, we propose a bottleneck design in IIEblock. First, we use a temporal projection to map the number of rows from  $H$  to  $F'$ . Then, the channel projection is applied  $F'$  times to communicate information. Finally, we use an output projection to map the features  $F'$  to the desired output length  $F$ . We call this bottleneck design since the hyperparameter  $F'$  is much smaller than  $H$  and  $F$ .\n\n# 3.5 Training Procedure\n\nNow, we can describe the training procedure of LightTS. In the first part, we transform each time series with length  $T$  to a  $2D$  matrix with shape  $C \\times \\frac{T}{C}$ . We then use two IIEBlocks (IEBlock-A and IIEBlock-B in the Figure 1) to extract the corresponding temporal features and obtain the feature matrices with shape  $\\mathbb{R}^{F \\times \\frac{T}{C}}$ . Each feature matrix is then down-projected to  $\\mathbb{R}^F$  with a simple linear mapping  $\\mathbb{R}_{\\overline{C}}^{\\underline{T}} \\to \\mathbb{R}$ . Hence, we map each time series to a feature vector of dimension  $F$ . This part focuses on capturing the long/short-term temporal patterns without considering the correlations of different time series variables.\n\nIn the second part, we concatenate all the obtained features in the first part. We concatenate the features from continuous and interval sampling on the temporal dimension and all the time series variables on the channel dimension. We thus obtain an input matrix of shape  $\\mathbb{R}^{2F\\times N}$ . Finally, we introduce another IIEblock-C of  $\\mathbb{R}^{2F\\times N}\\rightarrow \\mathbb{R}^{\\bar{L}\\times N}$ . IIEblock-C aims to combine the short-term local patterns and long-term global patterns from the temporal dimension and the correlations among different input variables from the channel dimension. The output of IIEblock-C is our final prediction.\n",
  "experiments": "# 4 EXPERIMENTS\n\nWe validate LightTS on eight public benchmark datasets, from short sequence forecasting to long sequence forecasting. Following the previous studies [8, 27], we use the single-step setting for short sequence forecasting and the multi-step setting for long sequence forecasting. We demonstrate the advantages of LightTS over previous methods in terms of accuracy, efficiency, and robustness, respectively.\n\nTable 1: Dataset statistics  \n\n<table><tr><td>Datasets</td><td>Variants</td><td>Timesteps</td><td>Granularity</td><td>Start time</td></tr><tr><td>ETTh1</td><td>7</td><td>17420</td><td>1 hour</td><td>2016/7/1</td></tr><tr><td>ETTh2</td><td>7</td><td>17420</td><td>1 hour</td><td>2016/7/1</td></tr><tr><td>ETTm1</td><td>7</td><td>69680</td><td>15 minutes</td><td>2016/7/1</td></tr><tr><td>Weather</td><td>12</td><td>35064</td><td>1 hour</td><td>2010/1/1</td></tr><tr><td>Solar-Energy</td><td>137</td><td>17544</td><td>10 minutes</td><td>2006/1/1</td></tr><tr><td>Traffic</td><td>862</td><td>52560</td><td>1 hour</td><td>2015/1/1</td></tr><tr><td>Electricity</td><td>321</td><td>26304</td><td>1 hour</td><td>2012/1/1</td></tr><tr><td>Exchange-Rate</td><td>8</td><td>7588</td><td>1 day</td><td>1990/1/1</td></tr></table>\n\n# 4.1 Experimental Settings\n\n4.1.1 Datasets. In Table 1, we summarize the statistics of eight benchmark datasets. Following the experimental setting in the previous studies [11, 27], for short sequence forecasting, we use the SolarEnergy, Traffic, Electricity and Exchange-Rate datasets [8, 24]; for long sequence forecasting, we use the Electricity Transformer Temperature (ETTh1, ETTh2, ETTm1), Electricity and Weather datasets [27]. We provide more details about the datasets in Appendix A.1.\n\n4.1.2 Evaluation Metrics. For a fair comparison, we use the same evaluation metrics as previous studies [8, 27]. We use the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for long sequence forecasting. We use the Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR) for short sequence forecasting. Readers can find more detailed information in Appendix A.2.\n\n# 4.2 Baseline Methods for Comparisons\n\nWe summarize the baseline methods in the following:\n\n# 4.2.1 Short sequence forecasting.\n\n- AR: An auto-regressive model.  \n- VARMLP [26]. A hybrid model that combines the AR model and an MLP.  \nGP [16]. A Gaussian Process model.  \n- RNN-GRU [24]. A recurrent neural network with GRU hidden units.  \n- LSTNet [8]. A hybrid model that combines convolutional neural networks and recurrent neural networks.  \n- TPR-LSTM [17]. A hybrid model that combines recurrent neural networks and the attention mechanism.  \n- TCN [2]. A typical temporal convolutional network.  \n- SCINet [11]. A forecasting model with sample convolution and interaction.  \n- MTGNN [24]. A GNN-based method that explicitly models the correlations among different time series.\n\n4.2.2 Long sequence forecasting. In addition to LSTNet and SCINet, we compare with the following baseline methods for long sequence forecasting:\n\n- LogTrans [9]. A variant of Transformers with LogSparse self-attention mechanism.  \n- Reformer [7]. An efficient variant of Transformers with locality-sensitive hashing.  \n- LSTM [1]. A variant of recurrent neural networks with dynamic length of encoding vectors.  \n- Informer [27]. A variant of Transformers with ProbSparse self-attention mechanism.  \n- Autoformer [23]. A variant of Transformers with a decomposition forecasting architecture.\n\n# 4.3 Main Results\n\nTable 2 and Table 3 show the main results of LightTS. We can observe that LightTS can achieve state-of-the-art or comparable results in most cases. In the following, we discuss the results of short sequence forecasting and long sequence forecasting, respectively.\n\n4.3.1 Long sequence forecasting. Table 2 presents the experimental results on long sequence forecasting tasks. LightTS achieves state-of-the-art results over all the horizons in ETTh1, ETTh2, ETTm1, and Electricity datasets and achieves second best results in the Weather dataset. In particular, for the longest prediction horizon, LightTS lowers MSE by  $9.21\\%$ ,  $33.90\\%$ ,  $34.18\\%$ , and  $13.60\\%$  on the ETTh1, ETTh2, ETTm1, and Electricity datasets, respectively. Compared with transformer models (LogTrans, Reformer, Informer), RNN-based models (LSTNet, LSTM), and CNN-based models (TCN, SCINet), LightTS achieves significant improvement in long sequence forecasting tasks. On the one hand, continuous and interval sampling can help the model capture the short-term local patterns and long-term global dependencies. On the other hand, such a down-sampling design is critical for the model to process a very long input sequence efficiently.\n\n4.3.2 Short sequence forecasting. Table 3 presents the experimental results on the task of short sequence forecasting. We can observe that LightTS achieves state-of-the-art results on the Solar-Energy dataset. In particular, on Solar-Energy dataset, LightTS lowers down RSE by  $4.16\\%$ ,  $4.61\\%$ ,  $3.90\\%$ ,  $2.73\\%$  on horizon 3, 6, 12, 24 respectively. We also observe an inconsistency in the evaluation metrics of RSE and CORR. For example, on Traffic datasets, LightTS achieves state-of-the-art results in terms of RSE but slightly lags behind on CORR. For Traffic, Electricity, and Exchange-Rate datasets, none of the existing models can consistently outperform other models. LightTS provides comparable results to MTGNN and SCINet on these datasets. Moreover, as we will see in the next section, LightTS is efficient and has significant advantages over MTGNN and SCINet in FLOPS and running time.\n\n# 4.4 Comparisons on FLOPS and Running Time\n\nWe compare the FLOPS and running time of LightTS with previous SOTA models (Autoformer, MTGNN, SCINet). We present the results of the three largest datasets in short sequence forecasting and four datasets in long sequence forecasting in Table 4 and 5. The hyperparameters of LightTS, MTGNN, and SCINet align with\n\nTable 2: Baseline comparisons under multi-step setting for long sequence time series forecasting tasks.  \n\n<table><tr><td rowspan=\"3\">Methods</td><td rowspan=\"3\">Metrics</td><td colspan=\"3\">ETTh1</td><td colspan=\"3\">ETTh2</td><td colspan=\"3\">ETTm1</td><td colspan=\"3\">Weather</td><td colspan=\"3\">Electricity</td></tr><tr><td colspan=\"3\">horizon</td><td colspan=\"3\">horizon</td><td colspan=\"3\">horizon</td><td colspan=\"3\">horizon</td><td colspan=\"3\">horizon</td></tr><tr><td>168</td><td>336</td><td>720</td><td>168</td><td>336</td><td>720</td><td>96</td><td>288</td><td>672</td><td>168</td><td>336</td><td>720</td><td>336</td><td>720</td><td>960</td></tr><tr><td rowspan=\"2\">LogTrans</td><td>MSE</td><td>0.888</td><td>0.942</td><td>1.109</td><td>3.944</td><td>3.711</td><td>2.817</td><td>0.674</td><td>1.728</td><td>1.865</td><td>0.649</td><td>0.666</td><td>0.741</td><td>0.305</td><td>0.311</td><td>0.333</td></tr><tr><td>MAE</td><td>0.766</td><td>0.766</td><td>0.843</td><td>1.573</td><td>1.587</td><td>1.356</td><td>0.674</td><td>1.656</td><td>1.721</td><td>0.573</td><td>0.584</td><td>0.611</td><td>0.395</td><td>0.397</td><td>0.413</td></tr><tr><td rowspan=\"2\">Reformer</td><td>MSE</td><td>1.686</td><td>1.919</td><td>2.177</td><td>4.484</td><td>3.798</td><td>5.111</td><td>1.267</td><td>1.632</td><td>1.943</td><td>1.228</td><td>1.770</td><td>2.548</td><td>1.507</td><td>1.883</td><td>1.973</td></tr><tr><td>MAE</td><td>0.996</td><td>1.090</td><td>1.218</td><td>1.650</td><td>1.508</td><td>1.793</td><td>0.795</td><td>0.886</td><td>1.006</td><td>0.763</td><td>0.997</td><td>1.407</td><td>0.978</td><td>1.002</td><td>1.185</td></tr><tr><td rowspan=\"2\">LSTM</td><td>MSE</td><td>1.058</td><td>1.152</td><td>1.682</td><td>3.987</td><td>3.276</td><td>3.711</td><td>1.195</td><td>1.598</td><td>2.530</td><td>0.948</td><td>1.497</td><td>1.314</td><td>0.778</td><td>1.528</td><td>1.343</td></tr><tr><td>MAE</td><td>0.725</td><td>0.794</td><td>1.018</td><td>1.560</td><td>1.375</td><td>1.520</td><td>0.785</td><td>0.952</td><td>1.259</td><td>0.713</td><td>0.889</td><td>0.875</td><td>0.629</td><td>0.945</td><td>0.886</td></tr><tr><td rowspan=\"2\">LSTNet</td><td>MSE</td><td>1.865</td><td>2.477</td><td>1.925</td><td>1.442</td><td>1.372</td><td>2.403</td><td>2.654</td><td>1.009</td><td>1.681</td><td>0.676</td><td>0.714</td><td>0.773</td><td>0.357</td><td>0.442</td><td>0.473</td></tr><tr><td>MAE</td><td>1.092</td><td>1.193</td><td>1.084</td><td>2.389</td><td>2.429</td><td>3.403</td><td>1.378</td><td>1.902</td><td>2.701</td><td>0.585</td><td>0.607</td><td>0.643</td><td>0.391</td><td>0.433</td><td>0.443</td></tr><tr><td rowspan=\"2\">Informer</td><td>MSE</td><td>0.878</td><td>0.884</td><td>0.941</td><td>1.512</td><td>1.665</td><td>2.340</td><td>0.642</td><td>1.219</td><td>1.651</td><td>0.592</td><td>0.623</td><td>0.685</td><td>0.311</td><td>0.308</td><td>0.328</td></tr><tr><td>MAE</td><td>0.722</td><td>0.753</td><td>0.768</td><td>0.996</td><td>1.035</td><td>1.209</td><td>0.626</td><td>0.871</td><td>1.002</td><td>0.531</td><td>0.546</td><td>0.575</td><td>0.385</td><td>0.385</td><td>0.406</td></tr><tr><td rowspan=\"2\">Autoformer*</td><td>MSE</td><td>0.634</td><td>0.724</td><td>0.898</td><td>1.101</td><td>1.386</td><td>2.445</td><td>0.539</td><td>0.575</td><td>0.599</td><td>0.359</td><td>0.492</td><td>0.527</td><td>0.257</td><td>0.259</td><td>0.291</td></tr><tr><td>MAE</td><td>0.590</td><td>0.651</td><td>0.743</td><td>0.803</td><td>0.892</td><td>1.226</td><td>0.504</td><td>0.527</td><td>0.542</td><td>0.413</td><td>0.491</td><td>0.503</td><td>0.357</td><td>0.361</td><td>0.381</td></tr><tr><td rowspan=\"2\">SCINet*</td><td>MSE</td><td>0.450</td><td>0.528</td><td>0.597</td><td>0.554</td><td>0.657</td><td>1.118</td><td>0.197</td><td>0.350</td><td>1.214</td><td>0.515</td><td>0.540</td><td>0.577</td><td>0.198</td><td>0.234</td><td>0.272</td></tr><tr><td>MAE</td><td>0.453</td><td>0.513</td><td>0.571</td><td>0.517</td><td>0.576</td><td>0.776</td><td>0.294</td><td>0.405</td><td>0.836</td><td>0.504</td><td>0.521</td><td>0.549</td><td>0.304</td><td>0.332</td><td>0.361</td></tr><tr><td rowspan=\"2\">LightTS</td><td>MSE</td><td>0.429</td><td>0.466</td><td>0.542</td><td>0.416</td><td>0.497</td><td>0.739</td><td>0.175</td><td>0.272</td><td>0.391</td><td>0.511</td><td>0.527</td><td>0.554</td><td>0.176</td><td>0.219</td><td>0.235</td></tr><tr><td>MAE</td><td>0.443</td><td>0.468</td><td>0.536</td><td>0.448</td><td>0.499</td><td>0.610</td><td>0.267</td><td>0.335</td><td>0.420</td><td>0.495</td><td>0.509</td><td>0.525</td><td>0.279</td><td>0.318</td><td>0.329</td></tr><tr><td rowspan=\"2\">Improvement</td><td>MSE</td><td>4.67%</td><td>11.74%</td><td>9.21%</td><td>24.91%</td><td>24.35%</td><td>33.90%</td><td>11.17%</td><td>22.29%</td><td>34.18%</td><td>-42.34%</td><td>-7.11%</td><td>-5.12%</td><td>11.11%</td><td>6.41%</td><td>13.60%</td></tr><tr><td>MAE</td><td>2.21%</td><td>8.77%</td><td>6.13%</td><td>13.35%</td><td>13.37%</td><td>21.39%</td><td>9.18%</td><td>17.28%</td><td>24.60%</td><td>-19.85%</td><td>-3.67%</td><td>-4.37%</td><td>8.22%</td><td>4.22%</td><td>8.86%</td></tr></table>\n\nResults are taken from [27] (for results from LogTrans to Informer). Best result in one setting is marked in bold, and second best is marked in italic. *We use 5 seeds to calculate the average result of SCINet and AutoFormer. We follow the same look-back settings in Informer and SCINet [11, 27] for each dataset.\n\nTable 3: Baseline comparisons under single-step setting for short sequence time series forecasting tasks.  \n\n<table><tr><td rowspan=\"3\">Methods</td><td rowspan=\"3\">Metrics</td><td colspan=\"4\">Solar-Energy</td><td colspan=\"4\">Traffic</td><td colspan=\"4\">Electricity</td><td colspan=\"4\">Exchange-Rate</td></tr><tr><td colspan=\"4\">horizon</td><td colspan=\"4\">horizon</td><td colspan=\"4\">horizon</td><td colspan=\"4\">horizon</td></tr><tr><td>3</td><td>6</td><td>12</td><td>24</td><td>3</td><td>6</td><td>12</td><td>24</td><td>3</td><td>6</td><td>12</td><td>24</td><td>3</td><td>6</td><td>12</td><td>24</td></tr><tr><td rowspan=\"2\">AR</td><td>RSE</td><td>0.2435</td><td>0.3790</td><td>0.5911</td><td>0.8699</td><td>0.5991</td><td>0.6218</td><td>0.6252</td><td>0.6300</td><td>0.0995</td><td>0.1035</td><td>0.1050</td><td>0.1054</td><td>0.0228</td><td>0.0279</td><td>0.0353</td><td>0.0445</td></tr><tr><td>CORR</td><td>0.9710</td><td>0.9263</td><td>0.8107</td><td>0.5314</td><td>0.7752</td><td>0.7568</td><td>0.7544</td><td>0.7591</td><td>0.8845</td><td>0.8632</td><td>0.8691</td><td>0.8595</td><td>0.9734</td><td>0.9656</td><td>0.9526</td><td>0.9357</td></tr><tr><td rowspan=\"2\">VARMLP [26]</td><td>RSE</td><td>0.1922</td><td>0.2679</td><td>0.4244</td><td>0.6841</td><td>0.5582</td><td>0.6579</td><td>0.6023</td><td>0.6146</td><td>0.1393</td><td>0.1620</td><td>0.1557</td><td>0.1274</td><td>0.0265</td><td>0.0394</td><td>0.0407</td><td>0.0578</td></tr><tr><td>CORR</td><td>0.9829</td><td>0.9655</td><td>0.9058</td><td>0.7149</td><td>0.8245</td><td>0.7695</td><td>0.7929</td><td>0.7891</td><td>0.8708</td><td>0.8389</td><td>0.8192</td><td>0.8679</td><td>0.8609</td><td>0.8725</td><td>0.8280</td><td>0.7675</td></tr><tr><td rowspan=\"2\">GP [16]</td><td>RSE</td><td>0.2259</td><td>0.3286</td><td>0.5200</td><td>0.7973</td><td>0.6082</td><td>0.6772</td><td>0.6406</td><td>0.5995</td><td>0.1500</td><td>0.1907</td><td>0.1621</td><td>0.1273</td><td>0.0239</td><td>0.0272</td><td>0.0394</td><td>0.0580</td></tr><tr><td>CORR</td><td>0.9751</td><td>0.9448</td><td>0.8518</td><td>0.5971</td><td>0.7831</td><td>0.7406</td><td>0.7671</td><td>0.7909</td><td>0.8670</td><td>0.8334</td><td>0.8394</td><td>0.8818</td><td>0.8713</td><td>0.8193</td><td>0.8484</td><td>0.8278</td></tr><tr><td rowspan=\"2\">RNN-GRU</td><td>RSE</td><td>0.1932</td><td>0.2628</td><td>0.4163</td><td>0.4852</td><td>0.5358</td><td>0.5522</td><td>0.5562</td><td>0.5633</td><td>0.1102</td><td>0.1144</td><td>0.1183</td><td>0.1295</td><td>0.0192</td><td>0.0264</td><td>0.0408</td><td>0.0626</td></tr><tr><td>CORR</td><td>0.9823</td><td>0.9675</td><td>0.9150</td><td>0.8823</td><td>0.8511</td><td>0.8405</td><td>0.8345</td><td>0.8300</td><td>0.8597</td><td>0.8623</td><td>0.8472</td><td>0.8651</td><td>0.9786</td><td>0.9712</td><td>0.9531</td><td>0.9223</td></tr><tr><td rowspan=\"2\">LSTNet [8]</td><td>RSE</td><td>0.1843</td><td>0.2559</td><td>0.3254</td><td>0.4643</td><td>0.4777</td><td>0.4893</td><td>0.4950</td><td>0.4973</td><td>0.0864</td><td>0.0931</td><td>0.1007</td><td>0.1007</td><td>0.0226</td><td>0.0280</td><td>0.0356</td><td>0.0449</td></tr><tr><td>CORR</td><td>0.9843</td><td>0.9690</td><td>0.9467</td><td>0.8870</td><td>0.8721</td><td>0.8690</td><td>0.8614</td><td>0.8588</td><td>0.9283</td><td>0.9135</td><td>0.9077</td><td>0.9119</td><td>0.9735</td><td>0.9658</td><td>0.9511</td><td>0.9354</td></tr><tr><td rowspan=\"2\">TCN</td><td>RSE</td><td>0.1940</td><td>0.2581</td><td>0.3512</td><td>0.4732</td><td>0.5459</td><td>0.6061</td><td>0.6367</td><td>0.6586</td><td>0.0892</td><td>0.0974</td><td>0.1053</td><td>0.1091</td><td>0.0217</td><td>0.0263</td><td>0.0393</td><td>0.0492</td></tr><tr><td>CORR</td><td>0.9835</td><td>0.9602</td><td>0.9321</td><td>0.8812</td><td>0.8486</td><td>0.8205</td><td>0.8048</td><td>0.7921</td><td>0.9232</td><td>0.9121</td><td>0.9017</td><td>0.9101</td><td>0.9693</td><td>0.9633</td><td>0.9531</td><td>0.9223</td></tr><tr><td rowspan=\"2\">TPR-LSTM</td><td>RSE</td><td>0.1803</td><td>0.2347</td><td>0.3234</td><td>0.4389</td><td>0.4487</td><td>0.4658</td><td>0.4641</td><td>0.4765</td><td>0.0823</td><td>0.0916</td><td>0.0964</td><td>0.1006</td><td>0.0174</td><td>0.0241</td><td>0.0341</td><td>0.0444</td></tr><tr><td>CORR</td><td>0.9850</td><td>0.9742</td><td>0.9487</td><td>0.9081</td><td>0.8812</td><td>0.8717</td><td>0.8717</td><td>0.8629</td><td>0.9439</td><td>0.9337</td><td>0.9250</td><td>0.9133</td><td>0.9790</td><td>0.9709</td><td>0.9564</td><td>0.9381</td></tr><tr><td rowspan=\"2\">MTGNN</td><td>RSE</td><td>0.1778</td><td>0.2348</td><td>0.3109</td><td>0.4270</td><td>0.4162</td><td>0.4754</td><td>0.4461</td><td>0.4535</td><td>0.0745</td><td>0.0878</td><td>0.0916</td><td>0.0953</td><td>0.0194</td><td>0.0259</td><td>0.0349</td><td>0.0456</td></tr><tr><td>CORR</td><td>0.9852</td><td>0.9726</td><td>0.9509</td><td>0.9031</td><td>0.8963</td><td>0.8667</td><td>0.8794</td><td>0.8810</td><td>0.9474</td><td>0.9316</td><td>0.9278</td><td>0.9234</td><td>0.9786</td><td>0.9708</td><td>0.9551</td><td>0.9372</td></tr><tr><td rowspan=\"2\">SCINet*</td><td>RSE</td><td>0.1788</td><td>0.2319</td><td>0.3049</td><td>0.4249</td><td>0.4203</td><td>0.4447</td><td>0.4536</td><td>0.4477</td><td>0.0758</td><td>0.0852</td><td>0.0934</td><td>0.0973</td><td>0.0179</td><td>0.0249</td><td>0.0344</td><td>0.0462</td></tr><tr><td>CORR</td><td>0.9849</td><td>0.9735</td><td>0.9529</td><td>0.9026</td><td>0.8931</td><td>0.8802</td><td>0.8760</td><td>0.8783</td><td>0.9493</td><td>0.9386</td><td>0.9296</td><td>0.9272</td><td>0.9744</td><td>0.9655</td><td>0.9493</td><td>0.9279</td></tr><tr><td rowspan=\"2\">LightTS</td><td>RSE</td><td>0.1704</td><td>0.2212</td><td>0.2930</td><td>0.4133</td><td>0.3973</td><td>0.4335</td><td>0.4403</td><td>0.4416</td><td>0.0762</td><td>0.0876</td><td>0.0935</td><td>0.0985</td><td>0.0178</td><td>0.0246</td><td>0.0339</td><td>0.0453</td></tr><tr><td>CORR</td><td>0.9866</td><td>0.9761</td><td>0.9564</td><td>0.9065</td><td>0.8900</td><td>0.8731</td><td>0.8696</td><td>0.8699</td><td>0.9432</td><td>0.9304</td><td>0.9238</td><td>0.9191</td><td>0.9798</td><td>0.9710</td><td>0.9548</td><td>0.9360</td></tr><tr><td rowspan=\"2\">Improvement</td><td>RSE</td><td>4.16%</td><td>4.61%</td><td>3.90%</td><td>2.73%</td><td>4.54%</td><td>2.52%</td><td>1.30%</td><td>1.36%</td><td>-2.28%</td><td>-2.82%</td><td>-2.07%</td><td>-3.36%</td><td>-2.30%</td><td>-2.07%</td><td>0.59%</td><td>-2.03%</td></tr><tr><td>CORR</td><td>0.14%</td><td>0.20%</td><td>0.37%</td><td>-0.18%</td><td>-0.70%</td><td>-0.81%</td><td>-1.11%</td><td>-1.26%</td><td>-0.64%</td><td>-0.87%</td><td>-0.62%</td><td>-0.87%</td><td>0.08%</td><td>-0.02%</td><td>-0.17%</td><td>-0.22%</td></tr></table>\n\nResults of previous models are taken from [11]. Best result in one setting is marked in bold, and second best is marked in italic. *We use 5 seeds to calculate the average result of SCINet. We follow the same look-back settings as previous studies [8, 11, 17, 24] for each dataset.\n\nTable 4: The FLOPS of Autoformer, MTGNN, SCINet and LightTS [19]. The best model is marked in bold. We report the FLOPS of the models corresponding to the longest forecasting horizon of each dataset in Table 2 and 3. Solar: SolarEnergy. ECL: Electricity.  \n\n<table><tr><td rowspan=\"2\">FLOPS (M)</td><td colspan=\"4\">Long Sequence</td><td colspan=\"3\">Short Sequence</td></tr><tr><td>ETTh1</td><td>ETTm1</td><td>Weather</td><td>ECL</td><td>Traffic</td><td>Solar</td><td>ECL</td></tr><tr><td>Autoformer</td><td>7581</td><td>7581</td><td>7600</td><td>11652</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MTGNN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>2370</td><td>377</td><td>883</td></tr><tr><td>SCINet</td><td>6</td><td>17</td><td>15</td><td>5078</td><td>16348</td><td>205</td><td>57</td></tr><tr><td>LightTS</td><td>4</td><td>3</td><td>7</td><td>328</td><td>90</td><td>10</td><td>30</td></tr></table>\n\n- dash denotes that the method does not implement on this task.\n\nTable 5: We report the running time of one epoch in seconds for Autoformer, MTGNN, SCINet and LightTS. We report the running time of each dataset with the longest forecasting horizon in Table 2 and 3. The experimental environment and batch size are the same for each model. The best model is marked in bold. Solar: Solar-Energy. ECL: Electricity.  \n\n<table><tr><td rowspan=\"2\">Time (s)</td><td colspan=\"4\">Long Sequence</td><td colspan=\"3\">Short Sequence</td></tr><tr><td>ETTh1</td><td>ETTm1</td><td>Weather</td><td>ECL</td><td>Traffic</td><td>Solar</td><td>ECL</td></tr><tr><td>Autoformer</td><td>80</td><td>355</td><td>280</td><td>330</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MTGNN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>1470</td><td>213</td><td>850</td></tr><tr><td>SCINet</td><td>80</td><td>850</td><td>260</td><td>950</td><td>455</td><td>275</td><td>3750</td></tr><tr><td>LightTS</td><td>2</td><td>9</td><td>7</td><td>160</td><td>33</td><td>33</td><td>50</td></tr></table>\n\n- dash denotes that the method does not implement on this task.\n\nthe ones that generate the forecasting results in Table 2 and 3. We can observe that LightTS has significant advantages in FLOPS and running time in both short sequence and long sequence forecasting. For the Traffic dataset (the largest dataset with the greatest number of variables), the FLOPS of LightTS is  $96.2\\%$  smaller than MTGNN and  $99.4\\%$  smaller than SCINet. In addition, LightTS achieves a speedup of  $44.5\\mathrm{x}$  over MTGNN and  $13.8\\mathrm{x}$  over SCINet in terms of running time per epoch. For the Electricity dataset in long sequence forecasting tasks, the FLOPS of LightTS is  $93.5\\%$  smaller than SCINet and  $97.2\\%$  smaller than Autoformer. We perform the experiments on a server with two 12-core Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz and one Tesla V100 PCIe 16GB.\n\n# 4.5 Robustness Analysis\n\nRobustness is a critical problem in long sequence time series forecasting. A wrong forecast on the trend or seasonality can gradually accumulate over time, resulting in incredibly misleading forecasting. During the experiments, we find that previous models for long sequence forecasting are not robust across random seeds, while LightTS can provide stable results. We present the standard deviation of AutoFormer, SCINet, and LightTS on the ETTh1, ETTm1, Weather, and Electricity datasets in Table 6. We can observe that LightTS has a much smaller variance in prediction accuracy than AutoFormer and SCINet. We also present the shaded area of the forecasting by different random seeds for LightTS, SCINet, and\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/f7d1b216f7c0dd969380a89a3587cf80a6df5bd90c07b0d5a2e1593242bc61e0.jpg)  \n(a) LightTS\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/e7cfc0e1cd982e28c015f28ebc62186a48c1ed7235170d58863e5998b18f1d8c.jpg)  \n(b) SCINet\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/4fb825fe09a23534f9b5f013c6f3f1e74e4b1f571f88927d1153cc898d06567b.jpg)  \n(c) AutoFormer\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/c08df1fcd7fb5d5d68d6c9a2cd3e0c331845120de1204d8408db51014315f55c.jpg)  \n(a) LightTS\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/fcb716311e06a9e5807c0476288a0401d30c899d95e73f160f4d8c3a87c29abf.jpg)  \n(b) SCINet\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/a2878b1b812d166a3e63e6e12893b42c3ef82f9c0a140fbb754e0ccf9d02979a.jpg)  \n(c) AutoFormer  \nFigure 3: Forecasting sequence randomly selected from ETTh1 (upper row) and Electricity (lower row). The shaded area is the range of forecasting results under five different random seeds for training.\n\nTable 6: The mean and standard deviation of the forecasting accuracy of LightTS, AutoFormer, and SCINet in long sequence forecasting. We report the results of the longest forecasting horizon for different datasets in Table 2. The results are in the form of  $mean_{std}$  estimated by five random seeds. LightTS has a much smaller variance in forecasting accuracy than AutoFormer and SCINet.  \n\n<table><tr><td>Methods</td><td>Metrics</td><td>ETTh1</td><td>ETTm1</td><td>Weather</td><td>Electricity</td></tr><tr><td rowspan=\"2\">AutoFormer</td><td>MSE</td><td>0.8980.039</td><td>0.5990.075</td><td>0.5270.059</td><td>0.2910.028</td></tr><tr><td>MAE</td><td>0.7430.019</td><td>0.5420.027</td><td>0.5030.033</td><td>0.3810.019</td></tr><tr><td rowspan=\"2\">SCINet</td><td>MSE</td><td>0.5970.013</td><td>1.2140.274</td><td>0.5770.003</td><td>0.2720.012</td></tr><tr><td>MAE</td><td>0.5710.010</td><td>0.8360.104</td><td>0.5490.002</td><td>0.3610.009</td></tr><tr><td rowspan=\"2\">LightTS</td><td>MSE</td><td>0.4980.002</td><td>0.3580.001</td><td>0.5540.003</td><td>0.2350.003</td></tr><tr><td>MAE</td><td>0.5120.002</td><td>0.3880.001</td><td>0.5250.002</td><td>0.3290.003</td></tr></table>\n\nAutoFormer in Figure 3. We can observe that LightTS has a much smaller shaded area of forecasting across different random seeds than SCINet and AutoFormer. LightTS has a significant advantage over existing methods in long sequence forecasting from both accuracy and robustness.\n\n# 4.6 Ablation Study\n\nWe conduct an ablation study to investigate the effectiveness of the components we propose in LightTS. We name LightTS without different components as follows:\n\n- w/o CP: LightTS without the channel projection. In this way, we do not rely on the interdependency of different time series to make predictions.  \n- w/o IS: LightTS without the interval sampling.  \n- w/o CS: LightTS without the continuous sampling.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/966637febb35bad64d70b5bb25e26ec54617b0ecc815b656f8450eaba468b129.jpg)  \n(a) LightTS\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/799f515f9b7ccbe5545492c8ee41a00ac1f8aa23dd95f24a9ccc9e25e81bb5d8.jpg)  \n(b) LightTS without Interval Sampling  \nFigure 4: The ground truth (black) and the forecast (orange) for one variable in the Traffic dataset with the forecasting horizon of 24. The variable has daily patterns (short-term local patterns) and weekly patterns (long-range patterns). The green arrows point at daily patterns, where (a) and (b) which have continuous sampling are more accurate in forecasting than (c). The blue arrows point at weekly patterns, where (a) and (c) which have interval sampling are more accurate in forecasting than (b).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/8098d4e1-4319-48ee-9d5c-206d0fb2a0fb/cf213e87dd26216d82d1080056c7bff7e01a5ca4edabe624c4ff87a91d3e9204.jpg)  \n(c) LightTS without Continuous Sampling\n\nWe repeat each experiment 5 times with different random seeds. We report the mean and standard deviation of RSE and CORR over five runs in Table 7. The introduction of channel projection significantly improves the performance of Solar and Electricity datasets. The interdependencies in these two datasets are evident. On the contrary, the interdependency in the Exchange-Rate dataset is not evident. The observation is similar in MTGNN [24] where MTGNN also fails to achieve desirable results in Exchange-Rate. Removing channel projection improves the results in the Exchange-Rate dataset. When the interdependencies among different time series are not evident, forcing the model to capture interdependency may negatively impact the model's performance. The effects of interval and continuous sampling are evident for all the datasets. We also demonstrate how interval sampling and continuous sampling affect the forecasting of LightTS in Figure 4. The results demonstrate that continuous sampling helps LightTS capture the short-term local patterns, and interval sampling helps LightTS capture the long-range patterns.\n\nTable 7: The results of the ablation study on different datasets. The forecasting horizon is set to 24 for each dataset. Best result in each setting is marked in bold. Solar: Solar-Energy. Exchange: Exchange-Rate.  \n\n<table><tr><td>Methods</td><td>Metrics</td><td>Solar</td><td>Traffic</td><td>Electricity</td><td>Exchange</td></tr><tr><td rowspan=\"2\">LightTS</td><td>RSE</td><td>0.4133</td><td>0.4416</td><td>0.0985</td><td>0.0453</td></tr><tr><td>CORR</td><td>0.9065</td><td>0.8699</td><td>0.9191</td><td>0.9360</td></tr><tr><td rowspan=\"2\">w/o CP</td><td>RSE</td><td>0.4469</td><td>0.4489</td><td>0.0998</td><td>0.0441</td></tr><tr><td>CORR</td><td>0.8912</td><td>0.8639</td><td>0.9119</td><td>0.9364</td></tr><tr><td rowspan=\"2\">w/o IS</td><td>RSE</td><td>0.4166</td><td>0.4571</td><td>0.1008</td><td>0.0486</td></tr><tr><td>CORR</td><td>0.9062</td><td>0.8591</td><td>0.9126</td><td>0.9345</td></tr><tr><td rowspan=\"2\">w/o CS</td><td>RSE</td><td>0.4174</td><td>0.4425</td><td>0.0997</td><td>0.0475</td></tr><tr><td>CORR</td><td>0.9044</td><td>0.8698</td><td>0.9193</td><td>0.9333</td></tr></table>\n",
  "hyperparameter": "Key hyperparameters include: C (sub-sequence length after sampling, determines how the input sequence of length T is divided into T/C sub-sequences), F (output feature dimension of IEBlock), F' (bottleneck feature dimension, where F' << F for efficiency, used in the temporal projection before channel projection). The paper uses look-back window T and forecasting horizon L that vary by dataset (e.g., T=168-720 for long sequence tasks). The model uses 3 IEBlocks total: 2 for sampling parts (IEBlock-A for continuous sampling, IEBlock-B for interval sampling) and 1 for prediction (IEBlock-C that maps from 2F×N to L×N). Batch sizes and learning rates follow standard settings from baseline papers but specific values are not explicitly mentioned in the method/experiments sections."
}