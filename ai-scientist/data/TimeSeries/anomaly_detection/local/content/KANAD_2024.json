{
  "id": "KANAD_2024",
  "paper_title": "KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks",
  "alias": "KANAD",
  "year": 2024,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "KAN-AD proposes a novel time series anomaly detection framework based on Kolmogorov-Arnold Networks that leverages the smoothness property of normal sequences. The core innovation is the Function Deconstruction (FD) mechanism, which decomposes time series into univariate Fourier series components (sine and cosine functions) and learns their coefficients through 1D CNNs. This approach transforms normal pattern modeling into a weighted combination of smooth univariate functions, filtering out local noise while capturing global periodic patterns. The method achieves state-of-the-art performance with minimal parameters (274 parameters) by avoiding overfitting to fine-grained structures and focusing on learning Fourier coefficients rather than complex feature representations.",
  "introduction": "# 1. Introduction\n\nTime Series Anomaly Detection (TSAD) serves as a critical component in modern IT infrastructure (Li et al., 2019; Qu et al., 2024) and manufacturing systems (Zhan et al., 2021; Wang et al., 2022), enabling rapid identification of potential anomalies and providing sufficient clues for fault localization (Sun et al., 2024; Kieu et al., 2022). The emergence of deep learning-based forecasting approaches (Xu et al., 2022; Wu et al., 2023; Zhou et al., 2023) have superseded traditional rule-based methods (Breunig et al., 2000; Siffer et al., 2017), establishing new state-of-the-art performance through their capacity to fit historical data and detect anomalies via prediction-observation comparisons.\n\nHowever, the effectiveness of the forecasting-based approach declines when encountering time series with localized disturbances. As illustrated in Figure 1, time series data frequently exhibit local peaks and drops that can significantly impact model learning. Existing deep learning methods (Tuli et al., 2022; Wu et al., 2023) often overfit to these local disturbances, compromising their ability to detect anomalies effectively. From the third column of Figure 2, we can observe that compared to training with clean data, TimesNet (Wu et al., 2023) trained on noisy data fails to detect anomalies in the samples.\n\nOur experimental analysis reveals that forecasting-based\n\nTSAD methods suffer performance degradation by attempting to model every detailed patterns in raw time series data. While these methods aim to identify anomalies through comparison with predicted behavior, such detailed modeling proves unnecessary and potentially detrimental, especially given that real-world time series typically contain various forms of anomalies and irrelevant disturbances, presenting two significant challenges: firstly, the difficulty in establishing a universal criterion for filtering these disturbances, and secondly, developing another model to ensure the forecasting model's input is free of local disturbances is resource-intensive. Given these inherent limitations in both filtering-based and dual-modeling approaches, researchers have explored VAE-based approaches to address the challenge of local disturbance mitigation. VAE-based approaches (Xu et al., 2018; Wang et al., 2024) assume that normal patterns in time series cluster in a low-dimensional latent space and can be effectively reconstructed, thereby overcoming interference from data perturbations. Nevertheless, as demonstrated in FCVAE (Wang et al., 2024), VAE-based approaches struggle with underfitting, which impairs their ability to reconstruct the original time series and limits their effectiveness.\n\nTo mitigate local disturbances, we reformulate TSAD by approximating time series using smooth univariate functions, building on the theoretical foundation that normal sequences exhibit greater local smoothness than abnormal ones (Xu et al., 2022). To achieve this formulation, Kolmogorov-Arnold Networks (KANs) (Liu et al., 2025) offer a promising direction by decomposing complex objectives into combinations of learnable univariate functions based on the Kolmogorov-Arnold representation theorem (Kolmogorov, 1957). This decomposition approach has shown remarkable effectiveness in various domains (Yu et al., 2024; Bodner et al., 2024). However, direct application of KAN to TSAD presents significant challenges. From the fourth column in the upper part of Figure 2, it can be observed that models trained on clean training samples can detect anomalies in the test samples. But we find that KAN fails to detect anomalies when the training samples contain noisy samples. The main reason is that, although KAN can specify univariate functions, i.e., B-spine function, these functions are not specifically designed for time series and can still overfit local features, failing to completely eliminate the impact of local peaks or drops.\n\nTo address these challenges, we propose KAN-AD, adopting KAN as our backbone. By considering the characteristics of time series, we redesign KAN in three aspects. First, we replace the B-spine function with Fourier series. Fourier series have local smoothness compared to spline functions, while their natural periodicity allows for better modeling of global patterns (Dym & HP, 1972; Stein & Shakarchi, 2011). Second, as the Fourier series contains unlimited\n\nterms which is computation intensive, we only use the first  $N$  terms of Fourier series. To overcome the limitation that the first N terms of Fourier series can only model periodic no smaller than  $\\frac{1}{N}$ , we designed an alternative index-based univariate function to capture the fine-scale periodic missing from the first N terms. Third, we incorporated differencing to isolate time series trend effects on coefficient estimation, leading to improved modeling accuracy through more precise coefficients.\n\nOur comprehensive evaluation demonstrates that KAN-AD achieves  $15\\%$  higher F1 accuracy while being  $50\\%$  faster than the original KAN architecture. Our code is publicly available at https://github.com/CSTCloudOps/KAN-AD. Our contributions are as follows:\n\n- We reformulate the problem to assist deep learning-based forecasting models for time series anomaly detection (TSAD) tasks by minimizing overfitting to local perturbations.  \n- We introduce KAN-AD, an innovative TSAD approach. KAN-AD, built meticulously on the KAN backbone, exhibits substantial improvements in both detection precision and inference efficiency.  \n- We performed comprehensive experiments on four publicly available datasets, verifying the effectiveness and efficiency against state-of-the-art TSAD benchmarks.\n\n",
  "method": "# 3. Methodology\n\nThe core challenge in time series anomaly detection (TSAD) lies in establishing accurate normal patterns while maintaining robustness to local disturbances (Li et al., 2021). Traditional approaches that directly predict based on historical data inevitably incorporate local noise into their learned patterns. Building on the observation that normal sequences exhibit greater smoothness than abnormal ones, we propose KAN-AD, a novel anomaly detection framework that leverages this smoothing feature to identify anomalies in complex time series data.\n\n# 3.1. Design of KAN-AD\n\nThe pipeline of KAN-AD consists of three main stages: mapping, reducing, and projection. In the mapping phase, we decompose the input time window into multiple univariate functions. The reducing phase then combines these functions through learned coefficients to reconstruct the \"normal\" pattern. Finally, the projection phase leverages this pattern to predict future behavior, enabling anomaly detection through comparison with real-time observations.\n\n$$\nf \\left(x _ {0: i}\\right) = A _ {0} + \\underbrace {\\sum_ {n = 1} ^ {N} \\left(A _ {n} \\cos \\left(n x _ {0 : i}\\right) + B _ {n} \\sin \\left(n x _ {0 : i}\\right)\\right)} _ {g \\left(x _ {0: i}\\right)} + \\epsilon \\tag {3}\n$$\n\n$$\n\\mathbf {H} = \\operatorname {S t a c k} \\left(\\cos \\left(x _ {0: i}\\right), \\sin \\left(x _ {0: i}\\right), \\dots , \\cos \\left(n x _ {0: i}\\right), \\sin \\left(n x _ {0: i}\\right)\\right)\n$$\n\n$$\n\\Theta \\left(x _ {0: i}\\right) = \\left[ A _ {1}, B _ {1}, A _ {2}, B _ {2}, \\dots , A _ {n}, B _ {n} \\right]\n$$\n\n$$\nx _ {0: i} ^ {\\prime} = A _ {0} + \\boldsymbol {\\Theta} \\left(x _ {0: i}\\right) \\times \\mathbf {H} \\tag {4}\n$$\n\nFormally, we employ Fourier series for normal pattern representation, motivated by two key advantages over alternative approaches such as B-spline functions. First, the constituent sine and cosine functions exhibit superior local smoothness, avoiding the potential overfitting to local noise. Second, Fourier series naturally capture global patterns, particularly excelling at modeling periodic behaviors in time series. Following this motivation, we introduce the function deconstruction (FD) mechanism, where  $f$ , the mapping between the historical window  $x_{0:i}$  and its next behavior  $x_{i+1}$ , can be expanded as shown in Equation (3). The normal pattern can be represented by the finite  $N$  terms of the series (Kolmogorov, 1957), denoted as  $g(x)$ , while the terms beyond  $N$  capture the stochastic observational noise  $\\epsilon$ . The normal pattern  $x_{0:i}'$  can then be expressed as in Equation (4), where  $\\mathbf{H}$  denotes the univariate function matrix. This decomposition combined with learnable coefficients filters out potential noise and significantly simplifies the construction of normal patterns.\n\n# 3.2. Mapping Phase\n\nAs shown in Figure 3b, the primary purpose of the mapping phase is to transform the original time series signal  $x_{0:i} \\in \\mathbb{R}^T$  into multiple new sets of values  $x_{0:i} \\in \\mathbb{R}^{T \\times (N + N)}$  through a series of univariate functions. Here,  $T$  is the size of the sliding window. The first  $N$  represents the number of sine series univariate functions, and the other  $N$  represents the number of cosine series univariate functions. The detailed calculation method is shown in Equation (3). Notably, besides the univariate function terms, an  $A_0$  term representing the average value within the sliding window is also present, which varies across different windows. To mitigate the impact of fluctuating  $A_0$  on coefficient fitting, a constant term elimination module is employed.\n\nConstant Term Elimination: In Fourier series,  $A_0$  represents the mean value of the function. Although normalization ensures that the entire time series has a mean of zero, individual time windows may still exhibit significant fluctuations in their means due to the presence of a trend. These variations in the constant term ultimately affect the model's accurate estimation of Fourier coefficients, leading to biases in the construction of the normal pattern.\n\nTo mitigate the impact of mean fluctuations on the model's approximation of normal time series patterns, we employ first-order differencing during data preprocessing to minimize the residual trend component in the data and subsequently renormalize the differenced data. This strategy\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/cdfe0879372669563d17104345b93afd8e7de9e02915c74dd3dc5bc856361d8f.jpg)\n\nKAN-AD (ours)  \n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/3a99cd3d7fad3eafbe5282cb8ffb087d0ba956653f46ef2337a2df99d76dbdeb.jpg)  \n(a) Illustration of learning components in KAN and KAN-AD. KAN-AD learns the coefficients on edges with fixed univariate functions, and performs weighted sum operations on nodes. Blue lines indicate edges with weights.\n\nFigure 3. Illustration of KAN-AD.  \n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/d9d36aba8b295ce1f395b365832a0cc0e799b14ed3e77175cfe6a5da93e0135d.jpg)  \n(b) Illustration of the KAN-AD process using a sliding window approach. During the mapping phase, raw time windows are transformed into multiple univariate functions. In the reducing phase, a one-dimensional convolutional kernel learns coefficients for these univariate functions, aggregating them into a normal pattern for the current time window. In the projection phase, a single-layer MLP predicts future normal patterns.\n\nallows the model to focus on estimating Fourier coefficients  $A_{1:n}$  and  $B_{1:n}$ , thereby avoiding the need to learn frequently changing constant terms. After this differential strategy, the normal pattern  $x_{0:i}^{\\prime}$  can be expressed as  $x_{0:i}^{\\prime} \\sim \\Theta(x_{0:i}) \\times \\mathbf{H}$\n\nPeriodic-Enhanced KAN-AD: Fourier series of finite  $N$  terms cannot model a period smaller than  $\\frac{1}{N}$ , which limits KAN-AD's ability to express time series containing more subtle periods.\n\nTo address this limitation and enhance the model's ability to capture periodic patterns in time series, we introduce additional univariate functions with different periods. Specifically, we incorporate trigonometric components  $\\cos\\left(\\frac{2\\pi ni}{T}\\right)$  and  $\\sin\\left(\\frac{2\\pi ni}{T}\\right)$  where  $i$  denotes the window index, with coefficients learned through one-dimensional convolution networks. Our implementation utilizes three complementary univariate functions shown in Equation (5): the raw time variable  $X$ , the Fourier series  $S_{n}$ , and the sine-cosine wave  $P_{n}$ . This integration of multi-periodic univariate functions enhances KAN-AD's capacity to model temporal patterns.\n\n$$\nX = x _ {0: i}\n$$\n\n$$\n\\mathrm {S} _ {\\mathrm {n}} = \\left\\{\\sin \\left(n x _ {0: i}\\right), \\cos \\left(n x _ {0: i}\\right) \\right\\} \\tag {5}\n$$\n\n$$\n\\mathrm {P _ {n}} = \\{\\sin (\\frac {2 \\pi n i}{T}), \\cos (\\frac {2 \\pi n i}{T}) \\}\n$$\n\n# 3.3. Reducing Phase\n\nAnother challenge in real-world time series anomaly detection is the high computational cost. Existing methods often sacrifice efficiency for accuracy, making them impractical in resource-constrained or large-scale settings.\n\nThe function deconstruction (FD) mechanism addresses this challenge by transforming the modeling of normal patterns into a weighted combination of univariate functions. This transformation substantially reduces the model's parameter quantity - instead of requiring numerous parameters for fine-grained feature modeling, FD mechanism achieves efficient representation through estimating coefficients of a small number of univariate functions.\n\n$$\n\\mathbf {H} ^ {(0)} = \\operatorname {S t a c k} (X, S _ {1}, P _ {1}, \\dots , S _ {n}, P _ {n}), \\forall n \\in [ 1, \\dots , N ] \\tag {6}\n$$\n\n$$\n\\mathbf {H} ^ {(l)} = \\operatorname {C N N} \\left(\\operatorname {C N N} \\left(\\mathbf {H} ^ {(l - 1)}\\right)\\right) \\quad \\forall l \\in [ 1, 2, \\dots , L ] \\tag {7}\n$$\n\n$$\n\\operatorname {C o n v} (\\mathbf {H}) = \\sum_ {c = 1} ^ {2 N} \\sum_ {m = 0} ^ {2} W _ {c} [ m ] \\cdot \\mathbf {H} _ {c} [ i + m - 1 ] \\tag {8}\n$$\n\n$$\nC N N (\\mathbf {H}) = \\operatorname {G E L U} (\\operatorname {B N} (\\operatorname {C o n v} (\\mathbf {H}))) \\tag {9}\n$$\n\nTo effectively estimate these univariate function coefficients, we employ a stacked one-dimensional convolutional neural network (1D CNN). This architecture choice is motivated by two key factors: 1D CNNs excel at sequence modeling through temporal dimension traversal, while their convolutional kernels naturally capture the diverse features introduced by the FD mechanism. As shown in Equation (6), KAN-AD first constructs a univariate function matrix  $\\mathbf{H}^{(0)}$  by combining the required functions for a given time window. This matrix is then processed through multiple stacked 1D convolutional layers with a kernel size of 3, progressively approximating the normal pattern through coefficient learning, as expressed in Equation (7). Here,  $L$  denotes the number of CNN blocks, with the network  $\\mathrm{CNN}(\\mathbf{H})$  and\n\nconvolution operation  $\\mathrm{Conv}(\\mathbf{H})$  defined in Equations (8) and (9). The convolution operation in Equation (8) applies a kernel  $W_{c}$  to each channel  $\\mathbf{H}_c$ , where indices  $m$  and  $t$  represent positions within the convolutional kernel and time window, respectively.\n\nTo ensure training stability and reduce internal covariate shift, we apply batch normalization (Ioffe & Szegedy, 2015) after each convolutional layer (Equation (9)), followed by Gaussian Error Linear Units (GELUs) (Hendrycks & Gimpel, 2016) for activation. The final stage of the reducing phase employs a residual connection (He et al., 2016) between the hidden state  $\\mathbf{H}^{(L)}$  and the original input  $\\mathbf{H}^{(0)}$  to maintain numerical stability, as shown in Equation (10). Finally, a 1-width convolutional kernel reduces the dimensionality of  $\\mathbf{H}^{(L)'}$  to generate the normal pattern approximation  $x_{0:i}'$  within the current time window:\n\n$$\n\\mathbf {H} ^ {(L) ^ {\\prime}} = \\mathbf {H} ^ {(L)} + \\mathbf {H} ^ {(0)} \\tag {10}\n$$\n\n$$\nx _ {0: i} ^ {\\prime} = \\operatorname {G E L U} (\\operatorname {B N} (\\operatorname {D o w n C o n v} \\left(\\mathbf {H} ^ {(L) ^ {\\prime}}\\right)) \\tag {11}\n$$\n\nHere,  $\\mathrm{DownConv}(\\mathbf{H}) = \\sum_{c=1}^{2N} W_c \\cdot \\mathbf{H}_c[i]$  denotes the convolution operation for reducing dimensions.\n\n# 3.4. Projection Phase\n\nAfter obtaining the current window's normal mode approximation  $x_{0:i}^{\\prime}$ , we predict the future behavior  $x_{i+1}$  through a single-layer MLP, leveraging KAN-AD's accurate approximation capability:\n\n$$\nx _ {i + 1} = W \\cdot x _ {0: i} ^ {\\prime} + b \\tag {12}\n$$\n\nwhere  $W$  and  $b$  denote the weight matrix and bias term of the linear layer.\n",
  "experiments": "# 4. Evaluation\n\nIn this section, we conduct comprehensive experiments primarily aimed at answering the following research questions.\n\nRQ1: How does KAN-AD compare to state-of-the-art anomaly detection methods in performance and efficiency?\n\nRQ2: How sensitive is KAN-AD to hyperparameters?\n\nRQ3: How effective is each design choice in KAN-AD?\n\nRQ4: How sensitive is KAN-AD to anomalies in the training data?\n\nIn addition, we also evaluate our method on a multivariate time series anomaly detection dataset to demonstrate the application potential of KAN-AD in more scenarios.\n\n# 4.1. Experimental settings\n\n# 4.1.1. DATASET\n\nWe evaluate KAN-AD on four publicly available UTS datasets: KPI (Competition, 2018), TODS (Lai et al., 2021),\n\nTable 1. Dataset Statistics.  \n\n<table><tr><td>Dataset</td><td>Curves</td><td>Train</td><td>Train</td><td>Ano%</td><td>Test</td><td>Test</td><td>Ano%</td></tr><tr><td>KPI</td><td>29</td><td>3,073,567</td><td colspan=\"2\">2.70%</td><td>3,073,556</td><td colspan=\"2\">1.85%</td></tr><tr><td>TODS</td><td>15</td><td>75,000</td><td colspan=\"2\">5.32%</td><td>75,000</td><td colspan=\"2\">6.38%</td></tr><tr><td>WSD</td><td>210</td><td>3,829,373</td><td colspan=\"2\">2.43%</td><td>3,829,537</td><td colspan=\"2\">0.76%</td></tr><tr><td>UCR</td><td>203</td><td>3,572,316</td><td colspan=\"2\">0.00%</td><td>7,782,539</td><td colspan=\"2\">0.47%</td></tr></table>\n\nWSD (Zhang et al., 2022), and UCR (Wu & Keogh, 2021). Dataset characteristics are summarized in Table 1, including curve counts, sizes, and anomaly rates. The anomaly interval length distributions, shown in Figure 6, reveal that while most anomalies span less than 10 points, WSD and UCR contain extended anomaly segments exceeding 300 points, enabling comprehensive evaluation. Detailed dataset descriptions are provided in Appendix A.1.\n\n# 4.1.2. MODEL TRAINING AND INFERENCE\n\nWe implement a systematic experimental protocol for both our method and baseline approaches. For each time series, we train dedicated KAN-AD models using consistent hyperparameters: batch size 1024, learning rate 0.01, and maximum 100 epochs. The validation strategy varies by dataset, with UCR reserving  $20\\%$  of training data and other datasets employing a 4:1:5 ratio for training, validation, and testing splits. To ensure fair comparison, we faithfully replicate all baseline methods following their original implementations and hyperparameter settings as specified in their respective papers. During inference, we standardize the batch size to 1 across all methods for comparable efficiency assessment. Results presented in Table 2 report means and standard deviations from five independent trials with different random seeds.\n\n# 4.1.3. BASELINES\n\nWe conducted comparative experiments with ten state-of-the-art time series anomaly detection methods: LST-MAD (Malhotra et al., 2015), FCVAE (Wang et al., 2024), SRCNN (Ren et al., 2019), FITS (Xu et al., 2024), TimesNet (Wu et al., 2023), OFA (Zhou et al., 2023), TranAD (Tuli et al., 2022), SubLOF (Breunig et al., 2000), Anomaly Transformer (Xu et al., 2022) (abbreviated as AnoTrans in the tables), KAN (Liu et al., 2025) and SAND (Boniol et al., 2021). Detailed descriptions of these methods can be found in Appendix A.2. For datasets not featured in the baseline literature, we meticulously tuned hyperparameters via grid search to optimize the performance of the baseline method on the respective evaluation metrics.\n\n# 4.1.4. EVALUATION METRICS\n\nIn practical applications, operations teams are less concerned with point-wise anomalies (i.e., whether individual data points are classified as anomalous) and more focused on detecting sustained anomalous segments within time se\n\nTable 2. Performance comparison. Best scores are highlighted in bold, and second best scores are highlighted in bold and underlined. Metrics include F1 (Best F1),  $\\mathsf{F1}_{\\mathrm{e}}$  (Event F1),  $\\mathsf{F1}_{\\mathrm{d}}$  (Delay F1), AUPRC (area under the precision-recall curve) and Avg  $\\mathsf{F1}_{\\mathrm{e}}$  (average  $\\mathsf{F1}_{\\mathrm{e}}$  score across four datasets).  \n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">KPI</td><td colspan=\"4\">TODS</td><td colspan=\"4\">WSD</td><td colspan=\"4\">UCR</td><td rowspan=\"2\">Avg F1e</td></tr><tr><td>F1</td><td>F1e</td><td>F1d</td><td>AUPRC</td><td>F1</td><td>F1e</td><td>F1d</td><td>AUPRC</td><td>F1</td><td>F1e</td><td>F1d</td><td>AUPRC</td><td>F1</td><td>F1e</td><td>F1d</td><td>AUPRC</td></tr><tr><td>SRCNN</td><td>0.4137</td><td>0.0994</td><td>0.2266</td><td>0.3355</td><td>0.6239</td><td>0.1918</td><td>0.4399</td><td>0.6076</td><td>0.4092</td><td>0.1185</td><td>0.1951</td><td>0.3080</td><td>0.5964</td><td>0.1369</td><td>0.1656</td><td>0.5109</td><td>0.1367</td></tr><tr><td>SAND</td><td>0.2710</td><td>0.0397</td><td>0.1097</td><td>0.2022</td><td>0.5372</td><td>0.1879</td><td>0.5103</td><td>0.5145</td><td>0.1761</td><td>0.0839</td><td>0.1267</td><td>0.1238</td><td>0.7044</td><td>0.5108</td><td>0.5116</td><td>0.6550</td><td>0.2056</td></tr><tr><td>AnoTrans</td><td>0.6103</td><td>0.3020</td><td>0.3623</td><td>0.5676</td><td>0.4875</td><td>0.1915</td><td>0.2918</td><td>0.4148</td><td>0.4348</td><td>0.2311</td><td>0.1517</td><td>0.3527</td><td>0.6135</td><td>0.1696</td><td>0.1084</td><td>0.5458</td><td>0.2236</td></tr><tr><td>TranAD</td><td>0.7553</td><td>0.5611</td><td>0.6399</td><td>0.7399</td><td>0.5035</td><td>0.2460</td><td>0.3619</td><td>0.4501</td><td>0.7570</td><td>0.6338</td><td>0.4158</td><td>0.7106</td><td>0.5278</td><td>0.1840</td><td>0.1554</td><td>0.4599</td><td>0.4062</td></tr><tr><td>SubLOF</td><td>0.7273</td><td>0.2805</td><td>0.4994</td><td>0.7015</td><td>0.7997</td><td>0.4795</td><td>0.7169</td><td>0.7809</td><td>0.8683</td><td>0.6585</td><td>0.4917</td><td>0.8353</td><td>0.8468</td><td>0.4772</td><td>0.4151</td><td>0.8001</td><td>0.4739</td></tr><tr><td>TimesNet</td><td>0.8022</td><td>0.6363</td><td>0.6995</td><td>0.8166</td><td>0.6232</td><td>0.3327</td><td>0.4495</td><td>0.6031</td><td>0.9406</td><td>0.8444</td><td>0.6170</td><td>0.9376</td><td>0.5273</td><td>0.1805</td><td>0.1439</td><td>0.4536</td><td>0.4985</td></tr><tr><td>FITS</td><td>0.9083</td><td>0.6353</td><td>0.8175</td><td>0.9359</td><td>0.7773</td><td>0.5416</td><td>0.6312</td><td>0.7725</td><td>0.9732</td><td>0.8391</td><td>0.6535</td><td>0.9771</td><td>0.6664</td><td>0.2926</td><td>0.2912</td><td>0.5969</td><td>0.5772</td></tr><tr><td>OFA</td><td>0.8810</td><td>0.6150</td><td>0.7952</td><td>0.9009</td><td>0.6928</td><td>0.5811</td><td>0.5588</td><td>0.7206</td><td>0.9564</td><td>0.8344</td><td>0.6250</td><td>0.9615</td><td>0.6294</td><td>0.3176</td><td>0.1503</td><td>0.5699</td><td>0.5870</td></tr><tr><td>FCVAE</td><td>0.9398</td><td>0.7556</td><td>0.8624</td><td>0.9572</td><td>0.8652</td><td>0.6995</td><td>0.7482</td><td>0.8798</td><td>0.9650</td><td>0.8610</td><td>0.6583</td><td>0.9653</td><td>0.7651</td><td>0.3812</td><td>0.2857</td><td>0.7145</td><td>0.6743</td></tr><tr><td>LSTMAD</td><td>0.9376</td><td>0.7742</td><td>0.8782</td><td>0.9624</td><td>0.8633</td><td>0.6981</td><td>0.7655</td><td>0.8740</td><td>0.9866</td><td>0.9028</td><td>0.6743</td><td>0.9849</td><td>0.7040</td><td>0.3482</td><td>0.3121</td><td>0.6432</td><td>0.6808</td></tr><tr><td>KAN</td><td>0.9411</td><td>0.7816</td><td>0.8666</td><td>0.9664</td><td>0.8109</td><td>0.6466</td><td>0.7518</td><td>0.8286</td><td>0.9879</td><td>0.8939</td><td>0.6650</td><td>0.9881</td><td>0.8016</td><td>0.4120</td><td>0.3971</td><td>0.7489</td><td>0.6835</td></tr><tr><td>KAN-AD</td><td>0.9442</td><td>0.7989</td><td>0.8755</td><td>0.9693</td><td>0.9425</td><td>0.8940</td><td>0.8391</td><td>0.9716</td><td>0.9888</td><td>0.8927</td><td>0.6623</td><td>0.9868</td><td>0.8554</td><td>0.5335</td><td>0.5177</td><td>0.8188</td><td>0.7798</td></tr><tr><td></td><td>±0.0007</td><td>±0.0054</td><td>±0.0024</td><td>±0.0008</td><td>±0.0040</td><td>±0.0022</td><td>±0.0055</td><td>±0.0035</td><td>±0.0005</td><td>±0.0025</td><td>±0.0022</td><td>±0.0009</td><td>±0.0040</td><td>±0.0046</td><td>±0.0042</td><td>±0.0041</td><td></td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/bbf4d6ea8917096d55784b1bcb6b452b4a78ae7244b4c0e823bb0d80a8d5b1d0.jpg)  \nFigure 4. Illustration of the adjustment strategy. Point-wise PA gives an inflated score when some anomaly segments persist for a long duration. Event-wise PA treats each anomaly segment as an event, completely disregarding the length of the anomaly segment.  $k$ -delay PA considers only anomalies detected within the first  $k$  points after the anomaly onset, treating any detected later as undetected.\n\nries data. Furthermore, due to the potential impact of such segments, early identification is crucial. Previous work (Xu et al., 2018) proposed the Best F1 metric, which iterates over all thresholds and applies a point adjustment strategy to calculate the F1 score. However, it has been criticized for performance inflation (Lai et al., 2021; Wu & Keogh, 2021).\n\nTo address this, we also adopt Delay F1 (Ren et al., 2019) and Event F1. Delay F1 is similar to Best F1 but uses a delay point adjustment strategy. As shown in Figure 4, the second anomaly was missed because the detection delay exceeded the threshold of five time intervals. In all experiments, a delay threshold of five was used across all datasets. Event F1, on the other hand, treats anomalies of varying lengths as anomalies with a length of 1, minimizing performance inflation caused by excessively long anomalous segments. For the sake of convenience, unless otherwise stated, we use Event F1 as the primary metric, as it is more alignment with the need for real-time anomaly detection in real-world situations.\n\n# 4.2. RQ1. Performance and Efficiency Comparison\n\nWe present a comprehensive evaluation of KAN-AD across multiple time series anomaly detection (TSAD) experiments,\n\nTable 3. Efficiency comparison on UCR dataset.  \n\n<table><tr><td>Method</td><td>GPU Time</td><td>CPU Time</td><td>Parameters</td><td>F1e</td></tr><tr><td>SAND</td><td>-</td><td>5637s</td><td>-</td><td>0.5108</td></tr><tr><td>SubLOF</td><td>-</td><td>299s</td><td>-</td><td>0.4772</td></tr><tr><td>OFA</td><td>220s</td><td>3087s</td><td>81.920 M</td><td>0.3176</td></tr><tr><td>AnoTrans</td><td>201s</td><td>1152s</td><td>4.752 M</td><td>0.1696</td></tr><tr><td>FCVAE</td><td>2327s</td><td>1743s</td><td>1.414 M</td><td>0.3812</td></tr><tr><td>TimesNet</td><td>182s</td><td>259s</td><td>73,449</td><td>0.1805</td></tr><tr><td>LSTMAD</td><td>73s</td><td>267s</td><td>10,421</td><td>0.3482</td></tr><tr><td>KAN</td><td>66s</td><td>34s</td><td>1,360</td><td>0.4120</td></tr><tr><td>FITS</td><td>32s</td><td>17s</td><td>624</td><td>0.2926</td></tr><tr><td>TranAD</td><td>113s</td><td>62s</td><td>369</td><td>0.1840</td></tr><tr><td>KAN-AD</td><td>42s</td><td>36s</td><td>274</td><td>0.5335</td></tr></table>\n\nwith results summarized in Table 2. Our analysis focuses on three key dimensions: detection accuracy, model efficiency, and computational requirements. Across diverse experimental settings, KAN-AD demonstrates consistent and robust performance advantages. In the TODS dataset, where training data contains a substantial proportion of anomalies, KAN-AD significantly outperforms SOTA by  $27\\%$  on Event F1, highlighting its robust learning capabilities in handling noisy training data. For datasets exhibiting strong periodic characteristics (WSD and KPI), KAN-AD achieves comparable or superior performance relative to state-of-the-art approaches. Even in the challenging UCR dataset scenario, where the training set lacks anomaly samples and contains significant periodic variations, KAN-AD effectively captures normal patterns, whereas baseline methods show reduced effectiveness in pattern recognition. Quantitatively, KAN-AD achieves more than a  $15\\%$  improvement in average Event F1 score compared to existing state-of-the-art methods.\n\nThe computational efficiency analysis, presented in Table 3, reveals another distinctive advantage of KAN-AD. We note that several baseline methods are excluded from this comparison due to implementation constraints: SAND's CPU-only execution requirement and SubLOF's limited multi-core utilization capabilities preclude fair comparison in modern hardware acceleration contexts. Among the other mod\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/0e7e477849ee29bf56c24029a7b4ce06b46e5bed1f26fed5f959877488633512.jpg)  \nFigure 5. Case study on UCR InternalBleeding10. The black curve represents the original sample, the red curve represents the anomaly scores provided by the method, and the true anomaly segments are highlighted in pink.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/adc550df366b4a1d7eff97387158e08b70183aa2eaaa1c896116ac0d7fc8eaae.jpg)  \nFigure 6. Anomalous lengths distribution.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/14bd8c84d0943681f8cdd950698f8b6563bc261c6aed7589d33c4389a188d264.jpg)  \nFigure 7. Model performance under different hyperparameters.\n\nels, we observe a wide spectrum of model complexities, with parameter counts ranging from millions to hundreds. Large-scale models like OFA utilize 81.92M parameters, while established approaches such as Anomaly Transformer, FCAVE, and TimesNet employ between 73k and 4.75M parameters. In contrast, KAN-AD achieves competitive performance with remarkable efficiency, requiring only 274 parameters, a  $25\\%$  reduction compared to TranAD, the next most compact model in our comparison.\n\nThese empirical findings underscore KAN-AD's exceptional efficiency-performance in TSAD tasks. By achieving state-of-the-art or near state-of-the-art performance while significantly reducing the parameter footprint, KAN-AD demonstrates the effectiveness of our design principles in creating efficient, practical solutions. This combination of high detection accuracy and minimal computational requirements positions KAN-AD as an ideal choice for resource-constrained or cost-sensitive deployment scenarios, offering a compelling balance between model complexity and detection capabilities.\n\n# 4.2.1.CASE STUDY\n\nWe analyzed anomaly detection performance on UCR dataset samples to illustrate how various methods respond to identical anomalies, as shown in Figure 5. The selected sample displayed pattern anomalies, marked by significant deviations from typical behavior. Both TranAD and TimesNet exhibit difficulty establishing normal patterns. Minor varia\n\ntions among normal samples across cycles lead to periodic false alarms during normal segments, consistent with our observations in Figure 2. Among the methods listed, while OFA, LSTMAD, SubLOF, and FITS can detect anomalies, their high anomaly scores during normal segments indicate excessive sensitivity to minor fluctuations in normal data. In contrast, KAN-AD excels in identifying anomalies while maintaining minimal anomaly scores during normal segments.\n\n# 4.3. RQ2. Hyperparameter sensitivity\n\nThe KAN-AD model incorporates two key hyperparameters: the number of terms in univariate functions  $N$  and the window size  $T$ . To investigate the ultimate impact of these parameters on model performance, we conducted experiments on the UCR dataset while holding all other parameters constant. As findings summarized in Figure 7, a larger window size facilitates more accurate learning of normal patterns when  $N$  is fixed, leading to improved performance. When  $T$  is fixed, insufficient univariate functions limit KAN-AD's expressive power, while excessive  $N$  can lead to overfitting. Overall, KAN-AD achieved its best performance with  $T = 96$  and  $N = 2$ . Notably, even with suboptimal hyperparameter settings like  $T = 16$  and  $N = 1$ , we surpassed SOTA methods on the UCR dataset.\n\n# 4.4. RQ3. Ablation Studies\n\nIn this section, we investigated the impact of constant term elimination modules, different univariate function selections on algorithm performance and the influence of the function deconstruction mechanism.\n\n# 4.4.1. CONSTANT TERM ELIMINATION MODULE\n\nWe employed a constant term elimination (CTE) module during data preprocessing to mitigate the influence of the offset term  $A_0$  in Equation (3). Further experiments were conducted across all datasets to evaluate the impact of incorporating CTE within the preprocessing pipeline. As presented in Figure 8, the impact of CTE varies across datasets, reflecting inherent data characteristics. For datasets with pronounced periodicity or strong temporal stability (e.g., WSD), the benefits of CTE are less apparent. Conversely, for datasets exhibiting larger value fluctuations or trends (e.g., KPI, TODS and UCR), CTE yields significant improvements.\n\n# 4.4.2. SELECTION OF UNIVARIATE FUNCTIONS\n\nTo assess the impact of different univariate functions on model performance, we conducted experiments using common univariate functions listed in Table 4. In our implementations, due to varying input range requirements across\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/8f00f57a76a60bea51427f572a7ba50830571987103b2d63f06f39804da66481.jpg)  \nFigure 8. Model performance under different preprocessing.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/1549e53a7dfeb9e20331ada180249c6e22d9c7d4bb012b5f4398584fd24dabf6.jpg)  \nFigure 10. Model performance under different anomaly ratios in training.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/85141064-0032-45ce-9858-5b57ccd2f686/9569a2418a87e057c1dab0b592b7ee9070def4ba3227bd2ab51817442a102c19.jpg)  \nFigure 9. Model performance under different univariate function.\n\nTable 4. Commonly used univariate functions for time series approximation.  \n\n<table><tr><td>Name</td><td>Φn(x)</td></tr><tr><td>Taylor Series</td><td>xn</td></tr><tr><td>Fourier Series</td><td>cos(nx) + sin(nx)</td></tr><tr><td>Chebyshev Polynomial I</td><td>cos(narccos(x))</td></tr><tr><td>Chebyshev Polynomial II</td><td>sin((n+1)arccos(x)) / sin(arccos(x))</td></tr></table>\n\nunivariate functions, appropriate normalization techniques are employed. Specifically, min-max scaling to the range  $x \\in [-1, 1]$  was utilized for both types of Chebyshev polynomials, while z-score was employed for Taylor series and Fourier series. The performance of all four univariate functions was compared using the same configuration. As results presented in Figure 9, Fourier series consistently achieved the top two performance across all datasets. In contrast, Taylor series exhibited persistent bias due to non-zero function values in most cases, hindering optimal model performance. The objective of both types of Chebyshev polynomials is to minimize the maximum error, which potentially conflicts with anomaly detection methods that minimize mean squared prediction error, thus leading to suboptimal performance.\n\n# 4.5. RQ4. Robustness to Anomalous Data\n\nTo evaluate KAN-AD's robustness to anomalies in the training set, we conducted additional experiments using synthetic datasets constructed in accordance with the TODS dataset generation methodology. We synthesized test datasets containing local peaks and drops anomalies, and progressively increased the proportion of these anomalies in the initially anomaly-free training set. As illustrated in Figure 10, KAN-AD demonstrates stable performance across all anomaly ratios. Popular methods such as LSTMAD, perform well at lower anomaly ratios but experience a significant decline as the ratio increases. Other approaches, like TranAD, fail to achieve optimal performance due to overfitting to fine-grained structures within the time series.\n\nTable 5. Model performance on UCR dataset under different function deconstruction strategies.  \n\n<table><tr><td>Variation</td><td>F1e</td><td>F1d</td><td>AUPRC</td></tr><tr><td>KAN-AD</td><td>0.5335</td><td>0.5177</td><td>0.8188</td></tr><tr><td>w/o X</td><td>0.5153</td><td>0.4974</td><td>0.8066</td></tr><tr><td>w/o P</td><td>0.5081</td><td>0.4810</td><td>0.8007</td></tr><tr><td>w/o S</td><td>0.5056</td><td>0.5113</td><td>0.7998</td></tr><tr><td>w/o X&amp;P</td><td>0.4737</td><td>0.4583</td><td>0.7872</td></tr><tr><td>w/o X&amp;S</td><td>0.4698</td><td>0.4610</td><td>0.7767</td></tr><tr><td>w/o S&amp;P</td><td>0.4561</td><td>0.4637</td><td>0.7595</td></tr></table>\n\n# 4.6. Ablation on function deconstruction mechanism\n\nTo investigate the impact of the function deconstruction mechanism, we compared the model's detection capabilities under different univariate function combination strategies. For clarity, the specific definitions are provided in Equation (5). As the results presented in Table 5, the model's detection performance exhibited a notable improvement with an increasing number of univariate functions. Both Fourier series and cosine waves outperformed the raw input data, likely due to their smoother representations compared to the original signal, enabling higher detection accuracy. The combination of different features, particularly those involving Fourier series and cosine waves, resulted in significant performance gains as the feature count increased. Ultimately, KAN-AD achieved optimal detection performance by integrating all features. It is worth noting that even the variant of KAN-AD utilizing only the raw time series X outperforms KAN, clearly demonstrating the advantage of Fourier series over the use of spline functions for optimizing univariate functions.\n\n# 4.7. Performance on Multivariate Time Series\n\nTo extend KAN-AD's application to the multivariate time series (MTS) scenario, we adopt a channel-independent approach. Specifically, an MTS input with the shape (batch_size, window_length, n_features) is reshaped into (batch_size * n_features, window_length). Each of the n_features channels\n\nTable 6. Best F1 and parameter counts for multivariate time series anomaly detection. Best and second best results are in bold and underline.  \n\n<table><tr><td>Methods</td><td>SMD</td><td>MSL</td><td>SMAP</td><td>SWaT</td><td>PSM</td><td>Avg F1</td><td>Parameters@MSL</td></tr><tr><td>Informer (Zhou et al., 2021)</td><td>0.8165</td><td>0.8406</td><td>0.6992</td><td>0.8143</td><td>0.7710</td><td>0.7883</td><td>504,174</td></tr><tr><td>Anomaly Transformer (Xu et al., 2022)</td><td>0.8549</td><td>0.8331</td><td>0.7118</td><td>0.8310</td><td>0.7940</td><td>0.8050</td><td>4,863,055</td></tr><tr><td>DLinear (Zeng et al., 2023)</td><td>0.7710</td><td>0.8488</td><td>0.6926</td><td>0.8752</td><td>0.9355</td><td>0.8246</td><td>20,200</td></tr><tr><td>Autoformer (Wu et al., 2021)</td><td>0.8511</td><td>0.7905</td><td>0.7112</td><td>0.9274</td><td>0.9329</td><td>0.8426</td><td>325,431</td></tr><tr><td>FEDformer (Zhou et al., 2022)</td><td>0.8508</td><td>0.7857</td><td>0.7076</td><td>0.9319</td><td>0.9723</td><td>0.8497</td><td>1,119,982</td></tr><tr><td>TimesNet (Wu et al., 2023)</td><td>0.8462</td><td>0.8180</td><td>0.6950</td><td>0.9300</td><td>0.9738</td><td>0.8526</td><td>75,223</td></tr><tr><td>UniTS (Gao et al., 2024)</td><td>0.8809</td><td>0.8346</td><td>0.8380</td><td>0.9326</td><td>0.9743</td><td>0.8921</td><td>8,066,376</td></tr><tr><td>KAN-AD (ours)</td><td>0.8429</td><td>0.8501</td><td>0.9450</td><td>0.9350</td><td>0.9650</td><td>0.9076</td><td>4,491</td></tr></table>\n\nis thus treated as an independent univariate time series instance. KAN-AD is then applied to these individual series. This channel-independent strategy has proven effective (Nie et al., 2023). By adopting a similar principle, KAN-AD can leverage its robust univariate modeling capabilities across all channels of an MTS dataset. The model is trained on the collection of these reshaped univariate instances, allowing it to learn generalized normal patterns.\n\nWe implemented MTS versions of KAN-AD in popular time series library (THUML) and evaluated them on the common SMD (Su et al., 2019), MSL (Hundman et al., 2018a), SMAP (Hundman et al., 2018b), SWaT (Mathur & Tippenhauer, 2016), and PSM (Abdulaal et al., 2021) datasets. Our evaluation metric uses the Best F1 score which is consistent with the baseline methods. We introduce these datasets and baseline methods in detail in the Appendix B. As detailed in Table 6, KAN-AD achieves the highest average Best F1 score of 0.9076, across all five benchmark datasets, outperforming all listed SOTA methods. A significant advantage of KAN-AD is its exceptional parameter efficiency. With only 4,491 trainable parameters (measured on MSL), KAN-AD utilizes substantially fewer parameters than all other compared methods.\n",
  "hyperparameter": "batch_size: 1024; learning_rate: 0.01; max_epochs: 100; window_size (T): 96 (optimal); number of Fourier terms (N): 2 (optimal); CNN kernel_size: 3; number of CNN blocks (L): multiple stacked layers; delay threshold for Delay F1: 5; validation split: 20% for UCR, 4:1:5 (train:val:test) for other datasets; activation function: GELU; normalization: Batch Normalization; convolution kernel width for dimension reduction: 1; inference batch_size: 1"
}