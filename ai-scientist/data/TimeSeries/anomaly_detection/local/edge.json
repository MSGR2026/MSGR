[
  {
    "source": "Informer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. Reusable: Encoder/EncoderLayer structure for backbone.\n\n2. Reusable: Conv1d-based FFN (conv1, conv2) in encoder layers - but use `bias=False`.\n\n3. Reusable: Multi-head attention wrapper pattern (query/key/value projections).\n\n4. Reusable: Training pipeline with MSE loss.",
    "differences": "1. **[CRITICAL] Embedding**: Must use `DataEmbedding_wo_pos` (NO position encoding)! Autoformer relies on AutoCorrelation for temporal dependencies.\n\n2. **[CRITICAL] LayerNorm**: Must use `my_Layernorm` that removes seasonal bias: `x_hat - mean(x_hat, dim=1)`.\n\n3. **[CRITICAL - WILL HANG WITHOUT THIS] Time Delay Aggregation**:\n\n**TRAINING PHASE** - Use UNIFIED delay index across batch (torch.roll):\n```python\ndef time_delay_agg_training(self, values, corr):\n    # values: [B, H, D, L], corr: [B, H, D, L]\n    head, channel, length = values.shape[1], values.shape[2], values.shape[3]\n    top_k = int(self.factor * math.log(length))\n    mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)  # [B, L]\n    # CRITICAL: Average across batch to get UNIFIED delay indices!\n    index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]  # [top_k] - shared!\n    weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)  # [B, top_k]\n    tmp_corr = torch.softmax(weights, dim=-1)\n    delays_agg = torch.zeros_like(values).float()\n    for i in range(top_k):  # Only loop top_k (~3-5)\n        pattern = torch.roll(values, -int(index[i]), -1)  # Roll ENTIRE batch at once!\n        delays_agg += pattern * tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n    return delays_agg\n```\n\n**INFERENCE PHASE** - Use torch.gather with doubled values:\n```python\ndef time_delay_agg_inference(self, values, corr):\n    B, H, C, L = values.shape\n    init_index = torch.arange(L).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(B, H, C, 1).to(values.device)\n    top_k = int(self.factor * math.log(L))\n    mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n    weights, delay = torch.topk(mean_value, top_k, dim=-1)\n    tmp_corr = torch.softmax(weights, dim=-1)\n    tmp_values = values.repeat(1, 1, 1, 2)  # Double for circular indexing\n    delays_agg = torch.zeros_like(values).float()\n    for i in range(top_k):\n        tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, H, C, L)\n        pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n        delays_agg += pattern * tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, H, C, L)\n    return delays_agg\n```\n\n**FORBIDDEN PATTERNS (will cause hanging)**:\n❌ `for b in range(B): for h in range(H): ... delay[b,h].item()`\n❌ Per-sample torch.roll inside nested loops\n❌ Any `.item()` call inside loops\n\n4. NEW: series_decomp - AvgPool-based, Conv1d use `bias=False`.\n\n5. Architecture: `DataEmbedding_wo_pos → Encoder(my_Layernorm) → projection`."
  },
  {
    "source": "Reformer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. Reusable: Encoder/EncoderLayer backbone with residual connections.\n\n2. Reusable: Multi-head attention wrapper pattern (Q/K/V projections).\n\n3. Reusable: Conv1d-based FFN - but Autoformer uses `bias=False`.\n\n4. Both achieve O(L log L) complexity.",
    "differences": "1. **[CRITICAL] Embedding**: Must use `DataEmbedding_wo_pos` (NO position encoding)!\n\n2. **[CRITICAL] LayerNorm**: Must use `my_Layernorm`: `x_hat - mean(x_hat, dim=1)`.\n\n3. **[CRITICAL - WILL HANG] Time Delay Aggregation**:\n\n**TRAINING**: Use UNIFIED delay index across batch + torch.roll:\n```python\nmean_value = torch.mean(torch.mean(corr, dim=1), dim=1)  # [B, L]\nindex = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]  # Shared across batch!\nfor i in range(top_k):\n    pattern = torch.roll(values, -int(index[i]), -1)  # Roll entire batch!\n    delays_agg += pattern * weights[...]\n```\n\n**INFERENCE**: Use torch.gather with doubled values:\n```python\ntmp_values = values.repeat(1, 1, 1, 2)\ninit_index = torch.arange(L).view(1,1,1,-1).expand(B, H, C, L).to(device)\nfor i in range(top_k):\n    tmp_delay = init_index + delay[:, i].view(-1,1,1,1).expand_as(init_index)\n    pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n```\n\n**FORBIDDEN**: `for b in range(B): for h in range(H): ... .item()`\n\n4. NEW: series_decomp - AvgPool, Conv1d `bias=False`.\n\n5. Architecture: `DataEmbedding_wo_pos → Encoder(my_Layernorm) → projection`."
  },
  {
    "source": "DLinear_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt DMS forecasting where the entire prediction horizon is generated in one forward pass, avoiding autoregressive error accumulation. DLinear's encoder structure and forward pass logic can be directly reused as a baseline framework.\n\n2. **Decomposition-Based Preprocessing**: Both leverage seasonal-trend decomposition for time series preprocessing, with DLinear using moving average kernels from Autoformer. The `series_decomp` module and decomposition initialization code can be directly integrated into PatchTST's preprocessing pipeline.\n\n3. **Channel-Independent Processing**: Both support individual channel modeling where separate parameters are learned per variate. DLinear's `individual` flag implementation and ModuleList structure for per-channel linear layers provide reusable code patterns for PatchTST's channel-independent mode.\n\n4. **Reconstruction-Based Anomaly Detection**: Both use reconstruction error between predictions and actual values as anomaly scores in anomaly detection tasks. DLinear's `anomaly_detection` method and forward pass structure serve as a template for PatchTST's reconstruction framework.\n\n5. **Shared Training Infrastructure**: Both require identical data loading, normalization (zero-mean), loss computation (MSE), and evaluation pipelines. DLinear's training loop, dataset handlers, and metric calculation utilities can be directly adapted with minimal modifications.",
    "differences": "1. **Core Architecture Innovation**: DLinear uses simple temporal linear layers operating on full sequences, while PatchTST introduces patch-based tokenization where input sequences are segmented into non-overlapping patches and processed through Transformer self-attention. NEW: Implement patch segmentation module, patch embedding layer, and multi-head self-attention blocks.\n\n2. **Temporal Modeling Mechanism**: DLinear performs direct weighted summation across time steps via linear projections, whereas PatchTST captures long-range dependencies through self-attention over patch tokens with positional encodings. NEW: Implement positional encoding injection, multi-head attention mechanism, and feedforward networks for patch-level feature extraction.\n\n3. **Input Representation**: DLinear processes raw time series points directly (shape: [B, L, C]), while PatchTST reshapes inputs into patch sequences (shape: [B, num_patches, patch_len, C]) before embedding. NEW: Develop patching function with configurable patch length and stride, plus linear/convolutional patch embedding layers.\n\n4. **Complexity and Scalability**: DLinear has O(L) complexity with no attention operations, while PatchTST reduces Transformer complexity from O(L²) to O((L/P)²) where P is patch length. NEW: Implement efficient attention computation on reduced patch sequence length and optional sparse attention patterns.\n\n5. **Feature Extraction Depth**: DLinear uses single-layer linear transformations per component (trend/seasonal), whereas PatchTST employs stacked Transformer encoder layers with residual connections and layer normalization. NEW: Build multi-layer Transformer encoder stack, implement residual connections, layer normalization, dropout regularization, and hierarchical feature aggregation across layers."
  },
  {
    "source": "FEDformer_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-only architecture for anomaly detection**: Both papers can adopt encoder-only structures for reconstruction-based anomaly detection tasks. FEDformer's encoder with frequency-enhanced blocks can be directly reused as a backbone, replacing attention mechanisms with patch-based processing in PatchTST.\n\n2. **Embedding and normalization modules**: FEDformer's DataEmbedding (positional + value encoding) and LayerNorm components are standard preprocessing steps. These can be directly transferred to PatchTST's implementation, requiring only dimension adjustments for patch-based inputs.\n\n3. **Decomposition-based preprocessing**: Both benefit from seasonal-trend decomposition for time series. FEDformer's series_decomp and MOEDecomp modules can be adapted for PatchTST's patch extraction stage, helping isolate normal patterns before patching.\n\n4. **Reconstruction loss for anomaly scoring**: Both use MSE/MAE between input and reconstructed output as anomaly scores. FEDformer's projection layer (nn.Linear) for final output can be reused, adjusting only output dimensions for patch-level reconstruction.\n\n5. **Multi-layer encoder stacking**: FEDformer's modular encoder design (EncoderLayer with self-attention + feedforward) provides a template. PatchTST can inherit this structure, substituting Fourier/Wavelet blocks with patch-based self-attention while keeping residual connections and normalization.",
    "differences": "1. **Core innovation - Patch-based tokenization vs. Frequency domain processing**: FEDformer operates in frequency domain (Fourier/Wavelet transforms) with O(L) complexity, while PatchTST segments time series into fixed-length patches as tokens. PatchTST requires NEW patch extraction logic (stride, patch_len parameters) and patch embedding layers, replacing FEDformer's FEB/FEA modules entirely.\n\n2. **Attention mechanism design**: FEDformer uses frequency-enhanced attention (FourierBlock/MultiWaveletTransform) with mode selection in spectral domain. PatchTST needs standard multi-head self-attention on patch tokens in time domain, requiring NEW vanilla Transformer attention implementation without frequency transforms.\n\n3. **Temporal dependency modeling**: FEDformer captures global dependencies through Fourier coefficients (selected modes) and wavelet decomposition across scales. PatchTST models local-to-global patterns via patch sequences with positional encodings, needing NEW patch-level positional embedding (learnable or sinusoidal) distinct from FEDformer's time-step embeddings.\n\n4. **Channel independence strategy**: FEDformer processes multivariate series jointly through shared encoder layers. PatchTST adopts channel-independent modeling, processing each variable separately then aggregating. This requires NEW channel-wise iteration logic and separate projection heads per channel.\n\n5. **Decoder removal and output projection**: FEDformer uses encoder-decoder with cross-attention (FEA modules) for forecasting. PatchTST eliminates decoders entirely, using only encoder + flattening + linear projection for reconstruction. NEW implementation needs patch-to-sequence reshaping logic and simplified output heads without cross-attention components.\n\n6. **Complexity and scalability**: FEDformer achieves O(L) via sparse mode selection in frequency domain. PatchTST reduces complexity to O((L/P)²) where P is patch length, through reduced sequence length. NEW patch merging strategies and attention masking for variable-length sequences need implementation.\n\n7. **Anomaly scoring granularity**: FEDformer computes point-wise reconstruction errors across full sequence length. PatchTST can calculate patch-level errors then aggregate (mean/max pooling) to point-level scores, requiring NEW error aggregation functions from patch predictions to original time steps."
  },
  {
    "source": "Pyraformer_2021",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based Anomaly Detection**: Both papers employ reconstruction-based anomaly detection where the model learns to reconstruct normal patterns, and deviations indicate anomalies. The source's `anomaly_detection` method and `projection` layer can be directly adapted for PatchTST's reconstruction objective.\n\n2. **Transformer Encoder Architecture**: Both utilize Transformer encoder architectures with multi-head self-attention and feed-forward networks. PatchTST can reuse Pyraformer's `EncoderLayer`, `PositionwiseFeedForward`, and attention mechanisms, replacing only the attention pattern (pyramidal → full attention on patches).\n\n3. **Embedding Strategy**: Both papers use similar embedding approaches combining data, positional, and temporal encodings. The `DataEmbedding` module from Pyraformer can be directly reused, though PatchTST applies it to patches rather than raw time points.\n\n4. **End-to-End Training**: Both models are trained end-to-end with MSE/MAE reconstruction loss for anomaly detection tasks. The training loop, loss calculation, and optimizer setup from Pyraformer's implementation can be largely preserved.\n\n5. **Batch Processing Framework**: Both handle multivariate time series with batch processing (B, L, D format). The data loading pipeline, batch normalization, and output projection layers can be reused with minimal modifications.",
    "differences": "1. **Core Innovation - Patching vs Pyramidal Attention**: Pyraformer uses pyramidal multi-resolution attention (O(L) complexity) with coarse-to-fine temporal modeling, while PatchTST segments sequences into non-overlapping patches and applies standard attention to patches (O(N²) where N=L/patch_len). PatchTST requires implementing patch segmentation (`Unfold` or `reshape` operations) and patch-level attention, removing Pyraformer's `get_mask`, `refer_points`, and pyramidal graph construction.\n\n2. **Temporal Modeling Strategy**: Pyraformer explicitly models multi-scale temporal dependencies through C-ary tree structure with inter-scale and intra-scale connections via CSCM module. PatchTST treats each patch as a semantic token, relying on channel-independence and patch-level self-attention without explicit hierarchical structure. The entire `Bottleneck_Construct` and `ConvLayer` modules must be removed and replaced with simple patch embedding.\n\n3. **Attention Mechanism Implementation**: Pyraformer uses custom CUDA kernels for sparse pyramidal attention with `RegularMask` and scale-specific connectivity patterns. PatchTST uses standard full self-attention over patches without masking, allowing use of native PyTorch `nn.MultiheadAttention` or standard scaled dot-product attention, significantly simplifying implementation but increasing computational cost per patch.\n\n4. **Sequence Length Handling**: Pyraformer processes full sequences with length L through pyramidal downsampling (window_size=[4,4]), creating log(L) scales. PatchTST divides L into N patches of fixed length (e.g., patch_len=16), processing shorter effective sequences. This requires implementing patch extraction logic and adjusting positional encoding to patch indices rather than time steps.\n\n5. **Channel Modeling Philosophy**: Pyraformer processes all channels jointly through shared encoder with dimension d_model across features. PatchTST employs channel-independence strategy, processing each channel separately or using channel-mixing layers. Implementation requires either channel-wise iteration or explicit channel attention modules, fundamentally changing the model's input/output handling compared to Pyraformer's multivariate joint processing.\n\n6. **Reconstruction Granularity**: Pyraformer's projection layer maps concatenated multi-scale features `(len(window_size)+1)*d_model` back to original temporal resolution for point-wise reconstruction. PatchTST reconstructs at patch level then unpacks to time points, requiring implementation of patch-to-sequence conversion and potentially patch boundary handling for seamless reconstruction across the full sequence length."
  },
  {
    "source": "Informer_2020",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-based Architecture**: Both use Transformer encoder architectures for time series representation learning. PatchTST can reuse Informer's EncoderLayer structure, including multi-head attention, feedforward networks, and layer normalization components.\n\n2. **Embedding Strategy**: Both employ learnable positional embeddings and value embeddings for input sequences. PatchTST can adapt Informer's DataEmbedding module by modifying it to handle patched inputs instead of point-wise inputs.\n\n3. **Self-Attention Mechanism**: Both rely on self-attention for capturing temporal dependencies, though with different efficiency strategies. The basic AttentionLayer implementation from Informer (query/key/value projections, multi-head structure) provides a foundation for PatchTST's attention.\n\n4. **Anomaly Detection via Reconstruction**: Both papers can perform anomaly detection through reconstruction-based approaches using encoder representations. Informer's anomaly_detection method demonstrates the pattern: encode input, project to output space, compare with original.\n\n5. **Normalization and Regularization**: Both use LayerNorm and Dropout for training stability. These components (norm layers, dropout rates, activation functions) can be directly reused from Informer's implementation.",
    "differences": "1. **Core Innovation - Patching Mechanism**: PatchTST introduces patch-based tokenization, segmenting continuous time series into non-overlapping or overlapping patches. This requires NEW implementation: a patching module that reshapes input from [B, L, C] to [B, N_patches, patch_len, C], fundamentally different from Informer's point-wise processing.\n\n2. **Attention Efficiency Strategy**: Informer uses ProbSparse attention with query sparsity measurement (O(L log L) complexity), while PatchTST uses standard self-attention on reduced sequence length through patching. NEW: Remove ProbAttention entirely; implement vanilla scaled dot-product attention since patching already reduces computational cost from O(L²) to O((L/P)²).\n\n3. **Channel Independence**: PatchTST processes each channel independently (channel-independent strategy), while Informer processes all channels jointly. NEW: Implement channel-wise processing loop or reshape operations to handle [B, C, N_patches, patch_len] → process each channel separately → aggregate results.\n\n4. **No Decoder Architecture**: PatchTST uses encoder-only architecture for forecasting, eliminating Informer's decoder with masked attention and cross-attention. NEW: Remove entire Decoder module; implement direct projection from encoder output to prediction horizon using linear layers: [B, N_patches, d_model] → [B, pred_len, C].\n\n5. **Sequence Length Reduction**: Informer reduces sequence length through distilling with ConvLayer and MaxPooling, while PatchTST reduces through patching. NEW: Remove ConvLayer distilling mechanism; implement patch embedding that inherently reduces sequence length by factor of patch_length, requiring different positional encoding strategy for patches rather than time steps.\n\n6. **Forecasting Strategy**: Informer uses generative inference with start tokens and placeholder sequences, while PatchTST directly predicts future values from patch representations. NEW: Implement direct forecasting head that maps patch-level features to point-level predictions, handling the mismatch between patch granularity and prediction granularity.\n\n7. **Training Objective**: While both use MSE loss, PatchTST may incorporate patch-level auxiliary losses or masked patch prediction during pre-training. NEW: Potentially implement self-supervised pre-training with masked patch modeling, requiring masking strategies and reconstruction losses at patch level."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture**: Both papers utilize Transformer encoder architectures with self-attention mechanisms for time series modeling. Pyraformer's `EncoderLayer` and attention modules can serve as base components for implementing Nonstationary Transformer's encoder structure.\n\n2. **Embedding Strategy**: Both employ similar data embedding approaches combining value embeddings with temporal covariates (x_mark). Pyraformer's `DataEmbedding` module (enc_embedding) can be directly reused or adapted for the target paper's embedding layer.\n\n3. **Layer Normalization and Feed-Forward Networks**: Both architectures use layer normalization and position-wise feed-forward networks after attention layers. Pyraformer's `PositionwiseFeedForward` class with GELU activation and residual connections provides a reusable implementation pattern.\n\n4. **Multi-layer Encoder Design**: Both stack multiple encoder layers to capture hierarchical temporal patterns. The modular `nn.ModuleList` structure in Pyraformer's Encoder can guide the implementation of stacked layers in Nonstationary Transformer.\n\n5. **Reconstruction-based Anomaly Detection**: Both can be adapted for anomaly detection through reconstruction error. Pyraformer's `anomaly_detection` method and projection layer demonstrate how to map encoder outputs back to input space for reconstruction.",
    "differences": "1. **Core Innovation - Nonstationarity Handling**: Nonstationary Transformer addresses distribution shifts in time series through de-stationary attention and series stationarization modules, while Pyraformer focuses on computational efficiency via pyramidal attention. New components needed: (a) Series Stationarization layer to normalize input statistics, (b) De-stationary Attention mechanism that learns adaptive attention based on distribution changes.\n\n2. **Attention Mechanism Design**: Pyraformer uses pyramidal attention with C-ary tree structure and multi-resolution representation (O(L) complexity), while Nonstationary Transformer uses standard full attention augmented with de-stationary factors. Implementation requires: (a) Removing pyramidal mask and CSCM modules, (b) Adding learnable de-stationary projection matrices, (c) Implementing distribution-aware attention weights.\n\n3. **Input Processing Strategy**: Pyraformer constructs multi-scale representations through bottleneck convolutions (`Bottleneck_Construct`, `ConvLayer`), while Nonstationary Transformer applies normalization/stationarization to raw sequences. New implementation: (a) Replace CSCM with normalization layers (mean/std computation), (b) Add inverse transformation for output re-scaling, (c) Implement adaptive normalization parameters.\n\n4. **Temporal Pattern Modeling**: Pyraformer captures long-range dependencies through hierarchical pyramid structure with inter-scale and intra-scale connections, while Nonstationary Transformer models non-stationary patterns through attention re-weighting based on statistical properties. Required changes: (a) Remove `get_mask`, `refer_points` functions, (b) Implement statistical moment tracking, (c) Add de-stationary factor computation in attention.\n\n5. **Model Complexity and Scalability**: Pyraformer optimizes for O(L) complexity with custom CUDA kernels and sparse attention patterns (window_size, inner_size parameters), while Nonstationary Transformer maintains O(L²) complexity but improves modeling of distribution shifts. Implementation needs: (a) Remove complexity optimization components, (b) Use standard PyTorch attention operations, (c) Add normalization statistics tracking across sequence length, (d) Implement re-stationary projection for final outputs."
  },
  {
    "source": "Informer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture**: Both papers utilize encoder-decoder Transformer architectures for time series modeling. The target paper can directly reuse Informer's basic Transformer components including multi-head attention layers, feed-forward networks, and positional embeddings from the DataEmbedding module.\n\n2. **Encoder-Decoder Framework**: Both adopt the encoder-decoder paradigm for sequence-to-sequence modeling. The target paper can leverage Informer's Encoder and Decoder base classes, modifying only the attention mechanisms while keeping the overall structural framework intact.\n\n3. **Embedding Strategy**: Both papers employ similar input embedding approaches combining value embedding, temporal encoding, and positional information. The DataEmbedding class from Informer (with enc_in, d_model, embed type, freq parameters) can be directly reused for the target implementation.\n\n4. **Training Objective**: Both use reconstruction-based objectives with MSE loss for learning temporal patterns. The target paper can adopt Informer's loss computation framework and training loop structure, only modifying the normalization components within the forward pass.\n\n5. **Sequence Processing**: Both handle variable-length sequences with similar input/output formats (x_enc, x_mark_enc, x_dec, x_mark_dec). The target implementation can reuse Informer's data preprocessing pipeline and batch handling mechanisms for consistent sequence management.",
    "differences": "1. **Core Innovation - Nonstationarity Handling**: Informer assumes stationarity and uses standard attention, while the target paper (Non-stationary Transformer) addresses non-stationary time series through De-stationary Attention mechanisms. This requires implementing new normalization modules that extract and restore statistical properties (mean/variance) at the series level before and after attention operations.\n\n2. **Attention Mechanism Design**: Informer uses ProbSparse attention with query sparsity measurement (M(q,K)) and Top-u selection for efficiency in O(L log L) complexity. The target paper needs to replace this with De-stationary Attention that performs statistical normalization on queries/keys before computing attention scores, requiring new AttentionLayer implementations with normalization parameters.\n\n3. **Statistical Normalization Strategy**: Informer directly processes raw sequences without explicit statistical treatment. The target paper must implement instance normalization layers that compute and store series-level statistics (mean μ, std σ) at encoding time, apply z-score normalization before attention, and restore original scale after decoding for handling distribution shifts.\n\n4. **Encoder Architecture Modifications**: Informer employs distilling layers with ConvLayer and MaxPooling for progressive dimension reduction. The target paper may need to modify or remove distilling operations since statistical normalization changes feature map properties, requiring careful redesign of how encoder layers aggregate temporal information under non-stationary conditions.\n\n5. **Projection and Output Processing**: Informer's final projection directly maps d_model to c_out dimensions. The target paper needs additional de-normalization layers after projection to restore the original data distribution using stored statistics, ensuring predictions match the scale and location of non-stationary input sequences for accurate forecasting and anomaly detection."
  },
  {
    "source": "Reformer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Informer_2020",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Pyraformer_2021",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based Anomaly Detection**: Both employ reconstruction paradigms where the model learns normal patterns and detects anomalies via reconstruction error. The source's `anomaly_detection` method and projection layer can guide implementing TiDE's decoder structure.\n2. **Temporal Embedding Strategy**: Both use time-series embeddings combining observations with temporal covariates (x_mark). The source's `DataEmbedding` module (enc_embedding) provides reusable patterns for TiDE's feature encoding layer.\n3. **Multi-layer Architecture**: Both stack multiple processing layers to capture temporal dependencies. The source's `EncoderLayer` structure with residual connections and normalization offers implementation guidance for TiDE's dense layers.\n4. **Sequence-to-Sequence Mapping**: Both map input sequences to output sequences of similar length. The source's forward pass structure (x_enc → encoder → projection) can template TiDE's encoder-decoder flow.\n5. **Batch Processing Framework**: Both handle batched time-series data with shape [B, L, D]. The source's tensor manipulation patterns and device handling provide practical implementation references for TiDE's data pipeline.",
    "differences": "1. **Core Architecture Philosophy**: Pyraformer uses pyramidal attention with multi-resolution C-ary trees achieving O(L) complexity, while TiDE employs a pure MLP-based architecture with dense layers, completely avoiding attention mechanisms. TiDE requires implementing residual MLP blocks instead of attention.\n2. **Temporal Modeling Approach**: Pyraformer captures dependencies through hierarchical attention across scales (PAM with inter/intra-scale connections), whereas TiDE uses temporal projection layers that directly encode time steps as features. Need to implement dense temporal encoders replacing pyramidal structures.\n3. **Feature Extraction Mechanism**: Pyraformer uses CSCM with bottleneck convolutions and downsampling (window_size=[4,4]) for multi-scale features. TiDE uses simple linear projections for feature encoding/decoding. Must replace convolutional modules with fully-connected layers.\n4. **Complexity Reduction Strategy**: Pyraformer achieves efficiency through sparse pyramidal attention masks and custom CUDA kernels. TiDE gains efficiency through pure dense operations without attention overhead. Implementation needs standard PyTorch dense layers without custom kernels.\n5. **Decoder Design**: Pyraformer's projection layer concatenates multi-scale features ((len(window_size)+1)*d_model dimensions) for final prediction. TiDE uses a separate temporal decoder with lookback/horizon-specific dense layers. Must implement distinct encoder-decoder with time-step-wise projections instead of scale concatenation."
  },
  {
    "source": "FEDformer_2022",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers employ DMS forecasting where the entire future horizon is predicted in one forward pass, avoiding autoregressive error accumulation. The encoder-decoder structure from DLinear can be adapted for TiDE's global forecasting framework.\n\n2. **Time Series Decomposition for Preprocessing**: DLinear's series_decomp module that separates trend and seasonal components using moving average kernels can be directly reused in TiDE's preprocessing pipeline. This decomposition enhances predictability by handling different temporal patterns separately.\n\n3. **Temporal Linear Layers as Building Blocks**: Both architectures utilize linear transformations along the temporal dimension as core computational units. DLinear's Linear_Seasonal and Linear_Trend implementations provide a foundation for TiDE's dense layers operating on temporal features.\n\n4. **Reconstruction-Based Anomaly Detection**: Both papers frame anomaly detection as a reconstruction task where the model learns normal patterns during training. DLinear's anomaly_detection method that returns reconstructed sequences can serve as the baseline for TiDE's anomaly scoring mechanism.\n\n5. **Channel-Independent or Shared Processing**: DLinear's individual flag for channel-wise vs. shared processing across variates directly maps to TiDE's design choice. The ModuleList implementation for individual channels can be adapted for TiDE's feature projection layers.",
    "differences": "1. **Core Architecture Innovation**: DLinear uses simple decomposition with two parallel linear layers, while TiDE introduces a more complex encoder-decoder with residual connections, dense layers, and temporal feature encoders. New components needed: multi-layer dense blocks, residual skip connections, and attribute/covariate encoders.\n\n2. **Feature Engineering and Covariates**: DLinear operates purely on historical time series without external features, whereas TiDE incorporates static attributes and dynamic covariates (past/future). Implementation requires: attribute embedding layers, covariate projection modules, and feature concatenation mechanisms before dense processing.\n\n3. **Temporal Encoding Mechanisms**: DLinear relies on implicit temporal relationships through weighted sums, while TiDE explicitly encodes temporal information using learnable positional encodings and time-based features. New implementation: temporal feature extractors, positional encoding modules, and time-aware attention mechanisms.\n\n4. **Decoder Design Complexity**: DLinear's decoder simply sums seasonal and trend outputs, but TiDE employs a sophisticated decoder with lookback connections, residual mappings, and hierarchical temporal decoding. Required components: lookback residual blocks, multi-scale temporal decoders, and global-local feature fusion layers.\n\n5. **Anomaly Score Calculation**: DLinear uses simple reconstruction error (MSE between input and output), while TiDE may incorporate multi-horizon prediction errors, weighted temporal discounting, and ensemble scoring across different forecast horizons. New metrics: horizon-weighted error functions, confidence interval estimation, and adaptive threshold mechanisms based on prediction uncertainty."
  },
  {
    "source": "PatchTST_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. Both employ prediction-based anomaly detection where models forecast future values and anomalies are identified through prediction errors. The forecasting pipeline with MSE loss calculation can be directly reused from PatchTST implementation.\n\n2. Both utilize channel-independence modeling approach where each time series channel is processed separately. PatchTST's channel-wise processing loops and data handling utilities are directly applicable to TiDE's implementation.\n\n3. Both papers leverage normalization strategies (RevIN in PatchTST, instance normalization in TiDE) to handle distribution shifts. The normalization layer implementations can be adapted with minimal modifications.\n\n4. Both frameworks support multivariate time series with lookback-horizon architecture. PatchTST's data loading, batching mechanisms, and sliding window implementations can be reused for TiDE's input-output structure.\n\n5. Both use standard time series benchmarking datasets and evaluation protocols. PatchTST's evaluation harness including metric calculation (MSE, MAE) and cross-validation procedures transfers directly to TiDE experiments.",
    "differences": "1. Core architecture differs fundamentally: PatchTST uses Transformer with patch-based tokenization and self-attention mechanisms, while TiDE employs pure MLP architecture with residual connections. Need to implement dense layer stacks, feature projection layers, and temporal decoder from scratch.\n\n2. Input representation diverges significantly: PatchTST segments time series into patches and applies positional encoding, whereas TiDE uses dense encoders with attribute/time feature concatenation. Must implement TiDE's feature engineering pipeline for covariates and temporal attributes integration.\n\n3. Temporal modeling approach contrasts: PatchTST captures dependencies through attention mechanisms across patches, while TiDE relies on dense encoders-decoders with explicit lookback-horizon mapping. Requires building TiDE's two-stage encode-decode architecture without attention layers.\n\n4. Model complexity and efficiency goals differ: PatchTST focuses on efficient attention through patching for long sequences, TiDE emphasizes simplicity and computational efficiency through linear layers. Need to implement TiDE's lightweight residual MLP blocks optimized for speed.\n\n5. Feature handling mechanisms vary: PatchTST primarily processes raw time series values, while TiDE explicitly incorporates covariates, static attributes, and known future features. Must implement TiDE's multi-source feature fusion strategy and attribute embedding layers for enriched representations."
  },
  {
    "source": "PatchTST_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Input Segmentation**: Both papers segment time series into non-overlapping patches as basic processing units. The patching mechanism, stride, and embedding layers from PatchTST can be directly reused for MultiPatchFormer's multi-scale patch extraction.\n\n2. **Transformer Encoder Architecture**: Both utilize Transformer encoder blocks with multi-head self-attention and feed-forward networks. PatchTST's encoder implementation (attention mechanisms, normalization, positional encoding) provides the foundational architecture for MultiPatchFormer's hierarchical encoder.\n\n3. **Channel-Independent Modeling**: Both adopt channel-independent strategies where each variate is processed separately. PatchTST's channel-wise processing pipeline and batch handling can be adapted for MultiPatchFormer's multi-scale channel processing.\n\n4. **Reconstruction-based Anomaly Detection**: Both employ reconstruction paradigm where models learn normal patterns and detect anomalies via reconstruction errors. PatchTST's loss computation and error calculation modules are directly applicable.\n\n5. **Positional Encoding Strategy**: Both require positional information for patch sequences. PatchTST's learnable or fixed positional encoding implementations can serve as baseline for MultiPatchFormer's multi-scale positional encoding requirements.",
    "differences": "1. **Multi-Scale Patch Hierarchy**: MultiPatchFormer introduces hierarchical multi-scale patching (fine, medium, coarse) to capture patterns at different temporal resolutions. NEW implementation needed: multi-scale patch extraction module with different patch lengths, hierarchical feature fusion mechanism across scales, and scale-specific encoders.\n\n2. **Cross-Scale Attention Mechanism**: MultiPatchFormer employs cross-scale attention to integrate information across different patch granularities. NEW components required: cross-scale attention layers that enable interaction between fine-grained and coarse-grained patch representations, attention weight computation across scales.\n\n3. **Hierarchical Decoder Architecture**: Unlike PatchTST's single-scale decoder, MultiPatchFormer uses hierarchical decoders for multi-scale reconstruction. NEW implementation: scale-specific decoders for each patch granularity, multi-scale reconstruction loss aggregation, and weighted combination of reconstruction errors from different scales.\n\n4. **Adaptive Scale Weighting**: MultiPatchFormer dynamically weights contributions from different scales based on anomaly characteristics. NEW module needed: learnable scale importance weights, adaptive weighting mechanism that adjusts during training, and scale-wise anomaly score fusion strategy.\n\n5. **Multi-Scale Anomaly Score Fusion**: MultiPatchFormer computes anomaly scores at multiple scales and fuses them intelligently. NEW implementation required: scale-specific reconstruction error computation, weighted aggregation of multi-scale errors, and ensemble-based anomaly scoring that considers all temporal granularities simultaneously.\n\n6. **Temporal Resolution Alignment**: MultiPatchFormer requires aligning features and errors across different patch lengths. NEW components: upsampling/downsampling modules for cross-scale alignment, temporal interpolation mechanisms, and resolution-aware feature aggregation to ensure consistent anomaly detection across scales."
  },
  {
    "source": "DLinear_2022",
    "target": "MultiPatchFormer_2025",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Crossformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Embedding Architecture**: Both employ segment/patch-wise embedding to capture local temporal patterns, where Crossformer's DSW embedding (Dimension-Segment-Wise) divides time series into fixed-length segments similar to patching strategies. The `PatchEmbedding` module from Crossformer can be directly adapted for MultiPatchFormer's patch extraction mechanism.\n\n2. **Hierarchical Multi-Scale Processing**: Both utilize hierarchical encoder structures to capture dependencies at multiple temporal scales through progressive aggregation. Crossformer's `scale_block` with segment merging provides reusable code for implementing multi-scale feature extraction in MultiPatchFormer.\n\n3. **Reconstruction-based Anomaly Detection**: Both papers adopt reconstruction paradigm where the model learns to reconstruct input sequences, with anomaly scores derived from reconstruction errors. Crossformer's `anomaly_detection` method demonstrates the encoder-decoder reconstruction pipeline applicable to MultiPatchFormer.\n\n4. **Cross-dimensional Dependency Modeling**: Both explicitly model relationships across different variable dimensions in multivariate time series. Crossformer's Two-Stage Attention (TSA) with router mechanism for cross-dimension interaction provides foundational code for dimension-aware processing.\n\n5. **Positional Encoding Strategy**: Both incorporate learnable positional embeddings to preserve temporal ordering information in patch/segment representations. The `PositionalEmbedding` and parameter-based position encoding from Crossformer are directly transferable components.",
    "differences": "1. **Core Innovation - Multiple Patch Sizes**: MultiPatchFormer's key contribution is using multiple patch lengths simultaneously to capture patterns at different temporal granularities, requiring implementation of parallel patching branches with different window sizes. Crossformer uses fixed segment length (seg_len=12), necessitating new multi-branch patch extraction and fusion modules.\n\n2. **Attention Mechanism Design**: Crossformer employs Two-Stage Attention (cross-time then cross-dimension with router mechanism) for computational efficiency O(DL²). MultiPatchFormer likely uses standard or modified self-attention across multi-scale patches, requiring new attention modules that handle variable-length patch sequences and their interactions.\n\n3. **Feature Fusion Strategy**: MultiPatchFormer requires novel fusion mechanisms to combine representations from multiple patch sizes (e.g., concatenation, weighted averaging, or hierarchical fusion). Crossformer's hierarchical decoder simply sums predictions across scales, necessitating implementation of more sophisticated multi-scale fusion layers.\n\n4. **Anomaly Score Calculation**: MultiPatchFormer may compute anomaly scores by aggregating reconstruction errors across different patch sizes with learned or adaptive weighting schemes. Crossformer uses straightforward reconstruction error from single-scale output, requiring new multi-scale error aggregation and threshold determination strategies.\n\n5. **Decoder Architecture Adaptation**: MultiPatchFormer's decoder must handle multiple input streams from different patch-size encoders, potentially requiring cross-scale attention or gating mechanisms. Crossformer's decoder processes single hierarchical representation with layer-wise predictions, necessitating architectural redesign for multi-branch decoding and integration.\n\n6. **Computational Complexity Trade-offs**: Processing multiple patch sizes increases computational cost proportionally. Implementation requires efficient parallel processing strategies and potential pruning mechanisms not present in Crossformer's single-scale processing, demanding new optimization techniques for training and inference efficiency."
  },
  {
    "source": "FEDformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Frequency Domain Processing**: Both leverage frequency-domain transformations for capturing temporal patterns. FEDformer's Fourier/Wavelet blocks can guide implementing frequency-aware components in MultiPatchFormer if it uses spectral analysis.\n2. **Encoder-Decoder Architecture**: Both adopt encoder-decoder structures with attention mechanisms. FEDformer's `Encoder`/`Decoder` classes and embedding modules (`DataEmbedding`) are directly reusable as architectural templates.\n3. **Series Decomposition**: FEDformer's `series_decomp` and MOEDecomp for trend-seasonal separation can be adapted if MultiPatchFormer employs decomposition-based preprocessing for anomaly detection.\n4. **Attention Mechanism Design**: FEDformer's frequency-enhanced attention (FEB, FEA) provides a blueprint for designing attention modules. The multi-head attention infrastructure can be reused for MultiPatchFormer's patch-level attention.\n5. **Anomaly Detection via Reconstruction**: FEDformer's `anomaly_detection` method uses encoder-projection for reconstruction. This reconstruction-based anomaly scoring framework is directly transferable to MultiPatchFormer's implementation.",
    "differences": "1. **Patch-Based Tokenization**: MultiPatchFormer likely segments time series into patches (like ViT for images), requiring new patch embedding and positional encoding modules. FEDformer processes full sequences without explicit patching, necessitating implementation of patch extraction and aggregation layers.\n2. **Multi-Scale Patch Processing**: MultiPatchFormer may use multiple patch sizes simultaneously for multi-resolution analysis. This requires designing parallel patch encoders with different granularities, unlike FEDformer's single-scale frequency decomposition via fixed wavelet levels.\n3. **Anomaly Score Aggregation Across Patches**: MultiPatchFormer needs patch-level anomaly scoring and aggregation strategies (e.g., max-pooling, weighted averaging). FEDformer computes point-wise reconstruction errors without patch-wise scoring, requiring new scoring logic.\n4. **Local-Global Interaction Modeling**: MultiPatchFormer likely models intra-patch (local) and inter-patch (global) dependencies separately. FEDformer's frequency attention operates globally; new modules for hierarchical local-global attention must be implemented.\n5. **Threshold Determination Strategy**: MultiPatchFormer may use patch-level or sequence-level thresholding with adaptive mechanisms. FEDformer's implementation lacks explicit threshold logic, requiring development of dynamic thresholding based on patch statistics or validation-set-based calibration."
  },
  {
    "source": "PatchTST_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. Both employ patch-based segmentation for time series processing, dividing sequences into non-overlapping segments to capture local patterns. PatchTST's patching mechanism can be adapted for SegRNN's segment extraction with similar tokenization logic.\n2. Both use channel-independent processing strategies where each variate is modeled separately, avoiding complex cross-channel dependencies. The channel-wise iteration framework from PatchTST can be directly reused for SegRNN's parallel RNN processing.\n3. Both papers focus on long-term forecasting with prediction-based anomaly detection, calculating reconstruction errors between predictions and actual values. The MSE-based anomaly scoring pipeline from PatchTST is transferable to SegRNN.\n4. Both utilize similar data preprocessing pipelines including normalization (instance normalization/RevIN) and sliding window generation. PatchTST's data loading and normalization utilities can serve as foundation for SegRNN implementation.\n5. Both employ similar evaluation frameworks for time series tasks, using metrics like MSE and MAE for forecasting quality assessment. The evaluation harness and metric calculation modules are largely reusable across implementations.",
    "differences": "1. Core architecture differs fundamentally: PatchTST uses Transformer encoders with self-attention mechanisms for patch relationships, while SegRNN replaces this with simple multi-layer RNNs (GRU/LSTM) processing segments sequentially. New RNN cell implementations and recurrent processing loops must be built.\n2. Computational complexity philosophy diverges: PatchTST maintains O(L²) attention complexity (reduced by patching), whereas SegRNN achieves O(L) linear complexity through recurrent connections. The lightweight RNN backbone requires removing attention modules entirely and implementing stateful recurrent propagation.\n3. Positional encoding handling differs: PatchTST employs learnable positional embeddings for patch positions within Transformer, while SegRNN relies on RNN's inherent sequential processing without explicit positional encodings. Position embedding layers must be removed for SegRNN.\n4. SegRNN introduces segment-level revin normalization applied to each segment independently before RNN processing, contrasting with PatchTST's instance-level normalization across entire sequences. New per-segment normalization logic with separate statistics tracking needs implementation.\n5. Aggregation mechanisms differ: PatchTST uses attention-weighted aggregation of patch representations, while SegRNN employs simple linear projection of RNN hidden states concatenated across segments. The projection head architecture must be redesigned from multi-head attention to basic linear layers.\n6. SegRNN emphasizes extreme parameter efficiency (under 1K parameters) as core contribution, requiring careful dimension management and minimal layer stacking. PatchTST's deeper architecture with multiple attention layers must be stripped down to lightweight RNN configurations.\n7. Training efficiency differs: SegRNN claims faster training due to RNN simplicity versus PatchTST's attention computation overhead. Training loops may need optimization adjustments for RNN gradient handling (truncated BPTT) versus parallel attention training."
  },
  {
    "source": "TimesNet_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based Anomaly Detection**: Both papers employ reconstruction approaches where the model learns to reconstruct normal patterns, with reconstruction error serving as anomaly indicator. TimesNet's anomaly_detection method (normalization → embedding → transformation → projection → denormalization) provides a reusable pipeline structure.\n\n2. **Instance Normalization Strategy**: Both adopt instance-level normalization (mean subtraction and standard deviation scaling) before processing, followed by denormalization after reconstruction. TimesNet's normalization code (means/stdev calculation with 1e-5 epsilon) can be directly reused for SegRNN implementation.\n\n3. **Sequence-to-Sequence Framework**: Both process full input sequences and output reconstructions of the same length for anomaly detection. The input/output handling logic (batch processing, sequence length management) from TimesNet's forward pass is transferable.\n\n4. **Layer Normalization Usage**: Both employ layer normalization for stabilizing deep representations. TimesNet's layer_norm application pattern (after each block, before projection) provides guidance for SegRNN's normalization placement strategy.\n\n5. **Embedding and Projection Layers**: Both use linear embedding layers to project raw inputs into latent space and projection layers to reconstruct outputs. TimesNet's DataEmbedding and final projection structure offers reusable components for SegRNN's encoder-decoder design.",
    "differences": "1. **Core Architecture Philosophy**: TimesNet transforms 1D time series into 2D tensors based on FFT-discovered periodicities, using Inception blocks for multi-scale 2D convolution. SegRNN requires implementing segment-wise RNN architecture that divides sequences into fixed segments and processes each with independent RNN cells, fundamentally different from TimesNet's periodicity-based 2D transformation.\n\n2. **Temporal Pattern Modeling Mechanism**: TimesNet employs FFT_for_Period to discover multiple periodicities (top-k frequencies), reshapes data into period×frequency 2D grids, and aggregates multi-period representations with adaptive weights. SegRNN needs segment-based decomposition without frequency analysis, requiring new implementation of segment division logic, per-segment RNN processing, and segment-wise feature aggregation mechanisms.\n\n3. **Feature Extraction Backbone**: TimesNet uses parameter-efficient Inception blocks with multi-kernel 2D convolutions (kernel sizes 1,3,5,7,9,11) applied to reshaped 2D tensors, enabling simultaneous intra-period and inter-period variation capture. SegRNN requires implementing RNN cells (LSTM/GRU) that process sequential segments independently, needing new recurrent computation logic and hidden state management absent in TimesNet.\n\n4. **Multi-Scale Representation Strategy**: TimesNet's TimesBlock processes k different period-based 2D reshapes simultaneously through shared Inception blocks, then aggregates via softmax-weighted amplitude-based fusion. SegRNN needs implementing segment-level processing where each segment is treated independently by RNN, requiring new segment boundary handling, cross-segment information flow control, and segment-wise output concatenation logic.\n\n5. **Model Complexity and Parameterization**: TimesNet stacks multiple TimesBlocks (e_layers) with shared Inception blocks across periods, making model size independent of k. SegRNN requires implementing segment-specific or shared RNN parameters across segments, needing careful design of parameter sharing strategy, segment number determination logic, and potentially segment-wise attention mechanisms for aggregation not present in TimesNet's architecture."
  },
  {
    "source": "DLinear_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers employ DMS forecasting where the model directly predicts the entire future sequence in one forward pass, avoiding autoregressive error accumulation. DLinear's encoder structure can serve as a template for SegRNN's reconstruction framework.\n\n2. **Reconstruction-Based Anomaly Detection**: Both use time series forecasting/reconstruction as the core mechanism for anomaly detection, computing anomaly scores from prediction errors. The `anomaly_detection` method in DLinear's implementation provides a reusable pattern for SegRNN's reconstruction pipeline.\n\n3. **Temporal Decomposition Awareness**: DLinear explicitly decomposes series into trend and seasonal components using moving average kernels. SegRNN likely benefits from similar decomposition thinking when segmenting sequences, making the `series_decomp` layer potentially reusable for preprocessing.\n\n4. **Channel-Independent Processing**: DLinear's `individual` mode processes each variate independently without modeling cross-channel correlations. This design philosophy aligns with segment-based approaches, where the ModuleList structure for per-channel linear layers can guide SegRNN's per-segment processing.\n\n5. **Simple Linear Baseline Philosophy**: Both challenge complex Transformer architectures by demonstrating that simpler linear/RNN models can achieve competitive performance. The weight initialization strategy (`1/seq_len`) and training loop structure from DLinear provide implementation guidance for SegRNN's simplicity-focused design.",
    "differences": "1. **Core Architecture - Linear vs. RNN**: DLinear uses pure linear layers for temporal mapping, while SegRNN requires implementing RNN cells (likely GRU/LSTM) to process segmented subsequences. New components needed: RNN encoder/decoder modules, hidden state management, and segment-level recurrent processing logic.\n\n2. **Sequence Segmentation Mechanism**: SegRNN's key innovation is dividing time series into fixed-length segments and processing them sequentially with RNNs. Implementation requires: segment splitting logic, segment embedding layers, and mechanisms to aggregate segment-level outputs—none of which exist in DLinear's continuous temporal processing.\n\n3. **Temporal Pattern Modeling**: DLinear models patterns through weighted summation across the entire lookback window with explicit trend-seasonal decomposition. SegRNN models patterns through recurrent hidden states that capture segment-to-segment dependencies, requiring new state transition dynamics and segment-aware attention mechanisms.\n\n4. **Anomaly Score Calculation**: DLinear computes point-wise reconstruction errors directly from linear projections. SegRNN likely calculates anomaly scores based on segment-level reconstruction quality or hidden state deviations, necessitating new scoring functions that aggregate errors across segments and handle segment boundary effects.\n\n5. **Training Objectives and Loss Functions**: DLinear optimizes simple MSE/MAE between predictions and targets on the full sequence. SegRNN may require segment-wise loss computation, regularization on RNN hidden states to prevent overfitting, and potentially contrastive losses between normal/anomalous segments—requiring custom loss function implementations beyond DLinear's straightforward regression loss."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. Both papers target long-term time series forecasting (LTSF) using Direct Multi-Step (DMS) strategy, eliminating autoregressive error accumulation. FEDformer's decoder architecture with projection layers can guide DLinear's output projection implementation.\n2. Both employ seasonal-trend decomposition preprocessing using moving average kernels to separate trend and seasonal components. FEDformer's series_decomp module (moving_avg kernel) is directly reusable for DLinear's decomposition preprocessing.\n3. Both use DataEmbedding for input representation with temporal features (timestamps, positional encoding). FEDformer's embedding layer structure provides template for handling multi-variate time series inputs in DLinear.\n4. Both models output predictions through linear projection layers mapping hidden states to target dimensions. FEDformer's final projection mechanism (nn.Linear) demonstrates the weight-sharing strategy applicable to DLinear's temporal linear layers.\n5. Both handle variable-length sequences with padding/truncation strategies and support multi-variate forecasting. FEDformer's batch processing logic for (B, L, C) tensors guides DLinear's data pipeline implementation.",
    "differences": "1. Core architecture: FEDformer uses complex Transformer encoder-decoder with frequency-domain attention (Fourier/Wavelet blocks), requiring O(L) complexity frequency transforms. DLinear uses simple one-layer temporal linear regression (WX_i), requiring only basic matrix multiplication without any attention mechanism.\n2. Decomposition integration: FEDformer applies MOEDecomp (Mixture of Experts Decomposition) recursively within each encoder/decoder layer using learnable weights for multi-scale trend extraction. DLinear applies decomposition only once as preprocessing, using fixed moving average followed by separate linear layers for trend/seasonal components.\n3. Temporal modeling: FEDformer captures long-range dependencies through multi-head self-attention and cross-attention in frequency domain with learnable frequency mode selection. DLinear directly regresses on raw time steps via temporal linear weights, treating each time step as independent feature without modeling inter-step correlations.\n4. Parameter sharing: FEDformer uses separate parameters for each attention head, encoder/decoder layer, and frequency mode with complex hierarchical structure. DLinear shares single weight matrix W∈R^(T×L) across all variates, dramatically reducing parameters while NLinear variant adds simple normalization (subtract last value).\n5. Implementation complexity: FEDformer requires implementing FFT/DWT transforms, complex multiplication in frequency domain, wavelet basis functions (Legendre polynomials), and multi-scale reconstruction. DLinear needs only basic PyTorch linear layers, moving average pooling, and element-wise operations, making it 100× simpler to implement."
  },
  {
    "source": "Autoformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. Both employ series decomposition to extract trend and seasonal components from time series data. Autoformer's `series_decomp` module using moving average kernels can be directly reused in DLinear's decomposition preprocessing.\n2. Both adopt Direct Multi-Step (DMS) forecasting strategy for long-term predictions, avoiding autoregressive error accumulation. The encoder-only architecture pattern from Autoformer can guide DLinear's single-pass prediction implementation.\n3. Both use `DataEmbedding_wo_pos` for input processing with temporal embeddings and channel projections. The embedding layers from Autoformer's implementation can be adapted for DLinear's input normalization.\n4. Both frameworks share identical evaluation protocols for forecasting tasks (MSE/MAE metrics). Autoformer's training loop, loss computation, and metric calculation code can be reused with minimal modifications.\n5. Both handle multivariate time series with channel-wise processing. Autoformer's data loading pipeline and batch processing logic can be directly transferred to DLinear implementation.",
    "differences": "1. Core architecture differs fundamentally: Autoformer uses complex Transformer encoder-decoder with Auto-Correlation mechanism (O(L log L) complexity), while DLinear uses simple temporal linear layers (O(L) complexity). DLinear requires implementing lightweight `nn.Linear` layers instead of attention mechanisms.\n2. Temporal dependency modeling diverges completely: Autoformer discovers period-based dependencies via FFT-based autocorrelation and time-delay aggregation with multi-head attention, whereas DLinear directly applies weighted summation without any attention. New implementation needs single linear transformation per component.\n3. Decomposition integration differs: Autoformer embeds decomposition as inner blocks throughout encoder/decoder layers with progressive refinement, while DLinear applies decomposition only once as preprocessing before linear layers. Simpler one-time decomposition module needed.\n4. NLinear variant introduces novel normalization strategy: subtracting last value before linear transformation and adding back afterward to handle distribution shifts. This normalization scheme is absent in Autoformer and requires new implementation.\n5. Model complexity and parameter count differ drastically: Autoformer has multi-layer encoder/decoder with projection matrices and feedforward networks, while DLinear has only 1-2 linear layers per variate. Simplified model class with minimal parameters needed for DLinear."
  },
  {
    "source": "Informer_2020",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both employ DMS forecasting that predicts entire future sequences in one forward pass, avoiding autoregressive error accumulation. Informer's generative decoder architecture (feeding start tokens + placeholders) can be adapted for DLinear's linear forecasting implementation.\n\n2. **Encoder-Decoder Framework Foundation**: Both use encoder-style processing for input sequences followed by prediction generation. Informer's DataEmbedding layer (temporal encoding, positional encoding) provides reusable input preprocessing components that DLinear can leverage for consistent time series representation.\n\n3. **Time Series Decomposition Philosophy**: Both recognize decomposition benefits—Informer uses self-attention distilling for feature extraction, while DLinear explicitly decomposes trend-seasonal. Informer's ConvLayer (moving average-like operations) offers implementation patterns for DLinear's moving average kernel in trend extraction.\n\n4. **Univariate/Multivariate Handling**: Both support channel-independent and channel-mixed forecasting modes through projection layers. Informer's final projection layer (nn.Linear mapping d_model to c_out) demonstrates the architectural pattern DLinear adopts for multi-variate output generation.\n\n5. **Training Infrastructure and Loss Functions**: Both use MSE loss for optimization and standard PyTorch training loops. Informer's training pipeline (data loading, normalization, model forward/backward) provides complete boilerplate code that DLinear experiments can directly inherit and modify.",
    "differences": "1. **Core Architecture Paradigm Shift**: Informer uses complex ProbSparse self-attention with O(L log L) complexity requiring query sparsity measurement and attention distilling operations. DLinear replaces this entirely with single-layer temporal linear transformations (W ∈ R^(T×L)), requiring implementation of simple matrix multiplication without attention mechanisms.\n\n2. **Decomposition Implementation Location**: Informer applies optional decomposition through ConvLayer distilling between encoder layers as a feature refinement step. DLinear mandates explicit seasonal-trend decomposition (moving average kernel) as mandatory preprocessing before linear layers, requiring new decomposition module at input stage.\n\n3. **Positional and Temporal Embeddings**: Informer extensively uses learnable positional encodings, temporal embeddings (hour/day/week), and channel projections totaling d_model dimensions. DLinear eliminates all embedding layers, operating directly on raw normalized values, requiring removal of embedding infrastructure and simplified input pipeline.\n\n4. **Model Complexity and Parameter Count**: Informer stacks multiple encoder/decoder layers (e_layers, d_layers) with multi-head attention (n_heads), feed-forward networks (d_ff=4*d_model), creating deep architectures. DLinear uses only one or two linear layers with minimal parameters, requiring drastically simplified model initialization and forward propagation logic.\n\n5. **Normalization Strategy Innovation**: Informer uses standard zero-mean normalization across sequences for training stability. DLinear introduces NLinear variant with last-value subtraction normalization (subtracting sequence's last value before linear transformation), requiring implementation of novel normalization schemes that preserve distribution shift robustness through additive residual connections."
  },
  {
    "source": "Pyraformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. Both papers address long-term time series forecasting (LTSF) with direct multi-step (DMS) prediction strategies to avoid error accumulation. Pyraformer's projection layer for multi-step output can guide DLinear's implementation of simultaneous T-step predictions.\n\n2. Both utilize data embedding modules for temporal feature extraction before prediction. Pyraformer's DataEmbedding component (combining value, position, and temporal embeddings) provides a reusable template for DLinear's input preprocessing, though DLinear simplifies this significantly.\n\n3. Both employ normalization techniques for stability: Pyraformer uses LayerNorm in feed-forward networks, while DLinear uses subtraction-based normalization (NLinear variant). The normalization patterns in Pyraformer's PositionwiseFeedForward can inform DLinear's preprocessing implementation.\n\n4. Both models output predictions with shape [B, T, D] for multivariate forecasting. Pyraformer's final projection layer structure (mapping encoded features to output dimensions) directly guides DLinear's linear layer design for channel-independent predictions.\n\n5. Both papers evaluate on common LTSF benchmarks (Electricity, ETT datasets) with similar train/validation/test splits. Pyraformer's data loading and evaluation pipeline (MSE, MAE metrics) can be directly reused for DLinear experiments.",
    "differences": "1. Core architecture philosophy: Pyraformer uses complex pyramidal attention with O(L) complexity and multi-scale temporal modeling through C-ary trees, while DLinear strips away all attention mechanisms, using only simple linear layers (W∈R^(T×L)) with O(L) operations. DLinear requires implementing minimal components: decomposition module, linear projection, and normalization.\n\n2. Temporal dependency modeling: Pyraformer captures dependencies through hierarchical attention across multiple scales (PAM with inter-scale and intra-scale connections), requiring CSCM convolution layers and custom CUDA kernels. DLinear models dependencies through direct weighted summation of historical points, requiring only matrix multiplication without any attention or convolution operations.\n\n3. Feature extraction approach: Pyraformer uses bottleneck convolutions (Bottleneck_Construct) with downsampling at multiple resolutions and stacked encoder layers with self-attention. DLinear variants (DLinear/NLinear) require implementing seasonal-trend decomposition using moving average kernels and simple subtraction-based normalization, eliminating all encoder complexity.\n\n4. Model complexity and parameters: Pyraformer has O(N(HDD_K+DD_F)+(S-1)CD_K²) parameters with multiple encoder layers, attention heads, and pyramid scales. DLinear has minimal parameters (single weight matrix per variate), requiring implementation of channel-independent linear layers where each variate has separate weights, fundamentally different from Pyraformer's shared representations.\n\n5. Innovation focus: Pyraformer innovates on efficient attention mechanisms for long-range dependency capture with pyramidal graphs and proves O(1) maximum path length. DLinear challenges Transformer necessity, demonstrating simple linear models outperform complex architectures, requiring implementation of ablation studies comparing against Transformer baselines and analysis of why simple models suffice for LTSF tasks."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": null,
    "differences": null
  },
  {
    "source": "DLinear_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt DMS forecasting where the entire future sequence is predicted at once rather than autoregressively. DLinear's direct linear projection framework (predicting T future steps simultaneously) can be reused as the baseline decoder strategy in Crossformer's implementation.\n\n2. **Series Decomposition for Preprocessing**: DLinear uses moving average kernels to extract trend-cyclical and seasonal components, identical to Autoformer's decomposition scheme. Crossformer's hierarchical encoder can directly reuse DLinear's `series_decomp` layer implementation for data preprocessing and intermediate feature refinement.\n\n3. **Channel-Independent Processing Option**: DLinear's `individual` parameter enables separate linear layers per variate, avoiding cross-dimension modeling. This design philosophy aligns with Crossformer's dimension-wise processing in the Cross-Time Stage, where each dimension processes temporal segments independently before cross-dimension aggregation.\n\n4. **Permutation of Input Dimensions**: Both papers manipulate tensor dimensions extensively—DLinear permutes `(B,L,D)` to `(B,D,L)` for temporal linear layers. Crossformer's DSW embedding and TSA layers require similar dimension permutations, allowing reuse of DLinear's tensor reshaping utilities.\n\n5. **Linear Projection for Final Prediction**: DLinear uses simple linear layers (`nn.Linear`) to map from input length to prediction length. Crossformer's decoder also employs linear projection matrices (`W^l`) to convert segment embeddings to time series predictions, enabling direct adaptation of DLinear's projection mechanism.",
    "differences": "1. **Core Architecture Paradigm**: DLinear uses purely linear layers without attention mechanisms, achieving O(1) complexity per layer. Crossformer implements a full Transformer with Two-Stage Attention (TSA) layers, requiring new implementation of multi-head self-attention for both cross-time (O(DL²)) and cross-dimension stages with router mechanism (O(DL)).\n\n2. **Input Embedding Strategy**: DLinear operates on raw point-wise time series values with optional decomposition but no explicit segmentation. Crossformer introduces Dimension-Segment-Wise (DSW) embedding that divides each dimension into segments of length L_seg, requiring new segment tokenization, learnable projection matrices E∈R^(d_model×L_seg), and 2D positional embeddings E^(pos).\n\n3. **Cross-Dimension Dependency Modeling**: DLinear explicitly avoids modeling spatial correlations between variates (shares weights or uses separate models per variate). Crossformer's key innovation is the router mechanism in Cross-Dimension Stage, using c learnable router vectors to aggregate and distribute information across D dimensions, requiring completely new attention module implementation.\n\n4. **Hierarchical Multi-Scale Processing**: DLinear processes data at a single temporal resolution throughout. Crossformer implements a hierarchical encoder-decoder with N layers where segment merging (concatenating adjacent segments via learnable matrix M) creates coarser scales, and multi-scale predictions from all decoder layers are summed, necessitating new hierarchical architecture with scale-specific processing.\n\n5. **Anomaly Detection Mechanism**: DLinear performs anomaly detection through direct reconstruction error (prediction vs. actual). Crossformer's hierarchical structure enables multi-scale anomaly scoring where discrepancies at different temporal granularities can be weighted differently, requiring new implementation of scale-aware anomaly score aggregation and potentially different threshold strategies per scale."
  },
  {
    "source": "FEDformer_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Hierarchical Encoder-Decoder Architecture**: Both employ multi-layer encoder-decoder structures with segment-based processing. FEDformer's encoder/decoder framework (Eq. 1-2) can be adapted by replacing FEB/FEA modules with Crossformer's TSA layers while maintaining the overall hierarchical structure and residual connections.\n\n2. **Seasonal-Trend Decomposition Philosophy**: Both integrate decomposition mechanisms throughout the network. FEDformer's MOEDecomp blocks (Eq. 10) provide a reusable foundation for implementing Crossformer's implicit decomposition, particularly the mixture-of-experts approach with learnable averaging filters for trend extraction.\n\n3. **Position Embedding and Linear Projection**: Both use learnable position embeddings and linear projections for input/output transformations. FEDformer's DataEmbedding module can be directly adapted for Crossformer's DSW embedding (Eq. 2) by modifying the projection dimensions to handle segment-wise rather than point-wise embeddings.\n\n4. **Layer Normalization and Residual Connections**: Both employ identical normalization strategies with skip connections around attention blocks. FEDformer's LayerNorm implementation in encoder/decoder layers can be reused without modification for Crossformer's TSA layers (Eq. 3-4, 7).\n\n5. **Multi-Layer Feedforward Networks**: Both use two-layer MLPs with identical activation patterns after attention mechanisms. FEDformer's FeedForward modules can be directly transplanted to Crossformer's TSA and decoder layers, maintaining the same hidden dimension expansion ratios.",
    "differences": "1. **Core Attention Mechanism - Frequency vs. Dimension-Time**: FEDformer operates in frequency domain using Fourier/Wavelet transforms (Eq. 3-7) with mode selection, while Crossformer uses spatial Two-Stage Attention on 2D arrays. NEW: Implement Cross-Time Stage (Eq. 3) with MSA on temporal dimension and Cross-Dimension Stage (Eq. 4) with router mechanism requiring learnable router vectors R and dual MSA operations.\n\n2. **Input Embedding Strategy - Point-wise vs. Segment-wise**: FEDformer embeds individual time points across all dimensions, while Crossformer uses Dimension-Segment-Wise embedding (Eq. 1-2). NEW: Implement segment partitioning logic that reshapes input from [B,T,D] to [B,T/L_seg,D,L_seg], then applies per-segment linear projection with 2D position embeddings for (segment_index, dimension) pairs.\n\n3. **Hierarchical Information Aggregation**: FEDformer uses fixed-depth frequency decomposition, while Crossformer employs segment merging (Eq. 6) that concatenates adjacent temporal segments and projects via learnable matrix M. NEW: Implement merging operation that halves temporal resolution per layer and multi-scale decoder (Eq. 7-8) that sums predictions across all hierarchical levels.\n\n4. **Cross-Dimension Dependency Modeling**: FEDformer treats dimensions independently in frequency domain, while Crossformer explicitly models cross-dimension interactions through router mechanism. NEW: Implement router-based attention (Eq. 4) with fixed number c of learnable routers that aggregate information from all D dimensions via MSA₁ and redistribute via MSA₂, reducing complexity from O(D²L) to O(DL).\n\n5. **Decoder Architecture and Prediction Strategy**: FEDformer uses single-scale cross-attention between encoder-decoder with accumulated trend components (Eq. 2), while Crossformer employs hierarchical decoder with scale-specific predictions. NEW: Implement N+1 decoder layers matching encoder scales, each with TSA self-attention, MSA cross-attention to corresponding encoder layer, and independent linear projections (Eq. 8) that sum for final multi-scale prediction."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Multi-resolution Temporal Modeling**: Both employ hierarchical structures to capture temporal dependencies at multiple scales. Pyraformer's pyramidal C-ary tree and Crossformer's segment merging in the encoder both aggregate information from finer to coarser scales, enabling efficient long-range dependency modeling. The coarser-scale construction module (CSCM) with convolution layers in Pyraformer can guide implementation of Crossformer's segment merging mechanism.\n\n2. **Segment-based Embedding Strategy**: Both methods move away from point-wise embeddings to segment-based representations. Pyraformer constructs multi-scale nodes representing temporal segments, while Crossformer uses Dimension-Segment-Wise (DSW) embedding with explicit segment length L_seg. Pyraformer's embedding layer combining observation, covariate, and positional embeddings provides a template for Crossformer's segment embedding implementation.\n\n3. **Complexity Reduction via Sparse Attention**: Both achieve O(L) complexity through sparse attention mechanisms instead of O(L²) full attention. Pyraformer's pyramidal attention module (PAM) limits node connections to neighbors, children, and parents; Crossformer's router mechanism uses fixed routers to aggregate cross-dimension information. The custom CUDA kernel implementation approach in Pyraformer can inform Crossformer's efficiency optimizations.\n\n4. **Hierarchical Encoder Architecture**: Both utilize multi-layer encoders where upper layers process coarser-scale representations. Pyraformer stacks attention layers on the pyramidal graph; Crossformer stacks TSA layers with progressive segment merging. The encoder layer structure with self-attention, layer normalization, and feedforward networks is directly reusable from Pyraformer's EncoderLayer implementation.\n\n5. **Multi-scale Prediction Aggregation**: Both aggregate predictions from multiple scales for final output. Pyraformer concatenates features from all pyramid scales before prediction; Crossformer sums predictions from all decoder layers (Equation 8). Pyraformer's prediction module with fully connected layers provides implementation guidance for Crossformer's linear projection approach across scales.",
    "differences": "1. **Core Attention Mechanism Design**: Pyraformer uses pyramidal attention with inter-scale (parent-child) and intra-scale (neighbor) connections in a C-ary tree structure. Crossformer introduces Two-Stage Attention (TSA) that explicitly separates cross-time and cross-dimension attention stages. NEW IMPLEMENTATION NEEDED: Cross-dimension stage with router mechanism (MSA_1^dim and MSA_2^dim in Equation 4), where routers aggregate and distribute information across dimensions.\n\n2. **Dimension Handling Philosophy**: Pyraformer treats multivariate time series by embedding all dimensions at each time step together, focusing on temporal pyramidal structure. Crossformer fundamentally redesigns this with Dimension-Segment-Wise (DSW) embedding, creating a 2D array (time × dimension) where each vector represents a univariate segment. NEW IMPLEMENTATION NEEDED: 2D array structure management, separate linear projections per dimension, and position embeddings for (i,d) positions.\n\n3. **Cross-Dimension Dependency Modeling**: Pyraformer implicitly captures cross-dimension dependencies through shared embeddings at time steps. Crossformer explicitly models cross-dimension dependencies via the router mechanism with learnable router vectors R and two-stage MSA operations. NEW IMPLEMENTATION NEEDED: Router vector initialization, MSA_1^dim for aggregation (routers as query), MSA_2^dim for distribution (dimension vectors as query), achieving O(DL) complexity.\n\n4. **Decoder Architecture Strategy**: Pyraformer uses either single-step prediction with end tokens or autoregressive decoder with two full attention layers taking prediction tokens. Crossformer employs a hierarchical decoder with N+1 layers, each corresponding to an encoder scale, using TSA layers and cross-attention to encoded features. NEW IMPLEMENTATION NEEDED: Multi-scale decoder with learnable position embeddings E^(dec), per-layer TSA processing, and scale-specific linear projections W^l.\n\n5. **Segment Construction and Merging**: Pyraformer constructs coarser scales through convolution layers with bottleneck structure (dimension reduction/restoration) applied sequentially. Crossformer merges segments by concatenating adjacent temporal segments and applying learnable matrix M for each dimension separately. NEW IMPLEMENTATION NEEDED: Segment merging module with concatenation operation [Z_2i-1 · Z_2i] and dimension-specific learnable matrices, integrated into encoder layers for progressive coarsening."
  },
  {
    "source": "Autoformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. Both employ encoder-decoder Transformer architectures for time series analysis, using multi-layer stacks with attention mechanisms and feedforward networks. Autoformer's `Encoder` and `Decoder` classes with layer stacking can be directly adapted for Crossformer's hierarchical structure.\n\n2. Both utilize series decomposition concepts to separate trend and seasonal components, though applied differently. Autoformer's `series_decomp` module and moving average operations provide reusable foundation for understanding temporal patterns in both forecasting and anomaly detection contexts.\n\n3. Both incorporate LayerNorm, residual connections, and MLP blocks in their layer designs following standard Transformer conventions. Autoformer's `EncoderLayer` and `DecoderLayer` implementation patterns (attention + decomp + FFN) can guide Crossformer's TSA layer structure.\n\n4. Both use learnable embeddings to transform input time series into higher-dimensional representations before processing. Autoformer's `DataEmbedding_wo_pos` with `TokenEmbedding` (Conv1d-based) and position embeddings can inform Crossformer's DSW embedding implementation approach.\n\n5. Both models output reconstructed/predicted sequences for anomaly detection by comparing with ground truth. Autoformer's `projection` layer and final output aggregation strategy can be adapted for Crossformer's multi-scale prediction fusion mechanism.",
    "differences": "1. **Embedding Strategy**: Autoformer embeds each time step across all dimensions into single vectors (point-wise), while Crossformer uses Dimension-Segment-Wise (DSW) embedding creating 2D arrays where each vector represents a segment of single dimension. Need to implement segment-based linear projection and 2D position embeddings.\n\n2. **Attention Mechanism**: Autoformer uses Auto-Correlation with FFT-based period discovery and time-delay aggregation for series-wise connections. Crossformer requires implementing Two-Stage Attention (TSA) with separate cross-time MSA and cross-dimension router mechanism, fundamentally different from Auto-Correlation's frequency domain approach.\n\n3. **Hierarchical Processing**: Autoformer processes single-scale representations with progressive decomposition extracting trends at each layer. Crossformer needs hierarchical encoder with segment merging (concatenating adjacent segments) and multi-scale decoder that aggregates predictions from all hierarchy levels.\n\n4. **Cross-Dimension Modeling**: Autoformer implicitly handles multivariate dependencies through shared processing of concatenated dimensions. Crossformer explicitly models cross-dimension dependency via router mechanism with learnable router vectors aggregating and distributing information across dimensions, requiring new implementation.\n\n5. **Complexity and Scalability**: Autoformer's Auto-Correlation achieves O(L log L) via FFT for temporal dependencies. Crossformer's TSA layer has O(DL²) complexity with router reducing cross-dimension from O(D²L) to O(DL), necessitating different computational optimization strategies for large D scenarios."
  },
  {
    "source": "Informer_2020",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both use Transformer-based encoder-decoder frameworks with multi-layer stacking. Crossformer can reuse Informer's basic encoder-decoder structure, LayerNorm, MLP, and skip connections from the implementation code.\n\n2. **Embedding Strategy**: Both embed input sequences into high-dimensional representations before attention computation. Crossformer can adapt Informer's DataEmbedding class for position encoding, though segment-wise embedding requires modification to handle 2D arrays instead of 1D sequences.\n\n3. **Multi-Head Attention Mechanism**: Both rely on multi-head self-attention as core dependency modeling. Crossformer can reuse Informer's AttentionLayer structure for query/key/value projections and multi-head computation, adapting it for cross-time and cross-dimension stages.\n\n4. **Hierarchical Feature Extraction**: Both use hierarchical architectures to capture multi-scale patterns. Crossformer can adapt Informer's ConvLayer for segment merging operations, replacing MaxPool with learnable matrix merging while maintaining the pyramid structure concept.\n\n5. **Cross-Attention in Decoder**: Both decoders use cross-attention between encoder outputs and decoder queries. Crossformer can directly reuse Informer's cross-attention implementation in DecoderLayer, applying it dimension-wise to connect encoder-decoder features at multiple scales.",
    "differences": "1. **Embedding Granularity (NEW IMPLEMENTATION)**: Informer embeds time steps point-wise (x_t → h_t), while Crossformer uses Dimension-Segment-Wise (DSW) embedding, creating 2D vector arrays where each vector represents a segment in one dimension. Requires implementing segment partitioning and 2D position encoding.\n\n2. **Attention Computation Strategy (NEW IMPLEMENTATION)**: Informer uses ProbSparse attention with query sparsity measurement (O(L log L)), while Crossformer employs Two-Stage Attention (TSA) separating cross-time (O(DL²)) and cross-dimension (O(DL)) stages. Requires implementing router mechanism with learnable aggregators for dimension-wise dependency.\n\n3. **Dimension Dependency Modeling (NEW IMPLEMENTATION)**: Informer treats multivariate series as independent channels within point embeddings, while Crossformer explicitly models cross-dimension dependency through router-based attention. Requires implementing MSA₁^dim and MSA₂^dim with router vectors to aggregate/distribute information across dimensions.\n\n4. **Hierarchical Merging Operation (NEW IMPLEMENTATION)**: Informer uses 1D convolution + max pooling for distilling, while Crossformer merges adjacent time segments using learnable matrices (M[Z_{2i-1,d}·Z_{2i,d}]). Requires replacing ConvLayer's pooling with concatenation-based merging that preserves dimension axis.\n\n5. **Multi-Scale Prediction Aggregation (NEW IMPLEMENTATION)**: Informer outputs single-scale predictions from the final decoder layer, while Crossformer generates predictions at each hierarchical level (N+1 layers) and sums them. Requires implementing per-layer linear projections (W^l) and aggregating predictions across scales for final forecasting."
  },
  {
    "source": "Autoformer_2021",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Philosophy**: Both papers adopt explicit decomposition of time series into trend and seasonal components as a core architectural principle. Autoformer uses `series_decomp` blocks with moving average filters throughout encoder-decoder layers, while ETSformer separates level, growth, and seasonal components. **Code Reuse**: The `series_decomp` module from Autoformer (moving average kernel) can be adapted as a baseline for ETSformer's seasonal extraction, though ETSformer replaces it with Frequency Attention. The decomposition block structure (extracting components, residual learning) is directly transferable.\n\n2. **Encoder-Decoder Architecture with Progressive Refinement**: Both employ stacked encoder-decoder architectures where each layer progressively refines component representations. Autoformer's encoder extracts seasonal patterns layer-by-layer while accumulating trend in decoder; ETSformer's encoder iteratively extracts growth/seasonal from residuals across N stacks. **Code Reuse**: The overall `Encoder` and `Decoder` class structures with layer stacking (from Autoformer's `nn.ModuleList(attn_layers)`) can be directly reused. The progressive component accumulation pattern in Autoformer's decoder (Equation 4: `T_de^l = T_de^(l-1) + W*T_de^(l,i)`) provides template for ETSformer's composition in Equation 3.\n\n3. **Embedding and Input Processing**: Both use convolutional token embeddings without relying heavily on positional encodings for temporal information. Autoformer's `TokenEmbedding` uses Conv1d (kernel=3, circular padding), and ETSformer uses similar Conv operation (kernel=3) for input embedding. **Code Reuse**: Autoformer's `DataEmbedding_wo_pos` class (especially `TokenEmbedding` with Conv1d) can be directly adapted for ETSformer's input embedding module, requiring only minor channel dimension adjustments.\n\n4. **Efficient Frequency Domain Operations**: Both leverage FFT for computational efficiency. Autoformer computes autocorrelation via FFT (Equation 8: Wiener-Khinchin theorem) achieving O(L log L) complexity; ETSformer uses DFT for Frequency Attention (Equation 4) also with O(L log L) complexity. **Code Reuse**: The FFT computation patterns in Autoformer's `AutoCorrelation.forward()` (`torch.fft.rfft`, `torch.fft.irfft`) provide direct templates for implementing ETSformer's FA mechanism, particularly for amplitude/phase extraction and inverse transforms.\n\n5. **Residual Connections and Layer Normalization**: Both employ residual connections with layer normalization after attention/decomposition blocks. Autoformer uses `my_Layernorm` (special seasonal-focused normalization) and standard LN; ETSformer uses LN after each component extraction. **Code Reuse**: The residual connection patterns (e.g., Autoformer Equation 3: `x + dropout(attention(x))`) and normalization modules can be directly transferred to ETSformer's encoder pipeline.\n\n6. **Anomaly Detection via Reconstruction**: For anomaly detection tasks, both papers use reconstruction-based approaches where the model learns normal patterns during training and anomalies are detected via reconstruction error. **Code Reuse**: Autoformer's `anomaly_detection()` method provides the exact template: encode input, project to output dimension, compare with actual values. This entire pipeline is reusable for ETSformer.",
    "differences": "1. **Core Attention Mechanism Innovation**: Autoformer introduces **Auto-Correlation** mechanism that discovers period-based dependencies via autocorrelation in frequency domain, then aggregates similar sub-series using time-delay rolling (Equation 6). ETSformer proposes two novel mechanisms: **(a) Exponential Smoothing Attention (ESA)** - a non-adaptive attention with exponential decay based on time lag rather than content similarity (recursive formula with learnable α parameter), and **(b) Frequency Attention (FA)** - selects top-K frequency components by amplitude for seasonal pattern extraction. **New Implementation Required**: (1) ESA requires implementing Algorithm 1 with efficient cross-correlation via FFT for O(L log L) complexity, including learnable smoothing parameter α and initial state v0; (2) FA requires DFT-based amplitude/phase extraction, Top-K frequency selection logic, and inverse DFT reconstruction (Equation 4); (3) Multi-head versions of ESA (MH-ESA) with successive differencing for growth extraction.\n\n2. **Component Extraction Strategy**: Autoformer extracts trend via moving average pooling (AvgPool with padding) and seasonal as residual (Equation 1), applied uniformly across all layers. ETSformer uses **three distinct mechanisms**: (a) **Level** via exponential smoothing equation (similar to classical Holt-Winters, with learnable α), (b) **Growth** via MH-ESA with successive differencing, (c) **Seasonal** via FA with frequency domain selection. **New Implementation Required**: (1) Level module implementing exponential smoothing recursion (Section 4.1.2) with learnable α parameter and Linear projections; (2) Growth extraction through successive differencing `(Z_t - [Z_(t-1), v0])` before ESA; (3) Integration of FA for seasonal extraction replacing moving average decomposition.\n\n3. **Decoder Forecasting Mechanism**: Autoformer's decoder uses **cross-attention** between decoder queries and encoder outputs, with trend accumulation via weighted sum of extracted trends (Equation 4). ETSformer's decoder has **no cross-attention** but instead uses: (a) **Growth Damping (GD)** module with learnable damping parameter γ for multi-step trend forecasting (cumulative sum with exponential decay), (b) **FA extrapolation** of seasonal patterns beyond lookback window, (c) **Level repetition** of last timestep. **New Implementation Required**: (1) Growth Damping module implementing Equation for trend damping with multi-head learnable γ parameters; (2) FA extrapolation logic to generate seasonal representations for forecast horizon (S_(t:t+H)); (3) Simple repeat operation for level forecasting; (4) Final composition layer (Equation 3) combining level + Linear(growth + seasonal).\n\n4. **Temporal Dependency Modeling Philosophy**: Autoformer models dependencies through **series-wise connections** - finding similar sub-processes at same phase positions across periods via autocorrelation, then rolling and aggregating entire sub-series (Figure 3d). ETSformer models dependencies through **exponential smoothing principle** - assigning higher weights to recent observations via exponential decay (non-adaptive, time-lag based), combined with frequency domain pattern selection (Figure 3e,f). **New Implementation Required**: Replace Autoformer's `time_delay_agg_training/inference` methods with ESA's exponential weighting scheme; implement frequency-based attention weights (non-learnable, amplitude-driven) instead of autocorrelation-based weights.\n\n5. **Input Requirements and Feature Engineering**: Autoformer uses `DataEmbedding_wo_pos` which optionally incorporates temporal embeddings (month, day, hour via `TemporalEmbedding` or `TimeFeatureEmbedding`) as auxiliary inputs (x_mark). ETSformer **explicitly avoids** manual time-dependent covariates, relying solely on raw signals with Conv embedding, claiming FA automatically uncovers seasonal patterns. **New Implementation Required**: Modify input pipeline to exclude temporal feature processing; ensure Conv embedding operates on raw signals only; FA mechanism must be robust enough to discover patterns without covariate hints.\n\n6. **Anomaly Score Calculation Details**: While both use reconstruction error, the **composition differs**: Autoformer's anomaly detection directly projects encoder output to observation space (`projection(enc_out)`), treating the entire reconstructed sequence uniformly. ETSformer's reconstruction is **compositional** (Equation 3): level (repeated last value) + Linear(sum of all layer growth + seasonal representations), making anomaly scores interpretable by component contribution. **New Implementation Required**: Implement compositional reconstruction with separate level, growth, and seasonal pathways; modify anomaly scoring to potentially leverage component-wise errors (e.g., seasonal vs. trend anomalies); add Linear projection layer for growth+seasonal aggregation before final composition."
  },
  {
    "source": "Informer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the encoder-decoder paradigm for time-series forecasting. From the source implementation, the basic encoder-decoder structure (Encoder/Decoder classes, layer stacking mechanism) can be directly reused. The ETSformer can leverage Informer's modular design where encoders process lookback windows and decoders generate forecasts.\n\n2. **Embedding and Input Processing**: Both use input embedding modules to map raw signals to latent space. Informer's DataEmbedding class (combining value embedding, positional encoding, temporal encoding) provides a strong foundation. ETSformer's Conv-based input embedding (kernel_size=3) can reuse the embedding infrastructure, only replacing the embedding mechanism from linear projection to 1D convolution.\n\n3. **Multi-Layer Stacking with Residual Connections**: Both architectures stack multiple encoder layers to progressively extract features. Informer's EncoderLayer with residual connections and layer normalization can be adapted. ETSformer's residual learning approach (Z := Z - S, Z := Z - B) follows similar principles, so the layer stacking logic and normalization modules are reusable.\n\n4. **Feed-Forward Networks**: Both use position-wise feed-forward networks after attention mechanisms. Informer's FFN implementation (Conv1d layers with activation) can be directly reused in ETSformer's encoder layers, though ETSformer uses sigmoid activation instead of ELU/GELU.\n\n5. **Projection to Output Space**: Both project latent representations to observation space for final predictions. Informer's projection layer (nn.Linear) can be reused for ETSformer's Linear() operations that map from latent dimension d to observation dimension m.\n\n6. **Training Infrastructure**: Both use MSE loss for forecasting tasks. The training loop, data loading, and evaluation metrics infrastructure from Informer's implementation can be directly reused for ETSformer.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Informer uses ProbSparse self-attention (content-based, O(L log L) via query sparsity measurement and Top-u selection), while ETSformer introduces two completely new attention mechanisms:\n   - **Exponential Smoothing Attention (ESA)**: Non-adaptive, time-lag-based attention with exponential decay (α(1-α)^j weighting). Requires implementing Algorithm 1 with FFT-based cross-correlation for O(L log L) complexity.\n   - **Frequency Attention (FA)**: Non-learnable, frequency-domain attention using DFT to select Top-K dominant frequencies. Requires implementing DFT/IDFT operations and amplitude-based frequency selection (Equation 4).\n   Neither mechanism exists in Informer - both need complete new implementation.\n\n2. **Component Decomposition Philosophy - NEW ARCHITECTURE**: Informer focuses on efficient attention for long sequences without explicit decomposition, while ETSformer explicitly decomposes signals into Level (E), Growth (B), and Seasonal (S) components inspired by classical exponential smoothing:\n   - **Level Module**: Implements level smoothing equation with learnable α parameter (similar to Equation 1 in classical ES). Needs new implementation of the recurrent level update mechanism.\n   - **Growth Module (MH-ESA)**: Extracts growth via successive differences with multi-head ESA. Requires implementing the differencing operation and ESA mechanism.\n   - **Seasonal Module (FA)**: Extracts seasonality via frequency-domain analysis. Requires implementing FA mechanism.\n   This tripartite decomposition is entirely new and not present in Informer.\n\n3. **Decoder Design - NEW GENERATIVE STRATEGY**: \n   - Informer: Uses masked self-attention with start tokens (generative inference, feeding X_token + X_0 placeholder)\n   - ETSformer: Uses Growth+Seasonal (G+S) Stacks with Growth Damping (TD) and Level Stack:\n     * **Growth Damping (TD)**: Implements trend damping with learnable γ parameter (∑γ^i * B_t). Needs new implementation.\n     * **Level Repetition**: Repeats last level across forecast horizon. Simple but different from Informer's approach.\n     * **Component Composition**: Final forecast = E + Linear(∑(B + S)) across N stacks. Requires implementing the additive composition mechanism.\n   The entire decoder logic needs to be redesigned.\n\n4. **Distilling Operation vs. Component Extraction - DIFFERENT PROCESSING**:\n   - Informer: Uses self-attention distilling with Conv1d + MaxPool for memory efficiency (halving sequence length progressively)\n   - ETSformer: Uses component extraction and residual updates (Z := Z - S, Z := Z - B) at each layer\n   ETSformer does NOT need Informer's distilling/downsampling mechanism - this should be removed.\n\n5. **Input Requirements - SIMPLIFIED**:\n   - Informer: Relies on time features (x_mark_enc, x_mark_dec) including month, day-of-week, hour covariates\n   - ETSformer: Does NOT use manual time-dependent covariates - FA automatically discovers seasonal patterns\n   The data preprocessing pipeline needs modification to remove covariate engineering.\n\n6. **Complexity Source - DIFFERENT EFFICIENCY MECHANISMS**:\n   - Informer: Achieves O(L log L) via ProbSparse attention (query sampling and Top-u selection)\n   - ETSformer: Achieves O(L log L) via FFT-based ESA (cross-correlation) and DFT-based FA\n   Both are efficient but use completely different mathematical foundations requiring separate implementations.\n\n7. **Interpretability Focus - NEW DESIGN PRINCIPLE**:\n   - Informer: Black-box attention weights, not inherently interpretable\n   - ETSformer: Explicit level/growth/seasonal decomposition provides human-interpretable forecasts\n   This requires implementing visualization and analysis tools for the three components."
  },
  {
    "source": "Reformer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanism Design Philosophy**: Both papers address the computational complexity of standard Transformer attention. Reformer introduces LSH attention to reduce O(L²) complexity to O(L log L), while ETSformer's Frequency Attention (FA) also achieves O(L log L) complexity through FFT operations. The source paper's implementation of efficient attention computation patterns (avoiding full materialization of attention matrices, chunked processing) provides a valuable template for implementing ETSformer's efficient FA mechanism.\n\n2. **Multi-Head Attention Architecture**: Both models employ multi-head attention mechanisms. Reformer's multi-head LSH attention implementation with dimension splitting, parallel processing, and concatenation can be directly adapted for ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA). The source code's handling of d_model, n_heads, d_keys, and d_values parameters provides reusable infrastructure.\n\n3. **Layer Normalization and Residual Connections**: Both architectures use LayerNorm and residual connections extensively. Reformer's EncoderLayer structure with attention + feedforward + residual connections mirrors ETSformer's encoder design where seasonal/growth components are extracted and residuals are updated. The source implementation's EncoderLayer class can serve as a structural template.\n\n4. **Sequence Length Handling**: Reformer's fit_length() function that pads sequences to multiples of bucket_size demonstrates practical sequence length adjustment. ETSformer similarly needs to handle variable-length inputs for its lookback window (L) and forecast horizon (H), making this padding logic reusable.\n\n5. **Embedding Layer Design**: Both use embedding layers to project raw inputs to latent space. Reformer's DataEmbedding with temporal encoding can be adapted for ETSformer's Conv-based input embedding, though ETSformer explicitly avoids time-dependent covariates.",
    "differences": "1. **Core Task Objective**: Reformer is designed for general sequence modeling (language modeling, text generation) with causal masking for autoregressive generation, while ETSformer is specifically engineered for time-series forecasting with explicit encoder-decoder architecture for multi-step ahead prediction. ETSformer requires NEW implementation of: (a) Separate encoder-decoder structure, (b) Forecast horizon handling (H-step ahead), (c) Level-Growth-Seasonal decomposition framework.\n\n2. **Attention Mechanism Innovation**: Reformer uses content-based LSH attention (hashing queries/keys based on their vector representations with random projections) to find nearest neighbors, while ETSformer introduces TWO novel attention types: (a) **Exponential Smoothing Attention (ESA)**: Non-adaptive, time-lag-based attention with learnable α parameter and exponential decay (Equation: A_ES(V)_t = α*V_t + (1-α)*A_ES(V)_{t-1}), requiring NEW implementation of the efficient cross-correlation-based Algorithm 1 with FFT. (b) **Frequency Attention (FA)**: Non-learnable, frequency-domain attention using DFT to select top-K dominant frequencies (Equation 4), requiring NEW implementation of: DFT/inverse DFT operations, amplitude-based frequency selection, phase/amplitude extraction, and seasonal pattern reconstruction.\n\n3. **Component Extraction Philosophy**: Reformer extracts generic latent representations through attention, while ETSformer explicitly decomposes time series into interpretable components. NEW implementations needed: (a) **Level Module**: Exponential smoothing-based level extraction (E_t^(n) = α*(E_t^(n-1) - Linear(S_t^(n))) + (1-α)*(E_{t-1}^(n) + Linear(B_{t-1}^(n)))), (b) **Growth Module**: Successive differencing of residuals with MH-ESA (B = MH-A_ES(Z_diff)), (c) **Seasonal Module**: FA-based seasonal pattern extraction and extrapolation, (d) **Growth Damping**: Trend damping mechanism (TD(B_t)_j = Σγ^i*B_t) with learnable damping parameter γ.\n\n4. **Decoder Architecture**: Reformer uses standard autoregressive decoder, while ETSformer requires NEW decoder implementation with: (a) **Growth + Seasonal (G+S) Stacks**: N stacks processing growth damping and frequency attention, (b) **Level Stack**: Repeating final level across forecast horizon, (c) **Additive Composition**: Final forecast as E_{t:t+H} + Linear(Σ(B_{t:t+H}^(n) + S_{t:t+H}^(n))), ensuring interpretability.\n\n5. **Training Objective and Loss**: Reformer uses cross-entropy loss for next-token prediction (language modeling), while ETSformer requires NEW implementation of: (a) Multi-step forecasting loss (MSE/MAE over forecast horizon H), (b) Handling of lookback window (L) vs forecast horizon (H) split, (c) No masking for future positions in decoder (since all H steps are predicted jointly), (d) Potential auxiliary losses for component regularization.\n\n6. **Reversibility and Memory Optimization**: Reformer emphasizes reversible layers (RevNet) and chunking for memory efficiency in long sequences, while ETSformer does not employ reversibility. The reversible architecture components from Reformer are NOT directly applicable to ETSformer's decomposition-based design.\n\n7. **Positional Information**: Reformer relies on learned positional encodings and causal masking, while ETSformer's ESA inherently encodes temporal order through exponential decay weights, and FA captures periodic patterns through frequency domain analysis, reducing dependency on explicit positional encodings (though Conv embedding provides some local temporal context)."
  },
  {
    "source": "Autoformer_2021",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. Reusable: Encoder, Decoder, EncoderLayer, DecoderLayer, series_decomp, my_Layernorm from `layers/Autoformer_EncDec.py` - directly reusable.\n\n2. Reusable: AutoCorrelationLayer wrapper from `layers/AutoCorrelation.py` - FEDformer's FourierBlock MUST be wrapped with AutoCorrelationLayer to provide standard multi-head attention interface.\n\n3. Reusable: DataEmbedding from `layers/Embed.py` - both use identical embedding layers.\n\n4. Reusable: anomaly_detection method structure: embedding → encoder → projection, only replace attention mechanism.\n\n5. Reusable: Post-layer decomposition pattern: Attention → Decomposition → FeedForward → Decomposition.",
    "differences": "1. NEW: FourierBlock - Implement frequency-domain attention with key points: (a) Use `compl_mul1d` function with `einsum` for vectorized complex operations, **NEVER use Python for-loops over modes**; (b) Weight shape `[n_heads, in_channels//n_heads, out_channels//n_heads, len(index)]`; (c) Use `get_frequency_modes(seq_len, modes, mode_select_method)` to pre-select frequency indices.\n\n2. NEW: FourierCrossAttention - Frequency-domain cross-attention using `einsum('bhex,bhey->bhxy', xq_ft_, xk_ft_)` for attention scores, activation options: 'tanh' or 'softmax'.\n\n3. **Critical Implementation Rules**: (a) Complex multiplication must use `torch.complex()` and `einsum`, never loops; (b) Use indexed assignment pattern `out_ft[:, :, :, j] = xqkvw[:, :, :, i]`; (c) Normalize by dividing `in_channels * out_channels` after irfft.\n\n4. For anomaly detection: Simplify to encoder-only architecture, `anomaly_detection(x_enc)` returns `self.projection(self.encoder(self.enc_embedding(x_enc, None))[0])`.\n\n5. **Framework Component Reuse Priority**: Replace AutoCorrelation with `AutoCorrelationLayer(FourierBlock(...), d_model, n_heads)` in EncoderLayer, keep other structures (conv1, conv2, decomp) unchanged."
  },
  {
    "source": "Informer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. Reusable: DataEmbedding from `layers/Embed.py` for input embedding.\n\n2. Reusable: Encoder, EncoderLayer, series_decomp from `layers/Autoformer_EncDec.py` - FEDformer should reuse these component frameworks.\n\n3. Reusable: MSE loss function and training loop structure.\n\n4. Reusable: Multi-head attention Q/K/V projection and output projection patterns.\n\n5. Reusable: anomaly_detection method signature and return format [B, L, D].",
    "differences": "1. NEW: FourierBlock replaces ProbSparse attention - Key implementation: (a) `get_frequency_modes(seq_len, modes, 'random')` to get frequency indices; (b) `compl_mul1d(order, x, weights)` function for complex multiplication using `einsum` instead of loops; (c) Weight shape `[n_heads, in//n_heads, out//n_heads, len(index)]`.\n\n2. NEW: series_decomp integration - Import from `layers/Autoformer_EncDec` and use in EncoderLayer, placed after attention and feedforward.\n\n3. **Complex Multiplication Standard**: ```python\ndef compl_mul1d(order, x, weights):\n    x = torch.complex(x, torch.zeros_like(x)) if not torch.is_complex(x) else x\n    weights = torch.complex(weights, torch.zeros_like(weights)) if not torch.is_complex(weights) else weights\n    return torch.complex(\n        einsum(order, x.real, weights.real) - einsum(order, x.imag, weights.imag),\n        einsum(order, x.real, weights.imag) + einsum(order, x.imag, weights.real)\n    )\n```\n\n4. NEW: AutoCorrelationLayer wrapper - FourierBlock MUST be wrapped with `AutoCorrelationLayer(FourierBlock(...), d_model, n_heads)`, import from `layers/AutoCorrelation`.\n\n5. **Anomaly Detection Simplification**: For anomaly_detection task, use encoder + projection only, no decoder needed: `enc_out = self.encoder(self.enc_embedding(x_enc, None))[0]; return self.projection(enc_out)`."
  },
  {
    "source": "Reformer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. Reusable: Multi-layer Encoder stacking structure with residual connections.\n\n2. Reusable: DataEmbedding from `layers/Embed.py`.\n\n3. Reusable: FeedForward network pattern (Conv1d-ReLU-Conv1d).\n\n4. Reusable: LayerNorm and Dropout configurations.\n\n5. Reusable: Attention mechanism wrapper pattern - wrap custom attention into standard interface.",
    "differences": "1. NEW: FourierBlock implementation - **NEVER use for-loops over modes or batch**, must use vectorized operations:\n```python\nout_ft = torch.zeros(B, H, E, L//2+1, device=x.device, dtype=torch.cfloat)\nfor wi, i in enumerate(self.index):  # only iterate over pre-selected few modes\n    out_ft[:, :, :, wi] = compl_mul1d('bhi,hio->bho', x_ft[:, :, :, i], weights[:, :, :, wi])\n```\n\n2. NEW: series_decomp integration - Import `series_decomp` class from `layers/Autoformer_EncDec.py`, call `self.decomp(x)` in EncoderLayer returning `(seasonal, trend)`.\n\n3. **Framework Component Reuse Priority**: (a) MUST import Encoder, EncoderLayer from `layers/Autoformer_EncDec.py`; (b) MUST import AutoCorrelationLayer from `layers/AutoCorrelation.py` to wrap FourierBlock; (c) MUST import DataEmbedding from `layers/Embed.py`.\n\n4. NEW: Frequency mode selection - `get_frequency_modes(seq_len, modes=32, mode_select_method='random')` returns pre-selected frequency index list, determined once in __init__.\n\n5. **Anomaly Detection Adaptation**: Simplify to encoder-only architecture, `anomaly_detection(x_enc)` method: `enc_out = self.enc_embedding(x_enc, None); enc_out, _ = self.encoder(enc_out); return self.projection(enc_out)`. Do NOT implement full decoder structure."
  },
  {
    "source": "Informer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "Informer_2020",
    "type": "in-domain",
    "similarities": "1. Both employ encoder-decoder Transformer architectures for long sequence processing, sharing the fundamental multi-head attention mechanism with Q, K, V projections. Reformer's encoder structure with LayerNorm and residual connections can be directly reused as the base framework.\n\n2. Both address the O(L²) complexity problem of standard attention through query-key selection strategies, though with different mechanisms. The attention computation flow (softmax normalization, value aggregation) remains identical, allowing Informer to leverage Reformer's efficient attention infrastructure.\n\n3. Both utilize positional encoding and embedding layers to inject temporal information into input sequences. Reformer's DataEmbedding implementation (combining value embedding with temporal encoding) can be directly adapted for Informer's time-series forecasting task.\n\n4. Both apply multi-layer encoder stacks with feed-forward networks and dropout regularization for feature extraction. Reformer's EncoderLayer structure (attention + FFN + normalization) provides a reusable template for Informer's encoder implementation.\n\n5. Both require length-fitting mechanisms to handle variable sequence lengths efficiently. Reformer's padding strategy for bucket alignment can inform Informer's approach to managing input/output sequence length mismatches in forecasting scenarios.",
    "differences": "1. Core attention mechanism differs fundamentally: Reformer uses LSH-based hashing to cluster similar queries/keys into buckets (O(L log L)), while Informer employs ProbSparse attention with KL-divergence-based query sparsity measurement (selecting top-u dominant queries). NEW: Implement sparsity measurement M(q_i, K) = LSE - mean and Top-u query selection logic.\n\n2. Encoder architecture diverges significantly: Reformer uses reversible layers for memory efficiency without feature reduction, while Informer introduces self-attention distilling with Conv1d and MaxPooling to progressively halve sequence length across layers. NEW: Implement distilling operation with 1D convolution and multi-scale pyramid stacking.\n\n3. Decoder strategies are fundamentally different: Reformer maintains standard autoregressive decoding, while Informer uses generative inference with start tokens (X_token) and zero-padded placeholders (X_0) for one-shot prediction. NEW: Implement generative decoder feeding mechanism with Concat(X_token, X_0) and non-autoregressive output generation.\n\n4. Query-key relationship handling differs: Reformer enforces shared-QK (Q=K normalized) for LSH locality, while Informer maintains separate Q and K projections for sparsity measurement. NEW: Modify attention to compute separate Q, K with max-mean measurement for query selection instead of hash bucketing.\n\n5. Objective and evaluation differ: Reformer targets language modeling with next-token prediction, while Informer focuses on LSTF with MSE loss on multi-step forecasting horizons (24/48/168/336/720 steps). NEW: Implement multi-horizon forecasting loss, start token sampling strategy, and time-series specific evaluation metrics (MSE/MAE over prediction windows)."
  },
  {
    "source": "PatchTST_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Only Transformer Architecture**: Both papers utilize encoder-only Transformer architectures without decoder components, focusing on representation learning rather than generative forecasting. The source paper's Transformer block implementation (layer normalization, feed-forward networks, self-attention) can be directly reused with dimension adjustments.\n\n2. **Linear Projection for Prediction**: Both employ simple linear layers (MLP) for final predictions rather than complex decoder structures, validating that linear forecasters are competent. The source paper's projection head implementation can be adapted by changing input dimensions from patch-level to variate-level tokens.\n\n3. **Series Representation Learning**: Both emphasize learning comprehensive representations of time series through stacked Transformer blocks with FFN extracting temporal features. The source paper's FFN modules and layer stacking logic can be reused, only requiring token dimension inversion.\n\n4. **Flexible Attention Mechanisms**: Both architectures support pluggable efficient attention variants (e.g., FlashAttention) to handle scalability, enabling the same attention optimization strategies. Source paper's attention module interfaces can be directly inherited with modified token numbers.\n\n5. **Normalization for Stability**: Both apply layer normalization within Transformer blocks to improve training convergence and handle non-stationary data. The source paper's normalization implementation can be reused by changing normalization axis from temporal to variate dimension.",
    "differences": "1. **Core Innovation - Token Definition**: PatchTST treats temporal patches as tokens (time-centric), while iTransformer inverts this by treating entire variate series as tokens (variate-centric). Implementation requires NEW token embedding logic: instead of segmenting time series into patches, embed each complete variate series independently using per-variate MLPs.\n\n2. **Attention Semantic Meaning**: PatchTST's attention captures temporal dependencies between time patches, while iTransformer's attention reveals multivariate correlations between variates. NEW implementation needed: attention score interpretation changes from temporal relationships to variate-wise correlation matrices, requiring different visualization and analysis tools.\n\n3. **Layer Normalization Axis**: PatchTST normalizes across temporal patch dimensions, while iTransformer normalizes across each variate's series representation (Equation 2). NEW implementation: change normalization axis from temporal features to individual variate tokens, computing mean/variance along the feature dimension of each variate independently.\n\n4. **Position Embedding Strategy**: PatchTST uses explicit position embeddings for temporal ordering of patches, while iTransformer eliminates position embeddings entirely, storing temporal order implicitly in FFN neuron permutations. NEW implementation: remove position embedding modules and ensure FFN architecture can encode temporal patterns through its weight structure.\n\n5. **Input Flexibility and Generalization**: iTransformer allows variable numbers of input variates between training and inference, enabling training on arbitrary variate subsets. NEW implementation: design variable-length token handling mechanisms and test generalization to unseen variates, which PatchTST's fixed temporal structure doesn't support.\n\n6. **Feed-Forward Network Role**: In PatchTST, FFN processes patch-level temporal features; in iTransformer, FFN acts as universal series representation learner extracting intrinsic properties (amplitude, periodicity, frequency). NEW implementation: design FFN to learn shared temporal primitives applicable across all variates, requiring different initialization and regularization strategies.\n\n7. **Embedding Strategy**: PatchTST embeds temporal patches using convolutional or linear layers on time windows; iTransformer embeds entire lookback series per variate using MLPs (Embedding: ℝ^T → ℝ^D). NEW implementation: replace patch-based embedding with per-variate series embedding that processes full temporal sequences independently."
  },
  {
    "source": "Crossformer_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-based Reconstruction Architecture**: Both employ encoder-only Transformer architectures for anomaly detection via reconstruction. Crossformer's encoder with segment embeddings and iTransformer's variate token encoder can share the same reconstruction loss framework (MSE/MAE between input and reconstructed output).\n\n2. **Layer Normalization for Distribution Alignment**: Both apply LayerNorm to handle distribution heterogeneity across dimensions. Crossformer normalizes segment representations while iTransformer normalizes variate tokens; the same nn.LayerNorm module can be reused with adjusted application dimensions.\n\n3. **Multi-Head Self-Attention Mechanism**: Both leverage multi-head self-attention (MSA) for dependency modeling. Crossformer's AttentionLayer and iTransformer's self-attention share identical Q/K/V projection structures; the AttentionLayer implementation with FullAttention can be directly reused.\n\n4. **Feed-Forward Network Design**: Both use identical two-layer MLP structures (Linear→GELU→Linear) for representation transformation. Crossformer's MLP1/MLP2 modules in TSA layers can be directly transplanted to iTransformer's FFN blocks without modification.\n\n5. **Anomaly Score via Reconstruction Error**: Both detect anomalies by measuring reconstruction discrepancies. The anomaly scoring logic (point-wise MSE/MAE computation and threshold-based detection) from Crossformer's anomaly_detection method can be adapted for iTransformer's output.",
    "differences": "1. **Core Embedding Strategy**: Crossformer embeds temporal segments (DSW embedding with segment length L_seg) creating 2D arrays [batch, dimension, segment_num, d_model]. iTransformer embeds entire variate series as tokens [batch, variate_num, d_model], requiring NEW embedding layers that process full time series via MLP instead of patch-based linear projections.\n\n2. **Attention Application Dimension**: Crossformer applies Two-Stage Attention (cross-time then cross-dimension) on segment tokens with router mechanisms for dimension interaction. iTransformer applies self-attention directly on variate tokens for multivariate correlation, requiring NEW attention logic that operates on inverted dimensions without temporal segmentation or routers.\n\n3. **Hierarchical vs. Flat Architecture**: Crossformer uses hierarchical encoder-decoder with segment merging across scales and multi-scale prediction fusion. iTransformer uses flat encoder-only structure with single-scale processing, requiring REMOVAL of decoder layers, segment merging modules, and multi-scale aggregation logic.\n\n4. **Position Encoding Approach**: Crossformer uses learnable position embeddings for segment positions [segment_idx, dimension_idx]. iTransformer eliminates position embeddings entirely, relying on neuron permutation in FFN to implicitly encode temporal order, requiring DELETION of all position embedding parameters.\n\n5. **Token-to-Series Projection**: Crossformer projects segment tokens back to time series via linear layers per scale then sums predictions. iTransformer projects variate token representations directly to future series via MLP [d_model→pred_len], requiring NEW projection heads that generate full prediction horizons from single token representations without multi-scale fusion."
  },
  {
    "source": "TiDE_2023",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **MLP-based representation learning**: Both papers leverage multi-layer perceptrons for extracting series representations. iTransformer's feed-forward networks (FFN) and embedding/projection layers can reuse TiDE's MLP implementation patterns, particularly for temporal feature extraction from lookback windows.\n\n2. **Encoder-only architecture for forecasting**: Both adopt encoder-only structures rather than encoder-decoder frameworks, simplifying the generation process. iTransformer's linear projection for future series prediction aligns with TiDE's direct forecasting approach, enabling code reuse for the prediction head implementation.\n\n3. **Layer normalization for stability**: Both employ layer normalization to handle non-stationarity and improve training convergence. The normalization modules from TiDE can be directly adapted, though iTransformer applies it on variate dimensions rather than temporal dimensions.\n\n4. **Lookback window processing**: Both process historical observations of fixed length T to predict future S steps. The data loading, windowing, and batching utilities from TiDE's implementation can be reused with minimal modifications for iTransformer's input pipeline.\n\n5. **Linear layers for temporal mapping**: Both rely heavily on linear transformations for temporal feature processing. TiDE's efficient linear layer implementations (Embedding: R^T→R^D, Projection: R^D→R^S) can serve as templates for iTransformer's corresponding components.",
    "differences": "1. **Inverted tokenization strategy**: iTransformer treats each variate's entire time series as a token (N tokens of dimension D), while TiDE likely processes time steps as tokens. This requires implementing new embedding logic that transposes input from (T×N) to (N×D) where each variate series is independently embedded.\n\n2. **Self-attention for multivariate correlation**: iTransformer introduces self-attention mechanisms applied on variate tokens to capture multivariate dependencies through attention score maps A∈R^(N×N). This requires implementing multi-head attention modules (Q, K, V projections, scaled dot-product attention) that operate on the variate dimension, which TiDE likely doesn't include.\n\n3. **Variate-wise normalization**: iTransformer applies layer normalization across the feature dimension of each individual variate token (Equation 2), normalizing series representations rather than cross-variate features. This differs from typical temporal normalization and requires modifying normalization axis specifications in the implementation.\n\n4. **Stacked Transformer blocks**: iTransformer uses L stacked blocks combining attention and FFN with residual connections (Equation 1), requiring implementation of the TrmBlock module that orchestrates attention→normalization→FFN sequences. TiDE may use simpler sequential MLP stacks without attention mechanisms.\n\n5. **No positional encoding**: iTransformer explicitly removes positional embeddings, relying on FFN neuron permutation to implicitly encode temporal order. This simplifies implementation but requires careful FFN design to ensure temporal information preservation, contrasting with TiDE's potential use of explicit temporal encodings.\n\n6. **Attention interpretability for correlations**: iTransformer's pre-Softmax attention scores reveal multivariate correlations through normalized query-key products (A_ij ∝ q_i^T k_j). Implementing visualization tools for attention maps requires new analysis modules to extract and display N×N correlation matrices during inference.\n\n7. **Flexible variate handling**: iTransformer supports varying token numbers between training and inference, enabling generalization to unseen variates. This requires implementing dynamic attention mechanisms and ensuring embedding/projection layers can handle variable N, which may not be present in TiDE's fixed-dimension design."
  },
  {
    "source": "TimesNet_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based Anomaly Detection**: Both employ reconstruction approaches where models learn normal patterns from historical data and detect anomalies via reconstruction error. TimesNet's `anomaly_detection` method with projection layer can guide iTransformer's similar reconstruction pipeline implementation.\n\n2. **Instance Normalization Strategy**: Both apply normalization per sample (means/stdev calculation per instance) for handling non-stationary data. TimesNet's normalization code (subtract means, divide by stdev) in `anomaly_detection` is directly reusable for iTransformer's variate-wise normalization.\n\n3. **Embedding Layer Architecture**: Both use `DataEmbedding` to transform raw inputs into latent representations before processing. The embedding initialization and forward pass logic from TimesNet can be adapted for iTransformer's variate token embedding.\n\n4. **Residual Connection Pattern**: Both architectures stack multiple processing blocks with residual connections for gradient flow. TimesNet's `res = res + x` pattern provides implementation reference for iTransformer's layer-wise residual structure.\n\n5. **Linear Projection for Output**: Both use simple linear layers (`nn.Linear`) to project learned representations back to original space. TimesNet's `self.projection` implementation directly transfers to iTransformer's projection component for series forecasting.",
    "differences": "1. **Core Architectural Paradigm**: TimesNet transforms 1D time series into 2D tensors via FFT-based period detection and uses 2D convolutions (Inception blocks) for spatial-temporal modeling. iTransformer inverts the token definition—treating entire variate series as tokens rather than time points—requiring NEW implementation of variate-centric attention mechanisms and series-level embeddings.\n\n2. **Attention Mechanism Application**: TimesNet uses NO attention mechanisms, relying purely on convolutional operations for feature extraction. iTransformer requires NEW implementation of self-attention applied across variate tokens (not temporal tokens), calculating correlation maps between different variates' series representations with query-key-value projections.\n\n3. **Temporal Dependency Modeling**: TimesNet explicitly models multi-periodicity through FFT analysis (`FFT_for_Period`) and adaptive period-based 2D reshaping with weighted aggregation. iTransformer requires NEW feed-forward networks that process entire series representations per variate, encoding temporal patterns implicitly through neuron permutations without explicit period detection.\n\n4. **Layer Normalization Scope**: TimesNet applies `LayerNorm` across the feature dimension of temporal tokens (shape `[B, T, D]`). iTransformer needs NEW implementation of LayerNorm applied along the series representation dimension of each variate token independently (normalizing each `h_n` vector), reducing cross-variate interference.\n\n5. **Token Definition and Processing Flow**: TimesNet processes temporal tokens `[B, T, C]` where T time steps are tokens. iTransformer requires COMPLETE REDESIGN of data flow—transposing input to `[B, N, T]`, embedding each of N variates as independent tokens, and implementing variate-wise attention and projection, fundamentally inverting the processing dimension from temporal to variate-centric."
  },
  {
    "source": "DLinear_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. Both adopt Direct Multi-Step (DMS) forecasting strategy for time series prediction, avoiding autoregressive error accumulation. DLinear's encoder architecture can serve as baseline for iTransformer's projection layer implementation.\n\n2. Both apply series decomposition preprocessing to extract trend and seasonal components, enhancing predictability. iTransformer can directly reuse DLinear's `series_decomp` module from Autoformer for initial data preprocessing.\n\n3. Both use linear layers for final prediction generation, acknowledging their effectiveness for time series forecasting. iTransformer's Projection MLP can leverage DLinear's weight initialization strategy: `(1/seq_len) * torch.ones([pred_len, seq_len])`.\n\n4. Both support individual modeling per variate with channel independence option, avoiding spurious correlations. DLinear's `individual` flag and `nn.ModuleList` implementation pattern can be adapted for iTransformer's per-variate token processing.\n\n5. Both employ layer normalization for handling distribution shifts and non-stationarity in time series. DLinear's normalization approach (subtraction in NLinear variant) aligns with iTransformer's LayerNorm applied to variate tokens.",
    "differences": "1. Core architecture inverts token representation: DLinear operates on temporal dimension (time steps as features), while iTransformer treats entire variate series as tokens. Requires implementing variate-wise embedding MLP (`Embedding: R^T -> R^D`) instead of temporal linear layers.\n\n2. Attention mechanism application differs fundamentally: DLinear has no attention, while iTransformer applies self-attention across variate tokens to capture multivariate correlations. Must implement multi-head self-attention with queries, keys, values projected from variate representations (`Q, K, V ∈ R^(N×d_k)`).\n\n3. Feature extraction strategy diverges: DLinear uses shared temporal linear weights, iTransformer uses stacked feed-forward networks (FFN) on series representations. Need to implement MLP-based FFN blocks that process each variate token's temporal representation independently.\n\n4. Normalization targets differ: DLinear normalizes across time (subtracting last value), iTransformer normalizes across feature dimension of each variate token. Requires implementing LayerNorm as `(h_n - Mean(h_n)) / sqrt(Var(h_n))` for each token separately.\n\n5. Model depth and representation learning: DLinear is shallow (1-2 layers), iTransformer stacks L Transformer blocks for deep representation learning. Must implement iterative block structure with residual connections: `H^(l+1) = TrmBlock(H^l)` for l=0 to L-1.\n\n6. Position encoding handling: DLinear implicitly preserves order through temporal weights, iTransformer removes explicit positional encoding as order is stored in FFN neuron permutations. No position embedding implementation needed, simplifying token processing.\n\n7. Scalability consideration: DLinear complexity is O(T×C) for channels, iTransformer attention is O(N²) for variates. When implementing iTransformer, need to consider efficient attention mechanisms (FlashAttention, sparse attention) if variate count N is large.\n\n8. Interpretability mechanism: DLinear's weights directly show temporal importance, iTransformer's pre-Softmax attention scores `A_ij ∝ q_i^T k_j` reveal multivariate correlations. Must implement attention score extraction and visualization for correlation analysis between variate pairs.\n\n9. Generalization capability: DLinear trains on fixed channel count, iTransformer's attention allows variable token numbers at inference. Implementation should support flexible variate numbers by designing attention to handle arbitrary N tokens beyond training configuration.\n\n10. Training objective: DLinear minimizes reconstruction loss on decomposed components separately, iTransformer jointly optimizes all variate predictions through shared Transformer blocks. Need unified loss computation across all N variate tokens: `Loss = Σ_n ||Y_:,n - Ŷ_:,n||²`."
  },
  {
    "source": "TimesNet_2022",
    "target": "KANAD_2024",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based anomaly detection approach**: Both models employ reconstruction-based strategies where normal patterns are learned and deviations indicate anomalies. TimesNet's `anomaly_detection()` method with normalization and projection can serve as a template for KANAD's reconstruction pipeline, particularly the normalization strategy (mean subtraction and standard deviation division) which KANAD also requires.\n\n2. **Embedding and normalization preprocessing**: Both papers apply instance normalization (Non-stationary Transformer style) before model processing to handle non-stationarity. TimesNet's normalization code (`means = x_enc.mean(1, keepdim=True).detach()` and subsequent operations) can be directly reused in KANAD's preprocessing stage, as KANAD mentions renormalization after differencing.\n\n3. **Residual connection architecture**: Both models utilize residual connections for training stability. TimesNet's `res = res + x` pattern in TimesBlock and KANAD's `H^(L)' = H^(L) + H^(0)` (Equation 10) share identical design philosophy. The residual implementation from TimesNet can guide KANAD's residual connection between hidden states.\n\n4. **Convolutional neural network components**: Both leverage CNNs for feature extraction—TimesNet uses 2D Inception blocks while KANAD uses 1D CNNs. The `Inception_Block_V1` structure with batch normalization and GELU activation in TimesNet provides implementation reference for KANAD's CNN blocks with similar activation patterns (Equation 9).\n\n5. **Sliding window and sequence processing**: Both process time series through windowed segments with fixed lengths. TimesNet's `seq_len` and `pred_len` handling, including padding mechanisms (`torch.zeros` padding and truncation), can be adapted for KANAD's sliding window approach with window size `T` and prediction of `x_{i+1}`.",
    "differences": "1. **Core temporal modeling mechanism**: TimesNet transforms 1D series into 2D tensors via FFT-based period discovery and reshape operations (Equations 1-3), capturing intraperiod and interperiod variations. KANAD introduces Function Deconstruction (FD) using Fourier series expansion (Equation 3) with learnable coefficients for sine/cosine univariate functions. NEW: Implement Fourier basis function generation (`Stack(cos(nx), sin(nx))`) and coefficient learning via 1D convolution.\n\n2. **Pattern representation and aggregation**: TimesNet adaptively aggregates multiple period-based 2D representations using softmax-normalized FFT amplitudes as weights (Equation 6). KANAD reduces univariate function matrices through stacked 1D CNNs with kernel size 3, learning coefficients to reconstruct normal patterns (Equations 7-11). NEW: Implement multi-layer 1D CNN with channel-wise convolution for coefficient estimation and dimension reduction via `DownConv`.\n\n3. **Preprocessing and detrending strategy**: TimesNet directly applies instance normalization without explicit detrending. KANAD employs first-order differencing before normalization to eliminate constant term `A_0` and residual trends (Section 3.2), ensuring the model focuses on Fourier coefficients `A_{1:n}` and `B_{1:n}`. NEW: Implement differencing operation (`x[t] - x[t-1]`) and inverse differencing for final reconstruction.\n\n4. **Periodic feature enhancement**: TimesNet discovers top-k periods via FFT and processes each period separately with shared Inception blocks. KANAD introduces Periodic-Enhanced mechanism with three complementary univariate function types: raw time variable `X`, Fourier series `S_n`, and sine-cosine waves `P_n` with window-index-based periods (Equation 5). NEW: Implement `P_n = {sin(2πni/T), cos(2πni/T)}` generators for multi-scale periodic modeling.\n\n5. **Projection and prediction architecture**: TimesNet uses multi-layer TimesBlocks followed by LayerNorm and a linear projection layer for reconstruction. KANAD employs a single-layer MLP (Equation 12) directly on the normal pattern approximation `x'_{0:i}` for next-step prediction `x_{i+1}`. NEW: Implement lightweight MLP projection (`Wx' + b`) and anomaly scoring based on prediction error between `x_{i+1}` and actual observations, replacing TimesNet's full-sequence reconstruction approach."
  },
  {
    "source": "FEDformer_2022",
    "target": "KANAD_2024",
    "type": "in-domain",
    "similarities": "1. **Frequency Domain Analysis Foundation**: Both employ frequency-domain transformations (FEDformer uses DFT/DWT, KANAD uses Fourier series decomposition) to capture temporal patterns, enabling efficient sequence modeling. FEDformer's `get_frequency_modes()` and FFT operations can guide KANAD's implementation of sine/cosine basis functions.\n\n2. **Decomposition-Based Architecture**: Both adopt decomposition strategies—FEDformer uses MOEDecomp for seasonal-trend separation, while KANAD decomposes signals into univariate Fourier components. The `series_decomp` module from FEDformer can be adapted for KANAD's constant term elimination preprocessing.\n\n3. **Efficient Linear Complexity**: Both achieve O(L) complexity through selective frequency mode processing. FEDformer's mode selection mechanism (random/low modes with `index` parameter) directly parallels KANAD's finite N-term Fourier approximation strategy.\n\n4. **Sliding Window Processing**: Both use windowed input sequences for temporal modeling. FEDformer's encoder-decoder structure with window-based embeddings (`DataEmbedding`) provides reusable components for KANAD's sliding window implementation with shape [B, T, D].\n\n5. **Residual Connections for Stability**: FEDformer employs residual connections in encoder/decoder layers (Equation 1-2), while KANAD uses residuals between H^(L) and H^(0) (Equation 10). FEDformer's residual implementation pattern can be directly reused.",
    "differences": "1. **Core Task Paradigm Shift**: FEDformer performs forecasting with encoder-decoder architecture predicting future sequences, while KANAD performs anomaly detection through reconstruction-based pattern learning. KANAD requires implementing a three-phase pipeline (mapping/reducing/projection) instead of FEDformer's attention-based forecasting framework.\n\n2. **Frequency Representation Mechanism**: FEDformer learns parameterized frequency kernels (weights1/weights2 in `FourierBlock`) through complex multiplication, whereas KANAD uses fixed Fourier basis functions (sin/cos) with learnable coefficients via 1D CNN. Must implement KANAD's univariate function matrix H construction (Equation 6) and coefficient learning through Conv layers.\n\n3. **Pattern Modeling Strategy**: FEDformer models seasonal-trend components through attention mechanisms (FEB/FEA blocks), while KANAD constructs normal patterns by weighted aggregation of univariate functions. Need to implement KANAD's reducing phase with stacked 1D CNNs (Equations 7-9) replacing FEDformer's Fourier/Wavelet attention blocks.\n\n4. **Anomaly Detection vs. Forecasting Output**: FEDformer outputs multi-step predictions through decoder projection, while KANAD outputs single-step normal pattern reconstruction for anomaly scoring. Must implement KANAD's projection phase (Equation 12) with single-layer MLP and develop anomaly score calculation based on reconstruction error.\n\n5. **Preprocessing and Normalization**: FEDformer uses standard time series embedding, while KANAD requires specific constant term elimination through first-order differencing and renormalization to handle mean fluctuations. Need to implement KANAD's differential preprocessing pipeline and periodic-enhanced components (Equation 5) with multi-periodic univariate functions."
  },
  {
    "source": "Autoformer_2021",
    "target": "KANAD_2024",
    "type": "in-domain",
    "similarities": "1. Both employ decomposition-based architectures to extract normal patterns from time series, separating trend and seasonal components for cleaner modeling. Autoformer's series_decomp block and moving average operations can guide KANAD's preprocessing pipeline for constant term elimination and trend removal through differencing.\n\n2. Both utilize reconstruction-based anomaly detection by learning normal patterns from historical windows and comparing predictions with observations. Autoformer's encoder-decoder structure for sequence reconstruction provides architectural insights for KANAD's mapping-reducing-projection pipeline design.\n\n3. Both leverage frequency domain operations for pattern discovery: Autoformer uses FFT in Auto-Correlation for period detection, while KANAD employs Fourier series decomposition. The FFT implementation in AutoCorrelation (torch.fft.rfft/irfft) can be adapted for KANAD's univariate function computation.\n\n4. Both apply normalization and embedding techniques for stable training: Autoformer's my_Layernorm and batch normalization strategies directly transfer to KANAD's BatchNorm layers after convolutions. The dropout and activation (GELU) patterns are identical.\n\n5. Both use sliding window approaches for temporal modeling with configurable window sizes (seq_len in Autoformer, T in KANAD). The data batching and windowing logic from Autoformer's forward pass can be reused for KANAD's time window processing.",
    "differences": "1. Core architecture differs fundamentally: Autoformer uses Transformer-based Auto-Correlation mechanism with multi-head attention for period-based dependencies, while KANAD employs 1D CNN with fixed Fourier basis functions for coefficient learning. KANAD requires implementing stacked Conv1d layers (kernel_size=3) with specific univariate function matrices, not present in Autoformer.\n\n2. Pattern representation mechanisms diverge completely: Autoformer discovers periods through autocorrelation and time-delay aggregation (top-k selection, rolling operations), whereas KANAD explicitly constructs normal patterns via weighted Fourier series (sin/cos terms with learned coefficients). KANAD needs new univariate function matrix construction (Equation 6) and DownConv dimension reduction (Equation 11).\n\n3. Temporal modeling complexity differs: Autoformer maintains separate encoder-decoder paths with cross-attention and progressive trend accumulation across multiple layers (N encoders, M decoders), while KANAD uses simpler three-phase pipeline (mapping→reducing→projection) with L CNN blocks and single-layer MLP. KANAD's projection phase (Equation 12) requires minimal parameters compared to Autoformer's multi-layer decoder.\n\n4. Anomaly score calculation methods are distinct: Autoformer computes reconstruction error between predicted and actual sequences across all dimensions, while KANAD calculates point-wise prediction error after differential preprocessing and inverse transformation. KANAD needs implementing first-order differencing preprocessing and inverse differencing for final anomaly scoring.\n\n5. Feature engineering approaches differ: Autoformer uses learnable embeddings (TokenEmbedding, TemporalEmbedding) for input transformation and positional encoding, while KANAD directly operates on raw time series with fixed trigonometric transformations. KANAD requires implementing periodic-enhanced univariate functions (Equation 5: X, S_n, P_n terms) and residual connections (Equation 10) specific to its CNN architecture, which Autoformer lacks."
  },
  {
    "source": "DLinear_2022",
    "target": "KANAD_2024",
    "type": "in-domain",
    "similarities": "1. Both employ decomposition-based preprocessing for time series. DLinear uses seasonal-trend decomposition via moving average kernels; KANAD applies first-order differencing to eliminate constant terms. The `series_decomp` module from DLinear's implementation can be adapted for KANAD's trend removal preprocessing.\n\n2. Both utilize reconstruction-based anomaly detection through prediction. DLinear reconstructs future sequences via linear layers on decomposed components; KANAD reconstructs normal patterns using Fourier coefficients then predicts future behavior. DLinear's encoder-decoder structure for `anomaly_detection` task provides a template for KANAD's reconstruction pipeline.\n\n3. Both use sliding window approaches for temporal modeling. DLinear processes windows of length `seq_len` to predict `pred_len`; KANAD applies sliding windows of size T for mapping-reducing-projection. The window handling logic in DLinear's forward pass can be reused for KANAD's window-based processing.\n\n4. Both employ simple linear transformations as core operations. DLinear uses `nn.Linear` layers on temporal axis; KANAD uses 1D convolutions (which are linear operations) for coefficient learning. DLinear's weight initialization strategy `(1/seq_len) * torch.ones()` can inform KANAD's convolution kernel initialization.\n\n5. Both support channel-independent and channel-shared processing modes. DLinear's `individual` parameter controls per-channel vs. shared linear layers; KANAD processes univariate functions that can be applied per-channel or globally. DLinear's `ModuleList` implementation for individual channels provides a pattern for KANAD's multi-channel handling.",
    "differences": "1. Core innovation differs fundamentally: DLinear challenges Transformer complexity with embarrassingly simple linear models for forecasting, while KANAD introduces Kolmogorov-Arnold Networks with Fourier series decomposition for anomaly detection. KANAD requires implementing entirely new function deconstruction (FD) mechanism with trigonometric univariate functions (sine/cosine basis), which has no equivalent in DLinear's linear architecture.\n\n2. Temporal pattern modeling approaches are opposite: DLinear directly learns weights on raw temporal sequences through linear regression, treating time series as numerical values. KANAD transforms time series into Fourier basis functions (mapping phase), learns coefficients via 1D CNN (reducing phase), then reconstructs normal patterns. Implementation needs: Fourier transformation module with configurable N terms, stacked 1D CNN blocks with batch normalization and GELU activation, residual connections between hidden states.\n\n3. Anomaly score calculation mechanisms differ completely: DLinear computes anomaly scores by comparing predicted vs. actual values using reconstruction error (MSE/MAE) on decomposed seasonal-trend components. KANAD calculates anomaly scores by measuring deviation between predicted normal pattern and observed behavior after Fourier coefficient-based reconstruction. KANAD requires implementing: periodic-enhanced univariate functions (Equation 5), coefficient learning via Conv operations (Equations 8-9), dimension reduction via DownConv (Equation 11).\n\n4. Architecture complexity and computational strategy diverge: DLinear uses single-layer or two-layer linear networks (73 lines total) with optional decomposition, emphasizing simplicity. KANAD employs multi-stage pipeline: mapping (Fourier basis construction), reducing (L-layer stacked CNNs with kernel size 3), projection (MLP). Implementation requires: univariate function matrix construction (Equation 6), iterative CNN application (Equation 7), batch normalization and GELU after each layer, residual connections (Equation 10).\n\n5. Preprocessing and normalization strategies differ: DLinear applies zero-mean normalization or NLinear's last-value subtraction for distribution shift handling, operating directly on raw values. KANAD employs constant term elimination through first-order differencing followed by renormalization to stabilize Fourier coefficient learning, removing trend components. Implementation needs: differencing operation before mapping phase, coefficient matrix Θ learning mechanism, integration of three univariate function types (X, Sn, Pn) as defined in Equation 5."
  },
  {
    "source": "Informer_2020",
    "target": "KANAD_2024",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based Anomaly Detection Framework**: Both models employ encoder-based architectures to reconstruct normal patterns from input sequences, then detect anomalies by comparing reconstructions with observations. Informer's encoder with self-attention distilling can be adapted as the backbone for KANAD's pattern learning, reusing the multi-layer encoding structure and residual connections.\n\n2. **Sequence Embedding and Preprocessing**: Both papers utilize temporal embedding mechanisms to encode raw time series into higher-dimensional representations before processing. KANAD can directly reuse Informer's DataEmbedding module (positional encoding, temporal features) for initial input transformation, requiring only modifications to handle the differencing preprocessing.\n\n3. **Convolutional Dimensionality Reduction**: Informer's self-attention distilling uses Conv1d layers with ELU activation and max-pooling for progressive feature extraction. KANAD's reducing phase employs stacked 1D CNNs with similar kernel operations, allowing direct reuse of Informer's ConvLayer implementation with adjusted kernel sizes and pooling strategies.\n\n4. **Training Infrastructure and Loss Functions**: Both models optimize MSE loss for reconstruction accuracy and share common training components. Informer's training loop, data loading pipelines, normalization utilities, and evaluation metrics (MSE/MAE) can be directly transferred to KANAD's implementation with minimal modifications.\n\n5. **Batch Normalization and Activation Patterns**: Both architectures incorporate batch normalization for training stability and use GELU/ELU activations. Informer's normalization layers (LayerNorm, BatchNorm1d) and activation function implementations can be directly reused in KANAD's CNN blocks and projection layers.",
    "differences": "1. **Core Pattern Modeling Mechanism**: Informer uses ProbSparse self-attention with query sparsity measurement to capture long-range dependencies through learned attention weights. KANAD introduces Function Deconstruction (FD) mechanism, decomposing time series into Fourier basis functions (sine/cosine) with learned coefficients, requiring NEW implementation of univariate function matrices (Equation 3-5) and coefficient learning via 1D convolutions.\n\n2. **Architecture Paradigm Shift**: Informer follows encoder-decoder Transformer architecture with multi-head attention, query-key-value projections, and generative inference for forecasting. KANAD adopts a three-phase pipeline (mapping-reducing-projection) without attention mechanisms, requiring NEW implementation of: (a) Fourier series mapping module, (b) stacked CNN-based coefficient reducer with residual connections (Equation 7-11), (c) single-layer MLP projection.\n\n3. **Temporal Pattern Representation**: Informer models temporal dependencies through learned attention distributions over historical sequences, with ProbSparse mechanism selecting Top-u dominant queries. KANAD represents normal patterns as weighted combinations of fixed trigonometric basis functions (raw time X, Fourier series Sn, periodic waves Pn), requiring NEW periodic-enhanced univariate function generation and constant term elimination via first-order differencing.\n\n4. **Anomaly Score Computation Strategy**: Informer detects anomalies by computing reconstruction errors between encoder outputs and original inputs in the anomaly_detection method. KANAD calculates anomaly scores by comparing MLP-projected future predictions against actual observations after normal pattern reconstruction, requiring NEW implementation of: (a) differencing-based preprocessing, (b) Fourier coefficient estimation, (c) future-step prediction scoring.\n\n5. **Computational Complexity and Efficiency**: Informer achieves O(L log L) complexity through ProbSparse attention with random sampling and Top-u query selection, suitable for long sequences. KANAD achieves linear complexity O(L) by replacing attention with 1D convolutions over fixed univariate functions (2N channels), requiring NEW lightweight CNN architecture with kernel size 3 and dimension-reducing DownConv operations for real-time deployment."
  },
  {
    "source": "Autoformer_2021",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based Forecasting for Anomaly Detection**: Both models perform time series forecasting and can be adapted for anomaly detection by comparing predictions with actual values. Autoformer's encoder-decoder architecture with `anomaly_detection()` method can serve as a template for LightTS's prediction-based anomaly detection implementation.\n\n2. **Embedding Layer Reusability**: Both use data embedding mechanisms to transform raw time series inputs into higher-dimensional representations. Autoformer's `DataEmbedding_wo_pos` (TokenEmbedding + TemporalEmbedding) can be partially reused or adapted for LightTS's initial feature extraction, though LightTS uses simpler linear projections.\n\n3. **Multi-step Forecasting Architecture**: Both handle input-output sequences with configurable lengths (seq_len, pred_len). The basic training loop, data loading pipeline, and loss computation (MSE/MAE) from Autoformer's implementation can be directly reused for LightTS with minimal modifications.\n\n4. **Layer Normalization and Dropout**: Both employ regularization techniques including dropout and normalization layers. Autoformer's `my_Layernorm` and dropout strategies can be adapted, though LightTS uses standard LayerNorm without the seasonal-specific bias removal.\n\n5. **Batch Processing and Model Configuration**: Both use similar configuration objects (configs) with hyperparameters like d_model, dropout, activation. Autoformer's model initialization pattern and forward pass structure provide a solid template for implementing LightTS's modular architecture.",
    "differences": "1. **Core Architecture Paradigm**: Autoformer uses Transformer-based encoder-decoder with Auto-Correlation mechanism and series decomposition blocks; LightTS uses pure MLP-based architecture with sampling-oriented IEBlocks. Must implement: (a) Continuous/Interval sampling functions transforming [B,T,N]→[B,C,T/C,N], (b) IEBlock with temporal/channel/output projections, (c) Bottleneck design with F'<<F dimension reduction.\n\n2. **Temporal Pattern Modeling Strategy**: Autoformer decomposes series into trend-cyclical and seasonal components using moving average (series_decomp block) and models them separately; LightTS captures short/long-term patterns through two parallel sampling paths without explicit decomposition. Must implement: (a) Sampling transformation logic (Eq 1-2), (b) Parallel feature extraction paths, (c) Feature concatenation and fusion strategy.\n\n3. **Dependency Modeling Mechanism**: Autoformer discovers period-based dependencies via FFT-based autocorrelation (O(LlogL) complexity) and time-delay aggregation; LightTS models variable interdependencies through channel-wise MLPs after temporal feature extraction. Must implement: (a) Weight-sharing MLPs for temporal projection (R^H→R^F'), (b) Channel projection MLPs (R^W→R^W), (c) Sequential information exchange without attention/correlation mechanisms.\n\n4. **Feature Aggregation Approach**: Autoformer uses multi-head attention-like structure with Q,K,V projections and Roll operations for sub-series aggregation; LightTS uses simple linear down-projection (R^(T/C)→R^1) followed by concatenation across all variables. Must implement: (a) Linear down-projection layers, (b) Feature concatenation logic [2F×N matrix formation], (c) Final IEBlock-C for joint temporal-channel modeling.\n\n5. **Model Complexity and Scalability**: Autoformer maintains O(LlogL) complexity through FFT but requires encoder-decoder structure with multiple decomposition blocks; LightTS achieves O(T) complexity through down-sampling (C<<T) and bottleneck design (F'<<F). Must implement: (a) Hyperparameter selection logic (C, F, F' values), (b) Efficient matrix operations for sampling, (c) Lightweight prediction head without decoder accumulation structure.\n\n6. **[CRITICAL] Sequence Length Compatibility**: LightTS sampling requires seq_len to be divisible by chunk_size(C). Must implement: (a) In __init__, set `chunk_size = min(seq_len, chunk_size)` to ensure chunk_size doesn't exceed sequence length; (b) If `seq_len % chunk_size != 0`, adjust internal seq_len in __init__: `self.seq_len += (chunk_size - seq_len % chunk_size)` for padding alignment; (c) Save original seq_len for output; (d) In forward, pad input: `x = torch.cat([x, torch.zeros(B, self.seq_len - T, N).to(x.device)], dim=1)`; (e) num_chunks must be computed based on padded seq_len. Failing to implement this mechanism causes `reshape` operation to fail due to dimension mismatch."
  },
  {
    "source": "Informer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Paradigm for Forecasting**: Both adopt encoder-decoder architectures for long sequence time-series forecasting (LSTF), processing multivariate inputs through embedding layers. Informer's DataEmbedding and encoder structure can guide LightTS's input preprocessing pipeline, particularly for handling temporal and positional encodings before feature extraction.\n\n2. **Multi-Head Feature Extraction**: Both models extract features from multiple perspectives—Informer uses multi-head ProbSparse attention, while LightTS uses parallel continuous/interval sampling paths. The multi-head projection mechanism (query/key/value in Informer) can inform LightTS's dual-path feature extraction design, where weight-sharing MLPs replace attention.\n\n3. **Hierarchical Feature Aggregation**: Informer's distilling operation (ConvLayer with max-pooling) progressively reduces sequence length, similar to LightTS's down-sampling strategy. The ConvLayer implementation (Conv1d + BatchNorm + ELU + MaxPool) provides a template for LightTS's temporal dimension reduction, though LightTS uses sampling instead of convolution.\n\n4. **Generative Inference with Start Tokens**: Both use start tokens for efficient prediction—Informer concatenates historical tokens with placeholders, LightTS concatenates features from dual sampling paths. Informer's decoder input preparation (Concat operation in Eq.6) directly guides LightTS's feature concatenation before final prediction.\n\n5. **MSE Loss for Regression**: Both optimize Mean Squared Error for forecasting tasks. Informer's loss computation and backpropagation setup can be directly reused for LightTS training, requiring only output dimension adjustments for multi-step vs. single-step forecasting scenarios.",
    "differences": "1. **Core Architecture: Attention vs. Pure MLP**: Informer relies on ProbSparse self-attention (O(L log L) complexity with query sparsity measurement M(q,K)), while LightTS uses pure MLP-based IEBlocks with bottleneck design. NEW: Implement IEBlock with temporal/channel/output projections (3-stage MLP) and bottleneck architecture (F' << F) to replace all attention mechanisms.\n\n2. **Temporal Pattern Extraction Strategy**: Informer uses quadratic dot-product attention with KL-divergence-based query selection (Eq.2), focusing on dominant query-key pairs. LightTS employs continuous sampling (Eq.1) and interval sampling (Eq.2) to explicitly separate short-term local and long-term global patterns. NEW: Implement dual sampling paths that transform sequences into non-overlapping sub-sequences without token elimination.\n\n3. **Feature Dimension Handling**: Informer maintains constant d_model throughout encoder-decoder with LayerNorm and residual connections. LightTS uses variable dimensions (C→F'→F) with bottleneck design in IEBlocks, where F'<<H,F for computational efficiency. NEW: Implement dimension transformation logic (H×W→F'×W→F×W) with weight-sharing across rows/columns in MLPs.\n\n4. **Inter-Variable Dependency Modeling**: Informer processes all variables jointly through multi-head attention across the entire sequence. LightTS treats variables independently in Part I (per-variable feature extraction), then explicitly models interdependencies in Part II via channel projection. NEW: Implement two-stage processing where Part I extracts per-variable features (2F×1 per variable), Part II concatenates to 2F×N matrix for cross-variable learning.\n\n5. **Sequence Length Reduction Mechanism**: Informer uses learnable convolution-based distilling (Conv1d kernel=3, stride=2 MaxPool) with pyramid stacking for memory efficiency O((2-ε)L log L). LightTS uses deterministic sampling (T→C×T/C) with fixed reduction ratio, avoiding learnable parameters in down-sampling. NEW: Replace Informer's ConvLayer with sampling-based dimension reduction, implementing column-wise feature extraction (MLP: R^C→R^F) after sampling.\n\n6. **Decoder Complexity**: Informer uses masked multi-head attention in decoder with cross-attention to encoder outputs, requiring separate self-attention and cross-attention modules (DecoderLayer). LightTS uses single IEBlock-C (R^(2F×N)→R^(L×N)) without masking or cross-attention. NEW: Implement lightweight IEBlock-C that directly maps concatenated features to predictions, eliminating decoder's attention mechanisms entirely.\n\n7. **Computational Bottleneck Design**: Informer reduces complexity via sparse query selection (Top-u queries where u=c·ln(L_Q)), maintaining O(L ln L) per layer. LightTS reduces complexity via bottleneck MLPs (F'<<F) and sampling (processing C-length sub-sequences instead of T-length sequences). NEW: Implement bottleneck temporal projection (R^H→R^F') before channel projection to minimize repeated MLP applications across time steps.\n\n8. **[CRITICAL] Sequence Length and Sampling Compatibility**: LightTS core sampling reshapes sequence into [C, T/C] matrix, requiring T to be divisible by C. Implementation must: (a) `chunk_size = min(seq_len, chunk_size)` to prevent chunk_size exceeding seq_len; (b) If `seq_len % chunk_size != 0`, adjust internal seq_len in __init__ to `seq_len + (chunk_size - seq_len % chunk_size)`; (c) Compute `num_chunks = adjusted_seq_len // chunk_size`; (d) Pad input in forward: `x = torch.cat([x, torch.zeros(B, adjusted_seq_len - T, N, device=x.device)], dim=1)`; (e) Sampling reshape must use adjusted_seq_len, not original seq_len. Otherwise: RuntimeError: shape invalid for input of size."
  },
  {
    "source": "Reformer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Sequential Processing Architecture**: Both employ encoder-based architectures for time series processing, stacking multiple layers with normalization. The target can reuse the basic encoder structure (EncoderLayer, layer stacking) from Reformer's implementation for building LightTS's sequential blocks.\n2. **Embedding Layer Foundation**: Both utilize DataEmbedding modules to transform raw input into higher-dimensional representations before processing. LightTS can directly adapt Reformer's enc_embedding component for initial feature extraction from multivariate time series.\n3. **Projection for Output**: Both apply final linear projections to map learned representations to desired output dimensions. The target paper can reuse Reformer's projection layer structure (nn.Linear) for mapping extracted features to forecasting predictions.\n4. **Layer Normalization**: Both incorporate normalization mechanisms within their architectures to stabilize training. LightTS can leverage Reformer's LayerNorm implementation pattern for normalizing features between IEBlocks.\n5. **Batch Processing Framework**: Both handle batched inputs with shape [B, L, D] and support flexible sequence lengths. The target can adopt Reformer's batching infrastructure and forward pass organization for efficient training.",
    "differences": "1. **Core Mechanism - Attention vs MLP**: Reformer uses LSH-based self-attention with hashing mechanisms for O(L log L) complexity, while LightTS employs pure MLP-based Information Exchange Blocks (IEBlocks) without any attention. Need to implement: temporal/channel projection MLPs, bottleneck architecture with dimension reduction (F' << F).\n2. **Temporal Pattern Extraction**: Reformer processes full sequences with locality-sensitive hashing and bucketing strategies, while LightTS uses continuous and interval sampling to downsample sequences into non-overlapping sub-sequences (C × T/C matrices). Need to implement: two sampling functions transforming [B, T, N] to [B, C, T/C, N] with different strategies.\n3. **Multi-Variable Handling**: Reformer treats multivariate series uniformly through shared attention, while LightTS has a two-stage architecture: Part I processes each variable independently, Part II concatenates features for inter-variable correlation learning. Need to implement: independent variable processing loops, feature concatenation logic across temporal and channel dimensions.\n4. **Feature Dimension Transformation**: Reformer maintains consistent d_model dimensions throughout, while LightTS uses bottleneck design with temporal projection (H→F'), channel projection (W→W), and output projection (F'→F). Need to implement: three separate MLP types with weight sharing across columns/rows, dimension mapping F' << F for efficiency.\n5. **Reversibility and Memory Optimization**: Reformer employs reversible layers and chunking for memory efficiency with explicit backward pass design, while LightTS uses standard feedforward architecture without reversibility. LightTS requires: standard PyTorch autograd without custom backward, simpler memory footprint but no activation checkpointing mechanisms.\n6. **[CRITICAL] Sampling Sequence Length Alignment**: LightTS splits sequences into fixed-length C sub-sequences for sampling, requiring seq_len to be divisible by C. Must implement alignment logic: (a) In __init__: `self.chunk_size = min(configs.seq_len, chunk_size)` to ensure chunk_size doesn't exceed seq_len; (b) If `seq_len % chunk_size != 0`, then `self.seq_len = seq_len + (chunk_size - seq_len % chunk_size)` for padding alignment; (c) `self.num_chunks = self.seq_len // chunk_size`; (d) In encoder method, pad input: `x = torch.cat([x, torch.zeros((B, self.seq_len - T, N)).to(x.device)], dim=1)`; (e) Continuous sampling reshapes to `[B, num_chunks, chunk_size, N]`, interval sampling reshapes to `[B, chunk_size, num_chunks, N]`. Not implementing padding causes reshape dimension mismatch errors."
  },
  {
    "source": "FEDformer_2022",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Architecture**: Both employ seasonal-trend decomposition as core design principle, separating complex patterns into seasonal and trend components. FEDformer's MOEDecomp block with multiple average pooling kernels can be adapted for MICN's MHDecomp, requiring only weight aggregation strategy changes (mixture-of-experts vs. simple mean).\n\n2. **Multi-Scale Feature Extraction**: Both utilize multi-scale approaches for pattern modeling. FEDformer's frequency domain mode selection (randomly selecting M modes) parallels MICN's multi-scale branches with different kernel sizes. The hierarchical processing logic and parameter initialization strategies from FEDformer's FEB blocks are directly transferable.\n\n3. **Encoder-Only Forecasting Paradigm**: Both adopt simplified encoder-only architectures for long-term forecasting, eliminating traditional encoder-decoder complexity. FEDformer's embedding module (temporal features + positional + value encoding) is directly reusable in MICN's embedding layer with identical three-component structure.\n\n4. **Residual Connection and Layer Normalization**: Both employ residual connections with layer normalization after each transformation block. FEDformer's normalization strategy (my_Layernorm) and dropout mechanisms can be directly integrated into MICN's local-global modules without modification.\n\n5. **Linear Projection for Output**: Both use simple linear projection layers to map hidden states to prediction dimensions. FEDformer's final projection layer implementation (nn.Linear with bias) provides ready-to-use code template for MICN's seasonal part output projection.",
    "differences": "1. **Core Transformation Mechanism**: FEDformer operates in frequency domain using Fourier/Wavelet transforms with complex-valued operations and FFT/IFFT, requiring specialized complex multiplication functions. MICN operates purely in time domain using isometric convolution, needing NEW implementation of custom padding strategies (S-1 zeros for sequence length S) and causal convolution kernels without frequency domain components.\n\n2. **Attention vs. Convolution for Global Modeling**: FEDformer implements frequency-enhanced attention (FEA-f/FEA-w) with softmax/tanh activation on frequency coefficients for global correlation. MICN replaces attention entirely with isometric convolution followed by transposed convolution for upsampling, requiring NEW Conv1d/Conv1dTranspose implementations with specific kernel=stride=scale configurations absent in FEDformer.\n\n3. **Trend Prediction Strategy**: FEDformer accumulates trend components from inner decomposition blocks without explicit modeling strategy. MICN introduces NEW linear regression module (MICN-regre) or mean-based prediction (MICN-mean) for trend-cyclical forecasting, requiring separate trend prediction branch implementation with regression-based parameter learning.\n\n4. **Multi-Scale Integration Method**: FEDformer uses mixture-of-experts with learnable softmax weights (data-dependent) for combining multi-scale decompositions. MICN employs NEW Conv2d-based weighted merging after local-global processing across branches, requiring 2D convolution implementation to aggregate multi-scale patterns with channel-wise weight learning.\n\n5. **Local Feature Compression**: FEDformer maintains consistent sequence length through zero-padding in frequency domain operations. MICN introduces NEW downsampling via Conv1d (stride=kernel=scale) in local module before global modeling, then upsampling via transposed convolution, requiring hierarchical length transformation logic (I+O → (I+O)/i → I+O) absent in FEDformer's fixed-length processing."
  },
  {
    "source": "Autoformer_2021",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Foundation**: Both employ series decomposition blocks to separate trend-cyclical and seasonal components using moving average operations. The `series_decomp` module from Autoformer (with AvgPool and padding) can be directly reused as the basis for MICN's MHDecomp block, requiring only multi-kernel extension.\n\n2. **Embedding Architecture**: Both use multi-component embeddings (value, positional, temporal) for input representation. MICN's `DataEmbedding_wo_pos` structure from Autoformer can be adapted, replacing encoder-decoder inputs with the simpler concatenation strategy (X_s + X_zero).\n\n3. **Progressive Refinement Strategy**: Both architectures progressively refine predictions through stacked layers with residual connections and normalization. The layer stacking pattern (`EncoderLayer` structure) and normalization blocks (`my_Layernorm`) provide templates for MICN's MIC layer implementation.\n\n4. **Anomaly Detection via Reconstruction**: Both perform anomaly detection through reconstruction-based forecasting, using MSE between predictions and ground truth. Autoformer's `anomaly_detection` method (encoder + projection) establishes the baseline pattern for MICN's seasonal prediction output.\n\n5. **Trend Handling Mechanisms**: Both extract and accumulate trend components separately from seasonal modeling. Autoformer's trend accumulation strategy in decoder layers (W_l projection and summation) informs MICN's trend-cyclical prediction block design, particularly for the regression approach.",
    "differences": "1. **Core Architecture Paradigm**: Autoformer uses Transformer-based Auto-Correlation mechanism with FFT-based period discovery and time-delay aggregation; MICN replaces this with multi-scale isometric convolution (Conv1d downsampling + isometric Conv + Conv1dTranspose upsampling). This requires implementing entirely new convolutional modules instead of attention layers.\n\n2. **Multi-Scale Pattern Extraction**: MICN introduces parallel multi-scale branches (kernel sizes I/4, I/8, etc.) with Conv2d merging for weighted pattern integration; Autoformer uses single-scale decomposition with fixed kernel. The multi-kernel MHDecomp block and branch-wise Conv2d merger are new components requiring custom implementation.\n\n3. **Global Correlation Modeling**: Autoformer captures dependencies via Auto-Correlation (FFT-based autocorrelation + Roll aggregation with O(LlogL) complexity); MICN uses isometric convolution (kernel=sequence_length with S-1 zero padding) for global modeling. Isometric convolution module with sequential inference capability needs ground-up development.\n\n4. **Decoder Input Strategy**: Autoformer uses encoder-decoder architecture with I/2 historical input + O placeholders in decoder initialization; MICN adopts simpler encoder-only design with direct O-length zero padding concatenation. This simplification eliminates cross-attention layers and modifies the forward pass structure.\n\n5. **Trend Prediction Methods**: Autoformer accumulates trends via learnable projections (W_l weights) across decoder layers; MICN offers two explicit strategies: linear regression (MICN-regre) or mean value (MICN-mean) for trend-cyclical prediction. The regression-based trend modeling requires implementing separate linear regression modules outside the main architecture."
  },
  {
    "source": "Informer_2020",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-based architecture for sequence processing**: Both use stacked layers with residual connections and normalization. MICN can reuse Informer's LayerNorm, dropout, and feed-forward network structures from EncoderLayer implementation.\n\n2. **Embedding strategy with multiple components**: Both employ value embedding, positional encoding, and temporal feature encoding for input representation. MICN's DataEmbedding module can directly adapt Informer's embedding implementation (TFE+PE+VE summation).\n\n3. **Sequential prediction with placeholder strategy**: Both concatenate zero placeholders to input for generative forecasting. MICN's Concat(X_s, X_zero) mirrors Informer's decoder input strategy, allowing reuse of sequence concatenation logic.\n\n4. **Multi-layer feature extraction**: Both stack multiple processing layers (Informer: EncoderLayer, MICN: MIC layers) with progressive feature refinement. The stacking mechanism and intermediate output handling can be adapted from Informer's encoder structure.\n\n5. **Projection to output dimension**: Both use linear projection layers to map hidden states to prediction outputs. MICN can reuse Informer's projection layer implementation for final forecasting output generation.",
    "differences": "1. **Core attention vs. convolution mechanism**: Informer uses ProbSparse self-attention with query sparsity measurement (O(L log L)), while MICN replaces attention entirely with isometric convolution for global correlation modeling. MICN requires implementing Conv1d-based local feature extraction and isometric convolution (kernel=S with S-1 padding) from scratch.\n\n2. **Multi-scale decomposition approach**: Informer uses self-attention distilling with MaxPool downsampling for hierarchical features, whereas MICN employs multi-scale hybrid decomposition (MHDecomp) with multiple AvgPool kernels and mean aggregation. MICN needs new implementation of parallel AvgPool branches with different kernel sizes.\n\n3. **Trend-cyclical modeling strategy**: Informer lacks explicit trend-cyclical handling, while MICN introduces dedicated Trend-cyclical Prediction Block using linear regression or mean operation on decomposed trend component. This requires implementing separate trend extraction and prediction pathways.\n\n4. **Local-global feature fusion**: Informer processes sequences uniformly through attention, while MICN uses multi-scale branches with different downsampling rates (I/4, I/8, etc.) followed by Conv2d-based weighted merging. MICN needs implementing multi-branch parallel processing and Conv2d-based pattern fusion.\n\n5. **Decoder architecture elimination**: Informer uses encoder-decoder with cross-attention between encoded memory and decoder queries, while MICN adopts encoder-only architecture with direct projection. MICN eliminates the need for decoder layers, cross-attention, and masked self-attention entirely, simplifying to single-pass forward computation."
  },
  {
    "source": "Reformer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanism**: Both papers address the O(L²) complexity bottleneck of standard Transformer attention for long sequences. Reformer uses LSH attention while Pyraformer uses pyramidal attention, but both reduce complexity to O(L). The source implementation's attention layer structure (query/key/value projections, multi-head mechanism) can be directly reused as the base attention module.\n\n2. **Shared Query-Key Architecture**: Both employ shared Q-K formulation where queries and keys are derived from the same source (Q=K in Reformer, similar parent-child relationships in Pyraformer's tree). The source code's `ReformerLayer` with shared QK projections provides a template for implementing Pyraformer's attention computations.\n\n3. **Multi-Scale Temporal Modeling**: Reformer's hash bucketing groups similar temporal patterns, while Pyraformer's C-ary tree explicitly constructs multi-resolution representations. Both capture temporal dependencies at different granularities. The source's chunking strategy for processing sequences can inform Pyraformer's scale-wise processing.\n\n4. **Memory-Efficient Design Philosophy**: Both prioritize memory efficiency through sparse attention patterns rather than full attention matrices. Reformer's reversible layers and chunking reduce activation memory; Pyraformer's pyramidal graph reduces Q-K pairs. The source's `fit_length` padding logic can be adapted for Pyraformer's scale alignment.\n\n5. **Encoder-Based Architecture**: Both use encoder-only architectures for representation learning without autoregressive decoding during encoding. The source's `Encoder` and `EncoderLayer` structure with normalization and feed-forward networks provides the backbone that can be extended with Pyraformer's pyramidal attention.",
    "differences": "1. **Core Attention Mechanism**: Reformer uses locality-sensitive hashing (LSH) to cluster similar queries/keys into buckets, computing attention only within buckets. Pyraformer constructs an explicit pyramidal graph with inter-scale (parent-child) and intra-scale (neighboring nodes) connections. **NEW IMPLEMENTATION NEEDED**: Custom CUDA kernel for pyramidal attention as described in Section 3.1, implementing Equation (3) with neighborhood sets N_ℓ^(s) containing adjacent nodes, children, and parent.\n\n2. **Multi-Resolution Construction**: Reformer implicitly creates multi-scale patterns through hash bucketing without explicit hierarchy. Pyraformer explicitly builds a C-ary tree using the Coarser-Scale Construction Module (CSCM) with convolutions. **NEW IMPLEMENTATION NEEDED**: CSCM module with sequential convolutions (kernel size C, stride C) to generate coarse-scale nodes, plus bottleneck fully-connected layers for dimension reduction as shown in Figure 3.\n\n3. **Attention Computation Pattern**: Reformer sorts queries by hash buckets and attends within chunks of m consecutive queries plus one chunk back (Equation 5). Pyraformer's nodes attend to A adjacent nodes at same scale, C children, and 1 parent (Equation 2). **NEW IMPLEMENTATION NEEDED**: Pyramidal graph construction logic defining neighborhoods A_ℓ^(s), C_ℓ^(s), P_ℓ^(s), and efficient sparse attention computation respecting this connectivity.\n\n4. **Prediction Head Design**: Reformer uses simple linear projection for reconstruction/classification. Pyraformer offers two prediction modules: (1) batch multi-step forecasting from concatenated last nodes at all scales, (2) decoder with two full attention layers using prediction tokens. **NEW IMPLEMENTATION NEEDED**: Prediction modules gathering features from all pyramid scales, and optional decoder with cross-attention between prediction tokens F_p and encoder output F_e.\n\n5. **Complexity Analysis Focus**: Reformer achieves O(L log L) through sorting and bucketing with n_hashes rounds. Pyraformer achieves O(AL) where A is constant neighborhood size, with maximum path length O(1) when C satisfies Equation (5). **NEW IMPLEMENTATION NEEDED**: Scale parameter S, child count C, adjacency size A configuration logic ensuring Lemma 1 conditions (Equation 4) for global receptive field, plus validation of Propositions 1-2 constraints."
  },
  {
    "source": "Informer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both adopt Transformer-based encoder-decoder frameworks for time series forecasting, allowing direct reuse of the embedding layers (DataEmbedding) and basic decoder structure from Informer's implementation.\n2. **Attention Mechanism Foundation**: Both build upon scaled dot-product attention with Q-K-V transformations and multi-head attention, enabling reuse of AttentionLayer's linear projection logic (query_projection, key_projection, value_projection) from Informer.\n3. **Complexity Reduction Goal**: Both target O(L log L) or O(L) complexity to handle long sequences, sharing the motivation to reduce quadratic attention cost through sparse attention patterns rather than full O(L²) computation.\n4. **Positional and Temporal Encoding**: Both incorporate positional embeddings and temporal covariates (time stamps) through embedding layers, allowing Pyraformer to directly adapt Informer's DataEmbedding module for input representation.\n5. **Multi-Step Forecasting Output**: Both generate multi-step predictions in one forward pass rather than autoregressive decoding, enabling reuse of Informer's projection layer structure and batch prediction strategy for efficiency.",
    "differences": "1. **Core Attention Innovation**: Informer uses ProbSparse attention selecting top-u queries via sparsity measurement M(q,K) with random sampling; Pyraformer introduces Pyramidal Attention Module (PAM) with multi-resolution C-ary tree structure where nodes attend only to A neighbors, C children, and 1 parent, requiring custom CUDA kernel implementation.\n2. **Multi-Scale Architecture**: Informer employs self-attention distilling with Conv1d and MaxPool for progressive downsampling in a single-scale encoder; Pyraformer constructs explicit multi-resolution pyramidal graph via Coarser-Scale Construction Module (CSCM) using C-stride convolutions to create S scales, necessitating new hierarchical node initialization logic.\n3. **Attention Connectivity Pattern**: Informer computes attention on reduced query set Q̄ with all keys K in flattened manner; Pyraformer defines structured sparse attention via neighborhood sets N_ℓ^(s) = A_ℓ^(s) ∪ C_ℓ^(s) ∪ P_ℓ^(s), requiring implementation of inter-scale (parent-child) and intra-scale (adjacent) connection logic not present in Informer.\n4. **Complexity Achievement Method**: Informer achieves O(L log L) through query sampling with factor c·ln(L_Q) and distilling layers; Pyraformer achieves O(AL) ≈ O(L) through fixed sparse connectivity (A+C+1 nodes per attention) across S scales, demanding different sparse attention masking implementation unavailable in standard PyTorch.\n5. **Decoder Design Options**: Informer uses single decoder with masked ProbSparse self-attention and cross-attention; Pyraformer offers two prediction modules: (1) direct FC mapping from last nodes at all scales, or (2) decoder with two full attention layers using prediction tokens F_p and encoder output F_e concatenation, requiring new dual-attention decoder implementation.\n6. **Scale-Aware Feature Aggregation**: Informer concatenates outputs from distilling pyramid stacks as final encoder representation; Pyraformer explicitly gathers and concatenates features from last nodes at all S scales in pyramidal graph for prediction, necessitating scale-indexed node selection mechanism not in Informer.\n7. **Path Length Optimization**: Informer focuses on reducing attention computation cost; Pyraformer additionally optimizes maximum signal traversing path to O(1) through balanced tree depth S and connectivity C satisfying Equation (5), requiring careful hyperparameter selection (S, C, A) based on sequence length L.\n8. **Bottleneck in Coarsening**: Informer's distilling uses direct Conv1d on full d_model dimension; Pyraformer's CSCM employs bottleneck structure with dimension reduction before convolutions and restoration after to reduce parameters, requiring additional FC layers around convolution blocks not present in Informer's ConvLayer."
  },
  {
    "source": "DLinear_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Decomposition-based preprocessing**: Both employ series decomposition to separate trend and seasonal components. DLinear's `series_decomp` module with moving average kernels can be directly reused as a preprocessing step in TimesNet's anomaly detection pipeline before 2D transformation.\n\n2. **Reconstruction-based anomaly detection**: Both follow reconstruction paradigm where models learn to reconstruct normal patterns, then anomalies are detected via reconstruction error. DLinear's encoder-decoder structure for sequence reconstruction provides the foundational anomaly scoring mechanism (MSE between input and output) that TimesNet can adopt.\n\n3. **Direct multi-step forecasting strategy**: Both use DMS (Direct Multi-Step) approach rather than autoregressive generation, predicting entire future sequences simultaneously. This shared forecasting philosophy means DLinear's loss computation and training loop structure are directly transferable to TimesNet's implementation.\n\n4. **Channel-wise processing**: DLinear processes each variate independently (individual mode) or shares weights across channels. TimesNet similarly maintains channel dimension  $(C)$  throughout 2D transformations, allowing reuse of DLinear's channel handling logic and batch processing strategies.\n\n5. **Linear projection layers**: Both rely on linear transformations for embedding and projection. DLinear's `nn.Linear` initialization strategy (uniform weight initialization  $1/\\text{seq\\_len}$ ) and parameter-efficient design principles can guide TimesNet's embedding layer implementation before 2D transformation.",
    "differences": "1. **Core architecture paradigm shift**: DLinear uses simple temporal linear layers operating directly on 1D sequences, while TimesNet requires implementing completely new 2D convolution infrastructure. Key new components needed: FFT-based period detection (`Period()` function), 1D-to-2D reshaping with padding/truncation (`Reshape()`, `Padding()`, `Trunc()`), and Inception block with multi-scale 2D kernels for spatial-temporal feature extraction.\n\n2. **Multi-periodicity modeling mechanism**: TimesNet introduces frequency domain analysis absent in DLinear. Must implement: (1) FFT amplitude calculation and top-k frequency selection, (2) multiple parallel 2D transformation branches for different periods, (3) adaptive aggregation using softmax-normalized amplitudes as attention weights. DLinear has no equivalent multi-scale temporal reasoning.\n\n3. **Temporal 2D-variation representation**: TimesNet's innovation lies in converting 1D time series into 2D tensors  $(p_i \\times f_i \\times C)$  where rows capture interperiod-variation and columns capture intraperiod-variation. This requires implementing period-based reshaping logic and 2D spatial processing, fundamentally different from DLinear's direct temporal modeling on flattened sequences.\n\n4. **Inception block integration**: TimesNet employs parameter-efficient Inception blocks with multi-scale 2D convolutions (multiple kernel sizes) shared across different period branches. Must implement: parallel convolutional paths, bottleneck layers for efficiency, and residual connections within TimesBlock. DLinear uses only simple linear layers without any convolutional components.\n\n5. **Hierarchical residual architecture**: TimesNet stacks multiple TimesBlocks with residual connections  $(\\mathbf{X}_{1D}^l = \\text{TimesBlock}(\\mathbf{X}_{1D}^{l-1}) + \\mathbf{X}_{1D}^{l-1})$ , where each block performs full 1D→2D→1D transformation cycles. DLinear has flat architecture with single decomposition step. Need to implement: multi-layer stacking logic, intermediate feature dimension management  $(d_{\\text{model}})$ , and gradient flow through deep residual paths."
  },
  {
    "source": "FEDformer_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. Both employ encoder-decoder architectures with embedding layers (DataEmbedding) for feature extraction. FEDformer's embedding infrastructure (positional/temporal encoding) can be directly reused for TimesNet's initial feature projection before period analysis.\n2. Both utilize series decomposition to extract seasonal and trend components. FEDformer's MOEDecomp (mixture-of-experts decomposition) with multiple average pooling filters provides a foundation for TimesNet's multi-periodicity analysis framework.\n3. Both leverage frequency domain analysis for pattern discovery. FEDformer's FFT-based mode selection in FourierBlock (Equation 3) can be adapted for TimesNet's period discovery mechanism (Equation 1).\n4. Both adopt residual connections for deep feature learning. FEDformer's residual structure in encoder/decoder layers (Equations 1-2) directly transfers to TimesNet's TimesBlock residual design (Equation 4).\n5. Both use learnable aggregation mechanisms for multi-scale representations. FEDformer's attention-based aggregation in FEA modules provides implementation patterns for TimesNet's amplitude-weighted aggregation (Equation 6).",
    "differences": "1. Core innovation differs fundamentally: FEDformer enhances attention via frequency/wavelet transforms for O(L) complexity, while TimesNet transforms 1D series into 2D tensors to capture intraperiod and interperiod variations. TimesNet requires implementing 1D-to-2D reshape operations (Equation 3) and 2D convolution backbones.\n2. Temporal modeling approach: FEDformer processes sequences in frequency domain with sparse mode selection, whereas TimesNet discovers multiple periods via FFT amplitude analysis (Equation 1) and reshapes series into multiple 2D tensors. New components needed: Period() function for top-k frequency selection and Reshape operations.\n3. Representation learning mechanism: FEDformer uses Fourier/Wavelet blocks for self/cross-attention, while TimesNet employs 2D inception blocks on reshaped tensors. Must implement Inception() module with multi-scale 2D kernels to capture spatial-temporal variations simultaneously.\n4. Multi-scale fusion strategy: FEDformer selects fixed frequency modes randomly/sequentially, TimesNet adaptively aggregates k period-based representations weighted by normalized FFT amplitudes (Equation 6). Requires implementing Softmax-based amplitude weighting for adaptive fusion.\n5. Architectural paradigm: FEDformer follows Transformer encoder-decoder with frequency-enhanced attention, TimesNet stacks modular TimesBlocks processing multiple 2D tensors in parallel. Must implement: (1) parallel 2D tensor processing pipeline, (2) shared inception block across periods, (3) Trunc() for length alignment after inverse reshape."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Reconstruction-based anomaly detection**: Both models adopt encoder-decoder architectures that reconstruct input sequences, where anomalies are detected via reconstruction error thresholds. Pyraformer's encoder with pyramidal attention and TimesNet's TimesBlock encoder can both be reused for feature extraction before reconstruction.\n\n2. **Embedding layer architecture**: Both use similar embedding strategies combining value embeddings with temporal/positional encodings (DataEmbedding in Pyraformer, Embed() in TimesNet). The embedding initialization code from Pyraformer can be directly adapted for TimesNet's input processing pipeline.\n\n3. **Multi-scale temporal modeling**: Pyraformer's pyramidal structure (C-ary tree with scales S) and TimesNet's multi-period analysis (k different periods) both capture temporal patterns at multiple resolutions. The concept of aggregating multi-scale features can guide TimesNet's adaptive aggregation implementation.\n\n4. **Residual connections for deep networks**: Both employ residual connections (Pyraformer in feed-forward layers, TimesNet in TimesBlock stacking). Pyraformer's PositionwiseFeedForward with residual connections can serve as template for TimesNet's residual architecture in Equation 4.\n\n5. **Projection layers for output**: Both use linear projection layers to map learned representations back to input dimension for reconstruction (Pyraformer's self.projection, TimesNet's implicit final projection). This projection mechanism can be directly transferred between implementations.",
    "differences": "1. **Core innovation approach**: Pyraformer reduces complexity through pyramidal attention with O(L) complexity using sparse graph connections, while TimesNet transforms 1D time series into 2D tensors based on FFT-discovered periodicities. TimesNet requires implementing FFT-based period detection (Equation 1-2) and 1D-to-2D reshaping operations (Equation 3) entirely from scratch.\n\n2. **Attention mechanism vs. convolution**: Pyraformer uses customized sparse attention (PAM) with query-key-value operations across pyramidal graph nodes, whereas TimesNet employs 2D inception blocks with multi-scale convolutions. TimesNet needs new 2D convolutional modules (Inception block in Equation 5) instead of attention layers, requiring different computational primitives.\n\n3. **Temporal pattern extraction**: Pyraformer constructs multi-resolution hierarchy through CSCM with strided convolutions (stride C) on 1D sequences, while TimesNet discovers periods via frequency domain analysis (FFT amplitude selection). TimesNet must implement Period() function with FFT, Amp(), TopK operations, and frequency-to-period conversion logic.\n\n4. **Feature aggregation strategy**: Pyraformer concatenates features from all pyramid scales equally (gather + concatenate in refer_points), while TimesNet uses softmax-normalized amplitude-weighted aggregation (Equation 6). TimesNet requires implementing adaptive weighting based on frequency amplitudes, which is fundamentally different from Pyraformer's concatenation approach.\n\n5. **Structural representation**: Pyraformer maintains 1D sequence structure throughout processing with hierarchical nodes, while TimesNet explicitly reshapes to 2D tensors (p_i × f_i × d_model) for spatial convolutions. TimesNet needs Reshape, Padding, and Trunc operations to handle 1D↔2D transformations, plus handling variable tensor shapes across k different periods simultaneously."
  }
]
