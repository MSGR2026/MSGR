{
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "items": [
    {
      "id": "tsl_framework_spec",
      "title": "Time Series Library 框架规范",
      "content": "模型类命名为 Model，继承 nn.Module。__init__ 接收 configs 参数，包含 task_name, seq_len, enc_in, c_out, d_model, n_heads, e_layers, d_ff, dropout, anomaly_ratio 等配置。必须实现 anomaly_detection(x_enc) 方法返回重构序列，forward() 根据 task_name 分发。输入 x_enc 形状 [B, seq_len, enc_in]，输出 [B, seq_len, c_out]。异常检测基于重构误差：训练时最小化重构损失，测试时根据误差阈值判断异常。【组件自定义】若模型需要特殊组件（如带 moving_avg 的 EncoderLayer），而框架现有组件不支持所需参数，应在模型文件中内联定义该组件，避免 API 签名不匹配导致运行时错误。",
      "tags": ["framework", "interface", "critical"],
      "source": "framework_analysis",
      "priority": 10,
      "created_at": "2025-12-24T12:00:00.000000",
      "updated_at": "2025-12-24T12:00:00.000000"
    },
    {
      "id": "config_params",
      "title": "超参数配置规范",
      "content": "【禁止配置的参数】enc_in, dec_in, c_out 等特征维度参数由框架根据数据集自动确定，配置中不要设置这些参数，否则会导致维度不匹配错误。【可配置参数】seq_len, label_len, pred_len (序列长度)；d_model, n_heads, e_layers, d_layers, d_ff (模型结构)；moving_avg, factor, dropout, activation, top_k, num_kernels (模型特定)；train_epochs, batch_size, patience, learning_rate, loss, lradj (训练)；anomaly_ratio (任务特定)。【布尔参数规范】distil, inverse, use_amp, individual 等布尔参数只能设 true 或不写，禁止设 0/false。【默认值处理】代码中用 getattr(configs, 'param', default) 获取可选参数并设置默认值。",
      "tags": ["config", "hyperparameter", "critical"],
      "source": "run.py_argparse",
      "priority": 10
    },
    {
      "id": "architecture_paradigm",
      "title": "异常检测架构范式",
      "content": "时序异常检测采用 Encoder-only 重构架构：原预测任务的编解码器结构简化为仅编码器+投影层，输入输出维度相同 [B, L, D]。核心流程：Embedding → Encoder Layers → Linear Projection (nn.Linear(d_model, c_out))。该范式的优势：(1) 结构简单，无需设计复杂的解码器；(2) 训练稳定，仅需最小化重构损失；(3) 推理高效，单次前向即可完成。编码器可选 Transformer、CNN、MLP 或混合结构，关键是学习正常模式的紧凑表示，使异常数据重构误差显著。",
      "tags": ["architecture", "encoder", "reconstruction"],
      "source": "implementation_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "normalization_techniques",
      "title": "实例归一化与反归一化",
      "content": "时序模型广泛采用实例归一化处理非平稳性：前向时计算 means=x.mean(1, keepdim=True).detach()，stdev=sqrt(var(x, dim=1)+1e-5).detach()，归一化 x=(x-means)/stdev；输出时反归一化 out=out*stdev+means。该技术可有效处理时序的分布偏移，使模型专注于学习相对模式而非绝对数值。部分设计额外使用可学习仿射变换 (affine_weight, affine_bias) 增强表达能力。注意：detach() 阻止梯度回传到统计量，保证归一化的稳定性。",
      "tags": ["normalization", "non-stationary", "preprocessing"],
      "source": "implementation_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "series_decomposition",
      "title": "序列分解技术",
      "content": "序列分解将时序拆分为趋势 (Trend) 和季节性 (Seasonal) 成分，降低建模复杂度。实现方式：(1) 滑动平均分解：trend=AvgPool1d(x, kernel_size=moving_avg)，seasonal=x-trend，需两端填充保持长度；(2) 多尺度分解：使用多个 kernel_size 的滑动平均后取均值，捕获不同粒度的趋势。分解后可分别建模：用独立网络处理两个成分，或在编码器每层后渐进分解。该技术有效分离正常周期模式与异常偏离。moving_avg 参数典型值：25。",
      "tags": ["decomposition", "trend", "seasonal"],
      "source": "implementation_analysis",
      "priority": 8,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "frequency_domain",
      "title": "频域处理与周期建模",
      "content": "频域方法高效捕获时序周期性：(1) 周期发现：FFT 后按幅值 topk 选择主周期，可将 1D 序列 reshape 为 2D 张量进行建模；(2) 频域特征学习：在频域执行线性变换或注意力计算，对选定的频率模式做可学习变换；(3) 自相关计算：Q、K 做 FFT 后共轭相乘再 IFFT 得相关序列，高效发现周期依赖。频域处理的优势：(1) 天然捕获全局周期依赖；(2) FFT 复杂度 O(LlogL)；(3) 对噪声鲁棒。核心函数：torch.fft.rfft/irfft。",
      "tags": ["frequency", "FFT", "periodicity"],
      "source": "implementation_analysis",
      "priority": 8,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "attention_efficiency",
      "title": "高效注意力机制设计",
      "content": "长序列时序建模需优化标准 O(L²) 注意力：(1) 稀疏注意力：通过度量 Query 重要性仅选 topk 计算，O(LlogL)；(2) 频域注意力：在频域执行 Q-K 相似度，模式选择降至 O(L)；(3) 局部敏感哈希：哈希分桶后桶内注意力，近似 O(L)；(4) 低秩近似：通过核函数或线性化降低复杂度。选择依据：短序列(<512)可用标准注意力；长序列优先考虑稀疏或频域变体。注意力层通常配合 LayerNorm 和残差连接：x = norm(x + dropout(attn(x)))。",
      "tags": ["attention", "complexity", "efficiency"],
      "source": "implementation_analysis",
      "priority": 8,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "channel_strategy",
      "title": "多变量通道处理策略",
      "content": "多变量时序的通道处理策略影响模型表达力与泛化性：(1) 通道独立：各变量独立编码，将 [B,L,D] reshape 为 [B*D,L] 或为每个通道设独立参数。优势：泛化性强，避免变量间伪相关，适合变量异构场景；(2) 通道交互：显式建模变量依赖，可将变量作为 token 在变量维做注意力，或用双阶段分别处理时间和变量维度。选择依据：变量相关性强且稳定时用交互策略；变量数量可变或相关性弱时用独立策略。individual 参数控制是否共享参数。",
      "tags": ["channel", "multivariate", "independence"],
      "source": "implementation_analysis",
      "priority": 7,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "multiscale_modeling",
      "title": "多尺度建模方法",
      "content": "多尺度建模捕获不同粒度的时序模式：(1) 分段采样：将序列分为 chunks，连续采样 reshape(B,num_chunks,chunk_size,D) 捕局部模式，间隔采样 reshape(B,chunk_size,num_chunks,D) 捕全局模式；(2) 金字塔结构：逐层下采样构建多分辨率表示，层间连接不同尺度；(3) 多核卷积：并行不同 kernel_size 的卷积核 (1,3,5,...) 后融合；(4) 周期重塑：根据发现的周期将序列 reshape 为 2D 结构。多尺度设计增强模型对不同周期和粒度异常的检测能力。",
      "tags": ["multiscale", "hierarchy", "resolution"],
      "source": "implementation_analysis",
      "priority": 7,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "anomaly_evaluation",
      "title": "异常检测评估方法",
      "content": "时序异常检测的评估具有特殊性：(1) 阈值确定：验证集误差百分位数、POT (Peaks Over Threshold)、或使用 anomaly_ratio 参数；(2) 点级评估：每个时间点是否正确识别；(3) 事件级评估：整个异常段是否被检测到；(4) Point-Adjust 策略：检测到异常段中任一点则整段算正确，更符合实际应用。计算流程：计算重构误差 → 确定阈值 → 预测异常点 → (可选) Point Adjustment → 计算 Precision/Recall/F1。实现：sklearn.metrics.precision_recall_fscore_support。",
      "tags": ["evaluation", "threshold", "metrics"],
      "source": "model_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "pred_len_handling",
      "title": "异常检测中的序列长度处理",
      "content": "异常检测输入只有 seq_len 长度（无未来序列），而预测任务模型常假设输入长度为 seq_len+pred_len。迁移时需注意两类问题：(1) 长度计算错误：若模型内部使用 seq_len+pred_len 计算期望长度（如 2D reshape、padding），需改为仅用 seq_len，否则 reshape 会因元素数量不匹配报错；(2) 组件依赖 pred_len：若组件需要 pred_len>0（如频域外推、阻尼层），需在 __init__ 中设置 self.pred_len=self.seq_len。典型症状：RuntimeError shape invalid for input of size / IndexError dimension out of bounds。最佳实践：在 __init__ 中根据 task_name=='anomaly_detection' 调整长度相关逻辑。",
      "tags": ["pred_len", "seq_len", "dimension", "critical"],
      "source": "implementation_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "tensor_shape_consistency",
      "title": "张量形状变换一致性",
      "content": "复杂张量变换（reshape/permute/1D↔2D转换）需严格追踪形状变化，确保各组件输入输出维度匹配。常见陷阱：(1) Channel-independent 处理时将 [B,L,C] reshape 为 [B*C,L] 后接卷积，若卷积 in_channels 仍为原 C 则不匹配，应设为 1；(2) 1D→2D 变换时 permute 顺序错误导致通道维位置不对；(3) 多尺度/多周期处理时不同分支输出形状不一致无法聚合。调试方法：在关键变换点打印形状或添加 assert；设计时先写出完整的 shape 流：输入 [B,L,D] → embedding [B,L,d_model] → reshape [...] → conv in_channels=? → ...。nn.Conv2d(in_channels, out_channels, ...) 的 in_channels 必须与实际输入张量的第 1 维（索引从 0 开始）严格一致。",
      "tags": ["tensor", "shape", "dimension", "debugging", "critical"],
      "source": "implementation_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "weight_initialization",
      "title": "特殊权重初始化",
      "content": "部分模型的性能高度依赖特殊权重初始化，使用默认初始化会导致性能显著下降。常见模式：(1) 线性层初始化为均匀值 (1/seq_len)*ones，使初始状态等价于滑动平均；(2) 卷积层使用 Kaiming 初始化配合特定 nonlinearity；(3) 嵌入层使用正态分布或截断正态。复现时务必检查原始代码中 nn.Parameter 的显式赋值、自定义 _initialize_weights 方法、以及 nn.init.* 调用。若原始实现有特殊初始化而复现使用默认初始化，模型可能难以收敛或性能大幅下降。",
      "tags": ["initialization", "weight", "performance", "critical"],
      "source": "implementation_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "component_reuse",
      "title": "框架组件复用原则",
      "content": "优先复用 layers/ 目录下的框架组件（如 DataEmbedding, series_decomp, AutoCorrelationLayer, Encoder, Decoder 等），而非重新实现。原因：(1) 框架组件经过充分测试和调优；(2) 与框架其他部分（如 Exp 类）接口兼容；(3) 避免细微实现差异导致的性能差距。仅当框架组件不支持所需功能（如缺少某参数）时才内联自定义。内联时应尽量保持与原始组件相同的实现逻辑，特别是 forward 中的计算顺序、残差连接位置、LayerNorm 位置等细节。",
      "tags": ["component", "reuse", "layers", "framework"],
      "source": "implementation_analysis",
      "priority": 8,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "anomaly_detection_adaptation",
      "title": "预测模型转异常检测的适配要点",
      "content": "将预测任务模型适配为异常检测时，需特别注意 anomaly_detection 方法的实现：(1) 若原模型有 Encoder-Decoder 结构，异常检测通常只用 Encoder+Projection，但若 Decoder 包含关键逻辑（如指数平滑的阻尼层、频域外推、趋势累加），需保留完整调用而非自行简化聚合；(2) 反归一化的 repeat 维度应与实际输出长度一致，预测任务用 seq_len+pred_len，异常检测仅用 seq_len；(3) 分解类模型的 trend 初始化和逐层累加逻辑需正确处理，不可随意省略。最佳实践：参考 local/implementation 中同类模型的 anomaly_detection 方法实现。",
      "tags": ["adaptation", "encoder-decoder", "anomaly_detection", "critical"],
      "source": "implementation_analysis",
      "priority": 9,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-25T12:00:00.000000"
    },
    {
      "id": "vectorization_critical",
      "title": "⚠️ 防卡死：向量化操作规范（不遵守会导致代码卡死）",
      "content": "【禁止的模式 - 会导致卡死】\n❌ for b in range(B): for h in range(H): output[b,h]=... （嵌套循环遍历batch/heads）\n❌ for t in range(seq_len): state[t] = f(state[t-1]) （时间步递归）\n❌ tensor[b,h].item() 在循环内调用（GPU-CPU同步阻塞）\n❌ torch.roll() 在嵌套循环内逐样本调用\n\n【正确的向量化模式】\n\n✅ 时延聚合（训练阶段）- 跨batch平均得到统一延迟索引：\nmean_value = torch.mean(torch.mean(corr, dim=1), dim=1)  # [B, L]\n# 关键：跨batch平均得到共享的延迟索引！\nindex = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]  # [top_k] 共享\nfor i in range(top_k):  # 只循环top_k次（约3-5次）\n    pattern = torch.roll(values, -int(index[i]), -1)  # 整个batch一起roll！\n    delays_agg += pattern * weights[...]\n\n✅ 时延聚合（推理阶段）- 使用torch.gather和doubled values：\ntmp_values = values.repeat(1, 1, 1, 2)  # 加倍用于循环索引\ninit_index = torch.arange(L).view(1,1,1,-1).expand(B, H, C, L).to(device)\nfor i in range(top_k):\n    tmp_delay = init_index + delay[:, i].view(-1,1,1,1).expand_as(init_index)\n    pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)  # 向量化！\n\n✅ 周期发现 - 跨batch平均FFT，使用共享周期：\nxf = torch.fft.rfft(x, dim=1)\nfreq_amp = xf.abs().mean(0).mean(-1)  # 跨batch平均！\n_, top_periods = torch.topk(freq_amp, k)  # 共享周期\n\n✅ 指数平滑 - 使用FFT卷积替代递归：\nkernel = alpha * (1-alpha)**torch.arange(L)\noutput = torch.fft.irfft(torch.fft.rfft(x) * torch.fft.rfft(kernel))\n\n【自检清单】提交前检查：forward()中不应出现 'for b in range(B)' 或 '.item()' 调用。",
      "tags": ["vectorization", "batch", "performance", "critical", "hanging"],
      "source": "implementation_analysis",
      "priority": 10,
      "created_at": "2025-12-25T12:00:00.000000",
      "updated_at": "2025-12-26T12:00:00.000000"
    }
  ],
  "metadata": {
    "created_at": "2025-12-24T12:00:00.000000",
    "updated_at": "2025-12-25T23:00:00.000000",
    "item_count": 16
  }
}
