{
  "id": "TimeMixer_2024",
  "paper_title": "TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting",
  "alias": "TimeMixer",
  "year": 2024,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "TimeMixer proposes a decomposable multiscale mixing architecture for time series forecasting that addresses intricate temporal variations through two key innovations: (1) Past-Decomposable-Mixing (PDM) that decomposes multiscale time series into seasonal and trend components, then applies bottom-up mixing for seasonal parts (to capture detailed patterns) and top-down mixing for trend parts (to leverage macroscopic information); (2) Future-Multipredictor-Mixing (FMM) that ensembles predictions from multiple scales to utilize their complementary forecasting capabilities, where fine scales capture detailed seasonal patterns while coarse scales focus on macro trends.",
  "introduction": "# 1 INTRODUCTION\n\nTime series forecasting has been studied with immense interest in extensive applications, such as economics (Granger & Newbold, 2014), energy (Martín et al., 2010; Qian et al., 2019), traffic planning (Chen et al., 2001; Yin et al., 2021) and weather prediction (Wu et al., 2023b), which is to predict future temporal variations based on past observations of time series (Wu et al., 2023a). However, due to the complex and non-stationary nature of the real world or systems, the observed series usually present intricate temporal patterns, where the multitudinous variations, such as increasing, decreasing, and fluctuating, are deeply mixed, bringing severe challenges to the forecasting task.\n\nRecently, deep models have achieved promising progress in time series forecasting. The representative models capture temporal variations with well-designed architectures, which span a wide range of foundation backbones, including CNN (Wang et al., 2023; Wu et al., 2023a; Hewage et al., 2020), RNN (Lai et al., 2018; Qin et al., 2017; Salinas et al., 2020), Transformer (Vaswani et al., 2017; Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022b; Nie et al., 2023) and MLP (Zeng et al., 2023; Zhang et al., 2022; Oreshkin et al., 2019; Challu et al., 2023). In the development of elaborative model architectures, to tackle intricate temporal patterns, some special designs are also involved in these deep models. The widely-acknowledged paradigms primarily include series decomposition and multiperiodicity analysis. As a classical time series analysis technology, decomposition is introduced to deep models as a basic module by (Wu et al., 2021), which decomposes the complex temporal patterns into more predictable components, such as seasonal and trend, and thereby benefiting the forecasting process (Zeng et al., 2023; Zhou et al., 2022b; Wang et al., 2023). Furthermore,\n\nmultiperiodicity analysis is also involved in time series forecasting (Wu et al., 2023a; Zhou et al., 2022a) to disentangle mixed temporal variations into multiple components with different period lengths. Empowered with these designs, deep models are able to highlight inherent properties of time series from tanglesome variations and further boost the forecasting performance.\n\nGoing beyond the above mentioned designs, we further observe that time series present distinct temporal variations in different sampling scales, e.g., the hourly recorded traffic flow presents traffic changes at different times of the day, while for the daily sampled series, these fine-grained variations disappear but fluctuations associated with holidays emerge. On the other hand, the trend of macroeconomics dominates the yearly averaged patterns. These observations naturally call for a multiscale analysis paradigm to disentangle complex temporal variations, where fine and coarse scales can reflect the micro- and the macro-scopic information respectively. Especially for the time series forecasting task, it is also notable that the future variation is jointly determined by the variations in multiple scales. Therefore, in this paper, we attempt to design the forecasting model from a novel view of multiscale-mixing, which is able to take advantage of both disentangled variations and complementary forecasting capabilities from multiscale series simultaneously.\n\nTechnically, we propose TimeMixer with a multiscale mixing architecture that is able to extract essential information from past variations by Past-Decomposable-Mixing (PDM) blocks and then predicts the future series by the Future-Multipredictor-Mixing (FMM) block. Concretely, TimeMixer first generates multiscale observations through average downsampling. Next, PDM adopts a decomposable design to better cope with distinct properties of seasonal and trend variations, by mixing decomposed multiscale seasonal and trend components in fine-to-coarse and coarse-to-fine directions separately. With our novel design, PDM is able to successfully aggregate the detailed seasonal information starting from the finest series and dive into macroscopic trend components along with the knowledge from coarser scales. In the forecasting phase, FMM ensembles multiple predictors to utilize complementary forecasting capabilities from multiscale observations. With our meticulous architecture, TimeMixer achieves the consistent state-of-the-art performances in both long-term and short-term forecasting tasks with superior efficiency across all of our experiments, covering extensive well-established benchmarks. Our contributions are summarized as follows:\n\n- Going beyond previous methods, we tackle intricate temporal variations in series forecasting from a novel view of multiscale mixing, taking advantage of disentangled variations and complementary forecasting capabilities from multiscale series simultaneously.  \n- We propose TimeMixer as a simple but effective forecasting model, which enables the combination of the multiscale information in both history extraction and future prediction phases, empowered by our tailored decomposable and multiple-predictor mixing technique.  \n- TimeMixer achieves consistent state-of-the-art in performances in both long-term and short-term forecasting tasks with superior efficiency on a wide range of benchmarks.",
  "method": "# 3 TIMEMIXER\n\nGiven a series  $\\mathbf{x}$  with one or multiple observed variates, the main objective of time series forecasting is to utilize past observations (length-  $P$ ) to obtain the most probable future prediction (length-  $F$ ). As mentioned above, the key challenge of accurate forecasting is to tackle intricate temporal variations. In this paper, we propose TimeMixer of multiscale-mixing, benefiting from disentangled variations and complementary forecasting capabilities from multiscale series. Technically, TimeMixer consists of a multiscale mixing architecture with Past-Decomposable-Mixing and Future-Multipredictor-Mixing for past information extraction and future prediction respectively.\n\n# 3.1 MULTISCALE MIXING ARCHITECTURE\n\nTime series of different scales naturally exhibit distinct properties, where fine scales mainly depict detailed patterns and coarse scales highlight macroscopic variations (Mozer, 1991). This multiscale view can inherently disentangle intricate variations in multiple components, thereby benefiting temporal variation modeling. It is also notable that, especially for the forecasting task, multiscale time series present different forecasting capabilities, due to their distinct dominating temporal patterns (Ferreira et al., 2006). Therefore, we present TimeMixer in a multiscale mixing architecture to utilize multiscale series with distinguishing designs for past extraction and future prediction phases.\n\nAs shown in Figure 1, to disentangle complex variations, we first downsample the past observations  $\\mathbf{x} \\in \\mathbb{R}^{P \\times C}$  into  $M$  scales by average pooling and finally obtain a set of multiscale time series  $\\mathcal{X} = \\{\\mathbf{x}_0, \\dots, \\mathbf{x}_M\\}$ , where  $\\mathbf{x}_m \\in \\mathbb{R}^{\\lfloor \\frac{P}{2^m} \\rfloor \\times C}$ ,  $m \\in \\{0, \\dots, M\\}$ ,  $C$  denotes the variate number. The lowest level series  $\\mathbf{x}_0 = \\mathbf{x}$  is the input series, which contains the finest temporal variations, while the highest-level series  $\\mathbf{x}_M$  is for the macroscopic variations. Then we project these multiscale series into deep features  $\\mathcal{X}^0$  by the embedding layer, which can be formalized as  $\\mathcal{X}^0 = \\operatorname{Embed}(\\mathcal{X})$ . With the above designs, we obtain the multiscale representations of input series.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/8d4fcb4ccdef70cdc92eaf5bbd20c44fd75a510bc68d184b809831bcc31755e9.jpg)  \nFigure 1: Overall architecture of TimeMixer, which consists of Past-Decomposable Mixing and Future-Multipredictor-Mixing for past observations and future predictions respectively.\n\nNext, we utilize stacked Past-Decomposable-Mixing (PDM) blocks to mix past information across different scales. For the  $l$ -th layer, the input is  $\\mathcal{X}^{l-1}$  and the process of PDM can be formalized as:\n\n$$\n\\mathcal {X} ^ {l} = \\operatorname {P D M} \\left(\\mathcal {X} ^ {l - 1}\\right), l \\in \\{0, \\dots , L \\}, \\tag {1}\n$$\n\nwhere  $L$  is the total layer and  $\\mathcal{X}^l = \\{\\mathbf{x}_0^l,\\dots ,\\mathbf{x}_M^l\\} ,\\mathbf{x}_m^l\\in \\mathbb{R}^{\\lfloor \\frac{P}{2^m}\\rfloor \\times d_{\\mathrm{model}}}$  denotes the mixed past representations with  $d_{\\mathrm{model}}$  channels. More details of PDM are described in the next section.\n\nAs for the future prediction phase, we adopt the Future-Multipredictor-Mixing (FMM) block to ensemble extracted multiscale past information  $\\mathcal{X}^L$  and generate future predictions, which is:\n\n$$\n\\widehat {\\mathbf {x}} = \\operatorname {F M M} \\left(\\mathcal {X} ^ {L}\\right), \\tag {2}\n$$\n\nwhere  $\\widehat{\\mathbf{x}}\\in \\mathbb{R}^{F\\times C}$  represents the final prediction. With the above designs, TimeMixer can successfully capture essential past information from disentangled multiscale observations and predict the future with benefits from multiscale past information.\n\n# 3.2 PAST DECOMPOSABLE MIXING\n\nWe observe that for past observations, due to the complex nature of real-world series, even the coarsest scale series present mixed variations. As shown in Figure 1, the series in the top layer still present clear seasonality and trend simultaneously. It is notable that the seasonal and trend components hold distinct properties in time series analysis (Cleveland et al., 1990), which corresponds to short-term and long-term variations or stationary and non-stationary dynamics respectively. Therefore, instead of directly mixing multiscale series as a whole, we propose the Past-Decomposable-Mixing (PDM) block to mix the decomposed seasonal and trend components in multiple scales separately.\n\nConcretely, for the  $l$ -th PDM block, we first decompose the multiscale time series  $\\mathcal{X}_l$  into seasonal parts  $S^l = \\{\\mathbf{s}_0^l,\\dots ,\\mathbf{s}_M^l\\}$  and trend parts  $\\mathcal{T}^l = \\{\\mathbf{t}_0^l,\\dots ,\\mathbf{t}_M^l\\}$  by series decomposition block from Autoformer (Wu et al., 2021). As the above analyzed, taking the distinct properties of seasonal-trend parts into account, we apply the mixing operation to seasonal and trend terms separately to interact information from multiple scales. Overall, the  $l$ -th PDM block can be formalized as:\n\n$$\n\\mathbf {s} _ {m} ^ {l}, \\mathbf {t} _ {m} ^ {l} = \\mathrm {S e r i e s D e c o m p} (\\mathbf {x} _ {m} ^ {l}), m \\in \\{0, \\dots , M \\},\n$$\n\n$$\n\\mathcal {X} ^ {l} = \\mathcal {X} ^ {l - 1} + \\text {F e e d F o r w a r d} \\left(\\text {S - M i x} \\left(\\left\\{\\mathbf {s} _ {m} ^ {l} \\right\\} _ {m = 0} ^ {M}\\right) + \\text {T - M i x} \\left(\\left\\{\\mathbf {t} _ {m} ^ {l} \\right\\} _ {m = 0} ^ {M}\\right)\\right), \\tag {3}\n$$\n\nwhere FeedForward(\\cdot) contains two linear layers with intermediate GELU activation function for information interaction among channels, S-Mix(\\cdot), T-Mix(\\cdot) denote seasonal and trend mixing.\n\nSeasonal Mixing In seasonality analysis (Box & Jenkins, 1970), larger periods can be seen as the aggregation of smaller periods, such as the weekly period of traffic flow formed by seven daily changes, addressing the importance of detailed information in predicting future seasonal variations.\n\nTherefore, in seasonal mixing, we adopt the bottom-up approach to incorporate information from the lower-level fine-scale time series upwards, which can supplement detailed information to the seasonality modeling of coarser scales. Technically, for the set of multiscale seasonal parts  $\\mathcal{S}^l =$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/4a8aa24b8b259f679d2d6ee05318911b0340278d0ef93f158038f4f449b46ed1.jpg)  \n(a) Seasonal Mixing\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/8a2cc4f59d8473cc4ac74d40b62a008d904b01072917f1baf878b7913915aef5.jpg)  \n(b) Trend Mixing  \nFigure 2: The temporal linear layer in seasonal mixing (a), trend mixing (b) and future prediction (c).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/40f74234ff1637aa6e9594baf2c60c824328ebdb2de5262e6b4a3205d49db716.jpg)  \n(c) Future Prediction\n\n$\\{\\mathbf{s}_0^l,\\dots ,\\mathbf{s}_M^l\\}$ , we use the Bottom-Up-Mixing layer for the  $m$ -th scale in a residual way to achieve bottom-up seasonal information interaction, which can be formalized as:\n\n$$\n\\text {f o r} m: 1 \\rightarrow M \\text {d o :} \\mathbf {s} _ {m} ^ {l} = \\mathbf {s} _ {m} ^ {l} + \\text {B o t t o m - U p - M i x i n g} (\\mathbf {s} _ {m - 1} ^ {l}). \\tag {4}\n$$\n\nwhere Bottom-Up-Mixing  $(\\cdot)$  is instantiated as two linear layers with an intermediate GELU activation function along the temporal dimension, whose input dimension is  $\\left\\lfloor \\frac{P}{2^{m - 1}}\\right\\rfloor$  and output dimension is  $\\left\\lfloor \\frac{P}{2^m}\\right\\rfloor$ . See Figure 2 for an intuitive understanding.\n\nTrend Mixing Contrary to seasonal parts, for trend items, the detailed variations can introduce noise in capturing macroscopic trend. Note that the upper coarse scale time series can easily provide clear macro information than the lower level. Therefore, we adopt a top-down mixing method to utilize the macro knowledge from coarser scales to guide the trend modeling of finer scales.\n\nTechnically, for multiscale trend components  $\\mathcal{T}^l = \\{\\mathbf{t}_0^l,\\dots ,\\mathbf{t}_M^l\\}$ , we adopt the Top-Down-Mixing layer for the  $m$ -th scale in a residual way to achieve top-down trend information interaction:\n\n$$\n\\text {f o r} m: (M - 1) \\rightarrow 0 \\text {d o :} \\quad \\mathbf {t} _ {m} ^ {l} = \\mathbf {t} _ {m} ^ {l} + \\text {T o p - D o w n - M i x i n g} (\\mathbf {t} _ {m + 1} ^ {l}), \\tag {5}\n$$\n\nwhere Top-Down-Mixing(\\cdot) is two linear layers with an intermediate GELU activation function, whose input dimension is  $\\left\\lfloor \\frac{P}{2^{m + 1}}\\right\\rfloor$  and output dimension is  $\\left\\lfloor \\frac{P}{2^m}\\right\\rfloor$  as shown in Figure 2.\n\nEmpowered by seasonal and trend mixing, PDM progressively aggregates the detailed seasonal information from fine to coarse and dive into the macroscopic trend information with prior knowledge from coarser scales, eventually achieving the multiscale mixing in past information extraction.\n\n# 3.3 FUTURE MULTIPREDICTOR MIXING\n\nAfter  $L$  PDM blocks, we obtain the multiscale past information as  $\\mathcal{X}^L = \\{\\mathbf{x}_0^L,\\dots ,\\mathbf{x}_M^L\\} ,\\mathbf{x}_m^L\\in$ $\\mathbb{R}^{\\lfloor \\frac{P}{2^m}\\rfloor \\times d_{\\mathrm{model}}}$  . Since the series in different scales presents different dominating variations, their predictions also present different capabilities. To fully utilize the multiscale information, we propose to aggregate predictions from multiscale series and present Future-Multipredictor-Mixing block as:\n\n$$\n\\widehat {\\mathbf {x}} _ {m} = \\operatorname {P r e d i c t o r} _ {m} \\left(\\mathbf {x} _ {m} ^ {L}\\right), m \\in \\{0, \\dots , M \\}, \\widehat {\\mathbf {x}} = \\sum_ {m = 0} ^ {M} \\widehat {\\mathbf {x}} _ {m}, \\tag {6}\n$$\n\nwhere  $\\widehat{\\mathbf{x}}_m\\in \\mathbb{R}^{F\\times C}$  represents the future prediction from the  $m$  -th scale series and the final output is  $\\widehat{\\mathbf{x}}\\in \\mathbb{R}^{F\\times C}$ . Predictor $_m(\\cdot)$  denotes the predictor of the  $m$  -th scale series, which firstly adopts one single linear layer to directly regress length-  $F$  future from length- $\\lfloor \\frac{P}{2^m}\\rfloor$  extracted past information (Figure 2) and then projects deep representations into  $C$  variates. Note that FMM is an ensemble of multiple predictors, where different predictors are based on past information from different scales, enabling FMM to integrate complementary forecasting capabilities of mixed multiscale series.\n",
  "experiments": "# 4 EXPERIMENTS\n\nWe conduct extensive experiments to evaluate the performance and efficiency of TimeMixer, covering long-term and short-term forecasting, including 18 real-world benchmarks and 15 baselines. The detailed model and experiment configurations are summarized in Appendix A.\n\nTable 1: Summary of benchmarks. Forecastability is one minus the entropy of Fourier domain.  \n\n<table><tr><td>Tasks</td><td>Dataset</td><td>Variate</td><td>Predict Length</td><td>Frequency</td><td>Forecastability</td><td>Information</td></tr><tr><td rowspan=\"5\">Long-term forecasting</td><td>ETT (4 subsets)</td><td>7</td><td>96~720</td><td>15 mins</td><td>0.46</td><td>Temperature</td></tr><tr><td>Weather</td><td>21</td><td>96~720</td><td>10 mins</td><td>0.75</td><td>Weather</td></tr><tr><td>Solar-Energy</td><td>137</td><td>96~720</td><td>10min</td><td>0.33</td><td>Electricity</td></tr><tr><td>Electricity</td><td>321</td><td>96~720</td><td>Hourly</td><td>0.77</td><td>Electricity</td></tr><tr><td>Traffic</td><td>862</td><td>96~720</td><td>Hourly</td><td>0.68</td><td>Transportation</td></tr><tr><td rowspan=\"2\">Short-term forecasting</td><td>PEMS (4 subsets)</td><td>170~883</td><td>12</td><td>5min</td><td>0.55</td><td>Traffic network</td></tr><tr><td>M4 (6 subsets)</td><td>1</td><td>6~48</td><td>Hourly~Yearly</td><td>0.47</td><td>Database</td></tr></table>\n\n**Benchmarks** For long-term forecasting, we experiment on 8 well-established benchmarks: ETT datasets (including 4 subsets: ETTh1, ETTh2, ETTm1, ETTm2), Weather, Solar-Energy, Electricity, and Traffic following (Zhou et al., 2021; Wu et al., 2021; Liu et al., 2022a). For short-term forecasting, we adopt the PeMS (Chen et al., 2001) which contains four public traffic network datasets (PEMS03, PEMS04, PEMS07, PEMS08), and M4 dataset which involves 100,000 different time series collected in different frequencies. Furthermore, we measure the forecastability (Goerg, 2013) of all datasets. It is observed that ETT, M4, and Solar-Energy exhibit relatively low forecastability, indicating the challenges in these benchmarks. More information is summarized in Table 1.\n\nBaselines We compare TimeMixer with 15 baselines, which comprise the state-of-the-art long-term forecasting model PatchTST (2023) and advanced short-term forecasting models TimesNet (2023a) and SCINet (2022a), as well as other competitive models including Crossformer (2023), MICN (2023), FiLM (2022a), DLinear (2023), LightTS (2022), FEDformer (2022b), Stationary (2022b), Pyraformer (2021), Autoformer (2021), Informer (2021), N-HiTS (2023) and N-BEATS (2019).\n\nUnified experiment settings Note that experimental results reported by the above mentioned baselines cannot be compared directly due to different choices of input length and hyper-parameter searching strategy. For fairness, we make a great effort to provide two types of experiments. In the main text, we align the input length of all baselines and report results averaged from three repeats (see Appendix C for error bars). In Appendix, to compare the upper bound of models, we also conduct a comprehensive hyperparameter searching and report the best results in Table 14 of Appendix.\n\nImplementation details All the experiments are implemented in PyTorch (Paszke et al., 2019) and conducted on a single NVIDIA A100 80GB GPU. We utilize the L2 loss for model training. The number of scales  $M$  is set according to the time series length to trade off performance and efficiency.\n\n# 4.1 MAIN RESULTS\n\nLong-term forecasting As shown in Table 2, TimeMixer achieves consistent state-of-the-art performance in all benchmarks, covering a large variety of series with different frequencies, variate numbers and real-world scenarios. Especially, TimeMixer outperforms PatchTST by a considerable margin, with a  $9.4\\%$  MSE reduction in Weather and a  $24.7\\%$  MSE reduction in Solar-Energy. It is worth noting that TimeMixer exhibits good performance even for datasets with low forecastability, such as Solar-Energy and ETT, further proving the generality and effectiveness of TimeMixer.\n\nTable 2: Long-term forecasting results. All the results are averaged from 4 different prediction lengths, that is \\{96, 192, 336, 720\\}. A lower MSE or MAE indicates a better prediction. We fix the input length as 96 for all experiments. See Table 13 in Appendix for the full results.  \n\n<table><tr><td>Models</td><td>TimeMixer (Ours)</td><td>PatchTST (2023)</td><td>TimesNet (2023a)</td><td>Crossformer (2023)</td><td>MICN (2023)</td><td>FILM (2022a)</td><td>DLinear (2023)</td><td>FEDformer (2022b)</td><td>Stationary (2022b)</td><td>Autoformer (2021)</td><td>Informer (2021)</td></tr><tr><td>Metric</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>Weather</td><td>0.240 0.271</td><td>0.265 0.285</td><td>0.251 0.294</td><td>0.264 0.320</td><td>0.268 0.321</td><td>0.271 0.291</td><td>0.265 0.315</td><td>0.309 0.360</td><td>0.288 0.314</td><td>0.338 0.382</td><td>0.634 0.548</td></tr><tr><td>Solar-Energy</td><td>0.216 0.280</td><td>0.287 0.333</td><td>0.403 0.374</td><td>0.406 0.442</td><td>0.283 0.358</td><td>0.380 0.371</td><td>0.330 0.401</td><td>0.328 0.383</td><td>0.350 0.390</td><td>0.586 0.557</td><td>0.331 0.381</td></tr><tr><td>Electricity</td><td>0.182 0.272</td><td>0.216 0.318</td><td>0.193 0.304</td><td>0.244 0.334</td><td>0.196 0.309</td><td>0.223 0.302</td><td>0.225 0.319</td><td>0.214 0.327</td><td>0.193 0.296</td><td>0.227 0.338</td><td>0.311 0.397</td></tr><tr><td>Traffic</td><td>0.484 0.297</td><td>0.529 0.341</td><td>0.620 0.336</td><td>0.667 0.426</td><td>0.593 0.356</td><td>0.637 0.384</td><td>0.625 0.383</td><td>0.610 0.376</td><td>0.624 0.340</td><td>0.628 0.379</td><td>0.764 0.416</td></tr><tr><td>ETTh1</td><td>0.447 0.440</td><td>0.516 0.484</td><td>0.495 0.450</td><td>0.529 0.522</td><td>0.475 0.480</td><td>0.516 0.483</td><td>0.461 0.457</td><td>0.498 0.484</td><td>0.570 0.537</td><td>0.496 0.487</td><td>1.040 0.795</td></tr><tr><td>ETTh2</td><td>0.364 0.395</td><td>0.391 0.411</td><td>0.414 0.427</td><td>0.942 0.684</td><td>0.574 0.531</td><td>0.402 0.420</td><td>0.563 0.519</td><td>0.437 0.449</td><td>0.526 0.516</td><td>0.450 0.459</td><td>4.431 1.729</td></tr><tr><td>ETTm1</td><td>0.381 0.395</td><td>0.406 0.407</td><td>0.400 0.406</td><td>0.513 0.495</td><td>0.423 0.422</td><td>0.411 0.402</td><td>0.404 0.408</td><td>0.448 0.452</td><td>0.481 0.456</td><td>0.588 0.517</td><td>0.961 0.734</td></tr><tr><td>ETTm2</td><td>0.275 0.323</td><td>0.290 0.334</td><td>0.291 0.333</td><td>0.757 0.610</td><td>0.353 0.402</td><td>0.287 0.329</td><td>0.354 0.402</td><td>0.305 0.349</td><td>0.306 0.347</td><td>0.327 0.371</td><td>1.410 0.810</td></tr></table>\n\nTable 3: Short-term forecasting results in the PEMS datasets with multiple variates. All input lengths are 96 and prediction lengths are 12. A lower MAE, MAPE or RMSE indicates a better prediction.  \n\n<table><tr><td colspan=\"2\">Models</td><td>TimeMixer (Ours)</td><td>SCINet (2022a)</td><td>Crossformer (2023)</td><td>PatchTST (2023)</td><td>TimesNet (2023a)</td><td>MICN (2023)</td><td>FiLM (2022a)</td><td>DLinear FEDformer (2023)</td><td>Stationary (2022b)</td><td>Autoformer (2021)</td><td>Informer (2021)</td><td></td></tr><tr><td rowspan=\"3\">PEMS03</td><td>MAE</td><td>14.63</td><td>15.97</td><td>15.64</td><td>18.95</td><td>16.41</td><td>15.71</td><td>21.36</td><td>19.70</td><td>19.00</td><td>17.64</td><td>18.08</td><td>19.19</td></tr><tr><td>MAPE</td><td>14.54</td><td>15.89</td><td>15.74</td><td>17.29</td><td>15.17</td><td>15.67</td><td>18.35</td><td>18.35</td><td>18.57</td><td>17.56</td><td>18.75</td><td>19.58</td></tr><tr><td>RMSE</td><td>23.28</td><td>25.20</td><td>25.56</td><td>30.15</td><td>26.72</td><td>24.55</td><td>35.07</td><td>32.35</td><td>30.05</td><td>28.37</td><td>27.82</td><td>32.70</td></tr><tr><td rowspan=\"3\">PEMS04</td><td>MAE</td><td>19.21</td><td>20.35</td><td>20.38</td><td>24.86</td><td>21.63</td><td>21.62</td><td>26.74</td><td>24.62</td><td>26.51</td><td>22.34</td><td>25.00</td><td>22.05</td></tr><tr><td>MAPE</td><td>12.53</td><td>12.84</td><td>12.84</td><td>16.65</td><td>13.15</td><td>13.53</td><td>16.46</td><td>16.12</td><td>16.76</td><td>14.85</td><td>16.70</td><td>14.88</td></tr><tr><td>RMSE</td><td>30.92</td><td>32.31</td><td>32.41</td><td>40.46</td><td>34.90</td><td>34.39</td><td>42.86</td><td>39.51</td><td>41.81</td><td>35.47</td><td>38.02</td><td>36.20</td></tr><tr><td rowspan=\"3\">PEMS07</td><td>MAE</td><td>20.57</td><td>22.79</td><td>22.54</td><td>27.87</td><td>25.12</td><td>22.28</td><td>28.76</td><td>28.65</td><td>27.92</td><td>26.02</td><td>26.92</td><td>27.26</td></tr><tr><td>MAPE</td><td>8.62</td><td>9.41</td><td>9.38</td><td>12.69</td><td>10.60</td><td>9.57</td><td>11.21</td><td>12.15</td><td>12.29</td><td>11.75</td><td>11.83</td><td>11.63</td></tr><tr><td>RMSE</td><td>33.59</td><td>35.61</td><td>35.49</td><td>42.56</td><td>40.71</td><td>35.40</td><td>45.85</td><td>45.02</td><td>42.29</td><td>42.34</td><td>40.60</td><td>45.81</td></tr><tr><td rowspan=\"3\">PEMS08</td><td>MAE</td><td>15.22</td><td>17.38</td><td>17.56</td><td>20.35</td><td>19.01</td><td>17.76</td><td>22.11</td><td>20.26</td><td>20.56</td><td>19.29</td><td>20.47</td><td>20.96</td></tr><tr><td>MAPE</td><td>9.67</td><td>10.80</td><td>10.92</td><td>13.15</td><td>11.83</td><td>10.76</td><td>12.81</td><td>12.09</td><td>12.41</td><td>12.21</td><td>12.27</td><td>13.20</td></tr><tr><td>RMSE</td><td>24.26</td><td>27.34</td><td>27.21</td><td>31.04</td><td>30.65</td><td>27.26</td><td>35.13</td><td>32.38</td><td>32.97</td><td>38.62</td><td>31.52</td><td>30.61</td></tr></table>\n\nTable 4: Short-term forecasting results in the M4 dataset with a single variate. All prediction lengths are in [6, 48]. A lower SMAPE, MASE or OWA indicates a better prediction. *. in the Transformers indicates the name of *former. Stationary means the Non-stationary Transformer.  \n\n<table><tr><td colspan=\"2\">Models</td><td>TimeMixer (Ours)</td><td>TimesNet (2023a)</td><td>N-HiTS (2023)</td><td>N-BEATS* (2019)</td><td>SCINet (2022a)</td><td>PatchTST (2023)</td><td>MICN (2023)</td><td>FiLM (2022a)</td><td>LightTS (2022)</td><td>DLinear (2023)</td><td>FED. (2022b)</td><td>Stationary (2022b)</td><td>Auto. (2021)</td><td>Pyra. (2021)</td><td>In. (2021)</td></tr><tr><td rowspan=\"3\">Yearly</td><td>SMAPE</td><td>13.206</td><td>13.387</td><td>13.418</td><td>13.436</td><td>18.605</td><td>16.463</td><td>25.022</td><td>17.431</td><td>14.247</td><td>16.965</td><td>13.728</td><td>13.717</td><td>13.974</td><td>15.530</td><td>14.727</td></tr><tr><td>MASE</td><td>2.916</td><td>2.996</td><td>3.045</td><td>3.043</td><td>4.471</td><td>3.967</td><td>7.162</td><td>4.043</td><td>3.109</td><td>4.283</td><td>3.048</td><td>3.078</td><td>3.134</td><td>3.711</td><td>3.418</td></tr><tr><td>OWA</td><td>0.776</td><td>0.786</td><td>0.793</td><td>0.794</td><td>1.132</td><td>1.003</td><td>1.667</td><td>1.042</td><td>0.827</td><td>1.058</td><td>0.803</td><td>0.807</td><td>0.822</td><td>0.942</td><td>0.881</td></tr><tr><td rowspan=\"3\">Quarterly</td><td>SMAPE</td><td>9.996</td><td>10.100</td><td>10.202</td><td>10.124</td><td>14.871</td><td>10.644</td><td>15.214</td><td>12.925</td><td>11.364</td><td>12.145</td><td>10.792</td><td>10.958</td><td>11.338</td><td>15.449</td><td>11.360</td></tr><tr><td>MASE</td><td>1.166</td><td>1.182</td><td>1.194</td><td>1.169</td><td>2.054</td><td>1.278</td><td>1.963</td><td>1.664</td><td>1.328</td><td>1.520</td><td>1.283</td><td>1.325</td><td>1.365</td><td>2.350</td><td>1.401</td></tr><tr><td>OWA</td><td>0.825</td><td>0.890</td><td>0.899</td><td>0.886</td><td>1.424</td><td>0.949</td><td>1.407</td><td>1.193</td><td>1.000</td><td>1.106</td><td>0.958</td><td>0.981</td><td>1.012</td><td>1.558</td><td>1.027</td></tr><tr><td rowspan=\"3\">Monthly</td><td>SMAPE</td><td>12.605</td><td>12.670</td><td>12.791</td><td>12.677</td><td>14.925</td><td>13.399</td><td>16.943</td><td>15.407</td><td>14.014</td><td>13.514</td><td>14.260</td><td>13.917</td><td>13.958</td><td>17.642</td><td>14.062</td></tr><tr><td>MASE</td><td>0.919</td><td>0.933</td><td>0.969</td><td>0.937</td><td>1.131</td><td>1.031</td><td>1.442</td><td>1.298</td><td>1.053</td><td>1.037</td><td>1.102</td><td>1.097</td><td>1.103</td><td>1.913</td><td>1.141</td></tr><tr><td>OWA</td><td>0.869</td><td>0.878</td><td>0.899</td><td>0.880</td><td>1.027</td><td>0.949</td><td>1.265</td><td>1.144</td><td>0.981</td><td>0.956</td><td>1.012</td><td>0.998</td><td>1.002</td><td>1.511</td><td>1.024</td></tr><tr><td rowspan=\"3\">Others</td><td>SMAPE</td><td>4.564</td><td>4.891</td><td>5.061</td><td>4.925</td><td>16.655</td><td>6.558</td><td>41.985</td><td>7.134</td><td>15.880</td><td>6.709</td><td>4.954</td><td>6.302</td><td>5.485</td><td>24.786</td><td>24.460</td></tr><tr><td>MASE</td><td>3.115</td><td>3.302</td><td>3.216</td><td>3.391</td><td>15.034</td><td>4.511</td><td>62.734</td><td>5.09</td><td>11.434</td><td>4.953</td><td>3.264</td><td>4.064</td><td>3.865</td><td>18.581</td><td>20.960</td></tr><tr><td>OWA</td><td>0.982</td><td>1.035</td><td>1.040</td><td>1.053</td><td>4.123</td><td>1.401</td><td>14.313</td><td>1.553</td><td>3.474</td><td>1.487</td><td>1.036</td><td>1.304</td><td>1.187</td><td>5.538</td><td>5.879</td></tr><tr><td rowspan=\"3\">Weighted Average</td><td>SMAPE</td><td>11.723</td><td>11.829</td><td>11.927</td><td>11.851</td><td>15.542</td><td>13.152</td><td>19.638</td><td>14.863</td><td>13.525</td><td>13.639</td><td>12.840</td><td>12.780</td><td>12.909</td><td>16.987</td><td>14.086</td></tr><tr><td>MASE</td><td>1.559</td><td>1.585</td><td>1.613</td><td>1.559</td><td>2.816</td><td>1.945</td><td>5.947</td><td>2.207</td><td>2.111</td><td>2.095</td><td>1.701</td><td>1.756</td><td>1.771</td><td>3.265</td><td>2.718</td></tr><tr><td>OWA</td><td>0.840</td><td>0.851</td><td>0.861</td><td>0.855</td><td>1.309</td><td>0.998</td><td>2.279</td><td>1.125</td><td>1.051</td><td>1.051</td><td>0.918</td><td>0.930</td><td>0.939</td><td>1.480</td><td>1.230</td></tr></table>\n\n* The original paper of N-BEATS (2019) adopts a special ensemble method to promote the performance. For fair comparisons, we remove the ensemble and only compare the pure forecasting models.\n\nShort-term forecasting TimeMixer also shows great performance in short-term forecasting under both multivariate and univariate settings (Table 3-4). For PeMS benchmarks that record multiple time series of citywide traffic networks, due to the complex spatiotemporal correlations among multiple variates, many advanced models degenerate a lot in this task, such as PatchTST (2023) and DLinear (2023), which adopt the channel independence design. In contrast, TimeMixer still performs favourably in this challenging problem, verifying its effectiveness in handling complex multivariate time series forecasting. As for the M4 dataset for univariate forecasting, it contains various temporal variations under different sampling frequencies, including hourly, daily, weekly, monthly, quarterly, and yearly, which exhibits low predictability and distinctive characteristics across different frequencies. Remarkably, Timemixer consistently performs best across all frequencies, affirming the multiscale mixing architecture's capacity in modeling complex temporal variations.\n\n# 4.2 MODEL ANALYSIS\n\nAblations To verify the effectiveness of each component of TimeMixer, we provide detailed ablation study on every possible design in both Past-Decomposable-Mixing and Future-Multipredictor-Mixing blocks on all 18 experiment benchmarks. From Table 5, we have the following observations.\n\nThe exclusion of Future-Multipredictor-Mixing in ablation  $②$  results in a significant decrease in the model's forecasting accuracy for both short and long-term predictions. This demonstrates that mixing future predictions from multiscale series can effectively boost the model performance.\n\nFor the past mixing, we verify the effectiveness by removing or replacing components gradually. In ablations  $③$  and  $④$  that remove seasonal mixing and trend mixing respectively, also cause a decline\n\nTable 5: Ablations on both PDM (Decompose, Season Mixing, Trend Mixing) and FMM blocks in M4, PEMS04 and predict-336 setting of ETTm1.  $\\nearrow$  indicates the bottom-up mixing while  $\\swarrow$  indicates top-down. A check mark  $\\swarrow$  and a wrong mark  $\\times$  indicate with and without certain components respectively.  $①$  is the official design in TimeMixer (See Appendix F for complete ablation results).  \n\n<table><tr><td rowspan=\"2\">Case</td><td rowspan=\"2\">Decompose</td><td colspan=\"2\">Past mixing</td><td>Future mixing</td><td colspan=\"3\">M4</td><td colspan=\"3\">PEMS04</td><td colspan=\"2\">ETTm1</td></tr><tr><td>Seasonal</td><td>Trend</td><td>Multipredictor</td><td>SMAPE</td><td>MASE</td><td>OWA</td><td>MAE</td><td>MAPE</td><td>RMSE</td><td>MSE</td><td>MAE</td></tr><tr><td>①</td><td>✓</td><td>↗</td><td>↘</td><td>✓</td><td>11.723</td><td>1.559</td><td>0.840</td><td>19.21</td><td>12.53</td><td>30.92</td><td>0.390</td><td>0.404</td></tr><tr><td>②</td><td>✓</td><td>↗</td><td>↘</td><td>×</td><td>12.503</td><td>1.634</td><td>0.925</td><td>21.67</td><td>13.45</td><td>34.89</td><td>0.402</td><td>0.415</td></tr><tr><td>③</td><td>✓</td><td>×</td><td>↘</td><td>✓</td><td>13.051</td><td>1.676</td><td>0.962</td><td>24.49</td><td>16.28</td><td>38.79</td><td>0.411</td><td>0.427</td></tr><tr><td>④</td><td>✓</td><td>↗</td><td>×</td><td>✓</td><td>12.911</td><td>1.655</td><td>0.941</td><td>22.91</td><td>15.02</td><td>37.04</td><td>0.405</td><td>0.414</td></tr><tr><td>⑤</td><td>✓</td><td>↘</td><td>↘</td><td>✓</td><td>12.008</td><td>1.628</td><td>0.871</td><td>20.78</td><td>13.02</td><td>32.47</td><td>0.392</td><td>0.413</td></tr><tr><td>⑥</td><td>✓</td><td>↗</td><td>↗</td><td>✓</td><td>11.978</td><td>1.626</td><td>0.859</td><td>21.09</td><td>13.78</td><td>33.11</td><td>0.396</td><td>0.415</td></tr><tr><td>⑦</td><td>✓</td><td>↘</td><td>↗</td><td>✓</td><td>13.012</td><td>1.657</td><td>0.954</td><td>22.27</td><td>15.14</td><td>34.67</td><td>0.412</td><td>0.429</td></tr><tr><td>⑧</td><td>×</td><td>↗</td><td></td><td>✓</td><td>11.975</td><td>1.617</td><td>0.851</td><td>21.51</td><td>13.47</td><td>34.81</td><td>0.395</td><td>0.408</td></tr><tr><td>⑨</td><td>×</td><td>↘</td><td></td><td>✓</td><td>11.973</td><td>1.622</td><td>0.850</td><td>21.79</td><td>14.03</td><td>35.23</td><td>0.393</td><td>0.406</td></tr><tr><td>⑩</td><td>×</td><td>×</td><td></td><td>✓</td><td>12.468</td><td>1.671</td><td>0.916</td><td>24.87</td><td>16.66</td><td>39.48</td><td>0.405</td><td>0.412</td></tr></table>\n\nof performance. This illustrates that solely relying on seasonal or trend information interaction is insufficient for accurate predictions. Furthermore, in both ablations  $⑤$  and  $⑥$ , we employed the same mixing approach for both seasons and trends. However, it cannot bring better predictive performance. Similar situation occurs in  $⑦$  that adopts opposite mixing strategies to our design. These results demonstrate the effectiveness of our design in both bottom-up seasonal mixing and top-down trend mixing. Concurrently, in ablations  $⑧$  and  $⑨$ , we opted to eliminate the decomposition architecture and mix the multiscale series directly. However, without decomposition, neither bottom-up nor top-down mixing method can achieve a good performance, indicating the necessity of season-trend separate mixing. Furthermore, in ablations  $⑩$ , eliminating the entire Past-Decomposable-Mixing block causes a serious drop in the model's predictive performance. The above findings highlight the substantial influence of an appropriate past mixing method on the final performance of the model. Starting from the insights in time series, TimeMixer presents the best mixing method in past information extraction.\n\nSeasonal and trend mixing visualization To provide an intuitive understanding of PDM, we visualize temporal linear weights for seasonal mixing and trend mixing in Figure 3(a)~(b). We find that the seasonal and trend items present distinct mixing properties, where the seasonal mixing layer presents periodic changes (repeated blue lines in (a)) and the trend mixing layer is dominated by local aggregations (the dominating diagonal yellow line in (b)). This also verifies the necessity of adopting separate mixing techniques for seasonal and trend terms. Furthermore, Figure 3(c) shows the predictions of season and trend terms in fine (scale 0) and coarse (scale 3) scales. We can observe that the seasonal terms of fine-scale and trend parts of coarse-scale are crucial for accurate predictions. This observation provides insights for our design in utilizing bottom-up mixing for seasonal terms and top-down mixing for trend components.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/3f6623f55f32affeb0459be44f8254f3cfd61dbbfe451352837f4e6d4bfb711e.jpg)  \nFigure 3: Visualization of temporal linear weights in seasonal mixing (Eq. 4), trend mixing (Eq. 5), and predictions from multiscale season-trend items. All the experiments are on the ETTh1 dataset under the input-96-predict-96 setting.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/416c572200734c1558ba1ea3971b1da40392d2d0db4ed863b803c5adc092249c.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/e7f04a76ac6516c857e4c2a6d3654fd00cc03af751bb2b53e98028b3e8afc82b.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/c65c6ca3d0ee7584a949a66a7c9b64866567c7ca4f5215ed3abaaf7385b326d0.jpg)  \n(a) Multiscale mixing\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/1c2e3ab416d20290dc7cd06d798e9b6f98965a21da6bee5b24e8f1a87b9f86e2.jpg)  \n(b) Scale 0\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/b33479c1797636a6e1e22fb6c86e1d70876b195837db0ab027d3080ae60388ff.jpg)  \n(c) Scale 1\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/ff809841acaea7bf7e73faaacaac818956f612f7d01a9294265395a7e322fc0b.jpg)  \n(d) Scale 2\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/3f92261514b0ad5ed7280148c9bfec2ecf5c3150d21055d491d2c567b5578fb7.jpg)  \n(e) Scale 3\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/93cd0585c28bebf59da348eea10bbe6cfd2a1ab8e86ef96f4b816e285dc6edcd.jpg)  \nFigure 4: Visualization of predictions from different scales  $(\\widehat{\\mathbf{x}}_m^L$  in Eq. 6) on the input-96-predict-96 settings of the ETTh1 dataset. The implementation details are included in Appendix A.  \n(a) Memory Efficiency Analysis  \nFigure 5: Efficiency analysis in both GPU memory and running time. The results are recorded on the ETTh1 dataset with batch size as 16. The running time is averaged from  $10^{2}$  iterations.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/2880984a63465b7be6f5fcb6ec7b4e441ad38a8b0adeab00e348123337ca54bf.jpg)  \n(b) Running Time Efficiency Analysis\n\nMultipredictor visualization To provide an intuitive understanding of the forecasting skills of multiscale series, we plot the forecasting results from different scales for qualitative comparison. Figure 4(a) presents the overall prediction of our model with Future-Multipredictor-Mixing, which indicates accurate prediction according to the future variations using mixed scales. To study the component of each individual scale, we demonstrate the prediction results for each scale in Figure  $4(\\mathrm{b})\\sim (\\mathrm{e})$  . Specifically, prediction results from fine-scale time series concentrate more on the detailed variations of time series and capture seasonal patterns with greater precision. In contrast, as shown in Figure  $4(\\mathrm{c})\\sim (\\mathrm{e})$  , with multiple downsampling, the predictions from coarse-scale series focus more on macro trends. The above results also highlight the benefits of Future-Multipredictor-Mixing in utilizing complementary forecasting skills from multiscale series.\n\nEfficiency analysis We compare the running memory and time against the latest state-of-the-art models in Figure 5 under the training phase, where TimeMixer consistently demonstrates favorable efficiency, in terms of both GPU memory and running time, for various series lengths (ranging from 192 to 3072), in addition to the consistent state-of-the-art performances for both long-term and short-term forecasting tasks.\n\nAnalysis on number of scales We explore the impact from the number of scales  $(M)$  in Figure 6 under different series lengths. Specifically, when  $M$  increases, the performance gain declines for shorter prediction lengths. In contrast, for longer\n\nprediction lengths, the performance improves more as  $M$  increases. Therefore, we set  $M$  as 3 for long-term forecast and 1 for short-term forecast to trade off performance and efficiency.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/a0619f25-2376-45b0-839f-fd6cf2168d41/566f940e509d317cb502fdc67cdc139a00b702278d42503ef7ce297b289a1a3b.jpg)  \nFigure 6: Analysis on number of scales on ETTm1 dataset.",
  "hyperparameter": "Number of scales M: 3 for long-term forecasting, 1 for short-term forecasting (adjusted based on series length to balance performance and efficiency); Number of PDM layers L: not explicitly specified but uses stacked blocks; Model dimension d_model: not explicitly specified; Input length P: 96 for most experiments; Prediction lengths F: {96, 192, 336, 720} for long-term forecasting, 12 for PEMS datasets, {6-48} for M4 dataset; Batch size: 16 (used in efficiency analysis); Loss function: L2 loss; Optimizer: not specified; Learning rate: not specified; The model uses average pooling for downsampling with scale factor 2^m, where m ∈ {0,...,M}; FeedForward contains two linear layers with intermediate GELU activation; Bottom-Up-Mixing and Top-Down-Mixing use two linear layers with intermediate GELU activation."
}