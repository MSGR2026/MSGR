[
  {
    "source": "Informer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer encoder-decoder paradigm for time series forecasting. The source code's basic encoder-decoder structure (Encoder/Decoder classes with stacked layers) can be directly reused as the architectural skeleton. The DataEmbedding module for input representation and the overall forward flow (enc_embedding → encoder → dec_embedding → decoder) are directly transferable.\n\n2. **Instance Normalization Strategy**: Both apply instance normalization (mean subtraction and standard deviation division) on the encoder input before processing, with denormalization applied to final predictions. The normalization code in Informer's `short_forecast` method (computing mean_enc, std_enc, and applying them) can be directly copied to Autoformer's implementation.\n\n3. **Multi-Head Attention Framework**: Both use multi-head attention mechanisms with similar projection structures (query, key, value projections followed by output projection). The AttentionLayer class structure with d_model, n_heads, d_keys, d_values parameters can serve as a template, though the inner attention mechanism differs significantly.\n\n4. **Decoder Input Construction**: Both use a similar decoder initialization strategy with start tokens from the latter half of encoder input concatenated with placeholders for the prediction horizon. Informer's approach in Eq.(6) with X_token and X_0 directly maps to Autoformer's Eq.(2) with X_ens/X_ent and placeholders, allowing code reuse for this input preparation logic.\n\n5. **Layer Normalization and Feed-Forward Networks**: Both employ LayerNorm and feed-forward networks (Conv1d-based) within encoder/decoder layers. The DecoderLayer's conv1/conv2 structure and normalization layers can be adapted, though Autoformer adds series decomposition blocks between operations.\n\n6. **Long Sequence Forecasting Focus**: Both target LSTF problems with O(L log L) complexity mechanisms. The overall training loop, loss computation (MSE), and prediction extraction ([:, -pred_len:, :]) patterns are identical and directly reusable.",
    "differences": "1. **Core Mechanism - Auto-Correlation vs. ProbSparse Attention**: Autoformer's fundamental innovation is the Auto-Correlation mechanism (Eq. 5-7) that discovers period-based dependencies via FFT-based autocorrelation and time delay aggregation, completely replacing Informer's ProbSparse attention. NEW IMPLEMENTATION NEEDED: (a) FFT-based autocorrelation computation using Wiener-Khinchin theorem (Eq. 8), (b) Top-k period selection based on autocorrelation values, (c) Roll operation for time delay aggregation, (d) Softmax-weighted aggregation of rolled series. This requires implementing autocorrelation calculation, period detection logic, and series-wise (not point-wise) aggregation.\n\n2. **Series Decomposition Architecture**: Autoformer introduces series decomposition blocks as built-in operations throughout the model (Eq. 1, Figure 1), separating seasonal and trend-cyclical components at every layer. NEW IMPLEMENTATION NEEDED: (a) SeriesDecomp module with AvgPool-based moving average for trend extraction, (b) Integration of decomposition after each attention and feed-forward block in encoder (Eq. 3), (c) Dual-path decoder processing seasonal and trend components separately (Eq. 4), (d) Trend accumulation structure with learnable projectors W_l,i, (e) Final prediction as sum of refined seasonal (W_S * X_de^M) and accumulated trend (T_de^M). This is entirely absent in Informer.\n\n3. **Encoder Design Philosophy**: Informer uses self-attention distilling with ConvLayer and MaxPool for progressive dimension reduction (Eq. 5), creating a pyramid structure with halving inputs. Autoformer's encoder focuses solely on seasonal modeling with decomposition blocks eliminating trends at each layer (Eq. 3), without dimension reduction. NEW IMPLEMENTATION: Remove Informer's distilling layers and pyramid structure; implement decomposition-based encoder that preserves sequence length and outputs only seasonal information.\n\n4. **Decoder Accumulation vs. Generative Inference**: Informer uses generative-style one-forward prediction with masked attention. Autoformer implements a progressive trend accumulation mechanism (T_de^l = T_de^(l-1) + weighted sum of extracted trends) alongside seasonal refinement. NEW IMPLEMENTATION: (a) Dual initialization for seasonal (X_des) and trend (X_det) components, (b) Three decomposition blocks per decoder layer extracting trends at each stage, (c) Weighted trend accumulation across layers with separate projectors, (d) Separate processing paths for seasonal (through Auto-Correlation) and trend (through accumulation).\n\n5. **Information Aggregation Paradigm**: Informer aggregates point-wise via dot-product weighted values in attention. Autoformer aggregates sub-series-level information by rolling entire value series based on detected periods and combining them. NEW IMPLEMENTATION: Replace point-wise softmax(QK^T)V aggregation with series-wise Roll(V, τ_i) operations weighted by autocorrelation confidences, fundamentally changing how temporal information is combined.\n\n6. **Dependency Discovery Mechanism**: Informer discovers dependencies through query sparsity measurement (Eq. 2, 4) selecting top-u queries based on KL divergence from uniform distribution. Autoformer discovers dependencies through autocorrelation (Eq. 5) identifying periodic patterns. NEW IMPLEMENTATION: Replace the entire ProbSparse logic (_prob_QK, sparsity measurement M(q_i, K), max-mean approximation) with FFT-based autocorrelation computation and top-k period selection, requiring signal processing operations not present in Informer.\n\n7. **Complexity Source**: Informer achieves O(L log L) through sparse query selection (sampling U = L_K ln L_Q pairs). Autoformer achieves O(L log L) through FFT-based autocorrelation computation and aggregating O(log L) rolled series. NEW IMPLEMENTATION: FFT/IFFT operations for efficient autocorrelation across all lags, which is a completely different complexity reduction approach requiring numpy.fft or torch.fft integration."
  },
  {
    "source": "Reformer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-based Architecture Foundation**: Both papers build upon the Transformer encoder architecture for time series processing. The source code's `Encoder` and `EncoderLayer` structures with feed-forward networks and normalization can be directly reused. The basic model skeleton including `enc_embedding`, multi-layer encoder stacking, and projection layers provides a strong foundation for Autoformer's implementation.\n\n2. **Embedding and Normalization Strategy**: Both models use similar data embedding mechanisms (`DataEmbedding` with positional encoding) and instance normalization for input preprocessing. The source code's normalization approach (mean subtraction and std division: `x_enc = (x_enc - mean_enc) / std_enc`) and denormalization (`dec_out * std_enc + mean_enc`) can be directly adopted for Autoformer's short-term forecasting task.\n\n3. **Sequence Length Handling**: Both papers address the challenge of processing variable-length sequences. Reformer's `fit_length` method that pads sequences to meet bucketing requirements (multiples of `bucket_size * 2`) demonstrates a practical approach to sequence length management that can inform Autoformer's implementation, though Autoformer uses different padding strategies for trend-seasonal decomposition.\n\n4. **Multi-head Attention Framework**: Both utilize multi-head attention mechanisms as core components. While Reformer uses LSH attention and Autoformer uses Auto-Correlation, the multi-head structure with head concatenation and output projection (`MultiHead` operation with `Concat` and `W_output`) is conceptually similar. The attention layer interface (queries, keys, values input/output) in the source code can serve as a template for implementing Auto-Correlation layers.\n\n5. **Short-term Forecasting Setup**: Both models follow the input-I-predict-O paradigm for time series forecasting. The source code's `short_forecast` method demonstrates the workflow: input sequence processing, encoder transformation, and projection to prediction horizon. This structure (concatenating input with placeholder predictions, processing through encoder, final projection) can be adapted for Autoformer's encoder-decoder architecture.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses Locality-Sensitive Hashing (LSH) attention with random projections and hash bucketing (`LSHSelfAttention` with `bucket_size`, `n_hashes`), while Autoformer introduces Auto-Correlation mechanism based on Fast Fourier Transform (FFT) for period-based dependencies discovery. **Must implement**: (a) FFT-based autocorrelation computation using Wiener-Khinchin theorem (`R_XX(τ) = F^-1(S_XX(f))`), (b) Top-k period selection (`argTopk` on autocorrelation values), (c) Time delay aggregation with `Roll` operation to align sub-series, (d) Softmax-weighted aggregation of rolled series. This is entirely different from LSH's hash-based nearest neighbor search.\n\n2. **Decomposition Architecture - NEW IMPLEMENTATION REQUIRED**: Autoformer introduces series decomposition as a built-in operation throughout the model, which is completely absent in Reformer. **Must implement**: (a) `SeriesDecomp` block using moving average (`AvgPool` with padding) to extract trend-cyclical component and seasonal component (`X_t = AvgPool(Padding(X))`, `X_s = X - X_t`), (b) Progressive trend accumulation in decoder with weighted sum (`T_de^l = T_de^(l-1) + W_l,1*T_de^l,1 + ...`), (c) Decomposition blocks after each attention and feed-forward layer in both encoder and decoder, (d) Separate handling and refinement of seasonal and trend components throughout the architecture.\n\n3. **Encoder-Decoder Architecture - NEW IMPLEMENTATION REQUIRED**: Reformer uses encoder-only architecture with simple projection for forecasting, while Autoformer employs a sophisticated encoder-decoder structure with cross-attention. **Must implement**: (a) Decoder with dual-component initialization (`X_des` from seasonal part, `X_det` from trend part with mean padding), (b) Three-stage decoder layer: inner Auto-Correlation, encoder-decoder Auto-Correlation, and feed-forward with decomposition after each stage, (c) Cross-attention mechanism where decoder queries attend to encoder outputs for utilizing past seasonal information, (d) Final prediction combining refined seasonal component (`W_S * X_de^M`) and accumulated trend component (`T_de^M`).\n\n4. **Computational Complexity and Efficiency**: Reformer achieves O(L log L) complexity through LSH bucketing and chunking (with `bucket_size=4`, `n_hashes=4` creating sparse attention patterns), while Autoformer achieves O(L log L) through FFT-based autocorrelation and selecting O(log L) time delays. **Must implement**: (a) FFT-based efficient autocorrelation for all lags simultaneously (Equation 8), (b) Sparse aggregation selecting only top-k=⌊c×log L⌋ periods rather than all L positions, (c) Series-wise connections aggregating entire sub-series rather than point-wise attention weights. The efficiency mechanisms are fundamentally different: LSH uses spatial hashing while Auto-Correlation uses frequency domain transformation.\n\n5. **Input Processing and Prediction Strategy**: Reformer concatenates input sequence with zero-padded future placeholders and processes them jointly through the encoder (`x_enc = torch.cat([x_enc, x_dec[:, -pred_len:, :]])`), while Autoformer uses separate encoder-decoder processing with carefully initialized decoder inputs. **Must implement**: (a) Split input into encoder part (full I steps) and decoder initialization (I/2 steps for recent info + O placeholders), (b) Decompose the latter half of encoder input to initialize decoder's seasonal and trend components (Equation 2), (c) Use zero padding for seasonal placeholders and mean padding for trend placeholders, (d) Progressive refinement through decoder layers rather than single-pass encoding. This represents a fundamentally different prediction paradigm: joint processing vs. sequential encoder-decoder refinement."
  },
  {
    "source": "DLinear_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt the DMS approach where the model directly predicts the entire future horizon T in one shot, avoiding autoregressive error accumulation. The DLinear implementation's `forecast()` method structure can be reused as the backbone for PatchTST's prediction pipeline.\n\n2. **Series Decomposition for Trend-Seasonal Separation**: Both leverage decomposition techniques to handle time series components. DLinear's `series_decomp` module with moving average kernel can be directly reused in PatchTST for preprocessing or as part of the architecture, particularly for datasets with strong trend components.\n\n3. **Channel Independence Modeling**: Both support channel-independent (CI) modeling where each variate is processed separately. DLinear's `individual=True` mode with `nn.ModuleList()` for per-channel linear layers provides a direct template for implementing PatchTST's CI variant, requiring only replacement of linear layers with patch-based Transformer blocks.\n\n4. **Normalization Techniques**: Both papers emphasize input normalization to handle distribution shifts across datasets. DLinear's NLinear variant (subtracting last value) demonstrates instance normalization that can be adapted for PatchTST's RevIN (Reversible Instance Normalization) implementation.\n\n5. **Shared Weight Initialization Strategy**: DLinear initializes weights as `(1/seq_len) * torch.ones()` for uniform averaging baseline. This initialization philosophy can inform PatchTST's linear projection layers in the patch embedding module.\n\n6. **Task-Agnostic Architecture**: Both models use `configs` object for flexible hyperparameter management (seq_len, pred_len, enc_in). This configuration system in DLinear's `__init__()` can be directly extended for PatchTST's additional parameters (patch_len, stride, d_model, n_heads).",
    "differences": "1. **Core Architecture - Patch-based Transformer vs Pure Linear**: PatchTST requires implementing a complete Transformer encoder with multi-head self-attention, which is entirely absent in DLinear. NEW COMPONENTS NEEDED: (a) Patch segmentation module to divide time series into subsequences, (b) Patch embedding layer with linear projection, (c) Positional encoding for patch positions, (d) Multi-head self-attention layers, (e) Feed-forward networks, (f) Layer normalization, (g) Residual connections in Transformer blocks.\n\n2. **Patching Mechanism for Local Semantic Extraction**: PatchTST's key innovation is treating subseries (patches) as tokens to capture local semantic information. NEW IMPLEMENTATION: A patching module that splits input sequence [B, L, C] into patches [B, N, P, C] where N=number of patches, P=patch length, with configurable stride. This involves reshaping operations and handling boundary conditions not present in DLinear's direct temporal modeling.\n\n3. **Multi-Scale Temporal Modeling**: PatchTST captures dependencies at patch-level granularity rather than point-level. NEW COMPONENTS: (a) Configurable patch_len and stride hyperparameters to control temporal resolution, (b) Attention mechanism operating on patch embeddings rather than raw time points, (c) Aggregation strategy to combine patch-level representations for final predictions. DLinear only operates at single point-level scale.\n\n4. **Attention-Based Dependency Modeling**: PatchTST uses self-attention to model dependencies between patches across the sequence. NEW IMPLEMENTATION: (a) Query, Key, Value projections for each attention head, (b) Scaled dot-product attention computation with softmax, (c) Multi-head concatenation and output projection, (d) Attention masking strategies if needed. DLinear uses only weighted summation without learned attention.\n\n5. **Embedding Dimensions and Representation Learning**: PatchTST projects patches into high-dimensional embedding space (d_model, typically 128-512) for richer representations. NEW COMPONENTS: (a) Linear projection from patch dimension to d_model, (b) Learnable positional embeddings for patch positions, (c) Dimension management throughout encoder layers. DLinear operates directly in input space without embedding.\n\n6. **Computational Complexity Trade-off**: PatchTST has O(N²·d) complexity where N=num_patches << L=seq_len, making it more efficient than vanilla Transformer O(L²·d) but still higher than DLinear's O(L·T). NEW CONSIDERATION: Need to implement efficient attention computation, potentially with optimizations like FlashAttention, and manage GPU memory for attention matrices.\n\n7. **Head Flattening Strategy**: PatchTST uses a flatten head that concatenates outputs from all attention heads before final projection. NEW IMPLEMENTATION: A specialized decoder that takes multi-head outputs [B, N, n_heads, d_head], flattens to [B, N, d_model], and projects to prediction horizon, different from DLinear's simple linear projection.\n\n8. **Training Dynamics and Regularization**: PatchTST requires Transformer-specific training techniques. NEW COMPONENTS: (a) Dropout in attention layers and FFN, (b) Learning rate scheduling suitable for Transformers (warmup + decay), (c) Gradient clipping for stability, (d) Potentially pre-layer normalization for better convergence. DLinear's simple linear layers require minimal regularization."
  },
  {
    "source": "FEDformer_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Embedding Infrastructure**: Both papers use similar embedding mechanisms for input sequences. FEDformer's `DataEmbedding` class (combining value embedding, positional encoding, and temporal encoding) can be directly reused for PatchTST's initial input processing before patching operations.\n\n2. **Normalization Strategy**: Both models benefit from instance normalization techniques to handle distribution shifts across different time series. FEDformer's normalization approach (computing mean statistics) in the `forecast` method can be adapted for PatchTST's RevIN (Reversible Instance Normalization) implementation.\n\n3. **Multi-head Attention Framework**: While the attention mechanisms differ, both use multi-head attention structures. FEDformer's multi-head setup (with `n_heads` parameter) and the general attention computation flow provide a structural template that can be adapted for PatchTST's channel-independent multi-head self-attention.\n\n4. **Training Infrastructure**: Both models use similar training loops, loss functions (MSE), and optimization strategies. FEDformer's training pipeline, including the forward pass structure and batch processing, can serve as a baseline for PatchTST implementation.\n\n5. **Decoder-free Forecasting Option**: FEDformer's encoder can operate independently, similar to PatchTST's encoder-only architecture. The encoder structure in FEDformer (with layer stacking and normalization) provides a reference for building PatchTST's Transformer encoder backbone.",
    "differences": "1. **Core Architecture - Patching Mechanism**: PatchTST requires implementing a NEW patch extraction module that segments continuous time series into fixed-length patches (subseries). This patching operation (splitting sequence of length L into N patches of length P) is NOT present in FEDformer, which operates on point-wise representations. Need to implement: patch embedding layer, patch positional encoding, and patch-level tokenization.\n\n2. **Attention Paradigm Shift**: PatchTST uses standard self-attention on patches in the TIME domain, while FEDformer operates in FREQUENCY domain (Fourier/Wavelet transforms). Must implement: (a) vanilla scaled dot-product attention without frequency transforms, (b) remove all Fourier-related components (FourierBlock, FourierCrossAttention, get_frequency_modes), (c) remove wavelet components (MultiWaveletTransform, MWT_CZ1d, get_filter).\n\n3. **Channel Independence Strategy**: PatchTST processes each channel (variate) independently with separate model instances or channel-wise operations, then aggregates predictions. FEDformer processes all channels jointly through its d_model dimension. Need to implement: (a) channel-wise data splitting mechanism, (b) independent Transformer processing per channel, (c) optional channel aggregation strategies.\n\n4. **No Decomposition Architecture**: PatchTST does NOT use seasonal-trend decomposition (series_decomp, MOEDecomp) that is central to FEDformer. Must remove: (a) all decomposition blocks, (b) separate seasonal/trend pathways, (c) trend initialization in decoder. PatchTST uses direct end-to-end mapping from patches to predictions.\n\n5. **Encoder-Only vs Encoder-Decoder**: PatchTST uses a pure encoder-only architecture with direct projection to forecast horizon, while FEDformer uses encoder-decoder with cross-attention. Need to implement: (a) remove entire decoder structure, (b) implement simple linear projection head from encoded patch representations to prediction length, (c) modify forward pass to eliminate decoder input preparation (x_dec, seasonal_init, trend_init).\n\n6. **Sequence Length Handling**: PatchTST's effective sequence length is N (number of patches) rather than L (original length), enabling longer lookback with reduced computational cost. Need to implement: (a) patch-based positional encoding (positions 1 to N, not 1 to L), (b) stride mechanism for overlapping patches if needed, (c) reshape operations to convert between patch representation and time series format.\n\n7. **Prediction Head Design**: PatchTST requires a NEW flattening and projection mechanism: flatten all patch embeddings, then project to prediction horizon. FEDformer uses decoder output directly. Must implement: (a) patch representation flattening (N patches × d_model → N×d_model), (b) linear projection layer (N×d_model → pred_len × num_channels), (c) reshape to final output format."
  },
  {
    "source": "Pyraformer_2021",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Stationarity**: Both papers apply instance normalization (mean subtraction and standard deviation division) across the temporal dimension before feeding data into the model, then denormalize predictions. The normalization code in Pyraformer's `short_forecast` method (mean_enc, std_enc calculation and application) can be directly reused for PatchTST.\n2. **Transformer-based Architecture Foundation**: Both leverage Transformer architecture components including multi-head attention mechanisms, feed-forward networks, and layer normalization. The `PositionwiseFeedForward` class and basic attention infrastructure from Pyraformer can serve as foundational building blocks.\n3. **Embedding Strategy**: Both use data embedding combined with temporal covariates (x_mark_enc) for input representation. The `DataEmbedding` approach in Pyraformer (combining value embedding with temporal features) provides a reusable pattern.\n4. **Short-term Forecasting Focus**: Both target short-term time series forecasting tasks with fixed prediction horizons, using encoder-based architectures that process historical sequences to generate future predictions.\n5. **Direct Multi-step Prediction**: Both avoid autoregressive decoding by predicting all future time steps simultaneously through a projection layer, reducing error accumulation. Pyraformer's projection approach (Linear layer mapping to pred_len * enc_in) demonstrates this pattern.",
    "differences": "1. **Core Innovation - Patching vs Pyramidal Attention**: PatchTST introduces patch-based tokenization where continuous time steps are grouped into patches (subseries), requiring NEW implementation of: (a) patch extraction module to segment sequences into fixed-length patches with stride, (b) patch embedding layer to project each patch to d_model dimension, (c) modified positional encoding for patch positions rather than time step positions. Pyraformer uses pyramidal multi-resolution attention with C-ary tree structure which is fundamentally different.\n2. **Attention Mechanism Architecture**: PatchTST uses standard full self-attention among patches (O(N²) where N is number of patches, much smaller than sequence length L), while Pyraformer implements custom pyramidal attention (PAM) with sparse connections following a hierarchical graph structure. PatchTST needs STANDARD PyTorch attention implementation, NOT the custom CUDA kernel and mask generation (get_mask, refer_points) used in Pyraformer.\n3. **Multi-scale Modeling Approach**: Pyraformer explicitly constructs multi-resolution representations through CSCM (Bottleneck_Construct with ConvLayer modules performing downsampling at different scales). PatchTST achieves implicit multi-scale through patch size selection but does NOT require the explicit coarser-scale construction module - the entire CSCM component and conv_layers can be removed.\n4. **Channel Independence Strategy**: PatchTST typically employs channel-independent modeling where each variate is processed separately and the model learns univariate patterns, requiring implementation of channel-wise iteration or reshaping. Pyraformer processes all channels jointly through its encoder without explicit channel separation.\n5. **Encoder Architecture Simplification**: PatchTST uses a simpler encoder with standard Transformer blocks (standard attention + FFN), eliminating Pyraformer's complex components: (a) NO hierarchical mask generation, (b) NO inter-scale and intra-scale connection logic, (c) NO pyramid index gathering (refer_points), (d) standard attention layers instead of custom PAM. The entire Encoder class needs reimplementation with vanilla Transformer layers.\n6. **Projection Head Design**: PatchTST requires a NEW projection strategy that maps from patch-level representations back to time-step-level predictions, typically through: (a) flattening patch embeddings, (b) a projection layer mapping to pred_len outputs per channel. This differs from Pyraformer's approach of gathering multi-scale features and projecting concatenated representations.\n7. **Positional Encoding Adaptation**: PatchTST needs positional encoding for patch positions (1 to num_patches) rather than Pyraformer's time-step level positions (1 to seq_len), requiring modification of the positional embedding component."
  },
  {
    "source": "Informer_2020",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-only architecture for forecasting**: Both papers can utilize encoder-based architectures for direct forecasting without autoregressive decoding. PatchTST can reuse Informer's EncoderLayer structure, including the feed-forward network (Conv1d layers with activation), LayerNorm, and residual connections from the source code.\n2. **Embedding mechanisms**: Both require input embedding with positional encoding. PatchTST can adapt Informer's DataEmbedding class, particularly the temporal encoding components (time features) and the linear projection logic, though patch-based tokenization will replace point-wise embedding.\n3. **Instance normalization**: Both papers apply instance normalization (reversible normalization) to handle distribution shift across instances. The normalization code in Informer's short_forecast method (mean/std computation and denormalization) can be directly reused: `mean_enc = x_enc.mean(1, keepdim=True).detach()` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()`.\n4. **Multi-head attention framework**: Both use multi-head attention as core component. PatchTST can reuse the AttentionLayer structure including query/key/value projections and output projection, replacing only the inner attention mechanism (ProbAttention → standard self-attention).\n5. **Training objective**: Both use MSE loss for forecasting and follow similar training paradigms with teacher forcing on historical data. The forward pass structure (encoding → prediction → denormalization) can be adapted.\n6. **Batch processing and tensor operations**: Similar tensor manipulation patterns (B, L, D format) allow direct adaptation of batch processing logic, particularly the reshape operations in attention layers.",
    "differences": "1. **Core innovation - Patching mechanism**: PatchTST introduces patch-based tokenization where the input sequence is divided into non-overlapping patches, fundamentally different from Informer's point-wise tokens. NEW IMPLEMENTATION NEEDED: A patching module that reshapes input [B, L, D] → [B, N_patches, patch_len*D] where N_patches = L//patch_len, and a corresponding patch embedding layer.\n2. **Attention mechanism**: Informer uses ProbSparse attention (O(L log L) complexity with query sparsity measurement via LSE and random sampling), while PatchTST uses standard full self-attention on patches (O(N²) where N is number of patches << L). NEW IMPLEMENTATION NEEDED: Replace ProbAttention with standard scaled dot-product attention: `attention = softmax(QK^T/√d)V` without sparsity mechanisms, removing _prob_QK, _get_initial_context, and _update_context methods.\n3. **Channel independence strategy**: PatchTST processes each channel (variate) independently with separate model instances or channel-wise processing, while Informer processes all channels jointly in multivariate mode. NEW IMPLEMENTATION NEEDED: Channel-independent processing loop or batch dimension expansion to treat channels as separate samples.\n4. **Decoder elimination**: Informer uses encoder-decoder architecture with masked attention in decoder and cross-attention between encoder-decoder, while PatchTST uses encoder-only architecture. NEW IMPLEMENTATION NEEDED: Direct prediction head from encoder output to forecast horizon, removing the entire Decoder, DecoderLayer, and start token logic from Informer.\n5. **Distilling operation removal**: Informer uses self-attention distilling with ConvLayer (1D convolution + MaxPooling) to progressively reduce sequence length in encoder, while PatchTST maintains constant patch sequence length. REMOVE: ConvLayer modules and the distilling logic in encoder stack; PatchTST encoder layers have uniform input/output dimensions.\n6. **Positional encoding for patches**: PatchTST requires positional encoding for patch positions (not time points), different from Informer's time-stamp based positional encoding. NEW IMPLEMENTATION NEEDED: Learnable or fixed positional embeddings for N_patches positions instead of L time-step positions.\n7. **Output projection strategy**: Informer projects from d_model to c_out at decoder's final layer, while PatchTST needs to project from [B, N_patches, d_model] to [B, pred_len, c_out], requiring unpacking of patch representations. NEW IMPLEMENTATION NEEDED: A flatten or reshape operation followed by linear projection: patches → flattened sequence → prediction horizon, potentially with additional linear layers to map N_patches*d_model → pred_len*c_out.\n8. **Complexity and efficiency focus**: Informer focuses on reducing O(L²) complexity for LONG sequences via sparse attention, while PatchTST reduces complexity by working with fewer patches (N << L) using standard attention. The efficiency gain comes from different sources (sparsity vs. tokenization), requiring completely different optimization strategies."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture**: Both papers adopt encoder-decoder Transformer architectures for time series forecasting. The source code's `EncoderLayer` with self-attention and position-wise FFN can be adapted as a base structure, though Nonstationary Transformer replaces standard attention with De-stationary Attention.\n\n2. **Instance Normalization for Stationarization**: Both employ normalization along the temporal dimension to handle non-stationarity. Pyraformer's code shows `mean_enc = x_enc.mean(1, keepdim=True)` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1))` in `short_forecast()`, which directly implements the normalization operation. This exact normalization logic (Equation 1 in Nonstationary Transformer) can be reused, though Nonstationary Transformer extends it with learnable de-stationary factors.\n\n3. **De-normalization for Output**: Both restore original statistics to predictions. Pyraformer's `dec_out = dec_out * std_enc + mean_enc` implements the de-normalization step, matching Equation 2 in Nonstationary Transformer. This code block is directly transferable.\n\n4. **Embedding Layer Structure**: Both use `DataEmbedding` to combine observation, covariate, and positional embeddings before feeding into the encoder. The source code's `self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.dropout)` provides a reusable component for initial input processing.\n\n5. **Multi-head Attention Mechanism**: Both leverage multi-head attention (configs.n_heads) to capture temporal dependencies from different perspectives. The attention computation framework with Q, K, V projections is shared, though the specific attention patterns differ (pyramidal vs. de-stationary).\n\n6. **Layer Normalization and Residual Connections**: Pyraformer's `PositionwiseFeedForward` includes `self.layer_norm = nn.LayerNorm(d_in)` and residual connections (`x = x + residual`), which are standard Transformer components also used in Nonstationary Transformer.\n\n7. **Batch Processing and GPU Support**: The source code's `.to(x_enc.device)` and batch dimension handling (B x L x D) provide a template for efficient batch processing that Nonstationary Transformer requires.",
    "differences": "1. **Core Innovation - Attention Mechanism**: Pyraformer introduces Pyramidal Attention Module (PAM) with multi-scale C-ary tree structure and sparse attention masks (`get_mask()`, `refer_points()`), achieving O(L) complexity. Nonstationary Transformer proposes De-stationary Attention that learns scaling factor τ and shifting vector Δ from unstationarized series statistics (Equation 6). **NEW IMPLEMENTATION NEEDED**: MLP projectors to learn de-stationary factors from μ_x and σ_x, and modified attention computation `Softmax((τ * Q'K'^T + 1Δ^T)/√d_k)V'`.\n\n2. **Multi-scale Temporal Modeling**: Pyraformer explicitly constructs a pyramidal graph with `Bottleneck_Construct` using convolution layers (`ConvLayer` with stride C) to create coarser-scale representations at multiple resolutions (daily, weekly, monthly from hourly data). **NEW IMPLEMENTATION NEEDED**: Nonstationary Transformer does NOT use multi-scale architecture; instead, it operates on a single-scale with standard encoder-decoder, requiring removal of CSCM and PAM components.\n\n3. **Non-stationarity Handling Philosophy**: Pyraformer treats normalization as a preprocessing step for numerical stability. Nonstationary Transformer addresses over-stationarization by explicitly modeling non-stationary information through de-stationary factors. **NEW IMPLEMENTATION NEEDED**: Statistics preservation module that computes and passes μ_x, σ_x alongside normalized inputs through the network, and integration of these statistics into attention calculation.\n\n4. **Attention Computation Complexity**: Pyraformer uses sparse pyramidal attention with custom CUDA kernels (mentioned but not shown in code) connecting each node to only A+C+1 neighbors, achieving O(AL) = O(L) complexity. Nonstationary Transformer uses full attention O(L²) but with learned de-stationary rescaling. **NEW IMPLEMENTATION NEEDED**: Standard full attention implementation without pyramidal masking, but with τ and Δ factor integration.\n\n5. **Prediction Module Design**: Pyraformer offers two options: (1) direct projection from concatenated multi-scale features `self.projection = nn.Linear((len(window_size)+1)*self.d_model, self.pred_len * configs.enc_in)`, or (2) decoder with two full attention layers. **NEW IMPLEMENTATION NEEDED**: Nonstationary Transformer uses standard encoder-decoder with decoder processing initialized future tokens, requiring implementation of decoder self-attention and cross-attention layers wrapped with de-stationary factors.\n\n6. **Learnable vs. Fixed Statistics**: Pyraformer computes normalization statistics (mean, std) directly from input without learnable parameters. Nonstationary Transformer learns de-stationary factors through MLPs: `log τ = MLP(σ_x, x)` and `Δ = MLP(μ_x, x)`. **NEW IMPLEMENTATION NEEDED**: Two separate MLP modules (typically 2-3 layer networks) that take statistics and raw series as input to produce de-stationary factors shared across all attention layers.\n\n7. **Architectural Components**: Pyraformer includes specialized components like `ConvLayer` for downsampling, `Bottleneck_Construct` for multi-resolution tree construction, and `refer_points()` for gathering features from pyramid sequences. **NEW IMPLEMENTATION NEEDED**: These components should be removed; instead, implement standard Transformer encoder-decoder blocks with De-stationary Attention replacing vanilla attention in both encoder and decoder.\n\n8. **Input Processing Flow**: Pyraformer: Embedding → CSCM (build pyramid) → PAM (pyramidal attention) → gather multi-scale features → project. Nonstationary Transformer: Normalization → Embedding → Encoder with De-stationary Attention → Decoder with De-stationary Attention → De-normalization. **NEW IMPLEMENTATION NEEDED**: Simplified linear flow without pyramid construction, but with explicit statistics tracking and MLP-based factor generation at each attention layer."
  },
  {
    "source": "Informer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both papers adopt the standard Transformer encoder-decoder structure for time series forecasting. The source code's `Encoder` and `Decoder` classes with their layer stacking mechanisms can be directly reused. The target paper explicitly states it follows 'the standard Encoder-Decoder structure' similar to vanilla Transformers.\n\n2. **Embedding Layer**: Both use `DataEmbedding` for input representation combining value embedding with temporal features. The source implementation's `enc_embedding` and `dec_embedding` modules can be directly adapted, as both papers handle temporal marks (`x_mark_enc`, `x_mark_dec`).\n\n3. **Instance Normalization Strategy**: Both papers apply normalization on the temporal dimension. Informer's code shows `mean_enc = x_enc.mean(1, keepdim=True)` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)` in the `short_forecast` method, which directly corresponds to Non-stationary Transformer's Normalization module (Equation 1: μ_x and σ_x calculation). This normalization logic can be reused with minor modifications.\n\n4. **Multi-Head Attention Framework**: The `AttentionLayer` class with query/key/value projections and multi-head mechanism from Informer can serve as the base structure. The target paper needs to replace the attention computation inside but can keep the projection layers and head splitting logic.\n\n5. **Feed-Forward Network**: Both use standard FFN layers after attention. Informer's `DecoderLayer` contains `conv1`, `conv2` with activation and dropout, which matches the target paper's assumption of feed-forward layers. These components can be directly reused.\n\n6. **Layer Normalization**: Both apply LayerNorm. The source code's `norm_layer=torch.nn.LayerNorm(configs.d_model)` in encoder/decoder can be directly used.\n\n7. **Decoder Start Token Strategy**: Both use a start token approach. Informer's generative inference with `x_token` and placeholder `x_0` (Equation 6) aligns with the target paper's decoder design, allowing the decoder initialization code to be adapted.\n\n8. **Task Focus**: Both target long sequence time-series forecasting (LSTF) problems, though the target paper emphasizes handling non-stationarity. The overall forecasting pipeline structure (input → normalize → encode → decode → denormalize → output) is similar.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Informer uses ProbSparse Attention with query sparsity measurement (Equation 2-4) and Top-u selection for O(L log L) complexity. Non-stationary Transformer requires implementing De-stationary Attention (Equation 6) that incorporates de-stationary factors τ and Δ. This requires: (a) Adding MLP projectors to learn τ and Δ from μ_x and σ_x; (b) Modifying the attention computation to apply `τ * Q'K'^T + 1Δ^T` before Softmax; (c) Removing the entire ProbSparse query selection logic (`_prob_QK`, `_get_initial_context`, `_update_context` methods).\n\n2. **Normalization Scope and Philosophy**: Informer applies normalization only in the `short_forecast` method as a preprocessing step. Non-stationary Transformer requires Series Stationarization as a wrapper around the entire model with explicit De-normalization at output (Equation 2). NEW IMPLEMENTATION: The de-normalization must be applied after decoder output projection: `ŷ_i = σ_x ⊙ y'_i + μ_x`, requiring storage and reuse of original statistics throughout the forward pass.\n\n3. **Encoder Distilling Operation - REMOVAL REQUIRED**: Informer uses self-attention distilling with `ConvLayer` (1D convolution + MaxPool) to progressively reduce sequence length (Equation 5), creating a pyramid structure with multiple encoder stacks. Non-stationary Transformer uses standard Transformer encoders without distilling. The entire `ConvLayer` class and distilling logic in the encoder must be removed.\n\n4. **De-stationary Factor Learning - NEW IMPLEMENTATION REQUIRED**: Non-stationary Transformer introduces learnable de-stationary factors τ (scaling) and Δ (shifting) computed via MLPs from original series statistics (before normalization). This requires: (a) Two separate MLP modules taking (σ_x, x) and (μ_x, x) as inputs; (b) Sharing these factors across all attention layers; (c) Passing τ and Δ through the entire model forward pass as additional parameters.\n\n5. **Attention Mask Handling**: Informer's ProbMask for sparse attention is specifically designed for ProbSparse mechanism. Non-stationary Transformer uses standard causal masking. The `ProbMask` class and related masking logic in `_update_context` should be replaced with standard triangular causal masks.\n\n6. **Model Complexity Trade-off**: Informer achieves O(L log L) complexity through sparse attention and distilling. Non-stationary Transformer maintains O(L²) complexity with full attention but gains from stationarity-aware learning. The target implementation should remove all complexity-reduction mechanisms (sparse sampling, distilling) and use standard full attention.\n\n7. **Statistics Preservation**: Non-stationary Transformer requires preserving original series statistics (μ_x, σ_x) throughout the forward pass for both de-stationary factor computation and final de-normalization. NEW IMPLEMENTATION: Add instance variables or pass these as additional arguments through encoder and decoder, unlike Informer which only uses them locally in normalization.\n\n8. **Attention Input Transformation**: Informer directly uses embedded queries/keys/values. Non-stationary Transformer requires all attention inputs to be stationarized (Q', K', V' derived from normalized x'), then applies de-stationary factors. The attention layer must handle both stationarized inputs and original statistics simultaneously."
  },
  {
    "source": "Reformer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the standard Transformer encoder-decoder structure with multi-head attention mechanisms. The source code's `Encoder`, `EncoderLayer` components and basic embedding layers (`DataEmbedding`) can be directly reused as the backbone architecture for Nonstationary Transformer.\n\n2. **Sequence-to-Sequence Forecasting Framework**: Both implement encoder-only or encoder-decoder architectures for time series forecasting. The source code's `short_forecast` method structure (input embedding → encoder processing → projection) provides a direct template for implementing the target paper's forecasting pipeline.\n\n3. **Normalization on Input Sequences**: Both apply normalization techniques to stabilize training. The source code's normalization in `short_forecast` (computing mean_enc, std_enc and normalizing x_enc) is structurally similar to the target paper's Normalization module in Series Stationarization, though applied differently. This normalization logic can be adapted with modifications.\n\n4. **Linear Projection for Output**: Both use simple linear layers for final predictions. The source code's `self.projection = nn.Linear(configs.d_model, configs.c_out)` can be directly reused for the target paper's output projection.\n\n5. **Batch Processing and Sequence Padding**: The source code's `fit_length` method for handling variable sequence lengths provides a useful utility that can be adapted for the target paper's preprocessing needs.\n\n6. **Multi-layer Encoder Stacking**: Both stack multiple encoder layers with residual connections and layer normalization. The source code's layer stacking mechanism in the `Encoder` class can be directly reused for building the Nonstationary Transformer's encoder.",
    "differences": "1. **Core Innovation - Attention Mechanism**: Reformer introduces LSH (Locality-Sensitive Hashing) attention for O(L log L) complexity using hash bucketing and chunking, while Nonstationary Transformer proposes De-stationary Attention that learns scaling factor τ and shift vector Δ to recover non-stationary information. The target paper needs NEW implementation of: (a) MLP projectors to compute τ and Δ from input statistics, (b) Modified attention computation with τ*Q'K'^T + 1Δ^T term, (c) Integration of these factors across all attention layers.\n\n2. **Stationarization Strategy**: Reformer applies simple mean-std normalization only at input/output boundaries, while Nonstationary Transformer implements a comprehensive Series Stationarization wrapper with explicit De-normalization module. NEW implementation needed: (a) Normalization module computing μ_x, σ_x on temporal dimension per variable, (b) De-normalization module restoring statistics to predictions, (c) Passing original statistics (μ_x, σ_x) alongside normalized data through the model.\n\n3. **Non-stationary Information Flow**: Reformer discards statistical information after normalization, while Nonstationary Transformer explicitly preserves and reintegrates it. NEW implementation needed: (a) Mechanism to pass raw series statistics (μ_x, σ_x, original x) to all attention layers, (b) De-stationary factor computation modules shared across layers, (c) Modified forward pass to handle both stationarized queries/keys/values and de-stationary factors simultaneously.\n\n4. **Efficiency Focus vs. Predictability Focus**: Reformer optimizes for memory/computational efficiency with reversible layers and chunking (Table 3 shows memory complexity reduction), while Nonstationary Transformer focuses on improving forecasting accuracy for non-stationary series without efficiency optimizations. The target paper does NOT need: (a) LSH hashing mechanisms, (b) Reversible layer implementations, (c) Chunking strategies for feed-forward layers.\n\n5. **Attention Computation Complexity**: Reformer reduces attention to O(L log L) through hash bucketing (bucket_size, n_hashes parameters), while Nonstationary Transformer maintains O(L²) complexity but enhances it with de-stationary factors. NEW implementation needed: Standard scaled dot-product attention with de-stationary modifications, not hash-based approximate attention.\n\n6. **Parameter Sharing Philosophy**: Reformer shares Q-K projections (shared-QK) and normalizes key lengths for LSH compatibility, while Nonstationary Transformer uses separate Q, K, V projections with shared de-stationary factors τ, Δ across layers. NEW implementation needed: Separate projection matrices for Q, K, V without length normalization constraints.\n\n7. **Decoder Design**: Reformer's code shows minimal decoder logic (placeholder concatenation), while Nonstationary Transformer requires a full decoder with De-stationary Attention for refining predictions. NEW implementation needed: (a) Decoder layers with De-stationary Attention, (b) Proper decoder initialization and cross-attention mechanisms, (c) Integration of encoder outputs with decoder processing."
  },
  {
    "source": "Informer_2020",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Pyraformer_2021",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt the DMS forecasting paradigm where the model directly predicts all future time steps in one forward pass, avoiding autoregressive error accumulation. The source code's `forecast()` method that outputs `[B, pred_len, D]` can be directly adapted as the foundational prediction framework.\n\n2. **Temporal Decomposition Philosophy**: Both leverage decomposition concepts to handle complex time series patterns. DLinear's `series_decomp` module (moving average kernel for trend extraction) provides a reusable preprocessing component. TiDE can adapt this decomposition logic, though it may apply it differently in its encoder-decoder structure.\n\n3. **Linear Layer Based Architecture**: Both emphasize the effectiveness of simple linear transformations over complex attention mechanisms for time series forecasting. DLinear's `nn.Linear(seq_len, pred_len)` implementation demonstrates efficient temporal projection that can serve as building blocks for TiDE's dense layers.\n\n4. **Channel-wise Processing**: Both support independent processing of different variates. DLinear's `individual` parameter and `nn.ModuleList()` implementation for per-channel linear layers (lines 23-35) can be directly reused for TiDE's feature projection modules when handling multivariate time series.\n\n5. **Normalization for Distribution Shift**: Both papers recognize the importance of handling distribution shifts in time series data. While DLinear uses subtraction normalization (NLinear variant), this normalization philosophy and the permutation operations (`permute(0,2,1)`) in the code provide guidance for TiDE's instance normalization implementation.\n\n6. **Simple Weight Initialization**: DLinear's uniform weight initialization `(1/seq_len) * torch.ones([pred_len, seq_len])` (lines 32-33, 38-39) provides a sensible baseline initialization strategy that can be adapted for TiDE's dense layer initialization.",
    "differences": "1. **Encoder-Decoder Architecture vs. Pure Linear**: TiDE employs a full encoder-decoder architecture with residual connections, while DLinear uses only direct linear projections. NEW IMPLEMENTATION NEEDED: Multi-layer dense encoder with residual blocks, temporal decoder with attention-like mechanisms, and skip connections between encoder and decoder that are completely absent in DLinear.\n\n2. **Feature Projection and Dense Layers**: TiDE uses dense layers for feature projection to create a latent representation space before temporal processing, while DLinear directly applies linear layers to the temporal dimension. NEW IMPLEMENTATION NEEDED: Feature projection modules that transform `[B, L, D]` to `[B, L, hidden_dim]`, multiple stacked dense layers with activation functions (ReLU/GELU), and layer normalization components.\n\n3. **Temporal Encoding and Covariates**: TiDE explicitly incorporates time-varying covariates (past and future) and static covariates through dedicated encoding pathways, while DLinear only processes the raw time series values. NEW IMPLEMENTATION NEEDED: Separate embedding layers for categorical covariates, concatenation mechanisms for integrating covariates with encoded features, and covariate-aware projection layers.\n\n4. **Residual Connection Structure**: TiDE employs deep residual connections throughout the encoder-decoder structure to facilitate gradient flow and information preservation, while DLinear simply sums trend and seasonal components. NEW IMPLEMENTATION NEEDED: Skip connections from encoder output to decoder input, residual blocks within encoder/decoder layers, and potentially global residual connections from input to output.\n\n5. **Lookback and Horizon Decoupling**: TiDE uses separate dense layers to decode from encoded representation to prediction horizon, allowing flexible lookback-horizon ratios, while DLinear directly maps `seq_len -> pred_len`. NEW IMPLEMENTATION NEEDED: Temporal decoder module that projects from latent dimension to output horizon, potentially with different architectural choices than the encoder.\n\n6. **Hierarchical Feature Processing**: TiDE processes features through multiple transformation stages (input projection -> encoder -> latent -> decoder -> output projection), creating hierarchical representations, while DLinear uses single-stage linear transformations. NEW IMPLEMENTATION NEEDED: Multi-stage processing pipeline with intermediate feature transformations, bottleneck layers for dimensionality reduction/expansion, and stage-specific normalization schemes.\n\n7. **Training Objective and Loss Computation**: While both use MSE-like losses, TiDE may employ more sophisticated loss functions considering covariate information and multi-horizon weighting. NEW IMPLEMENTATION NEEDED: Potentially weighted loss functions across prediction horizons, covariate-conditional loss terms, and auxiliary loss components for regularization."
  },
  {
    "source": "PatchTST_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization Strategy**: Both papers employ instance normalization (RevIN in PatchTST, similar normalization in TiDE) to handle distribution shifts across different time series instances. The normalization and denormalization modules from PatchTST's implementation can be directly adapted, as both normalize each instance independently by subtracting mean and dividing by standard deviation before feeding into the model.\n\n2. **Multi-horizon Forecasting Framework**: Both models support direct multi-step forecasting where the model outputs all future horizons simultaneously rather than autoregressive prediction. The training loop structure, loss calculation (MSE/MAE), and evaluation metrics implementation from PatchTST can be reused for TiDE.\n\n3. **Channel Independence Assumption**: Both architectures treat each variate/channel independently (Channel Independence in PatchTST, separate encoding per feature in TiDE). The data loading pipeline that processes each channel separately and the batching strategy from PatchTST's implementation can be adapted.\n\n4. **Lookback Window Processing**: Both use a fixed lookback window to predict future horizons. The sliding window data preparation logic, including handling of different prediction lengths and the dataset creation utilities from PatchTST can be reused.\n\n5. **Benchmark Evaluation Protocol**: Both papers evaluate on similar datasets (ETT, Electricity, Weather, etc.) with consistent metrics (MSE, MAE). The evaluation framework, metric calculation functions, and result logging infrastructure from PatchTST's codebase can be directly leveraged.",
    "differences": "1. **Core Architecture Paradigm**: PatchTST uses Transformer-based architecture with self-attention mechanisms operating on patches, requiring implementation of patch embedding, positional encoding, and multi-head attention layers. TiDE uses a pure MLP-based encoder-decoder architecture with residual connections, requiring NEW implementation of: (a) dense layer stacks for temporal encoding, (b) feature projection layers, (c) dense decoder with residual connections, and (d) global residual skip connections from input to output. No attention mechanisms needed.\n\n2. **Input Representation and Tokenization**: PatchTST segments time series into non-overlapping patches and embeds them as tokens (requires patch creation logic, patch embedding layers). TiDE directly processes the raw lookback window through dense layers without patching. NEW implementation needed: simple flattening of lookback window and direct linear projection layers instead of patch-based tokenization.\n\n3. **Temporal Feature Encoding**: PatchTST relies on learned positional encodings added to patch embeddings to capture temporal order. TiDE uses explicit temporal feature encoding (time of day, day of week, etc.) concatenated with the lookback window. NEW implementation needed: (a) temporal feature extraction utilities (cyclical encoding of time attributes), (b) feature concatenation logic, (c) separate encoding pathways for past and future covariates.\n\n4. **Multi-scale and Hierarchical Processing**: PatchTST's patching mechanism inherently creates a coarser temporal scale. TiDE processes all timesteps at the original resolution through dense layers and uses explicit feature-temporal decoupling in the decoder. NEW implementation needed: (a) separate dense encoders for each feature dimension, (b) concatenation of encoded features with future temporal attributes, (c) dense decoder that jointly processes all encoded information.\n\n5. **Model Complexity and Efficiency**: PatchTST has O(L²) complexity due to self-attention over patches (where L is sequence length divided by patch size), requiring attention computation. TiDE has O(L) linear complexity with simple matrix multiplications through MLPs. NEW implementation needed: straightforward feed-forward networks with layer normalization and dropout, significantly simpler than attention mechanisms, making TiDE more computationally efficient and easier to implement.\n\n6. **Residual Connection Strategy**: PatchTST uses standard Transformer residual connections within attention blocks. TiDE employs a global residual connection that directly projects the input lookback window to output space and adds it to the decoder output (linear projection for distribution alignment). NEW implementation needed: (a) learnable linear layer for direct input-to-output projection, (b) dimension matching logic to align lookback and forecast horizons, (c) residual fusion at the final output stage."
  },
  {
    "source": "PatchTST_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Tokenization Foundation**: Both papers use patching mechanisms to segment time series into sub-sequences. PatchTST's patching module (stride, patch_len parameters) and embedding layers can be directly reused as the base tokenization component. The linear projection layers for patch embedding are transferable.\n\n2. **Transformer Backbone Architecture**: Both employ Transformer encoder architectures with multi-head self-attention mechanisms. PatchTST's attention layers, feed-forward networks, and layer normalization components can be adapted. The positional encoding mechanisms for patch sequences are reusable.\n\n3. **Instance Normalization Strategy**: Both papers apply instance normalization (RevIN-style) to handle distribution shifts across different time series. PatchTST's normalization implementation (mean/std computation per instance) can be directly integrated into MultiPatchFormer's preprocessing pipeline.\n\n4. **Channel Independence Assumption**: Both models process each variate independently (channel-independent strategy). PatchTST's channel-wise processing logic and separate modeling per channel can be reused without modification.\n\n5. **Prediction Head Design**: Both use linear projection heads to map encoded representations to forecasting horizons. PatchTST's flatten-and-project output layer structure provides a template for MultiPatchFormer's prediction mechanism.\n\n6. **Training Framework**: Standard supervised learning with MSE/MAE loss functions, similar optimization strategies (AdamW), and learning rate scheduling. PatchTST's training loop, loss computation, and validation logic are directly applicable.",
    "differences": "1. **Multi-Scale Patch Extraction (NEW)**: MultiPatchFormer introduces multiple patch lengths simultaneously to capture patterns at different temporal resolutions. NEW IMPLEMENTATION NEEDED: Multi-scale patching module that generates patches with different granularities (e.g., patch_len=[16, 32, 64]) and processes them in parallel, unlike PatchTST's single fixed patch length.\n\n2. **Cross-Scale Fusion Mechanism (NEW)**: MultiPatchFormer requires a fusion strategy to integrate information from different patch scales. NEW IMPLEMENTATION NEEDED: Cross-attention or concatenation-based fusion layers that combine multi-scale representations, hierarchical aggregation modules, or learnable weighting mechanisms for scale importance.\n\n3. **Scale-Specific Attention Patterns**: While PatchTST uses uniform attention across all patches, MultiPatchFormer may employ scale-aware attention. NEW IMPLEMENTATION NEEDED: Modified attention mechanisms that can handle variable-length patch sequences from different scales, potentially with scale-specific attention masks or adaptive attention pooling.\n\n4. **Hierarchical Feature Extraction**: MultiPatchFormer likely implements hierarchical processing where coarse-scale features guide fine-scale modeling. NEW IMPLEMENTATION NEEDED: Multi-level encoder architecture with information flow between scales, possibly top-down or bottom-up pathways, unlike PatchTST's single-level encoding.\n\n5. **Adaptive Patch Selection**: MultiPatchFormer may include mechanisms to adaptively weight or select relevant scales per dataset/series. NEW IMPLEMENTATION NEEDED: Learnable scale importance modules, gating mechanisms, or attention-based scale selection that aren't present in PatchTST's fixed-scale approach.\n\n6. **Increased Model Complexity**: Processing multiple scales increases computational cost. NEW IMPLEMENTATION NEEDED: Efficient parallel processing strategies, memory management for multi-scale representations, and potentially pruning mechanisms to maintain computational efficiency compared to PatchTST's simpler single-scale pipeline.\n\n7. **Enhanced Positional Encoding**: Different patch scales require adapted positional encodings. NEW IMPLEMENTATION NEEDED: Scale-aware positional encoding schemes that maintain temporal ordering across different granularities, potentially with hierarchical or multi-resolution positional embeddings."
  },
  {
    "source": "DLinear_2022",
    "target": "MultiPatchFormer_2025",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Crossformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Segment-based Embedding Architecture**: Both papers adopt segment-wise embedding strategies rather than point-wise embedding. Crossformer's DSW (Dimension-Segment-Wise) embedding divides time series into segments of length L_seg and embeds each segment into vectors. The source code's `PatchEmbedding` module (with `patch_len`, `stride`, `padding_patch_layer`, `value_embedding`) can be directly adapted for MultiPatchFormer's multi-patch embedding, requiring only modifications to support multiple patch lengths simultaneously.\n\n2. **Positional Encoding Mechanism**: Both employ learnable positional embeddings to encode temporal positions. Crossformer uses `nn.Parameter(torch.randn(...))` for `enc_pos_embedding` and `dec_pos_embedding`. The `PositionalEmbedding` class with sinusoidal encoding in the source code provides a reusable foundation that can be extended to support multi-scale positional encoding for different patch sizes in MultiPatchFormer.\n\n3. **Hierarchical Multi-scale Processing**: Crossformer's hierarchical encoder-decoder with `SegMerging` (win_size=2) progressively merges segments to capture coarser-scale patterns. The `scale_block` and `Encoder` architecture with multiple layers processing different segment numbers (in_seg_num, ceil(in_seg_num/win_size^l)) demonstrates multi-scale modeling. This hierarchical structure can be adapted for MultiPatchFormer's multi-patch processing, though the merging strategy needs modification to handle parallel patches rather than sequential merging.\n\n4. **Layer Normalization and Residual Connections**: Both use standard Transformer components with LayerNorm and residual connections. The source code's `self.norm1`, `self.norm2`, `self.norm3`, `self.norm4` in `TwoStageAttentionLayer` and `DecoderLayer` can be directly reused. The MLP structure `nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model))` is standard and reusable.\n\n5. **Attention-based Dependency Modeling**: Both rely on attention mechanisms for capturing dependencies. Crossformer's `FullAttention` and `AttentionLayer` classes with query/key/value projections provide a solid foundation. The basic attention computation `torch.einsum(\"blhe,bshe->bhls\", queries, keys)` and softmax operations can be reused, though MultiPatchFormer may need modifications for cross-patch attention patterns.\n\n6. **Decoder Architecture with Cross-Attention**: Crossformer's decoder uses self-attention followed by cross-attention to encoder outputs, with `DecoderLayer` containing both `self_attention` and `cross_attention`. This structure can be adapted for MultiPatchFormer's decoder, particularly the cross-attention mechanism for integrating multi-scale patch representations.\n\n7. **Padding Strategy for Sequence Length**: Both handle non-divisible sequence lengths through padding. Crossformer's `self.pad_in_len = ceil(1.0 * configs.seq_len / self.seg_len) * self.seg_len` and `self.padding_patch_layer = nn.ReplicationPad1d((0, padding))` in `PatchEmbedding` can be directly reused or adapted for handling multiple patch lengths.",
    "differences": "1. **Core Innovation - Multi-Patch vs Cross-Dimension**: Crossformer's main innovation is the Two-Stage Attention (TSA) with explicit cross-dimension modeling using router mechanisms (batch_router, dim_sender, dim_receiver) to capture D-to-D dependencies. MultiPatchFormer's core innovation is multi-patch tokenization with DIFFERENT patch lengths processed in PARALLEL to capture multi-scale temporal patterns. This requires NEW implementation of: (a) parallel patch extraction modules for multiple patch sizes, (b) multi-patch fusion mechanisms, (c) patch-specific attention or aggregation strategies. The source code's dimension-focused architecture needs complete redesign for multi-patch processing.\n\n2. **Embedding Strategy - Single Patch vs Multi-Patch**: Crossformer uses uniform segment length (self.seg_len=12) across all processing. MultiPatchFormer requires MULTIPLE patch lengths simultaneously (e.g., patches of 4, 8, 16 time steps). NEW implementation needed: (a) multiple `PatchEmbedding` instances with different patch_len parameters, (b) alignment mechanisms to handle different numbers of patches from same input sequence, (c) patch-specific projection matrices, (d) strategies to combine embeddings from patches of different granularities before feeding to encoder.\n\n3. **Attention Mechanism - Cross-Dimension vs Multi-Patch Attention**: Crossformer's TSA layer has two stages: cross-time attention within each dimension, then cross-dimension attention via routers. The router mechanism (`self.router = nn.Parameter(torch.randn(seg_num, factor, d_model))`, `dim_sender`, `dim_receiver`) is specific to dimension interaction. MultiPatchFormer needs NEW attention mechanisms: (a) intra-patch attention within each patch scale, (b) inter-patch attention across different patch scales, (c) potentially patch-aware attention masks, (d) fusion attention to combine multi-scale patch representations. The two-stage dimension-focused attention cannot be directly used.\n\n4. **Hierarchical Processing - Sequential Merging vs Parallel Multi-Scale**: Crossformer uses sequential segment merging (`SegMerging` with win_size=2) where each encoder layer processes progressively coarser segments (L → L/2 → L/4). MultiPatchFormer processes multiple patch scales IN PARALLEL from the input, requiring NEW implementation: (a) parallel encoding branches for each patch size, (b) synchronization mechanisms to align representations from different scales, (c) multi-scale feature fusion modules, (d) potentially shared vs separate parameters for different patch encoders.\n\n5. **Decoder Design - Hierarchical Prediction vs Multi-Patch Fusion**: Crossformer's decoder has (e_layers+1) layers corresponding to each encoder scale, with predictions summed across scales (`final_predict = final_predict + layer_predict`). MultiPatchFormer needs NEW decoder design: (a) mechanisms to decode from fused multi-patch representations, (b) potentially patch-specific decoders, (c) strategies to aggregate predictions from different patch perspectives, (d) handling of different prediction granularities from different patches, (e) final projection that respects multi-scale information.\n\n6. **Computational Complexity Focus**: Crossformer optimizes for large dimension D using router mechanism (O(DL) instead of O(D²L)) for cross-dimension attention. MultiPatchFormer's complexity concerns are different: managing multiple parallel patch streams increases memory and computation. NEW implementation needed: (a) efficient multi-patch processing to avoid multiplicative complexity, (b) selective attention mechanisms across patches, (c) parameter sharing strategies across patch scales, (d) potentially dynamic patch selection based on input characteristics.\n\n7. **Feature Representation - 2D Array vs Multi-Scale Tensors**: Crossformer maintains 2D arrays [batch, dimension, segment_num, d_model] throughout processing, with explicit dimension and time axes. MultiPatchFormer requires NEW tensor organization: (a) 3D or 4D tensors to accommodate [batch, patch_scale, patch_num, d_model], (b) indexing and manipulation strategies for multi-scale patches, (c) broadcasting and alignment operations across different patch numbers, (d) potentially nested list structures to handle variable patch numbers per scale."
  },
  {
    "source": "FEDformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "PatchTST_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization Strategy**: Both papers employ instance normalization (RevIN) to handle distribution shift across time series instances. PatchTST's normalization module can be directly reused - it normalizes each instance by subtracting mean and dividing by standard deviation, then denormalizes predictions. The same normalization infrastructure (forward normalize → model → denormalize) applies to SegRNN.\n\n2. **Channel Independence Assumption**: Both architectures treat each variate independently (channel-independent strategy). PatchTST's implementation of processing each channel separately through the model can be adapted - the data loading and channel-wise iteration logic is directly transferable to SegRNN's implementation.\n\n3. **Lookback Window Segmentation**: Both methods segment the input time series into smaller units. PatchTST uses patching (non-overlapping segments), while SegRNN uses overlapping sliding windows, but the core concept of dividing sequences is shared. The windowing/segmentation utilities from PatchTST can be modified for SegRNN's sliding window approach.\n\n4. **Univariate Forecasting Focus**: Both papers emphasize univariate time series forecasting capabilities and are evaluated on similar benchmarks (ETT, Electricity, Weather datasets). PatchTST's data preprocessing pipelines, dataset loaders, and evaluation metrics (MSE, MAE) can be directly reused.\n\n5. **Embedding and Projection Layers**: Both use linear projection layers - PatchTST projects patches to embedding dimension, SegRNN projects segments. The linear layer implementation pattern and initialization strategies from PatchTST can be adapted for SegRNN's segment encoding.\n\n6. **Training Infrastructure**: Standard supervised learning setup with MSE loss, Adam optimizer, and learning rate scheduling. PatchTST's training loop, validation logic, early stopping mechanism, and hyperparameter management can be directly reused for SegRNN.",
    "differences": "1. **Core Architecture - Transformer vs RNN**: PatchTST uses self-attention Transformer encoder blocks with multi-head attention mechanisms, while SegRNN uses simple RNN cells (GRU/LSTM) for temporal modeling. NEW IMPLEMENTATION NEEDED: Replace entire Transformer encoder stack with RNN layers; implement GRU/LSTM cell processing for segment sequences; handle hidden state initialization and propagation across segments.\n\n2. **Segmentation Strategy - Non-overlapping vs Overlapping**: PatchTST uses non-overlapping patches (stride = patch_length), while SegRNN uses overlapping sliding windows with configurable stride (typically stride < segment_length for overlap). NEW IMPLEMENTATION NEEDED: Sliding window generator with overlap control; segment aggregation mechanism to combine overlapping predictions; handling of boundary segments.\n\n3. **Positional Information Encoding**: PatchTST uses learnable positional embeddings added to patch embeddings to preserve temporal order information. SegRNN relies on RNN's inherent sequential processing without explicit positional encodings. NEW IMPLEMENTATION NEEDED: Remove positional embedding modules; ensure RNN processes segments in correct temporal order through sequential feeding.\n\n4. **Attention Mechanism vs Sequential Processing**: PatchTST's self-attention allows each patch to attend to all other patches globally (complexity O(L²) where L is number of patches). SegRNN processes segments sequentially through RNN (complexity O(L) for L segments). NEW IMPLEMENTATION NEEDED: Sequential segment feeding mechanism; RNN hidden state management across time steps; no attention weight computation needed.\n\n5. **Multi-scale Modeling Approach**: PatchTST achieves multi-scale through variable patch lengths (different receptive fields per patch). SegRNN achieves multi-scale through variable segment lengths and overlapping ratios. NEW IMPLEMENTATION NEEDED: Segment length and stride hyperparameter tuning system; mechanism to test different overlap configurations; aggregation strategies for overlapping segment outputs.\n\n6. **Output Projection Strategy**: PatchTST uses a flattened representation from all patch embeddings followed by linear projection to forecast horizon. SegRNN uses the final RNN hidden state or aggregated segment representations for prediction. NEW IMPLEMENTATION NEEDED: RNN final state extraction logic; segment-level output aggregation (mean/max pooling over segments); direct mapping from RNN hidden dimension to forecast horizon.\n\n7. **Model Complexity and Parameter Efficiency**: PatchTST has higher parameter count due to multi-head attention (Q, K, V projections, multiple heads, multiple layers). SegRNN is designed for simplicity with fewer parameters (single/few RNN layers). NEW IMPLEMENTATION NEEDED: Lightweight RNN configuration; parameter counting and efficiency monitoring; simplified architecture without attention overhead.\n\n8. **Handling of Segment Boundaries**: PatchTST's non-overlapping patches have clear boundaries with no information sharing between adjacent patches (except through attention). SegRNN's overlapping segments share information at boundaries, requiring careful handling during inference. NEW IMPLEMENTATION NEEDED: Segment overlap resolution strategy; boundary padding mechanisms; consistent segment extraction during training and inference."
  },
  {
    "source": "TimesNet_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Short-term Time Series Forecasting Task**: Both papers target short-term forecasting problems and are evaluated on similar benchmarks including the M4 dataset with multiple seasonal patterns (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly). The basic forecasting setup with seq_len and pred_len can be directly reused from TimesNet's implementation.\n\n2. **Instance Normalization Strategy**: Both employ instance normalization (z-score normalization) on the input sequences to handle non-stationarity. TimesNet's normalization code in the forecast() method (computing means and stdev per instance, then denormalizing predictions) can be directly adapted: `means = x_enc.mean(1, keepdim=True).detach()` and `x_enc = x_enc.sub(means).div(stdev)`.\n\n3. **Residual Connection Architecture**: Both utilize residual connections in their architectures. TimesNet's TimesBlock implements `res = res + x` which is a standard residual pattern. This design principle and implementation pattern can guide SegRNN's residual structure between segments.\n\n4. **Embedding and Projection Layers**: Both require input embedding and output projection mechanisms. TimesNet's DataEmbedding and final projection layer (`self.projection = nn.Linear(configs.d_model, configs.c_out)`) provide reusable patterns for SegRNN's input/output transformations.\n\n5. **Multi-variate Handling**: Both models process multivariate time series with shape [B, T, C]. The dimension handling logic in TimesNet's forward pass can be adapted for SegRNN's segment-wise processing.",
    "differences": "1. **Core Modeling Paradigm - NEW IMPLEMENTATION REQUIRED**: TimesNet transforms 1D time series into 2D space using FFT-based period detection and processes with 2D CNNs (Inception blocks), while SegRNN segments the time series into patches and processes each segment with simple RNNs. SegRNN needs NEW segment division logic, RNN cells (GRU/LSTM), and segment-wise processing that are completely absent in TimesNet.\n\n2. **Periodicity Analysis vs Segment Division - NEW IMPLEMENTATION REQUIRED**: TimesNet uses FFT (`torch.fft.rfft`) to discover dominant periods and creates multiple 2D representations based on top-k frequencies. SegRNN does NOT use frequency analysis; instead it needs NEW implementation for fixed-length segment division (sliding window or non-overlapping chunks) and segment encoding mechanisms.\n\n3. **2D Convolution vs RNN Processing - NEW IMPLEMENTATION REQUIRED**: TimesNet's core is the Inception_Block_V1 with multi-scale 2D convolutions (`nn.Conv2d` with kernels 1x1, 3x3, 5x5, etc.). SegRNN requires NEW RNN-based components (nn.GRU or nn.LSTM layers) for sequential segment processing, with completely different parameter structures and forward logic.\n\n4. **Multi-Period Aggregation vs Segment Aggregation - NEW IMPLEMENTATION REQUIRED**: TimesNet aggregates k different period-based representations using softmax-weighted amplitude values (`period_weight = F.softmax(period_weight, dim=1)`). SegRNN needs NEW aggregation logic to combine information across segments, likely using attention mechanisms or simple concatenation/averaging of segment representations.\n\n5. **Complexity and Efficiency Trade-off**: TimesNet has O(k × d_model × d_ff) complexity with multiple 2D convolutions and FFT operations per layer. SegRNN aims for extreme simplicity with linear complexity O(num_segments × hidden_size), requiring NEW lightweight RNN implementations optimized for segment-level processing without the heavy FFT and 2D convolution overhead.\n\n6. **Temporal Alignment Strategy - NEW IMPLEMENTATION REQUIRED**: TimesNet uses `predict_linear` to align temporal dimensions before processing (`self.predict_linear(enc_out.permute(0, 2, 1))`). SegRNN needs NEW segment-to-prediction mapping logic, potentially using a final MLP or linear layer that maps from segment representations to the full prediction horizon.\n\n7. **Layer Stacking Philosophy**: TimesNet stacks multiple TimesBlocks (e_layers) with LayerNorm between blocks for deep hierarchical feature learning. SegRNN likely uses shallower architecture with fewer layers, requiring NEW shallow RNN stacking logic focused on segment-level patterns rather than deep temporal hierarchies."
  },
  {
    "source": "DLinear_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt the DMS approach where the model directly predicts the entire forecast horizon in one shot, avoiding autoregressive error accumulation. The source code's `forecast()` method and output slicing `dec_out[:, -self.pred_len:, :]` can be directly reused as the prediction framework.\n\n2. **Time Series Decomposition for Trend Handling**: Both methods leverage decomposition to separate trend and seasonal components. DLinear's `series_decomp` module with moving average kernel can be directly adapted or reused in SegRNN for preprocessing, as shown in `self.decompsition = series_decomp(configs.moving_avg)`.\n\n3. **Channel-Independent Processing**: Both architectures process each variate independently without modeling cross-channel correlations. DLinear's `individual` mode implementation with `nn.ModuleList()` for per-channel processing provides a direct template for SegRNN's channel-independent design.\n\n4. **Normalization for Distribution Shift**: Both papers recognize the importance of handling distribution shifts in time series data. DLinear's NLinear variant (subtraction normalization) and the general normalization pipeline can guide SegRNN's instance normalization implementation.\n\n5. **Simple Linear Baseline Philosophy**: Both challenge complex Transformer architectures by demonstrating that simpler approaches can be more effective. The source code's minimalist design pattern (direct weight initialization, simple forward pass) provides architectural guidance for SegRNN's simplicity-focused implementation.\n\n6. **Input-Output Dimension Handling**: Both use similar input/output configurations with `seq_len` (lookback window) and `pred_len` (forecast horizon). The source code's dimension permutation logic `permute(0, 2, 1)` for temporal axis processing can be reused.",
    "differences": "1. **Core Architecture - RNN vs Linear Layers**: DLinear uses pure linear transformations (`nn.Linear`) on the temporal dimension, while SegRNN requires implementing RNN-based sequential processing. NEW IMPLEMENTATION NEEDED: RNN cells (LSTM/GRU) with segment-wise processing, hidden state management, and recurrent connections not present in DLinear's feedforward architecture.\n\n2. **Segment-Based Temporal Modeling**: SegRNN introduces segment-level processing where the input sequence is divided into multiple segments, each processed by RNN units. NEW IMPLEMENTATION NEEDED: Segment partitioning logic, segment encoding mechanism, and segment-to-segment information propagation, whereas DLinear processes the entire sequence as a whole through single linear layers.\n\n3. **Hierarchical Multi-Scale Representation**: SegRNN likely employs hierarchical segment representations to capture patterns at different temporal scales. NEW IMPLEMENTATION NEEDED: Multi-scale segment aggregation, hierarchical feature extraction across segments, and scale-aware prediction heads, while DLinear only operates at a single temporal resolution.\n\n4. **Recurrent State Propagation**: SegRNN maintains and propagates hidden states across segments through RNN mechanisms. NEW IMPLEMENTATION NEEDED: Hidden state initialization, state transfer between segments, and state-based prediction generation, whereas DLinear is stateless with independent transformations.\n\n5. **Parameter Initialization Strategy**: DLinear uses uniform initialization `(1/self.seq_len) * torch.ones()` for linear weights. NEW IMPLEMENTATION NEEDED: RNN-specific initialization schemes (Xavier, orthogonal) for recurrent weights, forget gate biases, and segment embedding initializations appropriate for recurrent architectures.\n\n6. **Computational Complexity**: DLinear has O(L) complexity with single matrix multiplication per channel. NEW IMPLEMENTATION NEEDED: Sequential computation management for RNN's O(L×H²) complexity where H is hidden size, including efficient segment-wise batching and potential parallelization strategies across segments.\n\n7. **Decoder Architecture**: DLinear directly outputs predictions through linear projection. NEW IMPLEMENTATION NEEDED: RNN-based decoder that may use attention mechanisms over segment representations, multi-step decoding from final hidden states, or segment-aware output generation layers not present in DLinear's simple summation approach."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Seasonal-Trend Decomposition**: Both papers employ moving average-based decomposition to separate time series into trend and seasonal components. FEDformer's `series_decomp` class (using moving average kernel) can be directly reused for DLinear's decomposition preprocessing. The implementation in FEDformer already provides the exact decomposition mechanism needed for DLinear.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both models predict the entire future horizon in one forward pass rather than autoregressive step-by-step prediction. FEDformer's decoder architecture demonstrates this DMS approach, and the same forecasting paradigm (predicting all T future steps simultaneously) applies to DLinear.\n\n3. **Normalization for Distribution Shift**: Both papers recognize the importance of handling distribution shifts in time series data. FEDformer uses standard normalization in its embedding layers (`DataEmbedding` class), which provides a foundation for understanding NLinear's subtraction-based normalization approach.\n\n4. **Univariate Processing Philosophy**: Both models process each variate independently without modeling cross-variate correlations. FEDformer's architecture separates channels in processing, similar to DLinear's weight-sharing across variates but independent prediction per variate.\n\n5. **Benchmark Dataset Evaluation**: Both papers evaluate on similar LTSF benchmarks (ETT, Weather, Electricity datasets), allowing direct performance comparison. The data loading and preprocessing pipeline from FEDformer's codebase can be reused for DLinear experiments.",
    "differences": "1. **Core Architecture - Complexity vs Simplicity**: FEDformer uses complex frequency-domain operations (Fourier/Wavelet transforms with O(L) complexity) and multi-layer encoder-decoder Transformer architecture, requiring implementation of `FourierBlock`, `FourierCrossAttention`, `MultiWaveletTransform`, attention mechanisms, and multiple encoder/decoder layers. DLinear requires NEW implementation of an extremely simple single-layer linear regression model (`nn.Linear`) along temporal axis - no attention, no frequency transforms, no multi-layer stacks.\n\n2. **Feature Extraction Mechanism**: FEDformer extracts features through frequency-enhanced attention blocks that operate in Fourier/Wavelet domain with learnable parameters (`weights1`, `weights2` in frequency space), complex multiplication operations, and multi-head attention. DLinear needs NEW implementation of direct temporal linear projection `W ∈ R^(T×L)` that simply performs weighted sum - no frequency domain operations, no attention mechanism.\n\n3. **Input Embedding Strategy**: FEDformer uses sophisticated embedding with positional encoding, temporal embeddings, and value embeddings (`DataEmbedding` class combining multiple embedding types). DLinear requires NEW implementation of minimal or no embedding - raw input values directly fed to linear layer, with only optional normalization preprocessing.\n\n4. **Model Components**: FEDformer requires encoder-decoder architecture with multiple layers (e_layers, d_layers), layer normalization (`my_Layernorm`), feed-forward networks, and AutoCorrelation layers wrapping attention mechanisms. DLinear needs NEW implementation of just two simple components: (1) one linear layer for vanilla/NLinear, or (2) two parallel linear layers (one for trend, one for seasonal) for DLinear variant.\n\n5. **Normalization Approach**: FEDformer uses standard zero-mean normalization embedded in the model pipeline. DLinear requires NEW implementation of two specific normalization variants: (a) NLinear's last-value subtraction normalization: `subtract last value → linear → add back`, and (b) DLinear's decomposition-based approach: `decompose → two separate linears → sum results`.\n\n6. **Computational Paradigm**: FEDformer operates in frequency domain with FFT/IFFT operations, mode selection, complex number arithmetic, and recursive wavelet decomposition. DLinear needs NEW implementation operating purely in time domain with simple matrix multiplication - no transforms, no complex operations.\n\n7. **Parameter Scale**: FEDformer has millions of parameters across attention heads, frequency kernels, multiple layers, and embeddings. DLinear requires NEW implementation with minimal parameters: only `T×L` weights for the temporal linear layer, making it orders of magnitude smaller.\n\n8. **Trend Handling**: FEDformer handles trend through decoder initialization and progressive refinement across layers with mixture of experts decomposition (`MOEDecomp`). DLinear needs NEW implementation of explicit parallel processing: separate linear layer dedicated solely to trend component after moving average extraction."
  },
  {
    "source": "Autoformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers adopt seasonal-trend decomposition as a core preprocessing technique. Autoformer uses `series_decomp` module with moving average kernel (implemented as `moving_avg` class with `nn.AvgPool1d`), and DLinear's decomposition variant directly reuses this concept. The source code's `series_decomp` class (kernel_size configurable, padding with front/end repetition) can be **directly reused** for DLinear implementation.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both methods predict the entire future horizon in one forward pass without autoregressive decoding. Autoformer's decoder outputs `dec_out[:, -self.pred_len:, :]` directly, avoiding error accumulation. This forecasting paradigm and the data preparation logic (creating decoder inputs with label_len + pred_len structure) can guide DLinear's implementation.\n\n3. **Normalization Techniques**: Both handle distribution shifts in time series data. Autoformer applies zero-mean normalization implicitly through decomposition (subtracting trend), while DLinear's NLinear variant explicitly subtracts the last value. The source code's handling of mean calculation (`torch.mean(x_enc, dim=1)`) and data normalization patterns provide implementation templates.\n\n4. **Input/Output Data Pipeline**: Both use similar data formatting with shape `[Batch, Length, Channels]`. Autoformer's data embedding structure (`DataEmbedding_wo_pos` with value/temporal embeddings) and the forward pass signature `(x_enc, x_mark_enc, x_dec, x_mark_dec)` establish a reusable data processing framework, though DLinear simplifies this significantly.\n\n5. **Channel-Independent Processing**: Autoformer processes each channel through the same transformation (shared weights in encoder/decoder), similar to DLinear's channel-independent linear layers (`W` applied to each variate separately). The source code's dimension handling logic can be adapted.",
    "differences": "1. **Core Architecture Complexity**: Autoformer implements a full Transformer-based architecture with encoder-decoder structure, multi-head auto-correlation mechanism (`AutoCorrelation`, `AutoCorrelationLayer`), series decomposition blocks embedded in each layer, and progressive trend accumulation. DLinear requires **NEW implementation** of extremely simple one-layer or two-layer linear models (`nn.Linear` along temporal axis) - this is fundamentally different and much simpler than Autoformer's deep architecture.\n\n2. **Temporal Dependency Modeling**: Autoformer captures period-based dependencies through Auto-Correlation mechanism using FFT (`torch.fft.rfft`, `torch.fft.irfft`) to compute autocorrelation, TopK delay selection, and time-delay aggregation (`time_delay_agg_training/inference`). DLinear requires **NEW implementation** of pure temporal linear regression (`W ∈ R^(T×L)`) without any attention or correlation mechanism - just weighted sum of historical points.\n\n3. **Model Depth and Components**: Autoformer has multiple encoder layers (`e_layers`) and decoder layers (`d_layers`), each containing attention, feedforward networks (`conv1`, `conv2` with 1D convolutions), and decomposition blocks. DLinear needs **NEW implementation** of shallow architecture: (a) Vanilla Linear: single `nn.Linear(L, T)`; (b) DLinear: decomposition + two parallel linear layers for trend/seasonal; (c) NLinear: subtraction normalization + single linear + addition back.\n\n4. **Embedding Strategy**: Autoformer uses complex embedding schemes including `TokenEmbedding` (1D convolution with kernel_size=3), `PositionalEmbedding`, and `TemporalEmbedding` (learnable or fixed embeddings for time features). DLinear requires **NEW implementation** of minimal or no embedding - direct input to linear layers, possibly with simple normalization only.\n\n5. **Computational Complexity**: Autoformer has O(L log L) complexity due to FFT-based auto-correlation and multi-layer processing. DLinear requires **NEW implementation** optimized for O(L) complexity with single matrix multiplication, making it orders of magnitude faster and simpler.\n\n6. **Decoder Design**: Autoformer accumulates trend progressively across decoder layers (`trend = trend + residual_trend`) and refines seasonal components through cross-attention with encoder outputs. DLinear needs **NEW implementation** with no decoder concept - direct mapping from input window to prediction window through linear transformation(s).\n\n7. **Normalization Philosophy**: Autoformer's normalization is implicit through decomposition and LayerNorm (`my_Layernorm` with bias subtraction). DLinear's NLinear requires **NEW implementation** of explicit last-value subtraction/addition normalization: `output = Linear(input - input[:, -1:, :]) + input[:, -1:, :]`."
  },
  {
    "source": "Informer_2020",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers employ DMS forecasting where the model predicts the entire future sequence in one forward pass rather than autoregressive generation. From Informer's code, the decoder architecture that outputs `dec_out[:, -self.pred_len:, :]` demonstrates this strategy and can be adapted - specifically, the non-autoregressive prediction paradigm and the concept of generating all future timesteps simultaneously can guide DLinear's implementation structure.\n\n2. **Normalization via Mean-Std Standardization**: Both papers apply instance normalization on input sequences. Informer's code shows `mean_enc = x_enc.mean(1, keepdim=True).detach()` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()` followed by denormalization `dec_out = dec_out * std_enc + mean_enc`. This exact normalization logic can be directly reused in DLinear (for NLinear variant) and provides a proven implementation for handling distribution shifts across different time series.\n\n3. **Encoder-Decoder Architecture Framework**: Both utilize an encoder-decoder structure where encoder processes historical data and decoder generates predictions. While DLinear simplifies this drastically, Informer's overall pipeline structure (data embedding → encoding → decoding → projection) provides a template. The projection layer `nn.Linear(configs.d_model, configs.c_out, bias=True)` in Informer's decoder can be adapted as the final linear layer in DLinear.\n\n4. **Temporal Embedding and Time Feature Handling**: Both papers process temporal features (timestamps). Informer's `DataEmbedding` class that combines value embedding with temporal encodings (`x_mark_enc`, `x_mark_dec`) demonstrates how to handle time features. This embedding infrastructure, particularly the temporal feature processing logic, can be simplified and reused for DLinear's input preparation.\n\n5. **Training Loop and Loss Function**: Both use MSE loss for optimization. Informer's training framework with forward pass structure `forward(x_enc, x_mark_enc, x_dec, x_mark_dec)` and the loss computation setup provides a direct template for DLinear's training pipeline, requiring minimal modification.\n\n6. **Handling Multiple Variates**: Both support multivariate time series forecasting. Informer's handling of channel dimensions (`configs.enc_in`, `configs.dec_in`, `configs.c_out`) and the batch processing logic can be directly adapted, though DLinear applies channel-independent modeling with shared weights across variates.",
    "differences": "1. **Core Architecture - Attention vs Linear**: Informer uses complex ProbSparse self-attention mechanism with O(L log L) complexity including query sparsity measurement, top-u query selection, and multi-head attention. DLinear requires implementing simple one-layer linear transformations `W ∈ R^(T×L)` along temporal axis only. NEW IMPLEMENTATION NEEDED: Single linear layer `nn.Linear(lookback_window, forecast_horizon)` applied per channel, completely replacing all attention mechanisms, encoder layers, and decoder layers from Informer.\n\n2. **Decomposition Strategy**: Informer uses optional decomposition only in specific variants, while DLinear (the D variant) mandates seasonal-trend decomposition as core preprocessing. NEW IMPLEMENTATION NEEDED: Moving average kernel for trend extraction `trend = AvgPool1d(kernel_size=25)(input)` and seasonal component calculation `seasonal = input - trend`, followed by separate linear layers for each component and summation for final prediction. This decomposition must be implemented as a fundamental module, not optional.\n\n3. **Model Complexity and Component Elimination**: Informer includes ConvLayer with ELU activation, BatchNorm, MaxPooling for distilling operation, multi-layer encoder stacks with attention distilling, and complex decoder with masked attention. DLinear requires REMOVING all of these: no convolutional layers, no activation functions (pure linear), no normalization layers within model, no attention mechanisms, no multi-layer stacks. NEW IMPLEMENTATION NEEDED: Minimalist architecture with only 1-2 linear layers total.\n\n4. **Normalization Approach Variants**: Informer uses standard mean-std normalization universally. DLinear introduces NLinear variant with last-value subtraction normalization. NEW IMPLEMENTATION NEEDED: `normalized_input = input - input[:, -1:, :]` before linear layer, then `output = linear_output + input[:, -1:, :]` after prediction. This is a different normalization philosophy (last-value anchoring vs. mean-std) requiring separate implementation.\n\n5. **Embedding and Positional Encoding Removal**: Informer uses DataEmbedding with value embedding, positional encoding, and temporal encoding combined. DLinear requires COMPLETE REMOVAL of all embedding layers - input is raw time series values directly fed to linear layer. NEW IMPLEMENTATION NEEDED: Direct tensor reshaping from `[batch, lookback, channels]` to linear layer input without any embedding transformation.\n\n6. **Decoder Design Philosophy**: Informer's decoder uses start tokens, cross-attention between encoder and decoder, self-attention with masking, and generative-style inference with placeholder tokens. DLinear requires NO DECODER - the linear layer directly maps input length to output length. NEW IMPLEMENTATION NEEDED: Single-shot prediction `output = W @ input` where W directly transforms lookback window to forecast horizon, eliminating all decoder complexity.\n\n7. **Channel Independence vs. Cross-Channel Modeling**: Informer models cross-channel dependencies through attention across all dimensions simultaneously. DLinear enforces strict channel independence with shared weights. NEW IMPLEMENTATION NEEDED: Loop or vectorized operation applying same linear weights to each channel separately `for i in range(channels): output[:,:,i] = linear(input[:,:,i])`, preventing any cross-channel information flow during forward pass.\n\n8. **Computational Complexity Target**: Informer optimizes from O(L²) to O(L log L) through sparse attention. DLinear targets O(L) complexity with pure linear operations. NEW IMPLEMENTATION NEEDED: Implementation must avoid any O(L²) operations - no attention matrices, no pairwise computations, only matrix-vector multiplications with complexity linear in sequence length."
  },
  {
    "source": "Pyraformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Normalization Strategy**: Both papers apply instance normalization with zero-mean and standard deviation scaling. Pyraformer's code shows `mean_enc = x_enc.mean(1, keepdim=True).detach()` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()`, followed by normalization and denormalization. DLinear's NLinear variant uses similar normalization concepts. The normalization module from Pyraformer can be directly reused for DLinear implementation.\n\n2. **Direct Multi-Step (DMS) Forecasting**: Both models predict all future time steps simultaneously rather than autoregressively. Pyraformer uses `self.projection = nn.Linear((len(window_size)+1)*self.d_model, self.pred_len * configs.enc_in)` to output all predictions at once, avoiding error accumulation. DLinear follows the same DMS strategy with `W ∈ R^(T×L)` mapping input to output directly. The projection layer structure can be adapted.\n\n3. **Task Configuration Framework**: Both implementations use a configs object with standard attributes (seq_len, pred_len, enc_in). Pyraformer's code structure `def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None)` provides a template for handling input/output shapes and temporal covariates that can be simplified for DLinear.\n\n4. **Handling Multiple Variates**: Both models process multivariate time series with shape [B, L, D]. Pyraformer's encoder processes all variates together, and DLinear applies linear layers per variate. The data flow structure for multivariate handling can be reused.\n\n5. **Decomposition Awareness**: While Pyraformer doesn't explicitly decompose, it acknowledges multi-scale temporal patterns through pyramidal structure. DLinear explicitly uses moving average decomposition. The conceptual understanding of trend-seasonal separation is shared, though implementation differs.",
    "differences": "1. **Core Architecture - Complexity vs Simplicity**: Pyraformer uses complex pyramidal attention with O(L) complexity involving multi-scale C-ary trees, CSCM with bottleneck convolutions, PAM with custom CUDA kernels, and multi-layer encoders. DLinear requires implementing only simple linear layers (nn.Linear) with O(L) operations - no attention mechanism, no multi-scale hierarchy, no convolutions. NEW: Single-layer linear transformation `W ∈ R^(T×L)` per variate.\n\n2. **Temporal Modeling Mechanism**: Pyraformer captures dependencies through attention mechanism with queries, keys, values across pyramidal graph structures (inter-scale and intra-scale connections). DLinear uses direct weighted summation of historical values - no attention, no learned relationships between time steps beyond linear weights. NEW: Pure temporal linear layer `ŷ_i = WX_i` without any attention computation.\n\n3. **Decomposition Implementation**: Pyraformer has no explicit decomposition module. DLinear requires NEW implementation of: (a) Moving average kernel for trend extraction: `trend = AvgPool1d(kernel_size=K)(x)`, (b) Seasonal component as residual: `seasonal = x - trend`, (c) Separate linear layers for trend and seasonal: `y = Linear_trend(trend) + Linear_seasonal(seasonal)`. This is entirely absent in Pyraformer.\n\n4. **Multi-Scale Processing**: Pyraformer explicitly constructs multi-resolution representations with ConvLayer modules (stride=window_size), Bottleneck_Construct with multiple conv_layers, and processes information at scales S=1 to S with different resolutions (L, L/C, L/C², etc.). DLinear operates at single temporal resolution only - no hierarchical structure. NEW: Flat single-scale architecture.\n\n5. **Input Processing Pipeline**: Pyraformer uses DataEmbedding with positional encodings, temporal embeddings (x_mark_enc), and channel projections before encoder. DLinear requires NEW minimal preprocessing: (a) For NLinear: subtract last value, apply linear, add back last value, (b) For DLinear: decompose, apply two separate linears, sum results. No embedding layers, no positional encoding, no covariate integration.\n\n6. **Parameter Sharing Strategy**: Pyraformer shares parameters across time through attention weights but has separate parameters for different scales and layers (e_layers=3 typically). DLinear requires NEW implementation of weight sharing across all variates (same W for all channels) or channel-independent weights. Much simpler parameter structure.\n\n7. **Model Components to Remove**: For DLinear implementation, completely eliminate: EncoderLayer, AttentionLayer, FullAttention, PositionwiseFeedForward, ConvLayer, Bottleneck_Construct, CSCM, PAM, mask generation (get_mask), refer_points, all attention mechanisms. Replace entire encoder with 1-2 linear layers.\n\n8. **Normalization Variants**: Pyraformer uses standard normalization. DLinear's NLinear requires NEW last-value subtraction normalization: `x_norm = x - x[:, -1:, :]` before linear layer, then `y = y + x[:, -1:, :]` after. This specific normalization is not in Pyraformer.\n\n9. **Computational Graph Simplicity**: Pyraformer has deep computational graph with multiple layers, attention operations, gather operations (torch.gather), and complex tensor manipulations. DLinear requires NEW extremely shallow graph: input → [optional decomposition] → linear(s) → output. No intermediate representations beyond trend/seasonal split."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "",
    "differences": ""
  },
  {
    "source": "DLinear_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers utilize seasonal-trend decomposition as a core preprocessing technique. DLinear implements `series_decomp` with moving average kernels to extract trend and seasonal components, which can be directly reused in Crossformer's preprocessing pipeline. The decomposition logic (moving average for trend extraction, subtraction for seasonal component) is identical and the implementation code is readily adaptable.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both models employ DMS forecasting where the entire prediction horizon is generated in one forward pass, avoiding autoregressive error accumulation. DLinear's approach of directly mapping input length to prediction length (seq_len → pred_len) aligns with Crossformer's decoder design that outputs all future steps simultaneously. The basic forecasting paradigm and loss computation framework can be shared.\n\n3. **Linear Projection Layers**: Both architectures use linear layers for final prediction generation. DLinear's `nn.Linear(seq_len, pred_len)` for temporal mapping and Crossformer's linear projection matrix W^l ∈ R^(L_seg × d_model) serve similar purposes - transforming learned representations to output predictions. The weight initialization strategy (uniform 1/seq_len) from DLinear can inform Crossformer's projection layer initialization.\n\n4. **Multivariate Time Series Handling**: Both models process multivariate data with dimension-aware operations. DLinear's optional individual channel processing (via `individual` flag and `nn.ModuleList`) provides a template for handling per-dimension parameters, which relates to Crossformer's dimension-wise operations in the Cross-Dimension Stage.\n\n5. **Layer Normalization and Residual Connections**: Both employ standard Transformer-style LayerNorm and skip connections. DLinear's implementation pattern of normalization after aggregation can be directly applied to Crossformer's TSA layers and decoder blocks.",
    "differences": "1. **Core Architecture Philosophy**: DLinear is fundamentally a linear model that questions the necessity of Transformers, using only linear layers and decomposition without any attention mechanism. Crossformer, conversely, is a sophisticated Transformer variant that requires implementing: (a) Two-Stage Attention (TSA) with separate cross-time and cross-dimension stages, (b) Multi-head self-attention mechanisms with Q, K, V projections, (c) Router mechanism with learnable router vectors for dimension aggregation, (d) Hierarchical encoder-decoder structure with segment merging. None of these attention-based components exist in DLinear.\n\n2. **Input Embedding Strategy**: DLinear operates on raw time series with simple decomposition, while Crossformer requires implementing Dimension-Segment-Wise (DSW) embedding that: (a) Segments each dimension into patches of length L_seg, (b) Projects segments via learnable matrix E ∈ R^(d_model × L_seg), (c) Adds 2D positional embeddings E^(pos)_{i,d} for both temporal and dimensional positions. This creates a 2D vector array representation completely absent in DLinear.\n\n3. **Multi-Scale Hierarchical Processing**: Crossformer implements a hierarchical structure requiring: (a) Segment merging operation (M[Z_{2i-1,d} · Z_{2i,d}]) to create coarser temporal scales, (b) Multiple encoder layers (N layers) with progressively merged segments, (c) Multi-scale decoder with layer-wise predictions that are summed (∑_{l=0}^N x^{pred,l}), (d) Cross-attention between encoder and decoder at each scale. DLinear has no multi-scale processing - it operates at a single temporal resolution.\n\n4. **Cross-Dimension Dependency Modeling**: Crossformer explicitly captures cross-dimension correlations through: (a) Router mechanism with c learnable router vectors that aggregate information from all D dimensions, (b) Two-stage MSA operations (MSA_1^dim and MSA_2^dim) for dimension-to-dimension communication, (c) Complexity reduction from O(D²) to O(D) via router design. DLinear either shares weights across dimensions or processes them independently, with no explicit cross-dimension attention mechanism.\n\n5. **Computational Complexity and Scalability**: DLinear has O(1) complexity with respect to sequence length (just linear layers), while Crossformer requires implementing: (a) O(DL²) complexity for cross-time attention where L = T/L_seg, (b) O(DL) complexity for router-based cross-dimension attention, (c) Hierarchical processing with complexity scaling across N encoder/decoder layers, (d) Efficient attention implementations with proper masking and batching for large D and T. The entire attention computation infrastructure needs to be built from scratch.\n\n6. **Decoder Architecture**: DLinear has no explicit decoder (just applies linear layers to encoded features), while Crossformer requires: (a) Learnable decoder position embeddings E^(dec), (b) TSA layers in decoder with both self-attention and cross-attention to encoder outputs, (c) Layer-wise decoding with N+1 decoder layers corresponding to encoder scales, (d) Dimension-wise cross-attention (MSA(Z̃_{:,d}^{dec,l}, Z_{:,d}^{enc,l}, Z_{:,d}^{enc,l})) connecting decoder queries to encoder keys/values at each scale."
  },
  {
    "source": "FEDformer_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture with Decomposition**: Both use hierarchical encoder-decoder structures with seasonal-trend decomposition. FEDformer's `series_decomp` and MOEDecomp blocks can be directly adapted, as Crossformer also benefits from decomposition though not explicitly shown. The encoder-decoder framework with layer normalization and residual connections from FEDformer is reusable.\n\n2. **Embedding Strategy**: Both employ learnable embeddings with positional encoding. FEDformer's `DataEmbedding` class (combining value embedding + positional + temporal encoding) can be adapted for Crossformer's DSW embedding by modifying the projection dimension to handle segments instead of single time steps.\n\n3. **Multi-Head Attention Mechanism**: Both leverage attention mechanisms extensively. FEDformer's MSA implementation in `AutoCorrelationLayer` wrapper can be reused as Crossformer uses standard MSA in cross-time stage and cross-attention between encoder-decoder. The query-key-value projection logic is directly transferable.\n\n4. **Hierarchical Multi-Scale Processing**: Both models process information at multiple scales through layer stacking. FEDformer's encoder-decoder layer structure with progressive information aggregation can serve as a template for Crossformer's hierarchical encoder with segment merging.\n\n5. **Feed-Forward Networks and Normalization**: Both use identical MLP blocks (two-layer feed-forward networks) with LayerNorm and residual connections. FEDformer's implementation of these components can be directly reused without modification.\n\n6. **Training Objective**: Both optimize MSE/MAE loss for forecasting tasks. FEDformer's loss computation and training loop infrastructure can be directly adapted for Crossformer with minimal changes.",
    "differences": "1. **Core Innovation - Dimension-Segment-Wise (DSW) Embedding**: Crossformer's key innovation is embedding segments of each dimension separately (creating 2D array) rather than embedding all dimensions at each time step. This requires NEW implementation: (a) segment partitioning logic to divide each dimension into L_seg length segments, (b) 2D position embedding for (time_segment, dimension) pairs, (c) reshaping operations to maintain 2D structure throughout the network. FEDformer's 1D sequential embedding cannot be directly used.\n\n2. **Two-Stage Attention (TSA) Layer**: Crossformer introduces a novel two-stage attention mechanism NOT present in FEDformer: (a) Cross-Time Stage: applies MSA independently to each dimension across time segments (requires NEW dimension-wise iteration logic), (b) Cross-Dimension Stage with Router Mechanism: uses learnable router vectors to aggregate and distribute information across dimensions with O(D) complexity instead of O(D²). This entire router mechanism (two separate MSA operations with intermediate router aggregation) needs NEW implementation from scratch.\n\n3. **Segment Merging in Encoder**: Crossformer employs hierarchical segment merging where adjacent time segments are concatenated and projected to coarser scales. This requires NEW implementation: (a) pairwise segment concatenation logic, (b) learnable merging matrix M for dimension reduction, (c) handling of variable segment lengths across encoder layers. FEDformer uses frequency domain transformations (Fourier/Wavelet) for multi-scale, which is fundamentally different.\n\n4. **Frequency Domain vs Spatial Domain Processing**: FEDformer's core is frequency-enhanced processing (FEB-f/FEB-w using FFT/DWT, frequency mode selection, complex number operations). Crossformer operates entirely in spatial/time domain without any frequency transformations. ALL of FEDformer's frequency components (FourierBlock, FourierCrossAttention, MultiWaveletTransform, etc.) are NOT applicable and Crossformer needs pure spatial attention mechanisms.\n\n5. **Decoder Architecture and Multi-Scale Prediction**: Crossformer's decoder makes predictions at EACH scale (N+1 layers) and sums them for final output, with separate learnable projection matrices W^l for each layer. FEDformer's decoder uses trend accumulation across layers with single final projection. NEW implementation needed: (a) per-layer prediction heads, (b) multi-scale prediction aggregation logic, (c) learnable position embeddings for decoder E^(dec).\n\n6. **Cross-Attention Design**: Crossformer uses dimension-wise cross-attention where each dimension independently attends to encoder outputs, requiring NEW per-dimension iteration. FEDformer's cross-attention (FEA-f/FEA-w) operates on full frequency representations with mode selection, which is structurally incompatible.\n\n7. **Complexity Optimization Strategy**: Crossformer achieves O(DL²) through router mechanism and dimension-wise processing. FEDformer achieves O(L) through frequency mode selection (selecting M<<L modes). The optimization strategies are fundamentally different - Crossformer needs NEW router implementation while FEDformer's mode selection logic is not applicable."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Multi-scale Hierarchical Architecture**: Both employ hierarchical encoder structures to capture temporal dependencies at different scales. Pyraformer uses C-ary tree with pyramidal attention across scales (coarser scales summarize C nodes from finer scales), while Crossformer uses segment merging to create coarser representations. The source code's `Bottleneck_Construct` module and hierarchical layer stacking pattern can be adapted for Crossformer's encoder hierarchy.\n\n2. **Attention Mechanism Foundation**: Both build on standard multi-head self-attention (MSA) with Query-Key-Value paradigm. Pyraformer's `AttentionLayer` and `FullAttention` components provide the base attention implementation that can be directly reused for Crossformer's cross-time stage MSA operations in Eq. 3.\n\n3. **Embedding Strategy with Position Encoding**: Both use learnable embeddings combined with positional information. Pyraformer's `DataEmbedding` class structure (combining data projection and position encoding) can be adapted for Crossformer's DSW embedding in Eq. 2, though the projection target differs (time segments vs. dimension-segment pairs).\n\n4. **Layer Normalization and Residual Connections**: Both employ LayerNorm with residual connections in their attention layers. Pyraformer's `EncoderLayer` implementation with `self.slf_attn` followed by `self.pos_ffn` demonstrates the same pattern used in Crossformer's TSA layer (Eq. 3-4), making these components directly reusable.\n\n5. **Feed-Forward Networks**: Both use position-wise FFN with GELU activation. Pyraformer's `PositionwiseFeedForward` module with two linear layers and dropout can be directly reused in Crossformer's MLP components in Eq. 3, 4, and 7.\n\n6. **Instance Normalization for Non-stationarity**: Both apply input normalization by subtracting mean and dividing by standard deviation before encoding, then denormalize predictions. Pyraformer's `short_forecast` method shows this pattern (mean_enc, std_enc operations) which is essential for Crossformer's forecasting pipeline.\n\n7. **Encoder-Decoder Framework**: Both use encoder-decoder architecture where encoder extracts multi-scale features and decoder generates predictions. Pyraformer's encoder output gathering and projection pattern provides a template for Crossformer's hierarchical decoder structure.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Pyraformer uses pyramidal attention with sparse connectivity (nodes attend to neighbors at same scale, children, and parent in C-ary tree, Eq. 2-3) achieving O(L) complexity. Crossformer introduces Two-Stage Attention (TSA) with separate cross-time and cross-dimension stages (Eq. 3-4), requiring NEW implementation of: (a) Cross-time MSA applied independently per dimension, (b) Router mechanism for cross-dimension stage with learnable router vectors to aggregate/distribute information (Eq. 4), reducing D²L to DL complexity. The router mechanism with MSA₁ᵈⁱᵐ and MSA₂ᵈⁱᵐ is completely novel.\n\n2. **Data Representation - NEW EMBEDDING REQUIRED**: Pyraformer embeds time steps into 1D sequence where each vector represents all dimensions at one time point. Crossformer uses Dimension-Segment-Wise (DSW) embedding creating 2D array representation (Eq. 1-2) where each vector represents a univariate segment (length L_seg) in specific dimension, requiring NEW implementation of: (a) Segment partitioning logic for each dimension separately, (b) 2D position embedding for (time_index, dimension_index) pairs, (c) Linear projection from segment to d_model.\n\n3. **Hierarchical Construction Strategy - NEW MODULE NEEDED**: Pyraformer builds hierarchy through convolution layers with stride C in time dimension (`ConvLayer` with kernel_size=window_size), creating temporal coarsening. Crossformer requires NEW segment merging module (Eq. 6) that: (a) Merges pairs of adjacent vectors in time for each dimension independently, (b) Uses learnable matrix M ∈ R^(d_model × 2d_model) for concatenation-based merging, (c) Maintains 2D structure throughout hierarchy rather than flattening to 1D.\n\n4. **Decoder Architecture - COMPLETELY NEW DESIGN**: Pyraformer uses simple decoder with either: (a) single projection from last encoder nodes, or (b) two full-attention layers with prediction tokens. Crossformer requires NEW hierarchical decoder (Eq. 7-8) with: (a) N+1 decoder layers matching N+1 encoder scales, (b) Each layer performs TSA on learnable position embeddings E^(dec), (c) Cross-attention between decoder queries and corresponding encoder scale outputs, (d) Multi-scale prediction fusion by summing predictions from all decoder layers (Eq. 8), not present in Pyraformer.\n\n5. **Cross-Dimension Dependency Modeling - NEW CAPABILITY**: Pyraformer processes all dimensions together in each vector, implicitly mixing dimension information but not explicitly modeling cross-dimension relationships. Crossformer explicitly captures cross-dimension dependency through: (a) Router mechanism in cross-dimension stage enabling dimension-to-dimension interaction, (b) Independent processing per dimension in cross-time stage followed by dimension mixing, requiring NEW implementation of dimension-aware attention routing.\n\n6. **Complexity Optimization Strategy**: Pyraformer reduces complexity through sparse pyramidal graph (O(AL) with A=3 or 5 neighbors) and custom CUDA kernel for non-standard attention patterns. Crossformer achieves O(DL²+DL) through: (a) Factorized two-stage attention separating time and dimension, (b) Router mechanism with fixed small c << D routers, requiring NEW implementation of factorized attention scheduling without custom CUDA kernels.\n\n7. **Scale-Specific Prediction Strategy - NEW FUSION REQUIRED**: Pyraformer makes single prediction by projecting concatenated multi-scale features from encoder's last nodes at all scales. Crossformer requires NEW multi-scale prediction fusion (Eq. 8): (a) Each decoder layer produces independent prediction at its scale using W^l projection, (b) Final prediction sums all scale-specific predictions, (c) Each scale contributes different temporal granularity information, requiring implementation of per-layer projection matrices and summation logic."
  },
  {
    "source": "Autoformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt Transformer-based encoder-decoder architectures with multiple layers. The source code's `Encoder` and `Decoder` classes with layer stacking logic can be directly adapted. The basic structure of `EncoderLayer` and `DecoderLayer` with attention mechanisms, feedforward networks, and residual connections provides a reusable template.\n\n2. **Embedding Components**: Both use sophisticated embedding strategies combining value embeddings with positional/temporal information. The source code's `DataEmbedding_wo_pos`, `TokenEmbedding` (Conv1d-based), `PositionalEmbedding`, and `TemporalEmbedding`/`TimeFeatureEmbedding` modules can be reused or adapted. Crossformer's DSW embedding requires segmentation logic but can leverage the existing token embedding structure.\n\n3. **Normalization and Regularization**: Both employ LayerNorm and Dropout extensively. The source code's `my_Layernorm` (specialized for seasonal components) and standard `nn.LayerNorm` usage patterns can be directly reused. Dropout placement strategies in attention and feedforward layers are consistent.\n\n4. **Multi-Head Attention Mechanism**: Both use multi-head attention as a core component. The source code's attention layer structure with query/key/value projections and multi-head processing provides a foundation. While Crossformer's TSA differs from Auto-Correlation, the projection logic (`query_projection`, `key_projection`, `value_projection`) and output projection patterns can be adapted.\n\n5. **Hierarchical Processing**: Both adopt hierarchical structures to capture multi-scale information. Autoformer's progressive decomposition in encoder/decoder and Crossformer's segment merging share conceptual similarity. The source code's layer-wise processing and intermediate feature extraction logic can inform Crossformer's hierarchical encoder-decoder implementation.\n\n6. **Feedforward Network Design**: Both use similar FFN structures with Conv1d layers. The source code's `conv1` and `conv2` pattern with activation functions (ReLU/GELU) and dropout can be directly reused in Crossformer's TSA layers.\n\n7. **Training Infrastructure**: Both require similar training loops, loss functions (MSE/MAE), and data processing pipelines. The source code's model forward pass structure with `x_enc`, `x_mark_enc`, `x_dec`, `x_mark_dec` inputs provides a template for Crossformer's data flow.",
    "differences": "1. **Core Attention Mechanism - REQUIRES NEW IMPLEMENTATION**: Autoformer uses Auto-Correlation with FFT-based period discovery and time delay aggregation, while Crossformer uses Two-Stage Attention (TSA) with separate cross-time and cross-dimension stages. The entire `AutoCorrelation` class and its `time_delay_agg_*` methods cannot be reused. NEW COMPONENTS NEEDED: (a) `CrossTimeAttention` applying standard MSA along time dimension for each variable independently, (b) `CrossDimensionAttention` with router mechanism using two sequential MSA operations (`MSA_1` and `MSA_2`) with learnable router vectors, (c) `TSALayer` combining both stages sequentially.\n\n2. **Embedding Strategy - REQUIRES NEW IMPLEMENTATION**: Autoformer embeds individual time points across all dimensions, while Crossformer uses Dimension-Segment-Wise (DSW) embedding that segments each dimension independently before embedding. NEW COMPONENTS NEEDED: (a) Segmentation logic to split input `[B, T, D]` into segments `[B, T/L_seg, D, L_seg]`, (b) Dimension-wise linear projection for each segment, (c) 2D positional embedding `E^(pos)_{i,d}` for position `(i,d)` in the segment-dimension grid, (d) Reshaping logic to create 2D vector arrays `[B, L, D, d_model]` instead of 1D sequences.\n\n3. **Decomposition Architecture - NOT NEEDED IN CROSSFORMER**: Autoformer's series decomposition block (`series_decomp`, `moving_avg`) that separates seasonal and trend components is central to its design, appearing in every encoder/decoder layer. Crossformer does NOT use decomposition at all. The entire `series_decomp` class, trend accumulation logic in decoder (`T_de` variables), and separate seasonal/trend processing paths should NOT be implemented for Crossformer.\n\n4. **Hierarchical Structure Implementation - REQUIRES NEW IMPLEMENTATION**: While both are hierarchical, the mechanisms differ fundamentally. Autoformer progressively decomposes within layers, while Crossformer uses explicit segment merging between encoder layers. NEW COMPONENTS NEEDED: (a) Segment merging operation `M[Z_{2i-1,d} · Z_{2i,d}]` that concatenates and projects adjacent segments in time, (b) Multi-scale decoder structure where each decoder layer operates at different temporal granularities corresponding to encoder layers, (c) Layer-wise prediction and aggregation logic where each decoder layer produces predictions that are summed.\n\n5. **Decoder Input and Processing - REQUIRES NEW IMPLEMENTATION**: Autoformer initializes decoder with decomposed seasonal/trend from encoder's latter half plus placeholders. Crossformer uses learnable position embeddings only. NEW COMPONENTS NEEDED: (a) Learnable decoder position embedding `E^(dec)` of shape `[τ/L_seg, D, d_model]` instead of data-derived initialization, (b) Encoder-decoder cross-attention applied dimension-wise (each dimension independently) rather than globally, (c) Multi-scale cross-attention where decoder layer `l` attends to encoder layer `l`.\n\n6. **Output Projection Strategy - REQUIRES NEW IMPLEMENTATION**: Autoformer uses single final projection after summing seasonal and trend components. Crossformer projects at each decoder layer and sums predictions. NEW COMPONENTS NEEDED: (a) Layer-specific projection matrices `W^l` for each decoder layer, (b) Segment-to-sequence reconstruction logic to convert `[B, τ/L_seg, D, d_model]` back to `[B, τ, D]`, (c) Multi-scale prediction summation across all decoder layers.\n\n7. **Router Mechanism for Dimension Attention - REQUIRES NEW IMPLEMENTATION**: Unique to Crossformer for handling large D efficiently. NEW COMPONENTS NEEDED: (a) Learnable router vectors `R ∈ R^{L×c×d_model}` where c is a small constant, (b) Two-stage cross-dimension attention: first aggregating from all dimensions to routers (`MSA_1`), then distributing from routers to all dimensions (`MSA_2`), (c) Separate MSA modules for aggregation and distribution stages.\n\n8. **Complexity Optimization Focus**: Autoformer optimizes for long sequence length using O(L log L) Auto-Correlation with FFT. Crossformer optimizes for large dimension count using O(DL) router mechanism instead of O(D²L) full cross-dimension attention. This requires completely different efficiency strategies in implementation."
  },
  {
    "source": "Informer_2020",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both use Transformer-based encoder-decoder architecture for time series forecasting. Crossformer can reuse Informer's basic encoder-decoder structure (Encoder, Decoder classes), LayerNorm, and MLP components from the implementation code.\n\n2. **Embedding Strategy**: Both employ learnable embeddings with positional encoding. Crossformer can adapt Informer's DataEmbedding class structure, though it needs to modify it for segment-wise rather than point-wise embedding (DSW embedding).\n\n3. **Multi-Head Attention Mechanism**: Both rely on multi-head attention for dependency modeling. Crossformer can reuse Informer's AttentionLayer structure (query/key/value projections, multi-head split/merge logic) but needs to replace ProbAttention with standard MSA and implement the two-stage attention pattern.\n\n4. **Hierarchical Feature Extraction**: Both use hierarchical structures to capture multi-scale information. Informer's ConvLayer and distilling operation (max pooling with stride 2) provide a reference for Crossformer's segment merging, though Crossformer uses learnable matrix merging instead of convolution.\n\n5. **Normalization and Residual Connections**: Both apply instance normalization (mean/std normalization on input) and LayerNorm with residual connections. Crossformer can directly reuse Informer's normalization logic in short_forecast() and LayerNorm layers.\n\n6. **Cross-Attention in Decoder**: Both decoders use cross-attention between decoder queries and encoder outputs. Crossformer can adapt Informer's DecoderLayer cross-attention mechanism, though it needs dimension-wise rather than full cross-attention.\n\n7. **Training Objective**: Both use MSE loss for forecasting and support multivariate time series prediction. The training loop and loss computation can be largely reused.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Informer uses ProbSparse attention with query sparsity measurement (Eq. 2-4) and Top-u query selection for O(L log L) complexity. Crossformer requires implementing standard full self-attention in a Two-Stage pattern: (a) Cross-Time Stage with dimension-wise MSA (Eq. 3), (b) Cross-Dimension Stage with router mechanism using two sequential MSA operations (Eq. 4). The router mechanism with learnable router vectors R is entirely new.\n\n2. **Embedding Paradigm - NEW IMPLEMENTATION REQUIRED**: Informer embeds each time point across all dimensions into a single vector (point-wise). Crossformer requires Dimension-Segment-Wise (DSW) embedding (Eq. 1-2) that divides each dimension into segments of length L_seg and embeds each segment independently, creating a 2D array structure [L×D×d_model] instead of 1D sequence.\n\n3. **Hierarchical Merging Strategy - NEW IMPLEMENTATION REQUIRED**: Informer uses ConvLayer with 1D convolution (kernel_size=3) and max pooling for distilling. Crossformer requires implementing segment merging (Eq. 6) using learnable matrix M to concatenate and project adjacent segments [Z_{2i-1}, Z_{2i}] → Z_i, without convolution operations.\n\n4. **Decoder Design - NEW IMPLEMENTATION REQUIRED**: Informer uses generative inference with start tokens from historical data and masked attention. Crossformer requires implementing: (a) learnable position embeddings E^(dec) for decoder initialization (no start tokens), (b) dimension-wise cross-attention between decoder and each encoder layer (Eq. 7), (c) multi-scale prediction aggregation from N+1 decoder layers (Eq. 8).\n\n5. **Prediction Strategy - NEW IMPLEMENTATION REQUIRED**: Informer outputs predictions from a single decoder projection layer. Crossformer requires implementing hierarchical predictions: each decoder layer l produces predictions via linear projection W^l, and final prediction is the sum of all layer predictions (Eq. 8), capturing information at different temporal scales.\n\n6. **Complexity and Scalability**: Informer achieves O(L log L) complexity through sparse attention, suitable for very long sequences. Crossformer has O(DL²) complexity in TSA layer, optimized for multivariate (large D) rather than extremely long sequences, with router mechanism reducing cross-dimension complexity from O(D²L) to O(DL).\n\n7. **Cross-Dimension Dependency Modeling - NEW IMPLEMENTATION REQUIRED**: Informer implicitly models cross-dimension dependency through point-wise embedding. Crossformer explicitly captures it through: (a) separate Cross-Dimension Stage in TSA, (b) router mechanism with c learnable routers aggregating and distributing information across D dimensions (Eq. 4), requiring implementation of MSA₁^dim and MSA₂^dim operations.\n\n8. **Multi-Scale Architecture**: Informer's encoder uses progressive distilling with replicas at different input scales (pyramid structure in Fig. 2). Crossformer requires implementing a different hierarchical structure where each encoder layer operates at progressively coarser scales through segment merging, and each decoder layer corresponds to one encoder scale for multi-scale prediction fusion."
  },
  {
    "source": "Autoformer_2021",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Architecture**: Both papers adopt decomposition-based architectures that explicitly separate time series into multiple components. Autoformer uses series_decomp blocks with moving average to extract trend-cyclical and seasonal parts, while ETSformer extracts level, growth, and seasonal components. The source paper's series_decomp module (moving_avg + residual calculation) can be adapted as a baseline for ETSformer's component extraction, though ETSformer uses different extraction mechanisms (ESA for growth, FA for seasonality).\n\n2. **Encoder-Decoder Framework**: Both utilize encoder-decoder architectures where the encoder processes the lookback window and the decoder generates forecasts. Autoformer's Encoder/Decoder class structure, layer stacking mechanism, and residual connections can be directly reused. The DataEmbedding_wo_pos module for input embedding (TokenEmbedding with Conv1d) is applicable to ETSformer, which also uses Conv-based embedding without positional encodings or time features.\n\n3. **Component Accumulation Strategy**: Both papers progressively refine predictions through multi-layer stacking. Autoformer accumulates trend components across decoder layers (T_de^l = T_de^(l-1) + W*T_de^(l,i)), similar to ETSformer's composition of level, growth, and seasonal forecasts. The decoder's progressive refinement structure and layer normalization (my_Layernorm) can be adapted for ETSformer's stacked G+S blocks.\n\n4. **Efficient Computation via FFT**: Both leverage Fast Fourier Transform for O(L log L) complexity. Autoformer uses FFT in Auto-Correlation for computing series autocorrelation (Wiener-Khinchin theorem), while ETSformer uses FFT in Frequency Attention for DFT-based seasonal pattern extraction. The FFT-based computation patterns (torch.fft.rfft, torch.fft.irfft) from Autoformer's AutoCorrelation module provide implementation guidance for ETSformer's FA mechanism.\n\n5. **Multi-Head Attention Paradigm**: Both employ multi-head mechanisms for representation learning. Autoformer's AutoCorrelationLayer with query/key/value projections and multi-head output concatenation can serve as a template for ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA), though the attention computation differs fundamentally (autocorrelation vs. exponential smoothing).\n\n6. **Feedforward Networks**: Both use position-wise feedforward networks with Conv1d layers for feature transformation. Autoformer's EncoderLayer/DecoderLayer feedforward structure (conv1 + activation + dropout + conv2) can be directly reused in ETSformer's encoder layers after seasonal/growth extraction.\n\n7. **Normalization and Residual Learning**: Both apply layer normalization and residual connections for stable training. Autoformer's my_Layernorm (with bias removal for seasonal part) and residual addition patterns can be adapted for ETSformer's encoder residual updates (Z := Z - S, Z := LN(Z - B)).",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Autoformer uses Auto-Correlation mechanism based on period-based dependencies via autocorrelation (R_QK(τ)) and time delay aggregation (Roll operation), while ETSformer introduces two novel attention mechanisms: (a) **Exponential Smoothing Attention (ESA)**: Non-adaptive, learnable attention with exponential decay weights based on relative time lag (α*V_t + (1-α)*A_ES(V)_(t-1)), requiring implementation of efficient cross-correlation via FFT (Algorithm 1); (b) **Frequency Attention (FA)**: Non-learnable attention selecting top-K Fourier bases by amplitude, requiring DFT decomposition, amplitude/phase extraction, Top-K selection, and inverse DFT reconstruction (Equation 4). Neither mechanism exists in Autoformer's codebase.\n\n2. **Component Extraction Philosophy - NEW IMPLEMENTATION REQUIRED**: Autoformer extracts components via moving average decomposition (trend = AvgPool, seasonal = residual), while ETSformer uses specialized attention mechanisms: ESA for growth (successive difference of smoothed residuals), FA for seasonal (Fourier-based pattern extraction), and level smoothing equation (E_t = α*(E_t - S_t) + (1-α)*(E_(t-1) + B_(t-1))). The level module with learnable smoothing parameter α and the growth extraction via MH-ESA need complete new implementation.\n\n3. **Decoder Forecasting Strategy - NEW IMPLEMENTATION REQUIRED**: Autoformer's decoder refines seasonal and trend components through Auto-Correlation layers with encoder cross-attention, while ETSformer uses: (a) **Growth Damping (GD)**: Applies damped trend forecasting (TD(B_t)_j = Σ γ^i * B_t) with learnable damping parameter γ; (b) **Level Repetition**: Simply repeats last level E_t across forecast horizon; (c) **Seasonal Extrapolation**: Uses FA to extrapolate seasonal patterns beyond lookback window. The GD module and level repetition strategy require new implementation.\n\n4. **Input Requirements and Embedding - SIMPLIFICATION**: Autoformer uses time-dependent covariates (x_mark_enc, x_mark_dec) with TemporalEmbedding/TimeFeatureEmbedding for month/day/hour features, while ETSformer eliminates manual time features entirely, relying solely on raw signals with Conv-based embedding. ETSformer's implementation should remove the temporal embedding components and x_mark inputs from Autoformer's DataEmbedding module.\n\n5. **Interpretability and Output Composition - NEW IMPLEMENTATION REQUIRED**: Autoformer outputs final prediction as sum of refined seasonal and trend (W_S * X_de^M + T_de^M), while ETSformer provides interpretable forecasts via explicit composition: ŷ = E_(t:t+H) + Linear(Σ(B^(n) + S^(n))). The final Linear projection from latent (d-dimensional) to observation space (m-dimensional) for growth+seasonal components, and the separate level forecast handling need new implementation.\n\n6. **Encoder Residual Processing - NEW IMPLEMENTATION REQUIRED**: Autoformer's encoder extracts seasonal component and discards trend via series_decomp, while ETSformer sequentially removes seasonal (Z := Z - S) then growth (Z := Z - B) with layer normalization between removals, followed by feedforward transformation. This sequential removal pattern with intermediate normalization differs from Autoformer's single decomposition step.\n\n7. **Computational Efficiency Focus - DIFFERENT OPTIMIZATION**: Autoformer's Auto-Correlation achieves O(L log L) through FFT-based autocorrelation and sparse top-k delay selection, while ETSformer's ESA achieves O(L log L) through efficient cross-correlation algorithm exploiting attention matrix structure (Algorithm 1), and FA achieves O(L log L) through FFT with small K hyperparameter. The ESA's efficient algorithm exploiting right-shift structure requires specialized implementation beyond standard FFT usage."
  },
  {
    "source": "Informer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both papers adopt the encoder-decoder paradigm for time series forecasting. The source code's basic encoder-decoder structure (Encoder, Decoder classes) and layer stacking mechanism can be directly reused. ETSformer can leverage the modular design pattern with encoder/decoder layers.\n\n2. **Instance Normalization Strategy**: Both apply instance normalization (mean subtraction and std division) on input sequences before processing. The source code's normalization implementation in `short_forecast()` method (mean_enc, std_enc computation and denormalization) can be directly adopted for ETSformer.\n\n3. **Embedding Module**: Both use embedding modules to project raw inputs to latent space. The source's `DataEmbedding` class structure (combining value embedding with temporal encodings) provides a template, though ETSformer uses simpler Conv-based embedding without explicit temporal features.\n\n4. **Multi-Head Mechanism**: Both employ multi-head attention mechanisms for feature extraction. The source's multi-head implementation pattern (query/key/value projections, head splitting/concatenation in `AttentionLayer`) can be adapted for ETSformer's Multi-Head Exponential Smoothing Attention.\n\n5. **Layer Normalization and Residual Connections**: Both use LayerNorm and residual connections throughout the architecture. The source code's `norm_layer=torch.nn.LayerNorm(configs.d_model)` and residual addition patterns can be directly reused in ETSformer's encoder layers.\n\n6. **Feedforward Networks**: Both include position-wise feedforward networks in their layers. The source's Conv1d-based feedforward pattern in `DecoderLayer` (conv1, conv2 with activation) can be adapted for ETSformer's FF module.\n\n7. **Training Objective**: Both optimize MSE loss for forecasting. The source's training loop and loss computation framework can be directly reused.\n\n8. **Batch Processing**: Both handle batched inputs with shape [B, L, D]. The source's tensor manipulation patterns and batch processing logic can be directly applied.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Informer uses ProbSparse Attention (content-based, adaptive, O(L log L) via query sampling), while ETSformer introduces two novel mechanisms: (a) **Exponential Smoothing Attention (ESA)**: Non-adaptive, time-lag-based attention with exponential decay, requires implementing Algorithm 1 with FFT-based cross-correlation for O(L log L) complexity; (b) **Frequency Attention (FA)**: Non-learnable, DFT-based seasonal pattern extraction with Top-K frequency selection. Both mechanisms need complete new implementations.\n\n2. **Decomposition Architecture - NEW IMPLEMENTATION REQUIRED**: Informer has no explicit decomposition, while ETSformer decomposes signals into Level, Growth (trend), and Seasonal components at each layer. Need to implement: (a) Level Module with exponential smoothing equation and learnable α parameter; (b) Growth extraction via successive differencing in MH-ESA; (c) Seasonal extraction via FA; (d) Residual learning pipeline with sequential component removal.\n\n3. **Encoder Design Philosophy - MAJOR MODIFICATION**: Informer uses self-attention distilling with ConvLayer and MaxPooling for progressive downsampling (reducing sequence length). ETSformer maintains constant sequence length and uses residual learning to iteratively extract components. Need to remove distilling mechanism and implement component-wise extraction with residual updates.\n\n4. **Decoder Forecasting Strategy - NEW IMPLEMENTATION REQUIRED**: Informer uses autoregressive-style decoder with start tokens and masked attention. ETSformer uses component-based composition: (a) **Growth Damping (GD)**: Implement trend damping with learnable γ parameter and multi-head damping; (b) **Level Forecasting**: Simple repetition of last level; (c) **Seasonal Forecasting**: FA extrapolation beyond lookback window; (d) Final forecast as Linear(sum of growth + seasonal) + level.\n\n5. **Input Representation - SIMPLIFICATION**: Informer relies on rich temporal features (x_mark_enc, x_mark_dec with time covariates like month, day-of-week). ETSformer explicitly avoids manual temporal features, using only raw values with Conv embedding. Need to remove temporal feature dependencies.\n\n6. **Frequency Domain Processing - NEW IMPLEMENTATION REQUIRED**: Informer operates entirely in time domain. ETSformer's FA requires implementing: (a) DFT along temporal dimension; (b) Amplitude and phase extraction; (c) Top-K frequency selection per dimension; (d) Inverse DFT for reconstruction; (e) Extrapolation to forecast horizon using selected frequencies.\n\n7. **Efficient Algorithm Implementation - NEW IMPLEMENTATION REQUIRED**: Informer's efficiency comes from query sampling in ProbSparse Attention. ETSformer requires implementing: (a) FFT-based cross-correlation for ESA (Algorithm 1); (b) Efficient DFT/IDFT for FA; Both achieve O(L log L) through different mechanisms.\n\n8. **Interpretability Design - ARCHITECTURAL CHANGE**: Informer produces black-box predictions. ETSformer explicitly separates and outputs interpretable Level, Growth, and Seasonal components. Need to implement component tracking, separate projection layers, and additive composition for final forecast.\n\n9. **Multi-Scale Handling - REMOVED FEATURE**: Informer uses hierarchical encoder with distilling for multi-scale. ETSformer handles scales implicitly through frequency selection in FA. Need to remove Informer's pyramid-style multi-stack encoder.\n\n10. **Learnable vs Non-learnable Attention - MIXED APPROACH**: Informer's attention is fully learnable (Q, K, V projections). ETSformer combines learnable ESA (α, v0 parameters) with non-learnable FA (pure frequency-based). Need to implement both paradigms appropriately."
  },
  {
    "source": "Reformer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanism via FFT**: Both papers leverage Fast Fourier Transform (FFT) for computational efficiency. Reformer uses LSH with angular hashing for O(L log L) attention, while ETSformer's Frequency Attention and Exponential Smoothing Attention both achieve O(L log L) complexity through FFT-based operations. The source code's FFT infrastructure and efficient attention computation patterns can be directly adapted.\n\n2. **Encoder-Decoder Architecture with Stacked Layers**: Both adopt multi-layer encoder structures with residual connections. Reformer's reversible layers and ETSformer's cascaded encoder layers both progressively extract representations. The source code's `Encoder` and `EncoderLayer` classes with layer normalization and feedforward networks can be reused as architectural templates.\n\n3. **Instance Normalization for Time Series**: Both apply instance-wise normalization on input sequences. Reformer's `short_forecast` method implements mean-std normalization (mean_enc, std_enc) that can be directly reused in ETSformer's preprocessing pipeline to handle distribution shifts across different time series.\n\n4. **Sequence Length Handling and Padding**: Reformer's `fit_length` method pads sequences to multiples of bucket_size for efficient processing. ETSformer similarly needs to handle variable-length inputs for its frequency-domain operations, making this padding logic directly transferable.\n\n5. **Multi-Head Mechanism**: Both employ multi-head attention patterns. Reformer's multi-head LSH attention structure (with n_heads parameter) provides a template for implementing ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA), where the head splitting and concatenation logic can be reused.\n\n6. **Linear Projection Layers**: Both use linear transformations to project between latent and observation spaces. Reformer's `projection` layer and the Linear operations in attention modules provide reusable components for ETSformer's level/growth/seasonal projections.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses Locality-Sensitive Hashing (LSH) with random projections and hash bucketing for approximate nearest-neighbor attention, while ETSformer introduces two completely novel attention types: (a) **Exponential Smoothing Attention (ESA)** - a non-adaptive, time-lag-based attention with learnable decay parameter α and initial state v₀, requiring implementation of Algorithm 1 with cross-correlation operations; (b) **Frequency Attention (FA)** - a non-learnable attention that selects Top-K frequency components via DFT amplitude analysis (Equation 4), requiring phase/amplitude extraction and inverse DFT reconstruction.\n\n2. **Time Series Decomposition Framework - NEW IMPLEMENTATION REQUIRED**: ETSformer explicitly decomposes signals into **Level, Growth (Trend), and Seasonal** components following exponential smoothing principles, requiring: (a) **Level Module** with learnable smoothing parameter α and recurrent exponential smoothing equations; (b) **Growth Damping (GD)** module with learnable damping parameter γ for multi-step trend forecasting; (c) **Residual learning** where seasonal and growth components are sequentially extracted and removed from the residual. Reformer has no such decomposition structure.\n\n3. **Decoder Architecture and Forecasting Strategy - NEW IMPLEMENTATION REQUIRED**: ETSformer uses a specialized decoder with **N G+S Stacks** (Growth+Seasonal) and a **Level Stack**, where: (a) Growth forecasting uses trend damping TD(B_t) = Σγⁱ·B_t; (b) Seasonal forecasting extrapolates via FA from lookback to forecast horizon; (c) Final prediction is compositional: X̂ = E + Linear(ΣB + ΣS). Reformer simply uses encoder output with a single projection layer, lacking this interpretable decomposition-based forecasting.\n\n4. **Frequency Domain Processing - NEW IMPLEMENTATION REQUIRED**: ETSformer's FA module requires: (a) DFT along temporal dimension to get complex-valued Fourier coefficients; (b) Amplitude and phase extraction (|F(Z)|, φ(F(Z))); (c) Top-K frequency selection based on amplitude ranking; (d) Inverse DFT reconstruction using selected frequencies with conjugate pairs. Reformer's LSH uses FFT only for hash computation, not for signal decomposition.\n\n5. **Input Embedding and Time Features**: Reformer uses `DataEmbedding` with time-dependent covariates (month, day-of-week via configs.freq), while ETSformer explicitly avoids manual time features, relying solely on a **temporal convolutional filter** (kernel size 3) for input embedding, as FA automatically uncovers seasonal patterns. This requires removing time feature dependencies.\n\n6. **Training Objective and Component Supervision**: ETSformer's compositional forecast (Level + Growth + Seasonal) enables interpretable component analysis and potentially component-wise loss functions, while Reformer uses standard sequence-to-sequence loss. ETSformer may require specialized loss weighting across components.\n\n7. **Reversible Layers and Memory Efficiency**: Reformer implements reversible residual networks (RevNets) with chunking to reduce memory from O(b·l·d_ff·n_l) to O(b·l·d_model), enabling 64K sequence lengths. ETSformer uses standard residual connections without reversibility, focusing on shorter sequences typical in time series forecasting (lookback window L and forecast horizon H).\n\n8. **Positional Encoding and Causality**: Reformer implements causal masking for autoregressive generation with shared Q-K formulation, while ETSformer's encoder processes the entire lookback window without causal constraints, and the decoder generates the full forecast horizon simultaneously rather than autoregressively."
  },
  {
    "source": "Autoformer_2021",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Decomposition Architecture**: Both papers adopt a progressive decomposition architecture that separates time series into seasonal and trend components. The source code's `series_decomp` module with moving average can be directly reused, including the `moving_avg` class and decomposition logic in encoder/decoder layers.\n\n2. **Encoder-Decoder Framework**: Both use similar encoder-decoder structures with multi-layer stacking. The source code's `Encoder` and `Decoder` classes, including the layer iteration logic and residual connections, can be adapted. The decoder's progressive trend accumulation mechanism (Equation 4 in Autoformer, Equation 2 in FEDformer) is nearly identical.\n\n3. **Embedding Strategy**: Both use DataEmbedding without positional encoding for input processing. The source code's `DataEmbedding_wo_pos`, `TokenEmbedding`, `TemporalEmbedding`, and `TimeFeatureEmbedding` modules can be directly reused without modification.\n\n4. **Input Initialization**: Both initialize decoder inputs with the latter half of encoder input plus placeholders. The source code's initialization logic in Equation 2 (Autoformer) matches FEDformer's approach, allowing direct code reuse for `x_dec` preparation.\n\n5. **Normalization**: Both apply LayerNorm for seasonal components. The source code's `my_Layernorm` class with bias subtraction can be directly reused for seasonal part normalization in FEDformer.\n\n6. **Feed-Forward Networks**: Both use 1D convolution-based feed-forward layers with activation and dropout. The source code's `conv1`, `conv2` structure in encoder/decoder layers can be reused.\n\n7. **Training Objective**: Both optimize for multi-step forecasting with the same loss computation on the prediction window. The source code's forward method structure and output slicing `[:, -self.pred_len:, :]` can be maintained.",
    "differences": "1. **Attention Mechanism (CORE INNOVATION)**: Autoformer uses Auto-Correlation with time-delay aggregation in time domain via FFT-based autocorrelation. FEDformer introduces Frequency Enhanced Blocks (FEB) and Frequency Enhanced Attention (FEA) that operate DIRECTLY in frequency domain. NEW IMPLEMENTATION NEEDED: (a) FEB-f: Fourier-based block with mode selection, parameterized kernel multiplication in frequency domain (Equations 3-4); (b) FEB-w: Wavelet-based block with recursive multi-scale decomposition using Legendre wavelets (Equation 8-9); (c) FEA-f: Cross-attention in frequency domain with Softmax/Tanh activation (Equations 6-7); (d) FEA-w: Wavelet-based cross-attention with decomposition-reconstruction stages.\n\n2. **Frequency Domain Processing**: Autoformer uses FFT only for computing autocorrelation (Equation 8), then returns to time domain for aggregation. FEDformer performs ALL operations (linear projection, attention, aggregation) in frequency/wavelet domain. NEW IMPLEMENTATION NEEDED: Mode selection logic (`Select` operator), zero-padding for inverse transform, parameterized frequency kernels `R ∈ C^(D×D×M)`.\n\n3. **Decomposition Strategy**: Autoformer uses fixed-window moving average (kernel_size=25 default). FEDformer introduces MOEDecomp (Mixture of Experts Decomposition) with multiple average filters and learnable data-dependent weights (Equation 10). NEW IMPLEMENTATION NEEDED: Multiple pooling filters with different kernel sizes, linear layer `L(x)` for weight generation, softmax-weighted mixing mechanism.\n\n4. **Multi-Scale Modeling**: Autoformer operates at single scale with period-based dependencies via autocorrelation peaks. FEDformer-w explicitly models multi-scale through recursive wavelet decomposition (L=3 levels default) with separate processing paths for high/low frequency components. NEW IMPLEMENTATION NEEDED: Wavelet decomposition matrices (H^(0), H^(1), G^(0), G^(1)), recursive decomposition/reconstruction logic across scales.\n\n5. **Mode Selection Strategy**: Autoformer selects top-k delays based on autocorrelation values (k=⌊c×log L⌋). FEDformer randomly selects M=64 fixed Fourier/Wavelet modes before transformation. NEW IMPLEMENTATION NEEDED: Pre-selection of mode indices, handling of selected modes in frequency domain operations.\n\n6. **Complexity Optimization**: Autoformer achieves O(L log L) via FFT and top-k selection. FEDformer achieves O(L) by fixing mode count M and avoiding full DFT computation. NEW IMPLEMENTATION NEEDED: Efficient implementation with pre-selected modes to avoid full frequency spectrum computation.\n\n7. **Cross-Attention Design**: Autoformer's encoder-decoder Auto-Correlation uses time-delay aggregation with rolled values. FEDformer's FEA performs attention-like operations (Q·K^T·V) entirely in frequency domain with activation functions. NEW IMPLEMENTATION NEEDED: Frequency domain query-key-value projection, activation selection (softmax vs tanh), frequency domain matrix multiplication logic.\n\n8. **Activation Functions**: Autoformer uses ReLU/GELU in feed-forward layers only. FEDformer adds activation functions (softmax/tanh) within FEA-f attention mechanism in frequency domain. NEW IMPLEMENTATION NEEDED: Activation function selection logic and integration into frequency attention computation."
  },
  {
    "source": "Informer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture with Decomposition**: Both papers adopt the encoder-decoder Transformer architecture for long-term time series forecasting. The source code's basic encoder-decoder structure (Encoder, Decoder, EncoderLayer, DecoderLayer classes) can be directly reused as the backbone. The embedding layers (DataEmbedding) and normalization techniques (instance normalization via mean/std subtraction in short_forecast) are identical and can be directly transferred.\n\n2. **Generative Inference Strategy**: Both use the same generative-style decoder inference where the decoder input is Concat(X_token, X_0) - a start token from historical data plus zero-padded placeholders for the target sequence. The source code's decoder forward mechanism (Eq. 6 implementation) and single-step prediction strategy can be reused without modification.\n\n3. **Multi-head Attention Framework**: Both maintain the multi-head attention mechanism structure with Query, Key, Value projections. The AttentionLayer class with query_projection, key_projection, value_projection, and out_projection can serve as the base template. The dimension handling (d_model, n_heads, d_keys, d_values) is identical.\n\n4. **Feed-Forward Network and Residual Connections**: Both use the same feed-forward network design with Conv1d layers and residual connections with layer normalization. The source code's DecoderLayer implementation with conv1, conv2, norm layers, and dropout can be directly reused in FEDformer's framework.\n\n5. **Training Objective**: Both use MSE loss for prediction and share the same problem formulation for long-term forecasting. The loss computation and backpropagation mechanism from the source code can be directly applied.",
    "differences": "1. **Core Attention Mechanism - Frequency Domain vs Sparse Attention**: Informer uses ProbSparse self-attention with query sparsity measurement (Eq. 2-4) and Top-u query selection in time domain, achieving O(L log L) complexity. FEDformer completely replaces this with frequency domain operations: (a) FEB-f uses Fourier transform with learnable kernel R in frequency domain (Eq. 3-4), (b) FEA-f performs attention in frequency domain after DFT (Eq. 6-7), (c) FEB-w/FEA-w use wavelet decomposition with recursive processing. **NEW IMPLEMENTATION REQUIRED**: Fourier/Wavelet transform modules, frequency domain attention computation, learnable frequency kernels, mode selection mechanism (not query selection).\n\n2. **Decomposition Architecture - Self-Attention Distilling vs MOEDecomp**: Informer uses self-attention distilling with Conv1d+MaxPool (Eq. 5) to progressively halve sequence length and build pyramid encoder stacks. FEDformer introduces Mixture of Experts Decomposition (MOEDecomp) with multiple average pooling filters of different sizes and learnable weights (Eq. 10) for seasonal-trend decomposition after each attention block. **NEW IMPLEMENTATION REQUIRED**: MOEDecomp module with multiple pooling experts, learnable mixing weights, trend accumulation mechanism (Eq. 2), seasonal-trend separation logic throughout encoder/decoder.\n\n3. **Encoder-Decoder Information Flow**: Informer's encoder outputs a single representation X_en^N, and decoder uses standard cross-attention. FEDformer maintains explicit seasonal (S) and trend (T) components: encoder outputs only seasonal X_en^N (Eq. 1), decoder accumulates trend across layers T_de^l = T_de^(l-1) + W*T_de^(l,i) (Eq. 2), and final prediction is W_S·X_de^M + T_de^M. **NEW IMPLEMENTATION REQUIRED**: Dual-component tracking system, trend accumulation with projection layers W_{l,i}, seasonal-trend fusion at output.\n\n4. **Frequency Domain Processing Strategy**: Informer operates entirely in time domain with sparse sampling of queries. FEDformer operates in frequency domain: (a) Random mode selection (M=64 modes) before DFT/DWT instead of query selection, (b) Learnable frequency kernels for feature transformation, (c) Wavelet's recursive decomposition-reconstruction with high/low frequency separation across L cycles, (d) Different activation functions (softmax/tanh) in frequency attention. **NEW IMPLEMENTATION REQUIRED**: DFT/DWT/inverse transform modules, random mode selection logic, frequency kernel parameterization, wavelet recursive decomposition/reconstruction pipeline.\n\n5. **Complexity Optimization Approach**: Informer achieves O(L log L) through sparse query selection (sampling U=L_K ln L_Q pairs) and max-mean measurement approximation. FEDformer achieves O(L) by: (a) Pre-selecting fixed M modes (M=64) before any transform, (b) Operating only on selected modes in frequency domain, (c) Avoiding quadratic attention computation entirely. **NEW IMPLEMENTATION REQUIRED**: Fixed-mode pre-selection mechanism, frequency-domain linear operations, mode-limited transform implementation to ensure O(L) complexity.\n\n6. **Multi-Scale Processing**: Informer's ConvLayer provides limited multi-scale via halving (stride-2 MaxPool) in encoder distilling. FEDformer-w provides explicit multi-scale through wavelet decomposition: recursive L-level decomposition (L=3 default) with separate processing of high-frequency (Ud), low-frequency (Us), and remaining coarse signals (s^L), using shared FEB-f modules across scales. **NEW IMPLEMENTATION REQUIRED**: Wavelet decomposition matrices (Legendre basis), recursive multi-level processing logic, high/low frequency signal routing, reconstruction stage combining multi-scale outputs."
  },
  {
    "source": "Reformer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Transformer Architecture**: Both use multi-layer encoder-decoder structures with residual connections and layer normalization. FEDformer can directly reuse Reformer's EncoderLayer and Encoder framework, replacing only the attention mechanism (LSHSelfAttention → FEB/FEA blocks).\n\n2. **Sequence Length Handling**: Both handle variable-length sequences through padding/fitting mechanisms. Reformer's fit_length() method that pads to bucket_size*2 multiples can be adapted for FEDformer's mode selection requirements (padding to support FFT operations).\n\n3. **Linear Projection Layers**: Both use linear projections for Q, K, V generation and final output projection. Reformer's projection layer (nn.Linear(d_model, c_out)) can be directly reused for FEDformer's seasonal component projection (W_S).\n\n4. **Embedding Layer**: Both use DataEmbedding for input encoding with positional and temporal features. FEDformer can directly inherit Reformer's enc_embedding implementation without modification.\n\n5. **Normalization Strategy**: Both apply instance normalization (mean-std normalization) on input sequences before processing. Reformer's normalization code (mean_enc, std_enc computation and denormalization) can be directly reused in FEDformer's short_forecast method.\n\n6. **Feed-Forward Networks**: Both use position-wise feed-forward networks in encoder/decoder layers. The FeedForward module structure and dropout mechanism from Reformer can be directly adopted.\n\n7. **Multi-Head Mechanism**: Both maintain multi-head parallel processing (n_heads parameter). The multi-head splitting and concatenation logic can be adapted from Reformer's implementation.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses LSH (Locality-Sensitive Hashing) attention with hash bucketing for O(L log L) complexity, while FEDformer introduces Frequency Enhanced Blocks (FEB-f/FEB-w) and Frequency Enhanced Attention (FEA-f/FEA-w) operating in frequency domain. NEED TO IMPLEMENT: (a) Discrete Fourier Transform operations with mode selection (Select operator), (b) Parameterized frequency kernel R ∈ C^(D×D×M) with complex multiplication (⊙ operator), (c) Discrete Wavelet Transform with recursive decomposition/reconstruction, (d) Multi-wavelet coefficients computation with fixed decomposition matrices (H^(0), H^(1), G^(0), G^(1)).\n\n2. **Seasonal-Trend Decomposition Architecture - NEW IMPLEMENTATION REQUIRED**: FEDformer uses deep decomposition with MOEDecomp blocks at multiple layers to separate seasonal and trend components, while Reformer has no decomposition mechanism. NEED TO IMPLEMENT: (a) MOEDecomp block with mixture of experts using multiple average pooling filters of different sizes, (b) Data-dependent weight generation (Softmax(L(x))) for combining multiple trends, (c) Separate tracking and accumulation of seasonal (S) and trend (T) components through decoder layers (Equation 2), (d) Trend projection layers W_{l,i} for each decomposition block.\n\n3. **Decoder Input Strategy - NEW IMPLEMENTATION REQUIRED**: FEDformer uses (I/2 + O) length decoder input with partial historical context, while Reformer concatenates full historical sequence with prediction placeholders (seq_len + pred_len). NEED TO IMPLEMENT: Decoder input preparation logic that takes only the latter half of encoder input plus prediction horizon.\n\n4. **Frequency Domain Processing - NEW IMPLEMENTATION REQUIRED**: FEDformer operates primarily in frequency domain through FFT/DWT, while Reformer operates entirely in time domain. NEED TO IMPLEMENT: (a) FFT-based forward transform F(·) and inverse transform F^(-1)(·), (b) Random mode selection mechanism (M << N modes from N total), (c) Zero-padding operation after frequency domain processing, (d) Wavelet recursive decomposition with L cycles producing high-frequency (Ud), low-frequency (Us), and remaining (X) components, (e) Wavelet reconstruction stage combining decomposed components.\n\n5. **Cross-Attention Design - NEW IMPLEMENTATION REQUIRED**: FEDformer uses frequency-enhanced cross-attention (FEA) between encoder-decoder with frequency domain query-key-value interaction, while Reformer uses shared-QK self-attention only. NEED TO IMPLEMENT: (a) FEA-f with frequency domain attention computation σ(Q̃·K̃^T)·Ṽ, (b) FEA-w with separate wavelet decomposition for queries, keys, values, (c) Activation function selection (softmax vs tanh) for frequency attention.\n\n6. **Complexity Optimization - NEW IMPLEMENTATION REQUIRED**: FEDformer achieves O(L) complexity through fixed mode selection (M=64), while Reformer achieves O(L log L) through LSH bucketing. NEED TO IMPLEMENT: Pre-selection of fixed number of Fourier/Wavelet modes before transformation to avoid full O(L log L) FFT cost.\n\n7. **Multi-Scale Wavelet Processing - NEW IMPLEMENTATION REQUIRED**: FEDformer-w variant uses multi-scale wavelet decomposition with recursive processing at different frequency scales (L=3 default levels), while Reformer has no multi-scale mechanism. NEED TO IMPLEMENT: (a) Legendre wavelet basis matrices, (b) Ladder-down decomposition with decimation factor 1/2, (c) Three shared FEB-f blocks for high-freq, low-freq, and remaining signals, (d) Single-layer perceptron F̄ for coarsest scale processing, (e) Recursive reconstruction with 2x upsampling per cycle."
  },
  {
    "source": "Informer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "Informer_2020",
    "type": "in-domain",
    "similarities": "1. **Transformer Architecture Foundation**: Both papers build on the standard Transformer encoder-decoder architecture with multi-head attention mechanisms. The source code's basic structure (DataEmbedding, EncoderLayer, Encoder stack) can be directly reused as the foundational framework for Informer, requiring only attention mechanism substitution.\n\n2. **Efficiency-Driven Attention Innovation**: Both papers address the O(L²) quadratic complexity bottleneck of standard self-attention for long sequences. Reformer uses LSH-based hashing to reduce complexity to O(L log L), while Informer uses ProbSparse attention with similar O(L log L) complexity. The source code's attention layer interface (forward signature with queries, keys, values) provides a template for implementing ProbSparse attention as a drop-in replacement.\n\n3. **Memory-Efficient Design Philosophy**: Both papers emphasize memory optimization for long sequences. Reformer implements reversible layers and chunking; Informer uses self-attention distilling. The source code's memory-efficient attention computation pattern (processing queries individually rather than materializing full QK^T matrix) can be adapted for Informer's selective attention computation.\n\n4. **Shared Query-Key Mechanism**: Reformer enforces Q=K for LSH hashing to work (normalized keys). Informer's ProbSparse attention also computes query-key similarities where queries and keys come from the same input sequence. The source code's shared-QK implementation pattern can inform Informer's query sparsity measurement implementation.\n\n5. **Layer Normalization and Residual Connections**: Both use standard Transformer components like LayerNorm and residual connections. The source code's EncoderLayer structure with normalization can be directly reused, only swapping the attention mechanism.\n\n6. **Input Embedding Strategy**: Both use positional encoding combined with token embeddings. The source code's DataEmbedding module (handling temporal and feature embeddings) can be reused for Informer's time-series input representation.\n\n7. **Multi-Head Attention Framework**: Both maintain the multi-head attention paradigm. The source code's multi-head structure can be preserved while implementing ProbSparse attention within each head, allowing different sparse query-key pairs per head as Informer requires.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses LSH (Locality-Sensitive Hashing) with random projections and hash bucketing (h(x) = argmax([xR; -xR])), while Informer introduces ProbSparse attention based on query sparsity measurement M(q_i, K) = ln(Σexp(q_i·k_j/√d)) - (1/L_K)Σ(q_i·k_j/√d). Must implement: (a) Query sparsity scoring function, (b) Top-u query selection mechanism (u = c·ln(L_Q)), (c) Max-mean approximation for efficient measurement, (d) Random sampling of U = L_K·ln(L_Q) dot-products for sparsity calculation.\n\n2. **Encoder Architecture - NEW IMPLEMENTATION REQUIRED**: Reformer uses standard stacked encoder layers, while Informer introduces self-attention distilling with convolutional downsampling. Must implement: (a) Conv1d layers (kernel_width=3) with ELU activation after each attention block, (b) MaxPooling with stride 2 for progressive halving, (c) Pyramid structure with multiple encoder stacks at different resolutions (main stack, half-slice stack, etc.), (d) Feature map concatenation across all pyramid stacks.\n\n3. **Decoder Strategy - NEW IMPLEMENTATION REQUIRED**: Reformer uses standard autoregressive decoding, while Informer employs generative-style inference with start tokens. Must implement: (a) Start token mechanism (X_token from earlier input slice), (b) Placeholder tensor X_0 for target sequence positions, (c) Single forward pass prediction instead of iterative decoding, (d) Concatenation of start token and placeholder as decoder input X_de = Concat(X_token, X_0).\n\n4. **Sequence Length Handling**: Reformer requires input length to be divisible by (bucket_size × 2), using padding (fit_length function adds zeros). Informer handles arbitrary lengths through distilling's progressive downsampling. Must implement flexible length handling without strict divisibility constraints.\n\n5. **Attention Pattern Philosophy**: Reformer clusters similar queries/keys in hash buckets (locality-based), attending within buckets and one chunk back. Informer selects dominant queries that deviate from uniform distribution (sparsity-based), attending to all keys but only from selected queries. Must implement the Top-u query selection logic based on KL-divergence-inspired measurement.\n\n6. **Time-Series Specific Features - NEW IMPLEMENTATION REQUIRED**: Informer is designed specifically for LSTF (Long Sequence Time-Series Forecasting) with: (a) Time stamp encoding in decoder placeholder X_0, (b) Masking mechanism for future prevention in masked multi-head attention, (c) MSE loss on target sequence positions only, (d) Normalization strategy (mean/std) for time-series inputs. These are absent in Reformer's general-purpose design.\n\n7. **Complexity Optimization Trade-offs**: Reformer achieves O(L log L) through hash-based bucketing with n_hashes repetitions and chunk-based batching. Informer achieves O(L log L) through selective query computation (Top-u queries) and encoder distilling (reducing sequence length by factor of 2^layers). Must implement Informer's sparsity-measurement-driven selection rather than Reformer's hash-driven clustering.\n\n8. **Reversibility and Chunking - NOT NEEDED**: Reformer implements reversible layers (RevNet-style Y1 = X1 + Attention(X2), Y2 = X2 + FeedForward(Y1)) and feed-forward chunking for memory efficiency. Informer does not use reversibility, instead relying on distilling for memory reduction. No need to implement reversible architecture.\n\n9. **Multi-Round Hashing - NOT NEEDED**: Reformer uses n_rounds distinct hash functions to reduce collision probability (P_i = ∪P_i^(r)). Informer's ProbSparse attention is deterministic based on sparsity scores, not probabilistic hashing. No need to implement multiple hash rounds.\n\n10. **Output Generation**: Reformer produces outputs through standard Transformer projection. Informer specifically targets multi-horizon forecasting with configurable prediction length (pred_len), requiring implementation of length-specific output slicing (dec_out[:, -pred_len:, :]) and potentially different output dimensions (d_y) for univariate vs multivariate forecasting."
  },
  {
    "source": "PatchTST_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Only Transformer Architecture**: Both papers adopt encoder-only Transformer architectures rather than encoder-decoder structures. PatchTST's basic Transformer blocks (multi-head attention, FFN, layer normalization) can be directly reused as the foundational building blocks for iTransformer, requiring only dimensional reorientation.\n\n2. **Instance Normalization for Non-Stationarity**: Both employ instance normalization techniques to handle non-stationary time series. PatchTST's RevIN (Reversible Instance Normalization) implementation can be adapted for iTransformer's per-variate normalization, though applied on different dimensions (time vs. variate).\n\n3. **Linear Projection Layers**: Both use simple linear layers for embedding and projection. PatchTST's embedding and head projection modules can be repurposed - the embedding projects from lookback window T to hidden dimension D, and projection from D to forecast horizon S, matching iTransformer's Embedding: ℝ^T → ℝ^D and Projection: ℝ^D → ℝ^S.\n\n4. **Channel Independence Philosophy**: PatchTST's Channel Independence (CI) mode processes each variate separately, which aligns with iTransformer's approach of treating each variate as an independent token. The CI implementation logic can guide how to handle per-variate processing in iTransformer.\n\n5. **No Positional Encoding Required**: PatchTST demonstrates that positional encoding can be optional when using patching. Similarly, iTransformer eliminates positional encoding as temporal order is implicitly captured in the feed-forward network's neuron permutation, allowing reuse of PatchTST's architecture without positional encoding components.\n\n6. **Multi-Head Self-Attention Mechanism**: Both utilize standard multi-head self-attention. PatchTST's attention implementation (Q, K, V projections, scaled dot-product attention, multi-head concatenation) can be directly reused, only changing what constitutes a 'token' (patches vs. variates).\n\n7. **Layer Normalization Integration**: Both integrate layer normalization within Transformer blocks following pre-norm or post-norm configurations. PatchTST's layer norm modules can be reused, though iTransformer applies normalization across the feature dimension of each variate token (Equation 2) rather than across variates.",
    "differences": "1. **Core Innovation - Token Definition (CRITICAL NEW IMPLEMENTATION)**: PatchTST treats temporal patches as tokens (patch-wise tokenization), while iTransformer inverts this by treating entire variates as tokens (variate-wise tokenization). This requires implementing a completely new tokenization strategy where input shape transforms from [Batch, Time, Variates] to [Batch, Variates, Time], then each variate series is embedded as a single token, resulting in [Batch, N_variates, D_model] instead of PatchTST's [Batch, N_patches, D_model].\n\n2. **Attention Semantic Meaning (NEW INTERPRETATION)**: In PatchTST, attention captures temporal dependencies between patches across time. In iTransformer, attention captures multivariate correlations between different variates. This requires no code change but represents a fundamental semantic shift - the attention map A ∈ ℝ^(N×N) now reveals variate-variate correlations rather than patch-patch temporal relationships, requiring new visualization and interpretation tools.\n\n3. **Input Preprocessing Pipeline (NEW IMPLEMENTATION)**: PatchTST requires implementing patching operations (stride, patch_len) to segment time series into overlapping or non-overlapping patches. iTransformer eliminates patching entirely - each complete variate series X[:,n] ∈ ℝ^T is directly fed to the embedding layer. This requires removing PatchTST's patching module and implementing direct per-variate embedding: Embedding(X[:,n]) for n=1,...,N.\n\n4. **Feed-Forward Network Application Scope (ARCHITECTURAL DIFFERENCE)**: PatchTST's FFN operates on temporal patch representations to extract local temporal features. iTransformer's FFN operates on entire series representations of each variate, serving as universal approximators for series-level features (amplitude, periodicity, frequency). While the FFN implementation code is reusable, its conceptual role shifts from temporal feature extraction to series property encoding.\n\n5. **Normalization Dimension (IMPLEMENTATION MODIFICATION)**: PatchTST normalizes across the time dimension for each variate (or across patches). iTransformer's LayerNorm normalizes across the feature dimension D for each variate token: (h_n - Mean(h_n))/sqrt(Var(h_n)), where h_n ∈ ℝ^D. This requires modifying the normalization axis in the implementation from dim=-1 (features) to ensure per-token normalization.\n\n6. **Scalability with Variate Number (NEW CAPABILITY)**: PatchTST's complexity scales with sequence length and number of patches. iTransformer's complexity scales with number of variates N, as attention is computed over N variate tokens. This necessitates implementing efficient attention mechanisms (Flash Attention, Linear Attention) specifically optimized for scenarios with large N, which may not be present in PatchTST's codebase.\n\n7. **Multi-Scale Temporal Modeling (ABSENT IN ITRANSFORMER)**: PatchTST explicitly models multiple temporal scales through different patch sizes, enabling hierarchical temporal feature extraction. iTransformer abandons explicit multi-scale modeling, relying instead on the FFN's ability to capture temporal patterns at various scales through its neuron activations. This means removing PatchTST's multi-scale patching logic entirely.\n\n8. **Generalization to Unseen Variates (NEW TRAINING PARADIGM)**: iTransformer's architecture allows training on arbitrary numbers of variates and inference on different variate counts, as each variate is independently embedded. PatchTST is typically trained with fixed variate numbers. Implementing this requires creating flexible input handling where the model can accept variable N during training/inference, requiring modifications to batch processing and embedding layers.\n\n9. **Interpretability Focus (NEW ANALYSIS TOOLS)**: iTransformer emphasizes that attention maps reveal multivariate correlations, providing inherent interpretability. This requires implementing new visualization tools to analyze the N×N attention score matrices as correlation patterns, different from PatchTST's temporal attention visualization which shows patch-level dependencies.\n\n10. **Embedding Strategy (IMPLEMENTATION CHANGE)**: PatchTST embeds each patch (local temporal segment) independently. iTransformer embeds entire time series X[:,n] ∈ ℝ^T → h_n ∈ ℝ^D using MLP, requiring the embedding layer to handle longer input sequences (full lookback window T rather than patch_len). This may require implementing deeper or wider MLPs to effectively encode complete series representations."
  },
  {
    "source": "Crossformer_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Only Transformer Foundation**: Both papers utilize encoder-based Transformer architectures (Crossformer has encoder-decoder but iTransformer is encoder-only). The basic Transformer components from Crossformer (AttentionLayer, FullAttention, LayerNorm) can be directly reused for iTransformer's implementation.\n\n2. **Embedding Strategy for Time Series**: Both employ learned linear projections to embed time series data into higher-dimensional representations. Crossformer's PatchEmbedding module with linear projection (nn.Linear(patch_len, d_model)) provides a template that can be adapted for iTransformer's series embedding (Embedding: R^T → R^D).\n\n3. **Layer Normalization Application**: Both papers apply LayerNorm extensively. Crossformer's normalization infrastructure (self.norm1, self.norm2, self.norm3, self.norm4 in TwoStageAttentionLayer) can be reused, though iTransformer applies it along the series dimension rather than time dimension.\n\n4. **Multi-Head Self-Attention Mechanism**: Both rely on multi-head self-attention for capturing dependencies. Crossformer's AttentionLayer and FullAttention implementations provide the core attention computation (torch.einsum operations, scaled dot-product) that can be directly leveraged for iTransformer.\n\n5. **Feed-Forward Network (MLP) Structure**: Both use identical FFN applied to tokens. Crossformer's MLP implementation (nn.Sequential with Linear→GELU→Linear) in TwoStageAttentionLayer matches iTransformer's FFN requirements and can be reused without modification.\n\n6. **Final Projection for Prediction**: Both use linear projection layers to generate final predictions. Crossformer's linear_pred (nn.Linear(d_model, seg_len)) in DecoderLayer demonstrates the pattern that iTransformer needs for its Projection: R^D → R^S.\n\n7. **Residual Connections and Dropout**: Both employ residual connections with dropout for training stability. Crossformer's pattern (x + self.dropout(attention_output)) can be directly applied to iTransformer's block structure.",
    "differences": "1. **CORE INNOVATION - Token Definition (NEW IMPLEMENTATION REQUIRED)**: Crossformer treats segments of time series as tokens (dimension-segment-wise embedding), while iTransformer inverts this by treating entire variates as tokens. NEED TO IMPLEMENT: New embedding logic that takes X[:, n] (entire time series of variate n) as input instead of segments, requiring a complete redesign of the input processing pipeline.\n\n2. **Attention Semantic Inversion (NEW IMPLEMENTATION REQUIRED)**: Crossformer's Two-Stage Attention captures cross-time then cross-dimension dependencies with router mechanism (complexity O(DL)), while iTransformer applies attention across variates to capture multivariate correlations. NEED TO IMPLEMENT: Standard self-attention on N variate tokens (complexity O(N²)) without the two-stage structure or router mechanism - this simplifies to vanilla attention but changes the semantic meaning entirely.\n\n3. **Hierarchical Multi-Scale Architecture vs. Single-Scale (REMOVE COMPONENTS)**: Crossformer uses hierarchical encoder-decoder with SegMerging for multi-scale processing across layers, while iTransformer uses a flat encoder-only architecture. NEED TO REMOVE: SegMerging module, scale_block hierarchy, and the entire Decoder structure. NEED TO IMPLEMENT: Simple stacking of identical Transformer blocks without scale changes.\n\n4. **Position Embedding Strategy (NEW IMPLEMENTATION REQUIRED)**: Crossformer uses explicit learnable 2D position embeddings (self.enc_pos_embedding with shape [1, enc_in, in_seg_num, d_model]), while iTransformer states \"position embedding is no longer needed\" as order is implicit in FFN neuron permutation. NEED TO IMPLEMENT: Removal of position embedding or optional lightweight position encoding.\n\n5. **Normalization Dimension (MODIFY EXISTING)**: Crossformer normalizes across the d_model dimension for each segment-dimension pair, while iTransformer normalizes each variate token's series representation (Equation 2: normalizing h_n individually). NEED TO IMPLEMENT: Modified LayerNorm that operates on axis=-1 (feature dimension) of individual variate tokens rather than temporal segments.\n\n6. **Decoder Structure Elimination (REMOVE COMPONENTS)**: Crossformer has a complex hierarchical decoder with cross-attention between encoder and decoder (DecoderLayer with self_attention and cross_attention), while iTransformer uses encoder-only with direct projection. NEED TO REMOVE: Entire Decoder class, DecoderLayer, cross-attention mechanisms, and dec_pos_embedding. NEED TO IMPLEMENT: Simple linear projection head after encoder.\n\n7. **Segment Merging and Multi-Resolution (REMOVE COMPONENTS)**: Crossformer's SegMerging module merges adjacent segments for coarser scales (self.win_size window merging), while iTransformer processes all variates at a single resolution. NEED TO REMOVE: SegMerging class and all window-based hierarchical processing. This simplifies the architecture significantly.\n\n8. **Input Padding Strategy (MODIFY EXISTING)**: Crossformer pads input to make it divisible by segment length (self.pad_in_len = ceil(seq_len/seg_len)*seg_len), while iTransformer directly processes the full lookback window without segmentation. NEED TO IMPLEMENT: Direct embedding of full T-length series without padding constraints for segment alignment.\n\n9. **Interpretability Focus (NEW ANALYSIS REQUIRED)**: iTransformer emphasizes that attention scores reveal multivariate correlations (A_{i,j} ∝ q_i^T k_j after normalization), while Crossformer's attention captures temporal-dimensional patterns. NEED TO IMPLEMENT: Analysis tools to visualize N×N correlation matrices rather than temporal attention patterns.\n\n10. **Generalization to Variable Variate Numbers (NEW CAPABILITY)**: iTransformer explicitly supports training on arbitrary numbers of variates and inference with different variate counts due to token-level processing, while Crossformer's architecture is fixed to the training dimension D. NEED TO IMPLEMENT: Flexible input handling that doesn't assume fixed N during initialization, allowing dynamic variate numbers at inference time."
  },
  {
    "source": "TiDE_2023",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Channel Independence Philosophy**: Both papers embrace the concept of treating each variate independently for feature extraction. TiDE's encoder processes each time series separately through dense layers, while iTransformer embeds each variate as an independent token. The source paper's implementation of channel-wise processing (separate encoding for each variate) can be directly adapted as the embedding layer structure in iTransformer.\n\n2. **MLP-based Representation Learning**: Both architectures rely heavily on MLPs/FFNs for learning series representations. TiDE uses stacked dense layers with residual connections for temporal feature extraction, and iTransformer's FFN serves the same purpose on series representations. The source paper's MLP implementation with layer normalization and residual connections can be reused as the FFN component in iTransformer blocks.\n\n3. **Instance Normalization for Non-stationarity**: Both papers apply normalization at the variate level to handle non-stationary data. TiDE likely uses instance normalization on each time series, and iTransformer explicitly applies Layer Normalization to each variate token (Equation 2). The source paper's normalization preprocessing pipeline can be directly integrated into iTransformer's layer normalization module.\n\n4. **Linear Projection for Forecasting**: Both architectures use simple linear layers for final prediction generation rather than complex autoregressive decoders. TiDE's decoder projects encoded representations to future horizons, and iTransformer's Projection module (MLP: ℝ^D → ℝ^S) serves the same purpose. The source paper's projection layer implementation can be adapted for iTransformer's output projection.\n\n5. **Encoder-only Architecture Preference**: Both papers move away from encoder-decoder Transformers, focusing on representation learning rather than autoregressive generation. TiDE uses a pure encoder-decoder structure with MLPs, while iTransformer adopts encoder-only Transformer. The source paper's training loop and loss computation logic can be reused for iTransformer's non-autoregressive training.",
    "differences": "1. **Core Architecture Paradigm**: TiDE is a pure MLP-based model without attention mechanisms, using stacked dense layers for both temporal and feature encoding. iTransformer introduces the inverted self-attention mechanism where attention operates on variate tokens (N×D) rather than temporal tokens. NEW IMPLEMENTATION NEEDED: Multi-head self-attention module that computes attention scores between variate representations, including Q/K/V projections (ℝ^(N×D) → ℝ^(N×d_k)) and attention score computation A_ij ∝ q_i^T k_j.\n\n2. **Token Definition and Embedding Strategy**: TiDE processes raw time series through direct dense transformations, while iTransformer explicitly defines each entire time series X_{:,n} ∈ ℝ^T as a token and embeds it into D-dimensional space. NEW IMPLEMENTATION NEEDED: Series-to-token embedding module that transforms the full lookback window (T timesteps) into a fixed-dimensional token representation (Embedding: ℝ^T → ℝ^D), likely implemented as MLP operating on the temporal dimension.\n\n3. **Multivariate Interaction Mechanism**: TiDE relies on implicit feature mixing through shared dense layers and explicit feature projection layers. iTransformer introduces explicit variate-variate correlation modeling through inverted self-attention, where the attention map A ∈ ℝ^(N×N) reveals multivariate dependencies. NEW IMPLEMENTATION NEEDED: Attention score interpretation and visualization tools to analyze the learned N×N correlation matrix between variates, and the softmax-weighted aggregation of value vectors across variates.\n\n4. **Stacked Block Structure**: TiDE uses a simpler encoder-decoder structure with residual dense layers. iTransformer employs L stacked Transformer blocks (TrmBlock), each containing Layer Normalization → Self-Attention → Layer Normalization → FFN with residual connections. NEW IMPLEMENTATION NEEDED: Complete Transformer block implementation following the structure H^(l+1) = TrmBlock(H^l), including proper residual connection placement and the specific ordering of LayerNorm, attention, and FFN components.\n\n5. **Position Encoding Philosophy**: TiDE implicitly encodes temporal order through the sequential processing of dense layers. iTransformer explicitly removes position embeddings, arguing that temporal order is implicitly stored in the neuron permutation of the FFN. NEW IMPLEMENTATION NEEDED: Validation that the model can learn temporal patterns without explicit positional encodings, and potentially ablation studies to confirm this design choice.\n\n6. **Scalability to Variable Numbers**: TiDE operates on fixed variate dimensions through its dense layers. iTransformer's attention-based design allows flexible token numbers, enabling training on arbitrary numbers of variates and inference on different variate sets. NEW IMPLEMENTATION NEEDED: Dynamic attention mask handling and padding strategies to support variable N during training and inference, plus mechanisms to handle unseen variates at test time.\n\n7. **Interpretability Focus**: TiDE provides limited interpretability beyond feature importance. iTransformer emphasizes attention map interpretability, where pre-Softmax scores A_ij reveal learned multivariate correlations. NEW IMPLEMENTATION NEEDED: Attention visualization pipeline to extract and analyze the N×N attention matrices across layers and heads, demonstrating how the model learns meaningful variate relationships."
  },
  {
    "source": "TimesNet_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Non-stationarity**: Both papers apply instance normalization (mean subtraction and standard deviation division) along the temporal dimension to handle non-stationary time series. TimesNet's code shows: `means = x_enc.mean(1, keepdim=True).detach(); x_enc = x_enc.sub(means); stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5); x_enc = x_enc.div(stdev)`. iTransformer adopts similar layer normalization on series representations. The normalization and denormalization code from TimesNet can be directly reused.\n\n2. **DataEmbedding Module**: Both papers use embedding layers to project raw input features into a higher-dimensional representation space. TimesNet's `DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)` can be adapted for iTransformer's embedding component, as both transform input series from dimension C to d_model.\n\n3. **Encoder-only Architecture with Residual Connections**: Both adopt encoder-only architectures with residual connections. TimesNet uses `res = res + x` in TimesBlock, and iTransformer follows similar residual patterns in Transformer blocks. The stacking mechanism `for i in range(self.layer): enc_out = self.layer_norm(self.model[i](enc_out))` can be adapted for iTransformer's L-layer structure.\n\n4. **Linear Projection for Final Prediction**: Both papers use linear layers for final output projection. TimesNet's `self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)` can be directly adapted for iTransformer's projection layer that maps from D-dimensional representation to S future time steps.\n\n5. **Training Infrastructure and Data Pipeline**: Both papers work on the same multivariate time series forecasting task with lookback window T and prediction horizon S. The data loading, batch processing, and loss computation frameworks from TimesNet's implementation can be reused for iTransformer.",
    "differences": "1. **Core Architecture: 2D Convolution vs. Inverted Attention**: TimesNet transforms 1D time series into 2D tensors based on FFT-detected periods and applies 2D Inception blocks for spatial-temporal modeling. This requires implementing FFT_for_Period, reshape operations, and Inception_Block_V1. iTransformer requires COMPLETELY NEW implementation of inverted self-attention mechanism where each variate (not timestamp) becomes a token, with attention computed across N variates instead of T time steps. This includes new Q, K, V projections with dimension [N×D] and attention map [N×N].\n\n2. **Token Definition and Embedding Strategy**: TimesNet treats temporal segments as implicit tokens through 2D reshaping (columns represent intraperiod-variation, rows represent interperiod-variation). iTransformer requires NEW implementation where each entire time series X[:,n] of length T is embedded as a single token, using MLP: Embedding: R^T → R^D. This fundamentally different tokenization needs new embedding layers that process whole series instead of time points.\n\n3. **Multi-scale Temporal Modeling**: TimesNet explicitly models multiple periodicities through FFT analysis (top-k frequencies) and adaptive aggregation with softmax weights: `period_weight = F.softmax(period_weight, dim=1); res = torch.sum(res * period_weight, -1)`. iTransformer does NOT have multi-period modeling and requires NEW implementation of feed-forward networks (FFN) that operate on series representations to implicitly capture temporal patterns through dense non-linear connections, serving as universal approximators for time series.\n\n4. **Attention Mechanism Purpose**: TimesNet does NOT use attention mechanisms at all - it relies on 2D convolutions. iTransformer requires NEW implementation of self-attention that reveals multivariate correlations: attention scores A[i,j] ∝ q_i^T k_j represent correlations between variate i and variate j. The attention map [N×N] provides interpretability for multivariate dependencies, which is fundamentally different from TimesNet's frequency-based aggregation.\n\n5. **Position Encoding and Layer Normalization**: TimesNet uses standard layer normalization on temporal features. iTransformer requires NEW implementation: (a) NO position embeddings needed as temporal order is implicitly stored in FFN neuron permutations; (b) Layer normalization applied per-variate token: LayerNorm(H) = {(h_n - Mean(h_n))/√Var(h_n) | n=1,...,N}, normalizing across the D-dimensional series representation rather than across variates.\n\n6. **Forecasting Head Design**: TimesNet uses `self.predict_linear = nn.Linear(self.seq_len, self.pred_len + self.seq_len)` to align temporal dimensions before processing. iTransformer requires NEW implementation where projection MLP operates on each variate token independently: Projection: R^D → R^S, generating future S steps from the D-dimensional representation learned by stacked inverted Transformer blocks.\n\n7. **Complexity and Scalability**: TimesNet's complexity depends on 2D convolution operations O(k·p·f·C²) where k is top frequencies, p is period length, f is frequency, C is kernel size. iTransformer requires implementing attention with O(N²·D) complexity for N variates, but can leverage efficient attention mechanisms (FlashAttention, etc.) as plugins, and supports training on variable numbers of variates due to flexible token count."
  },
  {
    "source": "DLinear_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Non-Stationarity**: Both papers address distribution shifts in time series. DLinear's NLinear variant uses subtraction of the last value for normalization, while iTransformer applies LayerNorm on individual variate tokens (Equation 2). The source code's normalization logic in the encoder can be adapted - the subtraction/addition pattern in NLinear is conceptually similar to iTransformer's per-variate normalization approach.\n\n2. **Series Decomposition Framework**: DLinear explicitly uses `series_decomp` from Autoformer (moving average kernel) to extract trend and seasonal components. iTransformer doesn't explicitly decompose but the FFN layers can implicitly learn such decompositions. The `self.decompsition = series_decomp(configs.moving_avg)` module from DLinear code can be reused as a preprocessing step or compared against iTransformer's implicit decomposition.\n\n3. **Channel-Independent Processing Philosophy**: DLinear's `individual=True` mode creates separate linear layers per channel (`nn.ModuleList()` for each variate), treating each time series independently. iTransformer similarly processes each variate as an independent token through the FFN, maintaining channel independence. The ModuleList structure in DLinear code provides implementation guidance for managing per-variate parameters.\n\n4. **Direct Multi-Step Forecasting (DMS)**: Both avoid autoregressive decoding. DLinear directly projects from `seq_len` to `pred_len` via linear layers. iTransformer uses MLP projection (`Projection: R^D → R^S`) to generate future steps. The direct forecasting logic `dec_out[:, -self.pred_len:, :]` in DLinear can inform iTransformer's projection layer implementation.\n\n5. **Linear Layer Initialization**: DLinear initializes weights as `(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])`, providing uniform averaging as a starting point. This initialization strategy can be adapted for iTransformer's embedding and projection MLPs to ensure stable training initialization.\n\n6. **Lookback Window Design**: Both use fixed-length historical windows (`configs.seq_len` in DLinear, `T` time steps in iTransformer) for forecasting. The input handling logic `x_enc` in DLinear's forward method demonstrates how to structure the lookback window, which is directly applicable to iTransformer's input processing.",
    "differences": "1. **Core Architecture - Linear vs Transformer**: DLinear uses simple linear layers (`nn.Linear`) operating on the temporal dimension, requiring only basic PyTorch layers. iTransformer requires implementing full Transformer encoder blocks including multi-head self-attention, FFN, and layer normalization stacks. NEW IMPLEMENTATION NEEDED: Self-attention mechanism with Q, K, V projections, attention score computation, multi-head attention, and stacked Transformer blocks (Equation 1).\n\n2. **Token Representation - Time Points vs Variates**: DLinear processes temporal dimension directly with shape `[batch, channels, time]` and applies linear transformation along time axis. iTransformer inverts this to `[batch, variates, features]` where each variate's entire time series becomes a token. NEW IMPLEMENTATION NEEDED: Embedding layer to convert raw series `R^T → R^D` (MLP-based), token-wise processing logic, and dimension permutation handling for inverted structure.\n\n3. **Multivariate Correlation Modeling**: DLinear with `individual=False` shares weights across variates but doesn't model inter-variate dependencies - it's purely temporal. iTransformer's self-attention explicitly captures multivariate correlations through attention maps `A ∈ R^(N×N)` between variate tokens. NEW IMPLEMENTATION NEEDED: Attention mechanism for variate-variate interaction, correlation score computation `q_i^T k_j`, and interpretable attention map generation.\n\n4. **Representation Learning Depth**: DLinear is shallow (1-2 layers: seasonal + trend linear layers) with minimal non-linearity. iTransformer stacks L Transformer blocks with deep FFN layers for hierarchical representation learning. NEW IMPLEMENTATION NEEDED: Deep stacking of TrmBlocks, residual connections, and progressive representation refinement across layers.\n\n5. **Feature Extraction Mechanism**: DLinear extracts features through weighted temporal aggregation (linear combination of time points). iTransformer uses FFN as universal approximators to extract complex series representations (amplitude, periodicity, frequency spectrums as mentioned). NEW IMPLEMENTATION NEEDED: Multi-layer FFN with non-linear activations (typically 2-layer with expansion factor), operating on series representations rather than time points.\n\n6. **Position Encoding Philosophy**: DLinear implicitly encodes position through the temporal dimension structure in linear weights. iTransformer explicitly states position embedding is NOT needed because \"order of sequence is implicitly stored in neuron permutation of FFN\". NEW IMPLEMENTATION NEEDED: Understanding of how FFN neurons encode temporal order without explicit positional encoding, different from standard Transformer practice.\n\n7. **Normalization Application**: DLinear's NLinear normalizes the entire input sequence by subtracting last value (global operation). iTransformer applies LayerNorm independently to each variate token's representation `(h_n - Mean(h_n))/sqrt(Var(h_n))`, normalizing across feature dimension. NEW IMPLEMENTATION NEEDED: Per-token LayerNorm implementation that operates on feature dimension, integrated within each Transformer block.\n\n8. **Scalability and Flexibility**: DLinear has fixed architecture with O(T×S) complexity per variate. iTransformer allows plugin of efficient attention mechanisms (mentioned: FlashAttention, etc.) and supports variable variate numbers between training/inference. NEW IMPLEMENTATION NEEDED: Modular attention interface for efficiency plugins, flexible token handling for varying N, and attention complexity management for large-scale multivariate scenarios."
  },
  {
    "source": "TimesNet_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Informer_2020",
    "target": "KANAD_2024",
    "type": "incomplete-target",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition Philosophy**: Both papers recognize the importance of decomposing time series into trend and seasonal components. Autoformer uses series_decomp with moving average (kernel_size=25 default) to extract trend-cyclical and seasonal parts. LightTS implicitly handles this through separate continuous and interval sampling paths. The source code's series_decomp module and moving_avg implementation can be adapted as a preprocessing step or ablation baseline for LightTS.\n\n2. **Multi-Scale Temporal Pattern Capture**: Both architectures capture patterns at different temporal scales. Autoformer's Auto-Correlation mechanism discovers period-based dependencies at multiple time delays (top-k periods via FFT), while LightTS uses continuous sampling (local patterns) and interval sampling (global patterns). The FFT-based autocorrelation computation in Autoformer's AutoCorrelation class (torch.fft.rfft) can be reused for analyzing optimal sampling intervals in LightTS.\n\n3. **Embedding and Input Processing**: Both use similar embedding strategies. Autoformer's DataEmbedding_wo_pos combines TokenEmbedding (Conv1d with kernel_size=3), TemporalEmbedding, and dropout. LightTS can directly reuse these embedding modules for initial feature extraction before applying sampling operations, especially the TokenEmbedding's Conv1d implementation for capturing local temporal patterns.\n\n4. **Normalization Techniques**: Both papers employ layer normalization. Autoformer uses my_Layernorm (custom LayerNorm with bias removal) and standard LayerNorm in encoder/decoder. LightTS can leverage these normalization implementations, particularly for stabilizing training in the IEBlock components where feature dimensions change significantly.\n\n5. **Residual Connections and Feature Aggregation**: Autoformer extensively uses residual connections (x + dropout(attention_output)) in EncoderLayer and DecoderLayer. LightTS's IEBlock architecture can adopt similar residual connection patterns from Autoformer's implementation to improve gradient flow, especially in the bottleneck design where F' << F.\n\n6. **Batch Processing and Efficient Computation**: Both models handle batch processing efficiently. Autoformer's time_delay_agg_training and time_delay_agg_inference methods show different strategies for training vs inference. LightTS can adopt similar dual-mode processing patterns for its sampling operations to optimize memory usage during training.",
    "differences": "1. **Core Architecture Paradigm - NEW IMPLEMENTATION REQUIRED**: Autoformer uses Transformer-based encoder-decoder with Auto-Correlation mechanism (replacing self-attention), while LightTS is a pure MLP-based architecture with IEBlock. Must implement from scratch: (a) IEBlock with temporal projection (MLP: R^H → R^F'), channel projection (MLP: R^W → R^W), and output projection (MLP: R^F' → R^F); (b) Weight sharing across columns/rows; (c) Bottleneck architecture where F' << F. No equivalent in Autoformer's attention-based code.\n\n2. **Sampling Strategy - NEW IMPLEMENTATION REQUIRED**: LightTS's continuous sampling (Eq. 1: consecutive C tokens) and interval sampling (Eq. 2: C tokens with fixed intervals) are completely novel. Must implement: (a) Continuous sampling: X_t^con[j] = {x_{t-T+(j-1)C+1}, ..., x_{t-T+jC}}; (b) Interval sampling: X_t^int[j] = {x_{t-T+j}, x_{t-T+j+⌊T/C⌋}, ...}; (c) Non-overlapping sub-sequence generation transforming R^T → R^{C×T/C}. Autoformer's Roll operation in Auto-Correlation is fundamentally different (circular shift for time delay aggregation).\n\n3. **Attention Mechanism vs Pure MLP - NO AUTOFORMER CODE REUSABLE**: Autoformer's AutoCorrelation, AutoCorrelationLayer with query/key/value projections, FFT-based period discovery, and time delay aggregation are entirely replaced in LightTS. LightTS requires implementing: (a) Three sequential MLP layers in IEBlock without any attention mechanism; (b) Information exchange purely through matrix operations and MLPs; (c) No query-key-value paradigm. The AutoCorrelation class (300+ lines) is completely unused.\n\n4. **Two-Part Architecture Design - NEW COORDINATION LOGIC**: LightTS has distinct Part I (independent time series feature extraction with IEBlock-A and IEBlock-B) and Part II (cross-variable learning with IEBlock-C). Must implement: (a) Independent processing of N variables in Part I; (b) Feature concatenation: R^{F×T/C} → R^F per variable; (c) Final concatenation: R^{2F×N} input to IEBlock-C; (d) Output mapping: R^{2F×N} → R^{L×N}. Autoformer's encoder-decoder architecture with cross-attention is structurally incompatible.\n\n5. **Decoder Structure and Trend Accumulation - AUTOFORMER-SPECIFIC**: Autoformer's progressive trend accumulation (T_de^l = T_de^{l-1} + W_{l,1}*T_de^{l,1} + ...) across decoder layers and seasonal/trend decomposition at each layer are absent in LightTS. LightTS uses direct end-to-end prediction without explicit trend accumulation. The DecoderLayer's three decomposition blocks and trend projection logic cannot be reused.\n\n6. **Input Initialization Strategy - DIFFERENT PARADIGMS**: Autoformer initializes decoder with decomposed components: X_des = Concat(X_ens, X_0) and X_det = Concat(X_ent, X_Mean) using label_len and pred_len. LightTS directly processes the entire look-back window T without decoder initialization or placeholder tokens. Must implement: (a) Direct window-based processing; (b) No seasonal/trend initialization; (c) Single forward pass without encoder-decoder separation.\n\n7. **Multi-Head Mechanism - NOT APPLICABLE**: Autoformer's multi-head Auto-Correlation (h heads, d_model/h per head) with separate query/key/value projections per head is absent in LightTS. LightTS requires: (a) Single-path processing without head splitting; (b) Direct feature dimension mapping through MLPs; (c) No concatenation of multiple attention heads. The MultiHead function and head-wise processing logic are not transferable.\n\n8. **Computational Complexity Optimization - DIFFERENT APPROACHES**: Autoformer achieves O(L log L) through FFT-based autocorrelation and top-k period selection. LightTS achieves efficiency through: (a) Down-sampling to C×T/C matrices; (b) Bottleneck design (F' << F) reducing channel projection cost; (c) Weight sharing across temporal/channel dimensions. Must implement custom optimization strategies not present in Autoformer's FFT-based approach."
  },
  {
    "source": "Informer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Distribution Shift**: Both papers apply instance normalization (mean-std normalization) on the encoder input to handle distribution shifts across different time series samples. Informer's `short_forecast` method computes `mean_enc` and `std_enc` per batch sample, then denormalizes decoder output. LightTS can directly reuse this normalization logic: compute mean/std on the look-back window (dimension 1), normalize input, and denormalize predictions.\n\n2. **Encoder-Decoder Architecture Paradigm**: Both adopt an encoder-decoder structure where the encoder extracts features from historical sequences and the decoder generates future predictions. Informer's `Encoder` class with multi-layer `EncoderLayer` and `Decoder` class can serve as architectural templates. LightTS can adapt the encoder concept (feature extraction from input) and decoder concept (prediction generation), though replacing attention mechanisms with MLP-based IEBlocks.\n\n3. **Embedding Layer for Input Representation**: Both use embedding layers to transform raw input into higher-dimensional representations. Informer's `DataEmbedding` (combining value embedding, positional encoding, and temporal encoding) can be partially reused. LightTS doesn't explicitly mention temporal embeddings but the embedding projection logic (linear transformation from input dimension to model dimension) is transferable.\n\n4. **Multi-Step Forecasting with Start Token Strategy**: Both handle multi-step forecasting by feeding decoder with a combination of known historical context and target placeholders. Informer's `x_dec` concatenates `x_token` (start token from historical data) and `x_0` (zero placeholders). LightTS can adopt similar input preparation: use last portion of look-back window as context and append zero-initialized target sequence placeholders.\n\n5. **Layer Normalization and Residual Connections**: Informer extensively uses `LayerNorm` in encoder/decoder layers with residual connections (`x = x + dropout(attention(...))`). LightTS's IEBlock can incorporate these components: apply LayerNorm after each MLP projection and add residual connections between temporal/channel projections to stabilize training and enable deeper architectures.\n\n6. **Dropout Regularization**: Both apply dropout for regularization. Informer's dropout in attention layers and feed-forward networks (configured via `configs.dropout`) can be directly transferred to LightTS's MLP layers within IEBlocks, applying dropout after each linear transformation or activation function.",
    "differences": "1. **Core Attention vs. MLP Mechanism**: Informer's innovation is ProbSparse self-attention with O(L log L) complexity through query sparsity measurement and top-u query selection. Implementation requires `_prob_QK` for sampling, `_get_initial_context`, and `_update_context` methods. LightTS completely abandons attention mechanisms, using pure MLP-based IEBlocks with temporal/channel projections. NEW IMPLEMENTATION NEEDED: (a) IEBlock class with three sequential MLP projections (temporal, channel, output); (b) Weight-sharing mechanism across columns/rows; (c) Bottleneck architecture with dimension reduction (H→F'→F).\n\n2. **Sampling-Oriented Feature Extraction**: Informer processes full sequences through attention, using distilling operations (Conv1d + MaxPool) for downsampling between encoder layers. LightTS introduces novel continuous and interval sampling strategies that transform sequences into 2D matrices (C × T/C) BEFORE feature extraction. NEW IMPLEMENTATION NEEDED: (a) `continuous_sampling` function splitting sequence into consecutive chunks (Eq. 1); (b) `interval_sampling` function creating strided sub-sequences (Eq. 2); (c) Parallel processing of both sampling outputs through separate IEBlock-A and IEBlock-B; (d) Feature concatenation logic merging continuous/interval features.\n\n3. **Variable Independence vs. Joint Modeling**: Informer treats multivariate time series jointly through multi-head attention, with Q/K/V projections operating on concatenated variables. LightTS adopts a two-stage approach: Part I processes each variable independently (N separate feature extractions), Part II concatenates all variable features (2F × N matrix) for interdependency learning. NEW IMPLEMENTATION NEEDED: (a) Loop/parallel processing for N independent variables in Part I; (b) Feature concatenation across both temporal (continuous/interval) and channel (variable) dimensions; (c) IEBlock-C designed specifically for shape transformation (2F × N → L × N).\n\n4. **Self-Attention Distilling vs. Linear Down-Projection**: Informer uses ConvLayer with 1D convolution (kernel=3), ELU activation, and MaxPooling (stride=2) for progressive sequence length reduction across encoder layers, creating a pyramid structure. LightTS uses simple linear mapping (R^(T/C) → R) to aggregate sub-sequence features after IEBlock processing. NEW IMPLEMENTATION NEEDED: (a) Linear down-projection layer per variable after IEBlock-A/B; (b) Removal of convolutional distilling operations; (c) Direct feature aggregation without multi-scale pyramid stacking.\n\n5. **Decoder Prediction Strategy**: Informer's decoder uses masked multi-head attention for auto-regressive prevention and cross-attention between decoder queries and encoder outputs, with `DecoderLayer` containing two attention blocks. LightTS's IEBlock-C directly maps concatenated features (2F × N) to predictions (L × N) through temporal and channel projections without cross-attention. NEW IMPLEMENTATION NEEDED: (a) Replace decoder attention layers with single IEBlock-C; (b) Remove masking mechanisms; (c) Direct output projection from feature space to prediction horizon; (d) No iterative decoding—single forward pass generates all L predictions.\n\n6. **Complexity and Efficiency Focus**: Informer achieves O(L log L) through sparse attention and distilling, requiring complex query selection logic (`M(q,K)` measurement, Top-u selection, random sampling for approximation). LightTS achieves O(L) through pure MLPs with bottleneck design (F' << F), avoiding quadratic attention costs entirely. NEW IMPLEMENTATION NEEDED: (a) Bottleneck dimension hyperparameter F' configuration; (b) Three-stage MLP architecture (temporal: H→F', channel: F'→F', output: F'→F); (c) Remove all attention-related computations (Q/K/V projections, softmax, attention score calculations); (d) Efficient weight-sharing implementation for column-wise and row-wise MLPs."
  },
  {
    "source": "Reformer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization Strategy**: Both papers apply instance normalization on input sequences. Reformer's code shows `mean_enc = x_enc.mean(1, keepdim=True).detach()` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()`, followed by `x_enc = (x_enc - mean_enc) / std_enc`. This exact normalization approach can be directly reused in LightTS for preprocessing multivariate time series before applying continuous/interval sampling.\n\n2. **Embedding Layer Architecture**: Both use DataEmbedding for encoding input sequences. Reformer's `self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)` can be adapted for LightTS's initial feature extraction, though LightTS processes each variable independently in Part I.\n\n3. **Projection Layer for Output**: Both employ linear projection layers to map hidden representations to prediction space. Reformer uses `self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)`, which can be adapted for LightTS's final output after IEBlock-C, mapping from `R^{L×N}` to prediction horizon.\n\n4. **Sequence Length Handling**: Both models handle variable-length sequences through padding/fitting mechanisms. Reformer's `fit_length()` method ensures sequences are divisible by bucket_size, similar to how LightTS requires input length T to be divisible by C for continuous/interval sampling.\n\n5. **Multi-layer Encoder Structure**: Reformer's encoder stack `Encoder([EncoderLayer(...) for l in range(configs.e_layers)])` with LayerNorm can inform LightTS's stacking of multiple IEBlocks, though IEBlocks have fundamentally different internal architectures.",
    "differences": "1. **Core Architecture - Attention vs MLP**: Reformer uses LSH attention mechanism with O(L log L) complexity through locality-sensitive hashing, bucketing, and sorting operations. LightTS requires **completely new MLP-based IEBlock implementation** with temporal projection (R^H → R^F'), channel projection (R^W → R^W), and output projection (R^F' → R^F), using weight-shared MLPs across dimensions - none of this exists in Reformer's codebase.\n\n2. **Sampling Strategy - Novel Component**: LightTS introduces **continuous sampling** (Equation 1: consecutive C tokens) and **interval sampling** (Equation 2: C tokens with fixed intervals) to create non-overlapping sub-sequences, transforming R^T to R^{C×T/C}. This dual-sampling approach for capturing short/long-term patterns is **entirely absent in Reformer** and requires new implementation, including the sampling logic and dual-path processing.\n\n3. **Two-Part Architecture Design**: LightTS has a fundamentally different structure: Part I processes each time series variable independently through continuous/interval sampling + IEBlock-A/B, then Part II concatenates features (R^{2F×N}) for inter-variable learning via IEBlock-C. Reformer processes all variables jointly through attention layers. **The entire two-stage pipeline with variable-wise independence followed by cross-variable interaction needs new implementation**.\n\n4. **Bottleneck Design in IEBlock**: LightTS uses a bottleneck architecture where F' << F to reduce computational cost when applying channel projection F' times instead of H times. This design pattern (temporal projection → bottleneck channel projection → output projection) with **specific dimension reduction strategy** is not present in Reformer's attention-based approach.\n\n5. **Feature Aggregation Method**: LightTS down-projects feature matrices R^{F×T/C} to R^F using linear mapping R^{T/C} → R, then concatenates across sampling methods and variables. Reformer maintains full sequence length through attention and only projects at the final output layer. **The feature aggregation and concatenation logic for dual-sampling paths needs new implementation**.\n\n6. **Reversibility and Chunking**: Reformer implements reversible layers (RevNet) to save memory by computing activations on-the-fly during backpropagation, and chunks feed-forward computations. LightTS has no such memory-saving mechanisms, relying instead on efficient MLP operations and downsampling to reduce complexity. **These architectural patterns are incompatible and cannot be transferred**.\n\n7. **Multi-round Hashing**: Reformer uses n_rounds of LSH with different hash functions to reduce collision probability (Equation 6), performing attention n_rounds times in parallel. LightTS has no hashing or probabilistic components - it uses deterministic sampling and MLP operations. **The entire LSH infrastructure (hash functions, bucket assignment, sorting, chunking) is irrelevant to LightTS**."
  },
  {
    "source": "FEDformer_2022",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Architecture**: Both papers adopt seasonal-trend decomposition as a core design principle. FEDformer uses MOEDecomp (Mixture of Experts Decomposition) with multiple average pooling kernels, while MICN uses MHDecomp (Multi-scale Hybrid Decomposition) with similar multi-kernel averaging. The `series_decomp` class from FEDformer's implementation can be directly adapted as the foundation for MICN's MHDecomp, requiring only modification from weighted mixture (Softmax) to simple mean operation.\n\n2. **Embedding Strategy**: Both models use similar embedding mechanisms combining value embedding, positional encoding, and temporal features. FEDformer's `DataEmbedding` class (combining `TokenEmbedding`, `PositionalEmbedding`, and `TemporalEmbedding`) can be directly reused for MICN's embedding layer with minimal modifications to handle the concatenation of input series with zero placeholders.\n\n3. **Normalization and Residual Connections**: Both architectures employ layer normalization and residual connections throughout their networks. FEDformer's `my_Layernorm` implementation and residual connection patterns in encoder/decoder layers can be directly adopted for MICN's MIC layers, particularly in the global module where `Norm(Y + Dropout(Tanh(...)))` pattern is used.\n\n4. **Multi-scale Pattern Modeling**: Both papers recognize the importance of capturing patterns at different temporal scales. FEDformer's multi-mode frequency selection (selecting subset of Fourier/Wavelet modes) shares conceptual similarity with MICN's multi-scale branches (I/4, I/8, etc.). The multi-branch architecture pattern from FEDformer can inform MICN's parallel branch design.\n\n5. **Trend Component Handling**: Both models separate trend prediction from seasonal prediction. FEDformer accumulates trend components through decoder layers with linear projection, while MICN uses simpler linear regression or mean strategies. The trend accumulation logic in FEDformer's decoder can be simplified and adapted for MICN's trend-cyclical prediction block.",
    "differences": "1. **Core Frequency vs. Spatial Operation**: FEDformer's main innovation is frequency-domain processing using Fourier/Wavelet transforms (FEB-f/FEB-w blocks with FFT/DWT operations), achieving O(L) complexity through mode selection. MICN completely abandons frequency domain operations in favor of **pure convolutional operations** (1D Conv, Isometric Conv, Conv2d for merging). This requires implementing: (a) Downsampling Conv1d with stride=kernel for local feature extraction, (b) Isometric Convolution (padding S-1 zeros, kernel=S) for global correlation, (c) Conv1dTranspose for upsampling, (d) Conv2d for multi-branch merging - none of these specialized convolution patterns exist in FEDformer.\n\n2. **Attention Mechanism Elimination**: FEDformer uses attention-based modules (FourierBlock as self-attention replacement, FourierCrossAttention for encoder-decoder interaction) with query-key-value paradigm. MICN **completely removes all attention mechanisms**, replacing them with isometric convolution for global modeling. This fundamental shift requires implementing the isometric convolution architecture from scratch, including the specific padding strategy (S-1 zeros) and sequential inference capability that differs entirely from FEDformer's attention computation.\n\n3. **Encoder-Decoder vs. Single-Path Architecture**: FEDformer uses traditional encoder-decoder structure with cross-attention between them, requiring separate processing paths and decoder input containing label_len/2 from encoder input. MICN uses a **simplified single-path architecture** with only zero-padding for future positions (X_zero), eliminating decoder complexity. This requires restructuring the forward pass to remove encoder-decoder separation and implementing the simpler Concat(X_s, X_zero) input strategy.\n\n4. **Local-Global Hierarchical Processing**: MICN introduces a novel **two-stage local-global module** within each MIC layer: local module compresses features via downsampling Conv1d (reducing sequence length by factor i), then global module models correlations on compressed sequence via isometric convolution, finally upsamples back. FEDformer processes full-length sequences throughout. This hierarchical compression-modeling-expansion pattern with explicit local/global separation needs complete new implementation.\n\n5. **Multi-branch Merging Strategy**: FEDformer's MOEDecomp uses learned Softmax weights (data-dependent) to combine multiple decomposition results. MICN uses **Conv2d-based learnable merging** after parallel multi-scale branches process features, with weights learned during training rather than computed from input. This requires implementing: (a) parallel branch execution with different kernel sizes (I/4, I/8, etc.), (b) Conv2d layer that takes multi-branch outputs as input channels and learns optimal combination weights, (c) integration with subsequent FeedForward layers.\n\n6. **Complexity and Computational Pattern**: FEDformer achieves O(L) complexity through sparse frequency mode selection and FFT efficiency. MICN's complexity depends on convolution operations: local Conv1d is O(L/i) per branch, isometric Conv is O((L/i)²) for kernel computation but O(L) for application, overall approximately O(L) but through different computational mechanisms. The implementation requires careful kernel size management and efficient convolution operations rather than FFT-based acceleration.\n\n7. **Trend Prediction Simplification**: FEDformer accumulates trend through multiple decoder layers with learned transformations and projections. MICN uses either **simple linear regression** (MICN-regre variant) or **mean operation** (MICN-mean variant) for trend prediction, with no learned parameters in trend path. This requires implementing lightweight trend prediction modules (sklearn-style linear regression or simple mean pooling) rather than FEDformer's complex trend accumulation logic."
  },
  {
    "source": "Autoformer_2021",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers adopt series decomposition to separate trend-cyclical and seasonal components. Autoformer's `series_decomp` module with moving average can be directly reused as the foundation for MICN's MHDecomp block. The basic decomposition logic (AvgPool + Padding, seasonal = input - trend) is identical and can be adapted by extending to multiple kernel sizes.\n\n2. **Embedding Strategy**: Both use similar embedding components including value embedding, positional encoding, and temporal features. Autoformer's `DataEmbedding_wo_pos` class (with `TokenEmbedding`, `PositionalEmbedding`, `TemporalEmbedding/TimeFeatureEmbedding`) can be directly reused for MICN's embedding layer with minimal modifications to handle the concatenated input (X_s + X_zero).\n\n3. **Normalization and Residual Connections**: Both employ layer normalization and residual connections throughout their architectures. Autoformer's `my_Layernorm` (special normalization for seasonal part with bias removal) can be adapted for MICN's seasonal prediction block to maintain zero-mean properties.\n\n4. **Training Infrastructure**: Both are encoder-decoder or encoder-like architectures for long-term forecasting with similar input-output formats (input-I-predict-O). The training loop, loss computation (MSE/MAE), and data preprocessing pipelines from Autoformer can be directly reused.\n\n5. **Seasonal Pattern Focus**: Both emphasize modeling complex seasonal patterns through dedicated seasonal component processing. The philosophy of separating and independently modeling seasonal vs. trend components is shared, allowing MICN to leverage Autoformer's decomposition insights.\n\n6. **Placeholder Strategy**: Both use placeholder filling for future time steps. Autoformer uses zeros for seasonal and mean for trend initialization in decoder input (Equation 2), which aligns with MICN's X_zero placeholder strategy, enabling code reuse for input preparation.",
    "differences": "1. **Core Architecture - Auto-Correlation vs. Multi-scale Isometric Convolution**: Autoformer uses Auto-Correlation mechanism with FFT-based period discovery and time-delay aggregation (O(L log L) complexity). MICN completely replaces this with Multi-scale Isometric Convolution (MIC) layers combining local Conv1d downsampling and global isometric convolution. **NEW IMPLEMENTATION NEEDED**: (a) MIC layer with multiple branches for different scales, (b) Local module with Conv1d for downsampling (kernel=stride=i), (c) Isometric Convolution with padding of S-1 zeros and kernel=S for global correlation, (d) Conv1dTranspose for upsampling back to original length.\n\n2. **Multi-scale Decomposition Strategy**: Autoformer uses single-kernel moving average decomposition. MICN introduces Multi-scale Hybrid Decomposition (MHDecomp) using multiple kernels (kernel_1, ..., kernel_n) and averaging results. **NEW IMPLEMENTATION NEEDED**: (a) MHDecomp module that applies multiple AvgPool operations with different kernel sizes, (b) Mean aggregation across multiple decomposition results, (c) Integration with multi-scale branches in seasonal prediction.\n\n3. **Trend Prediction Approach**: Autoformer accumulates trend through progressive decomposition in decoder layers with learned projections (W_l,1, W_l,2, W_l,3 in Equation 4). MICN uses simple linear regression or mean strategy without progressive refinement. **NEW IMPLEMENTATION NEEDED**: (a) Linear regression module for trend prediction (MICN-regre variant), (b) Simple mean-based trend prediction (MICN-mean variant), (c) Removal of Autoformer's complex trend accumulation structure.\n\n4. **Encoder-Decoder vs. Single-Pass Architecture**: Autoformer has explicit encoder-decoder structure with cross-attention between encoder output and decoder. MICN uses a simpler single-pass architecture without encoder-decoder separation, directly processing concatenated input (X_s + X_zero). **NEW IMPLEMENTATION NEEDED**: (a) Remove encoder-decoder separation, (b) Implement direct concatenation and embedding of input with zero placeholders, (c) Sequential MIC layers without cross-attention mechanism.\n\n5. **Pattern Merging Mechanism**: Autoformer doesn't explicitly merge multi-scale patterns (uses single-scale decomposition). MICN merges different scale branches using Conv2d with learnable weights instead of simple concatenation. **NEW IMPLEMENTATION NEEDED**: (a) Conv2d-based merge module to weight and combine outputs from different scale branches (Y_s,l^global,i), (b) Learnable weighting mechanism replacing simple averaging.\n\n6. **Global Correlation Modeling**: Autoformer uses Auto-Correlation with FFT-based autocorrelation computation and Roll operations for time-delay aggregation. MICN uses Isometric Convolution (large kernel convolution with specific padding) as an alternative to self-attention for global correlation. **NEW IMPLEMENTATION NEEDED**: (a) Isometric Convolution implementation with S-1 zero padding and kernel size S, (b) Tanh activation for isometric conv outputs, (c) Integration with local-global module architecture.\n\n7. **Complexity and Efficiency Trade-off**: Autoformer achieves O(L log L) through FFT-based operations and sparse time-delay selection. MICN's complexity depends on convolution operations across multiple scales with varying kernel sizes. **NEW IMPLEMENTATION NEEDED**: Careful implementation of multi-scale convolutions to balance between modeling capability and computational efficiency, potentially optimizing Conv1d/Conv2d operations for different scale branches.\n\n8. **Output Generation**: Autoformer sums decoder's seasonal output (after projection) with accumulated trend. MICN uses Truncate(Projection(Y_s,N)) for seasonal and adds simple trend prediction. **NEW IMPLEMENTATION NEEDED**: (a) Truncate operation to extract only the prediction horizon O from (I+O) sequence, (b) Simplified final prediction combining truncated seasonal and trend predictions without progressive accumulation."
  },
  {
    "source": "Informer_2020",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt an encoder-decoder paradigm for time series forecasting. MICN can reuse Informer's DataEmbedding module (time features encoding, positional encoding, value embedding) and the basic LayerNorm/Dropout infrastructure from the source code.\n\n2. **Instance Normalization Strategy**: Both papers apply instance normalization (mean subtraction and standard deviation division) on input sequences before processing. MICN can directly adapt Informer's normalization code: `mean_enc = x_enc.mean(1, keepdim=True).detach()` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()` for preprocessing.\n\n3. **Generative Inference with Placeholders**: Both use placeholder tokens for future predictions. Informer concatenates start tokens with zero placeholders (`Concat(X_token, X_0)`), while MICN uses `Concat(X_s, X_zero)`. The placeholder generation logic and concatenation operations can be reused from Informer's decoder input preparation.\n\n4. **Multi-layer Stacking with Residual Connections**: Both employ stacked layers with residual connections and normalization. MICN's MIC layers follow similar patterns to Informer's encoder layers (attention + FFN + residual + norm). The basic layer stacking framework and residual connection implementation can be adapted.\n\n5. **Convolution for Feature Processing**: Both use 1D convolution operations. Informer's ConvLayer (Conv1d with ELU activation and MaxPool) for distilling, and MICN uses Conv1d for downsampling/upsampling. The Conv1d implementation patterns and activation functions can be reused.\n\n6. **Linear Projection for Output**: Both use linear layers for final output projection. Informer's `nn.Linear(d_model, c_out)` in decoder projection can be directly adapted for MICN's Projection operation to map from hidden dimension D to output dimension d.",
    "differences": "1. **Core Attention Mechanism vs. Isometric Convolution**: Informer's innovation is ProbSparse self-attention with O(L log L) complexity using query sparsity measurement and Top-u selection. MICN completely abandons self-attention, introducing isometric convolution (padding S-1 zeros with kernel=S) for global correlation modeling. MICN needs NEW implementation of: (a) IsometricConv module with specific padding strategy, (b) Multi-scale convolution branches with different kernel sizes, (c) Conv2d-based merge operation for pattern fusion. These components are entirely absent in Informer.\n\n2. **Series Decomposition Strategy**: Informer uses simple moving average for trend-seasonal decomposition in a single scale. MICN introduces Multi-scale Hybrid Decomposition (MHDecomp) using multiple AvgPool kernels (kernel_1, ..., kernel_n) and mean aggregation to capture different temporal patterns. MICN needs NEW implementation of: (a) Multi-kernel AvgPool with mean fusion, (b) Separate trend-cyclical prediction block with linear regression strategy, (c) Coordination between decomposition scales and MIC layer scales.\n\n3. **Local-Global Hierarchical Processing**: Informer processes sequences globally through attention without explicit local feature extraction. MICN introduces a hierarchical local-global module: local module uses Conv1d (stride=kernel=i) for downsampling to compress local features, then global module applies isometric convolution on compressed sequences, followed by Conv1dTranspose for upsampling. MICN needs NEW implementation of: (a) Multi-scale local downsampling with different compression ratios (I/4, I/8, etc.), (b) Global isometric convolution on compressed representations, (c) Transposed convolution for length recovery, (d) Branch-wise processing and merging.\n\n4. **Distilling vs. Multi-scale Isometric Convolution**: Informer's encoder uses self-attention distilling (MaxPool with stride 2) to progressively halve sequence length, reducing memory to O((2-ε)L log L). MICN maintains constant sequence length (I+O) throughout all MIC layers, using multi-scale branches to capture different patterns without dimension reduction. MICN needs NEW implementation of: (a) Length-preserving MIC layers with N stacks, (b) Multi-branch parallel processing at each layer, (c) Conv2d-based weighted merging of branches.\n\n5. **Decoder Design Philosophy**: Informer uses standard masked multi-head attention in decoder with cross-attention to encoder outputs, requiring TriangularCausalMask for autoregressive prevention. MICN eliminates the decoder entirely, using a simpler complementary-O strategy where placeholders are directly concatenated to input, and the entire prediction is generated in one forward pass through MIC layers. MICN needs NEW implementation of: (a) Single-pass prediction without decoder, (b) Truncate operation to extract final O-length predictions, (c) Simplified embedding without decoder-specific components.\n\n6. **Complexity and Efficiency Trade-offs**: Informer achieves O(L log L) complexity through sparse attention with probabilistic sampling (U = L_K ln L_Q dot-products). MICN's complexity depends on multi-scale convolution operations: local Conv1d with varying kernels and global isometric convolution with kernel=S. MICN needs NEW implementation of: (a) Efficient isometric convolution with large kernels on short sequences, (b) Multi-branch parallel execution optimization, (c) Memory management for multiple scale representations without Informer's progressive dimension reduction strategy."
  },
  {
    "source": "Reformer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Attention Mechanism Foundation**: Both papers build upon the standard scaled dot-product attention from Transformer (Equation 1 in both papers). The core attention computation follows `softmax(QK^T/√d_k)V`, where Q, K, V are linearly projected from input. The source code's `ReformerLayer` with `LSHSelfAttention` demonstrates this pattern, and the basic attention structure (linear projections, multi-head mechanism) can be directly reused for implementing Pyraformer's PAM.\n\n2. **Encoder-Only Architecture for Time Series**: Both adopt encoder-based architectures without autoregressive decoding. Reformer's implementation shows `Encoder` with stacked `EncoderLayer` blocks, each containing attention + feed-forward layers. Pyraformer similarly uses stacked PAM layers (N layers) followed by prediction modules. The `Encoder` class structure and layer stacking logic from Reformer can be adapted for Pyraformer's encoder backbone.\n\n3. **Input Embedding Strategy**: Both use composite embeddings combining data values, temporal covariates, and positional information. Reformer's code shows `DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)` which handles value embedding, temporal encoding, and positional encoding. This embedding module can be directly reused for Pyraformer's embedding layer before CSCM.\n\n4. **Normalization for Short-term Forecasting**: Both implement instance normalization (mean-std normalization) per sample. Reformer's `short_forecast` method shows: `mean_enc = x_enc.mean(1, keepdim=True).detach()`, `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()`, then normalize input and denormalize output. This exact normalization code block can be transplanted to Pyraformer.\n\n5. **Feed-Forward Network Structure**: Both use position-wise feed-forward networks (FFN) after attention layers. Reformer's `EncoderLayer` includes `d_ff` parameter for FFN dimension expansion. Pyraformer also employs FFN with dimension D_F. The FFN implementation (two linear layers with activation) from Reformer's `EncoderLayer` can be reused.\n\n6. **Multi-Head Attention Framework**: Both employ multi-head attention with H heads to capture different representation subspaces. Reformer's code specifies `n_heads` parameter, and Pyraformer uses H heads. The multi-head splitting and concatenation logic can be adapted from Reformer's attention implementation.\n\n7. **Layer Normalization**: Both use LayerNorm for stabilizing training. Reformer's code shows `norm_layer=torch.nn.LayerNorm(configs.d_model)` in the Encoder. Pyraformer also applies LayerNorm within its PAM layers. This component can be directly reused.\n\n8. **Placeholder Padding for Future Steps**: Reformer's code concatenates zero-filled future steps: `x_enc = torch.cat([x_enc, x_dec[:, -self.pred_len:, :]], dim=1)`. Pyraformer similarly adds end tokens (z_{t+1}=0) for single-step forecasting. This padding strategy can be adapted for Pyraformer's prediction module.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses Locality-Sensitive Hashing (LSH) attention with hash bucketing (Equation 4-6, Figure 2) to achieve O(L log L) complexity by grouping similar queries/keys into buckets. Pyraformer introduces **Pyramidal Attention Module (PAM)** with multi-resolution C-ary tree structure (Equation 2-3, Figure 1d) achieving O(L) complexity. **NEW**: Implement custom CUDA kernel for sparse pyramidal attention where each node attends to only A adjacent nodes, C children, and 1 parent (∣N_ℓ^(s)∣ ≤ A+C+1), rather than Reformer's hash-bucket-based attention. The attention mask must encode the pyramidal graph topology defined in Equation 2.\n\n2. **Multi-Scale Hierarchical Structure - NEW IMPLEMENTATION REQUIRED**: Reformer operates at a single temporal resolution with all tokens at the same scale. Pyraformer constructs **Coarser-Scale Construction Module (CSCM)** with S scales forming a C-ary tree (Figure 3). **NEW**: Implement sequential convolution layers with kernel size C and stride C to progressively downsample the sequence from L to L/C, L/C^2, ..., L/C^(S-1) nodes. This creates a hierarchical representation where coarser scales capture long-range dependencies. The bottleneck structure (dimension reduction before convolution, restoration after) must be implemented to reduce parameters.\n\n3. **Graph-Based Information Propagation - NEW IMPLEMENTATION REQUIRED**: Reformer's LSH attention creates dynamic, content-based connectivity through hashing. Pyraformer uses **fixed pyramidal graph structure** with inter-scale (parent-child) and intra-scale (neighboring nodes) connections. **NEW**: Implement graph-based attention that respects the pyramidal topology, where information flows both vertically (across scales) and horizontally (within scales). This requires custom attention masking that enforces the connectivity patterns defined by A_ℓ^(s), C_ℓ^(s), P_ℓ^(s) in Equation 2.\n\n4. **Prediction Module Design - NEW IMPLEMENTATION REQUIRED**: Reformer uses simple linear projection on the encoded sequence: `dec_out = self.projection(enc_out)` followed by slicing `[:, -self.pred_len:, :]`. Pyraformer proposes **two distinct prediction modules**: (1) Single-step: gather last nodes at all scales, concatenate, and project; (2) Multi-step: either batch projection or **decoder with two full attention layers** using prediction tokens F_p. **NEW**: Implement scale-wise feature gathering (collecting last node from each of S scales), concatenation across scales, and the two-layer decoder architecture where first layer attends to encoder output F_e, second layer attends to concatenated F_{d1} and F_e.\n\n5. **Complexity Optimization Strategy - NEW IMPLEMENTATION REQUIRED**: Reformer achieves efficiency through LSH bucketing (sorting by hash, chunking, Equation 5) and reversible layers for memory efficiency. Pyraformer achieves **O(L) complexity through sparse pyramidal connectivity** (Proposition 1) and **O(1) maximum path length** (Proposition 2) by carefully choosing C to satisfy Equation 5. **NEW**: Implement the pyramidal attention with complexity O(AL) where A is constant (3 or 5), eliminating the need for LSH hashing, bucket sorting, and reversible layers. The custom CUDA kernel must efficiently compute only the sparse Q-K pairs defined by the pyramidal graph.\n\n6. **Receptive Field Expansion Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer expands receptive field through multiple rounds of hashing (n_rounds parameter, Equation 6) where each round uses different hash functions. Pyraformer expands receptive field through **multi-scale hierarchy** where coarser scales provide global context (Lemma 1). **NEW**: Implement the condition in Equation 4: L/C^(S-1) - 1 ≤ (A-1)N/2 to ensure global receptive field at the coarsest scale after N attention layers, rather than using multiple hash rounds.\n\n7. **Node Initialization Strategy - NEW IMPLEMENTATION REQUIRED**: Reformer directly embeds input tokens without hierarchical construction. Pyraformer requires **CSCM to initialize coarse-scale nodes** before PAM. **NEW**: Implement the scale-by-scale construction where each coarser scale is built by convolving the finer scale (Figure 3), creating a complete C-ary tree structure. The initialization must ensure that parent nodes meaningfully summarize their C children's information.\n\n8. **Attention Masking for Causality - NEW IMPLEMENTATION REQUIRED**: Reformer implements causal masking by comparing position indices after LSH sorting and forbids self-attention except when no other targets exist. Pyraformer must implement **pyramidal graph causality** where nodes can only attend to past nodes within their neighborhood N_ℓ^(s). **NEW**: Implement position-aware masking that respects both the pyramidal structure and temporal causality, ensuring nodes at time t cannot attend to nodes at time t' > t, even within the same hash bucket or scale.\n\n9. **Parameter Allocation - NEW IMPLEMENTATION REQUIRED**: Reformer's parameters are O(N(HD_KD_K + DD_F)) for N attention layers. Pyraformer adds **CSCM parameters** O((S-1)CD_K^2) for the convolution layers across S-1 coarser scales. **NEW**: Implement the CSCM convolution layers with proper parameter initialization and the bottleneck structure to manage the additional parameter overhead from multi-scale construction."
  },
  {
    "source": "Informer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer-based encoder-decoder architecture for time series forecasting. The source paper's `DataEmbedding`, `Encoder`, and `Decoder` base classes can be directly reused. Specifically, the embedding layer combining temporal features, positional encoding, and value embeddings (DataEmbedding in Informer) is directly applicable to Pyraformer's input processing.\n\n2. **Attention Mechanism Framework**: Both utilize modified attention mechanisms to reduce complexity from O(L²). The source paper's `AttentionLayer` wrapper structure (query/key/value projections, multi-head mechanism, output projection) can be adapted for Pyraformer. The core attention computation flow in `AttentionLayer.forward()` provides a template, though the inner attention logic differs.\n\n3. **Instance Normalization Strategy**: Both papers apply instance normalization on input sequences before encoding. Informer's normalization code in `short_forecast()` method (mean subtraction and standard deviation division, then denormalization after decoding) can be directly reused for Pyraformer's preprocessing pipeline.\n\n4. **Feed-Forward Network Components**: Both use position-wise feed-forward networks with Conv1D implementations. Informer's `DecoderLayer` contains Conv1D layers with activation and dropout that can be adapted. The pattern of `Conv1D → Activation → Dropout → Conv1D` is common to both architectures.\n\n5. **Multi-Head Attention Structure**: Both employ multi-head attention with d_keys and d_values projections. The source paper's multi-head splitting logic (reshaping to [B, L, H, D]) and concatenation can be reused in Pyraformer's attention implementation.\n\n6. **Training Objective**: Both use MSE loss for forecasting and support univariate/multivariate prediction through adjustable output dimensions. The loss computation and output projection layer from Informer can be directly applied.",
    "differences": "1. **Core Attention Mechanism - NEEDS NEW IMPLEMENTATION**: Pyraformer introduces Pyramidal Attention Module (PAM) with C-ary tree structure and multi-resolution modeling, fundamentally different from Informer's ProbSparse attention. Must implement: (a) Coarser-Scale Construction Module (CSCM) with strided convolutions to build C-ary tree (kernel_size=C, stride=C), (b) Custom sparse attention pattern following Equation (3) where each node attends to adjacent A nodes, C children, and 1 parent (N_ℓ^(s) = A_ℓ^(s) ∪ C_ℓ^(s) ∪ P_ℓ^(s)), (c) Custom CUDA kernel using TVM for efficient sparse Q-K pair computation to achieve true O(L) complexity, (d) Multi-scale node concatenation mechanism across pyramid levels.\n\n2. **Complexity Reduction Strategy - REQUIRES DIFFERENT APPROACH**: Informer uses query sparsity measurement M(q_i, K) with Top-u query selection and self-attention distilling with MaxPooling to reduce complexity. Pyraformer uses spatial pyramidal graph structure. Must implement: (a) Hierarchical scale construction instead of distilling layers, (b) Graph-based attention routing (intra-scale and inter-scale connections) instead of probability-based query selection, (c) No need for LSE-based sparsity measurement or random sampling of Q-K pairs.\n\n3. **Encoder Architecture - SUBSTANTIAL REDESIGN NEEDED**: Informer uses stacked EncoderLayers with optional ConvLayer distilling between layers (halving sequence length). Pyraformer requires: (a) CSCM module with multiple scales (S scales) constructed simultaneously using parallel convolutions, (b) Bottleneck structure (dimension reduction before CSCM, restoration after) to reduce parameters, (c) PAM layers operating on concatenated multi-scale sequences [L, L/C, L/C², ..., L/C^(S-1)], (d) No progressive distilling; all scales maintained throughout encoding.\n\n4. **Decoder and Prediction Module - TWO NEW STRATEGIES**: Informer uses standard decoder with masked attention and start token. Pyraformer offers: (a) Single-step forecasting: gather last nodes from all pyramid scales, concatenate, feed to FC layer (simpler than Informer's decoder), (b) Multi-step forecasting Option 1: direct mapping from concatenated last nodes to all M future steps (batch prediction), (c) Multi-step forecasting Option 2: decoder with two full attention layers using prediction tokens F_p, where both layers receive encoder output F_e directly (different from Informer's cross-attention pattern), (d) No autoregressive decoding; all predictions generated in one forward pass.\n\n5. **Positional Encoding and Scale Representation - NEW MECHANISM**: Informer uses standard temporal encoding. Pyraformer must implement: (a) Multi-scale positional encoding for nodes at different pyramid levels, (b) Scale-specific node features representing different temporal resolutions (hourly→daily→weekly→monthly), (c) Proper alignment of temporal information across scales in the C-ary tree structure.\n\n6. **Memory and Computation Pattern - FUNDAMENTALLY DIFFERENT**: Informer achieves O((2-ε)L log L) memory through distilling. Pyraformer achieves O(AL) time/space complexity where A is constant (3 or 5). Must implement: (a) Fixed-size attention neighborhood regardless of sequence length, (b) Sparse attention mask based on graph connectivity (not probability-based), (c) Custom memory allocation for pyramidal structure with O(L(1 + 1/C + 1/C² + ... + 1/C^(S-1))) ≈ O(L) total nodes, (d) Efficient tensor operations for non-uniform attention patterns across scales."
  },
  {
    "source": "DLinear_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Temporal Decomposition Foundation**: Both papers utilize seasonal-trend decomposition as a core preprocessing technique. DLinear's `series_decomp` module with moving average kernels (from Autoformer) can be directly reused in TimesNet for initial time series decomposition, as TimesNet also benefits from separating trend and seasonal components before further processing.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both adopt DMS forecasting where the entire prediction horizon is generated in one forward pass, avoiding autoregressive error accumulation. The input-output structure (seq_len → pred_len) and training loop from DLinear's implementation can be adapted for TimesNet's forecasting task.\n\n3. **Instance Normalization Approach**: Both papers handle distribution shifts through normalization techniques. DLinear's NLinear variant (subtract last value, process, add back) addresses non-stationarity, which is conceptually similar to TimesNet's need for stable inputs before FFT analysis and 2D transformation.\n\n4. **Channel-Independent Processing Option**: DLinear's `individual` parameter allows separate linear layers per channel, treating each variate independently. This design philosophy aligns with TimesNet's approach of processing temporal variations where the inception block operates on each channel dimension (d_model) independently after 2D transformation.\n\n5. **Residual Connection Architecture**: Both employ residual connections for stable training. DLinear's simple additive residuals and TimesNet's residual TimesBlock structure (Equation 4: X^l = TimesBlock(X^(l-1)) + X^(l-1)) share the same principle, allowing direct adaptation of the residual training framework.\n\n6. **Embedding Layer for Feature Projection**: Both require initial embedding to project raw inputs into higher-dimensional feature space. DLinear's channel projection mechanism can be adapted as TimesNet's initial Embed() operation that transforms X_1D ∈ R^(T×C) to X_1D^0 ∈ R^(T×d_model).",
    "differences": "1. **Core Innovation - Temporal 2D-Variation Modeling vs. Linear Simplicity**: TimesNet's fundamental contribution is transforming 1D time series into 2D tensors based on discovered periodicities, then applying 2D convolutional kernels (inception blocks) to capture intraperiod and interperiod variations simultaneously. This requires implementing: (a) FFT-based period detection (Equation 1-2), (b) 1D-to-2D reshaping with padding (Equation 3), (c) 2D inception block with multi-scale kernels, (d) 2D-to-1D transformation back. DLinear has NONE of these components - it only uses simple temporal linear layers.\n\n2. **Multi-Periodicity Discovery and Processing**: TimesNet requires implementing frequency domain analysis using FFT to identify top-k significant periods (Equation 1: A = Avg(Amp(FFT(X_1D)))), then creating k different 2D representations for each period. This involves: (a) torch.fft.rfft for frequency analysis, (b) amplitude calculation and top-k selection, (c) period length computation (p_i = ⌈T/f_i⌉), (d) processing k parallel 2D tensors through shared inception block. DLinear has no frequency analysis or multi-scale period handling.\n\n3. **2D Convolutional Architecture - Inception Block**: TimesNet needs a complete inception block implementation with multi-scale 2D convolutions (e.g., 1×1, 3×3, 5×5 kernels) to capture spatial patterns in the reshaped 2D tensors (p_i × f_i × d_model). This requires: (a) parallel conv2d layers with different kernel sizes, (b) max pooling branches, (c) concatenation and fusion of multi-scale features. DLinear only has 1D linear layers (nn.Linear) with no convolutional components.\n\n4. **Adaptive Aggregation with Amplitude Weighting**: TimesNet implements a sophisticated aggregation mechanism (Equation 6) where k different 1D representations are weighted by softmax-normalized amplitude values (Â_f_i) from FFT analysis. This requires: (a) maintaining amplitude values throughout forward pass, (b) softmax normalization across k frequencies, (c) weighted summation of k outputs. DLinear simply adds decomposed trend and seasonal outputs with no adaptive weighting.\n\n5. **Dynamic Reshaping Operations**: TimesNet requires implementing dynamic tensor reshaping based on discovered periods: (a) Padding() to make length compatible with period (length → p_i × f_i), (b) Reshape_{p_i,f_i}() to create 2D tensors with varying dimensions per layer, (c) Trunc() to restore original length after 2D processing. These operations must handle variable shapes across different periods and layers. DLinear operates on fixed-length sequences with simple permute operations.\n\n6. **Computational Complexity and Parameter Efficiency**: TimesNet's parameter-efficient design shares the same inception block across k different 2D tensors, making model size invariant to k. This requires careful implementation of: (a) shared weight mechanism for processing multiple 2D views, (b) batch processing of k tensors through single inception block, (c) managing memory for k parallel 2D representations. DLinear's complexity is O(T) with simple matrix multiplication, while TimesNet involves O(T log T) for FFT plus 2D convolution operations, requiring different optimization strategies."
  },
  {
    "source": "FEDformer_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Frequency Domain Analysis**: Both papers leverage frequency domain transformations to analyze time series patterns. FEDformer uses FFT in FourierBlock and FourierCrossAttention for mode selection, while TimesNet uses FFT to discover dominant periods (Equation 1). The FFT-based period detection logic in FEDformer's `get_frequency_modes()` can be adapted for TimesNet's `Period()` function to extract top-k frequencies and their corresponding periods.\n\n2. **Seasonal-Trend Decomposition Architecture**: Both employ decomposition strategies for time series. FEDformer uses `series_decomp` with moving average filters and MOEDecomp for extracting seasonal/trend components. TimesNet implicitly handles multi-periodicity through 2D transformation. The decomposition module `series_decomp` and the concept of separating different temporal components can be reused as preprocessing or within TimesBlock.\n\n3. **Embedding and Normalization**: Both use similar input embedding strategies. FEDformer's `DataEmbedding` class (combining value embedding, temporal encoding, positional encoding) can be directly reused for TimesNet's initial feature projection. The embedding approach handles multivariate time series with temporal marks, which is applicable to both architectures.\n\n4. **Encoder-Decoder Framework**: Both adopt encoder-decoder architectures with residual connections. FEDformer's residual structure in EncoderLayer/DecoderLayer (Equations 1-2) aligns with TimesNet's residual TimesBlock design (Equation 4). The overall stacking mechanism and layer normalization (`my_Layernorm`) can be adapted.\n\n5. **Multi-scale Feature Extraction**: FEDformer captures multiple frequency modes (random/low mode selection) to handle different temporal scales, while TimesNet extracts k different periods for multi-scale 2D-variations. The concept of selecting top-k important components (modes in FEDformer, periods in TimesNet) is shared, and FEDformer's mode selection logic can guide TimesNet's period selection.\n\n6. **Padding and Reshaping Operations**: FEDformer uses zero-padding in Fourier/Wavelet transforms to handle length compatibility, similar to TimesNet's `Padding()` operation before reshaping to 2D tensors. The padding utilities can be directly reused.",
    "differences": "1. **Core Innovation - Frequency Enhancement vs 2D Transformation**: FEDformer's main contribution is frequency-enhanced blocks (FEB-f/FEB-w) and attention (FEA-f/FEA-w) operating in frequency/wavelet domain with O(L) complexity. TimesNet's innovation is transforming 1D time series into 2D tensors based on discovered periods (Equation 3) and applying 2D vision backbones (Inception block). **NEW IMPLEMENTATION NEEDED**: TimesNet requires implementing the 1D-to-2D reshape operation `Reshape_{p_i,f_i}()` that converts temporal sequences into structured 2D tensors with rows representing interperiod-variation and columns representing intraperiod-variation.\n\n2. **Attention Mechanism vs 2D Convolution**: FEDformer uses Fourier/Wavelet-based attention mechanisms (FourierBlock, FourierCrossAttention, MultiWaveletTransform) for capturing dependencies. TimesNet replaces attention entirely with 2D convolutional operations through the Inception block. **NEW IMPLEMENTATION NEEDED**: TimesNet requires implementing a parameter-efficient Inception block with multi-scale 2D kernels (typically 1×1, 3×3, 5×5 convolutions) to simultaneously capture intraperiod and interperiod variations in the 2D space.\n\n3. **Multi-Periodicity Handling**: FEDformer handles multiple scales through random/low mode selection in frequency domain but processes all modes uniformly. TimesNet explicitly discovers k different periods via amplitude-based selection (Equation 1), creates k separate 2D tensors for each period (Equation 5), and adaptively aggregates them using softmax-normalized amplitudes (Equation 6). **NEW IMPLEMENTATION NEEDED**: (a) Period-specific 2D transformation for each of k periods; (b) Adaptive aggregation mechanism using amplitude-based weights; (c) Shared Inception block processing across different period representations.\n\n4. **Complexity and Efficiency**: FEDformer achieves O(L) time/memory complexity through fixed mode selection (M=64) in frequency domain. TimesNet's complexity depends on the Inception block's 2D convolutions applied to k reshaped tensors, which is O(k×p_i×f_i×kernel_size²) but benefits from parameter sharing across periods. **NEW IMPLEMENTATION NEEDED**: Efficient batched processing of multiple 2D tensors with different shapes (p_i×f_i varies for each period).\n\n5. **Cross-Attention Design**: FEDformer uses frequency-enhanced cross-attention (FEA-f/FEA-w) between encoder and decoder with queries/keys/values in frequency domain. TimesNet does not explicitly use cross-attention; instead, it relies on the encoder-only architecture with residual TimesBlocks. **NEW IMPLEMENTATION NEEDED**: TimesNet may need a simpler decoder design or direct forecasting from encoder outputs, eliminating the complex cross-attention modules present in FEDformer.\n\n6. **Wavelet Transform Support**: FEDformer includes comprehensive wavelet-based structures (FEB-w, FEA-w, MultiWaveletTransform, MWT_CZ1d) with recursive decomposition using Legendre/Chebyshev basis. TimesNet focuses solely on FFT-based period discovery without wavelet support. **NOT NEEDED**: The entire wavelet transformation infrastructure (get_filter, get_phi_psi, sparseKernelFT1d, etc.) is not required for TimesNet.\n\n7. **Truncation and Back-Transformation**: FEDformer maintains consistent sequence length through FFT/inverse FFT operations. TimesNet requires explicit truncation (`Trunc()`) after reshaping 2D representations back to 1D (Equation 5) to match original length T. **NEW IMPLEMENTATION NEEDED**: Implement the `Trunc()` operation that removes padded values when converting from 2D (length p_i×f_i) back to 1D (length T).\n\n8. **Vision Backbone Integration**: FEDformer is purely designed for time series with frequency/wavelet domain operations. TimesNet bridges time series analysis with computer vision by enabling the use of various 2D vision backbones (ResNet, ResNeXt, ConvNeXt, attention-based models). **NEW IMPLEMENTATION NEEDED**: Modular design allowing easy replacement of the Inception block with other 2D vision architectures, requiring a flexible interface for different backbone implementations."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Normalization Strategy for Forecasting**: Both papers employ instance normalization (RevIN-style) for forecasting tasks. Pyraformer's code shows `mean_enc = x_enc.mean(1, keepdim=True).detach()` and `std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()` followed by normalization and denormalization. TimesNet can directly reuse this normalization module for stabilizing training and improving generalization.\n\n2. **Multi-Resolution Temporal Representation**: Both papers decompose time series into multiple temporal scales. Pyraformer uses pyramidal multi-scale structure (C-ary tree with scales S) via CSCM with convolution layers (stride C, kernel C), while TimesNet discovers multiple periods via FFT and reshapes into 2D tensors. The concept of processing different temporal resolutions simultaneously is shared - TimesNet can adapt Pyraformer's convolution-based downsampling logic in `Bottleneck_Construct` for initial multi-scale feature extraction.\n\n3. **Embedding Layer Architecture**: Both use similar embedding strategies combining data embedding with positional/temporal encodings. Pyraformer's `DataEmbedding` module that fuses value embedding with temporal marks can be directly adapted for TimesNet's initial embedding layer before the 2D transformation, reducing implementation effort.\n\n4. **Residual Connection Design**: Both employ residual connections for deep architectures. Pyraformer uses residual in feed-forward layers, TimesNet uses `X_1D^l = TimesBlock(X_1D^{l-1}) + X_1D^{l-1}`. The residual connection pattern and layer normalization from Pyraformer's `PositionwiseFeedForward` can be reused in TimesNet's block structure.\n\n5. **Batch Processing and Projection Layers**: Both use linear projection layers for final predictions. Pyraformer's `self.projection = nn.Linear((len(window_size)+1)*self.d_model, self.pred_len * configs.enc_in)` demonstrates the projection pattern that TimesNet can adapt for its task-specific output heads across different applications (forecasting, classification, etc.).",
    "differences": "1. **Core Attention Mechanism vs 2D Convolution**: Pyraformer's innovation is pyramidal attention (PAM) with O(L) complexity using sparse attention on C-ary tree structure with custom CUDA kernels. TimesNet completely abandons attention mechanisms, instead using Inception blocks with 2D convolutions on reshaped tensors. NEW IMPLEMENTATION NEEDED: (a) FFT-based period detection module `Period()` to identify top-k frequencies and periods, (b) 1D-to-2D transformation `Reshape_{p_i,f_i}()` based on discovered periods, (c) Inception block with multi-scale 2D kernels (multiple parallel conv paths with different kernel sizes), (d) Adaptive aggregation using softmax-weighted amplitude-based fusion.\n\n2. **Temporal Dependency Modeling Philosophy**: Pyraformer models long-range dependencies through hierarchical attention across pyramid scales (inter-scale via parent-child, intra-scale via neighboring nodes with window A). TimesNet models temporal variations through 2D spatial structure where rows represent interperiod-variation and columns represent intraperiod-variation. NEW IMPLEMENTATION NEEDED: The entire 2D-variation modeling framework including padding/truncation logic to handle arbitrary sequence lengths when reshaping, and the parameter-efficient design where one Inception block processes all k different 2D tensors.\n\n3. **Multi-Scale Construction Strategy**: Pyraformer uses CSCM with fixed downsampling ratio C (typically 2 or 4) and bottleneck structure (d_model → d_bottleneck → d_model) to build coarser scales bottom-up via strided convolutions. TimesNet dynamically discovers k different periods per layer via FFT amplitude analysis, creating k parallel 2D representations with different (p_i, f_i) shapes. NEW IMPLEMENTATION NEEDED: (a) Dynamic period discovery that adapts per sample and per layer, (b) Handling variable-sized 2D tensors {X_2D^{l,1}, ..., X_2D^{l,k}} with different dimensions, (c) Amplitude-based importance weighting for aggregation instead of fixed hierarchical structure.\n\n4. **Computational Complexity and Scalability**: Pyraformer achieves O(AL) = O(L) complexity through sparse pyramidal attention with maximum path length O(1) when properly configured (Propositions 1-2). It requires custom CUDA kernels for efficient sparse attention. TimesNet's complexity depends on Inception block operations on 2D tensors: O(k × p_i × f_i × d_model²) where k is number of periods, but benefits from standard PyTorch 2D conv operations without custom kernels. NEW IMPLEMENTATION NEEDED: Efficient batched processing of multiple 2D tensors with different shapes, and optimization strategies for FFT computation at each layer.\n\n5. **Task Generality and Architecture Flexibility**: Pyraformer is specifically designed for forecasting with encoder-only (single-step) or encoder-decoder (multi-step) architectures using full attention in decoder. TimesNet is designed as a general time series analysis backbone applicable to 5 tasks (forecasting, classification, imputation, anomaly detection). NEW IMPLEMENTATION NEEDED: (a) Task-specific output heads beyond forecasting, (b) Modular design allowing vision backbone substitution (ResNet, ConvNeXt, attention models) instead of fixed Inception, (c) Backbone selection interface to swap 2D processing modules while maintaining the 1D↔2D transformation framework.\n\n6. **Feature Aggregation Mechanism**: Pyraformer concatenates features from all pyramid scales' last nodes: `all_enc.view(seq_enc.size(0), self.all_size[0], -1)` using fixed gather indices. TimesNet uses adaptive weighted aggregation: `Σ(Â_{f_i} × X̂_1D^{l,i})` where weights are softmax-normalized FFT amplitudes, allowing the model to dynamically emphasize important periods. NEW IMPLEMENTATION NEEDED: Amplitude-based softmax weighting module that normalizes across k periods and performs weighted sum, integrated with gradient flow for end-to-end learning of period importance."
  }
]
