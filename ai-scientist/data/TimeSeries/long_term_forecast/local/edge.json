[
  {
    "source": "Informer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both adopt the Transformer encoder-decoder structure for long sequence time-series forecasting (LSTF), with encoder processing historical sequences and decoder generating future predictions.\n2. **Embedding Strategy**: Both use DataEmbedding that combines token embeddings (via Conv1d with kernel_size=3) and temporal embeddings for input representation, encoding both value and time information.\n3. **Feed-Forward Network Design**: Both employ two-layer Conv1d-based feed-forward networks in their encoder/decoder layers with activation functions (ReLU/GELU) and dropout, following the standard Transformer FFN pattern.\n4. **Layer Normalization**: Both apply LayerNorm after each sub-layer (attention + FFN) with residual connections, maintaining the standard Transformer normalization scheme.\n5. **Multi-Head Mechanism**: Both implement multi-head attention variants (ProbSparse in Informer, Auto-Correlation in Autoformer) to capture diverse temporal patterns from different representation subspaces.\n6. **Decoder Input Strategy**: Both initialize decoder inputs by concatenating historical information with placeholders for future predictions, though Informer uses start tokens while Autoformer uses decomposed components (seasonal + trend).",
    "differences": "1. **Core Innovation**: Informer introduces ProbSparse self-attention to reduce quadratic complexity O(L²) to O(L log L) by selecting top-u dominant queries based on sparsity measurement M(q,K), using KL-divergence-inspired query selection. Autoformer proposes series decomposition as a built-in block and Auto-Correlation mechanism that discovers period-based dependencies via FFT-based correlation in O(L log L), fundamentally replacing dot-product attention with time-delay aggregation.\n2. **Attention Mechanism**: Informer uses sparse attention where queries are selected via max-mean measurement (top-k from sampled Q-K pairs), maintaining the softmax(QK^T/√d)V paradigm but with reduced queries. Autoformer completely abandons self-attention, using Auto-Correlation that computes time-delay correlations via FFT, identifies top-k periods, and aggregates values by rolling/shifting operations—no query-key similarity computation.\n3. **Decomposition Strategy**: Informer has no explicit decomposition mechanism; it relies on distilling operation (ConvLayer with MaxPool) in encoder for multi-scale feature extraction. Autoformer embeds series decomposition (moving average for trend extraction) as inner blocks throughout encoder/decoder, progressively refining seasonal and trend components separately at each layer (Equations 3-4).\n4. **Encoder Architecture**: Informer encoder uses stacked layers with ProbAttention + FFN + optional ConvLayer distilling for halving sequence length. Autoformer encoder focuses solely on seasonal modeling, explicitly extracting and discarding trend components at each layer via SeriesDecomp blocks (S_en^l = SeriesDecomp(AutoCorr(X) + X)).\n5. **Decoder Design**: Informer decoder uses standard masked self-attention (ProbAttention with causal mask) + encoder-decoder cross-attention. Autoformer decoder has dual-path processing: (1) progressive trend accumulation by summing extracted trends from hidden variables, (2) seasonal refinement via inner and encoder-decoder Auto-Correlation, with final prediction as sum of accumulated trend and refined seasonal components.\n6. **Complexity and Efficiency**: Informer achieves O(L log L) complexity via query sampling (U=L_K ln L_Q pairs) and top-u selection, with memory O(L_K ln L_Q). Autoformer achieves O(L log L) via FFT-based correlation computation, but aggregates all values with learned time-delay weights (top-k periods), potentially better information utilization than sparse point-wise connections.\n7. **Prediction Philosophy**: Informer predicts future sequences directly from sparse attention patterns on historical data, treating forecasting as sequence-to-sequence translation. Autoformer decomposes prediction into trend and seasonal forecasting, explicitly modeling periodicity through auto-correlation and accumulating long-term trends progressively, aligning better with time series analysis principles.\n8. **Decoder Initialization**: Informer initializes decoder with start tokens (zeros or mean values) for the entire prediction horizon. Autoformer initializes decoder with decomposed components: seasonal part from recent history + zero placeholders, trend part from recent history + mean placeholders (Equation 2), providing richer initialization."
  },
  {
    "source": "Reformer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both papers adopt the Transformer-based encoder-decoder framework for sequence modeling, with multiple stacked layers processing input sequences through attention mechanisms and feed-forward networks.\n2. **Efficiency Focus**: Both papers address the computational efficiency challenge of standard Transformers for long sequences - Reformer achieves O(L log L) complexity through LSH attention, while Autoformer achieves O(L log L) through FFT-based auto-correlation.\n3. **Embedding Strategy**: Both use token embedding (Reformer through standard linear projection, Autoformer through 1D convolution with circular padding) combined with positional encoding to process input sequences.\n4. **Layer Normalization**: Both apply layer normalization in their architectures to stabilize training, though positioned differently in their respective designs.\n5. **Multi-head Mechanism**: Both employ multi-head attention variants (Reformer uses multi-head LSH attention, Autoformer uses multi-head auto-correlation) to capture diverse patterns from different representation subspaces.",
    "differences": "1. **Core Innovation**: Reformer focuses on memory and computational efficiency through locality-sensitive hashing (LSH) to approximate full attention with O(L log L) complexity, primarily for NLP tasks. Autoformer introduces series decomposition as a built-in architectural component and replaces self-attention with auto-correlation mechanism specifically designed for time series forecasting with periodic patterns.\n2. **Attention Mechanism**: Reformer uses LSH-based sparse attention that hashes queries and keys into buckets (bucket_size=4, n_hashes=4) to find approximate nearest neighbors, attending only within hash buckets. Autoformer uses auto-correlation mechanism that discovers period-based dependencies through FFT in frequency domain and aggregates information based on time delay, selecting top-k delays based on autocorrelation strength.\n3. **Decomposition Strategy**: Reformer has no decomposition component. Autoformer introduces progressive series decomposition as a core architectural element, using moving average (AvgPool) to extract trend-cyclical components and residuals as seasonal components at each encoder/decoder layer, with trend accumulation in the decoder.\n4. **Query-Key Relationship**: Reformer enforces shared-QK (queries and keys are identical, generated from the same linear projection) to ensure LSH hashing works correctly for finding similar query-key pairs. Autoformer uses separate Q, K, V projections in its auto-correlation mechanism, where queries and keys can differ.\n5. **Sequence Length Handling**: Reformer's implementation requires input length to be divisible by (bucket_size × 2), padding sequences with zeros if necessary (fit_length function). Autoformer initializes decoder with concatenated past seasonal/trend components and placeholders (zeros for seasonal, mean values for trend) for future prediction.\n6. **Prediction Strategy**: Reformer uses direct encoder-only prediction by concatenating input with decoder placeholder and projecting the final hidden states. Autoformer employs a decomposition-based prediction strategy where the decoder progressively refines seasonal components through auto-correlation while accumulating trend components extracted at each layer.\n7. **Computational Complexity**: Reformer achieves O(L log L) through LSH bucketing and sorting operations with chunk-based local attention. Autoformer achieves O(L log L) through FFT-based correlation computation in frequency domain (O(L log L) for FFT) followed by top-k delay selection and aggregation.\n8. **Domain Specificity**: Reformer is designed as a general-purpose efficient Transformer for long sequences across NLP and other domains, with no time-series-specific components. Autoformer is specifically designed for time series forecasting with period-based dependencies, seasonal-trend decomposition, and time delay aggregation tailored to temporal patterns."
  },
  {
    "source": "DLinear_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Distribution Shift**: Both papers employ instance normalization to handle distribution shift between training and testing data. DLinear's code implementation uses `means = x_enc.mean(1, keepdim=True).detach()` and `stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)` with explicit `.detach()` to prevent gradient flow to statistics. PatchTST adopts the same normalization strategy with identical implementation including the detach operation and 1e-5 epsilon for numerical stability.\n2. **Channel Independence Strategy**: Both papers process each channel (variate) independently without modeling cross-channel correlations. DLinear implements this through separate linear layers per channel (when `individual=True`) or shared weights across channels. PatchTST explicitly splits multivariate series into M univariate series and feeds each independently through the Transformer backbone, enabling parallel processing and avoiding spurious correlations.\n3. **Direct Multi-Step (DMS) Forecasting**: Both papers use direct prediction strategy where the model outputs all T future steps simultaneously, avoiding autoregressive error accumulation. DLinear outputs predictions directly through linear layers, while PatchTST uses a flatten head to map encoder representations to the full prediction horizon.\n4. **Series Decomposition Awareness**: While implemented differently, both papers acknowledge the importance of handling trend and seasonal components. DLinear explicitly uses `series_decomp` from Autoformer with moving average kernel to separate trend and seasonal parts, applying separate linear layers to each. PatchTST, though not using explicit decomposition in the main architecture, discusses decomposition in the context of prior work and focuses on capturing local semantic patterns through patching.",
    "differences": "1. **Core Architecture Philosophy**: DLinear represents the extreme simplification approach, arguing that Transformers are ineffective for time series due to lack of semantic correlations between raw numerical values. It uses only linear layers (one-layer or two-layer with decomposition) to directly map input sequence to output. PatchTST counters this by demonstrating that Transformers can be effective when properly designed, using vanilla Transformer encoder with patching to extract local semantic information and achieve superior performance.\n2. **Temporal Modeling Mechanism**: DLinear uses simple weighted sum operations through temporal linear layers (W ∈ ℝ^(T×L)) without any attention mechanism, treating forecasting as pure linear regression. PatchTST employs multi-head self-attention with scaled dot-product attention (Softmax(QK^T/√d_k)V) to capture dependencies between patch tokens, leveraging the Transformer's ability to model long-range correlations.\n3. **Input Representation and Patching**: DLinear processes raw time series points directly at single-step granularity (each input is one time step value). PatchTST introduces patching where input series is divided into subseries of length P with stride S, creating N = ⌊(L-P)/S⌋ + 2 patch tokens. This reduces sequence length by factor of S, enabling quadratic complexity reduction in attention (from O(L²) to O((L/S)²)) and allowing the model to see longer historical sequences.\n4. **Model Complexity and Capacity**: DLinear has minimal parameters - only weight matrices for linear projections (individual mode: 2×M×L×T parameters; shared mode: 2×L×T parameters), with O(L) time complexity. PatchTST has significantly higher capacity with d_model dimensional embeddings, multi-layer Transformer encoder with H attention heads, feed-forward networks (d_ff dimensions), and learnable positional encodings, resulting in O((L/S)²) attention complexity but much richer representation power.\n5. **Feature Extraction Strategy**: DLinear relies on explicit decomposition (moving average kernel with configurable window size) to separate trend and seasonal components, then applies separate linear transformations. The implementation initializes weights as `(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])` for uniform averaging. PatchTST uses implicit feature learning through patch embeddings (linear projection W_p ∈ ℝ^(D×P)) and multi-layer self-attention, allowing the model to automatically discover relevant patterns without predefined decomposition.\n6. **Representation Learning Capability**: DLinear is purely supervised and task-specific, designed only for forecasting without transfer learning capability. PatchTST extends to self-supervised representation learning through masked autoencoding at patch level, where random patches are masked and reconstructed, enabling pre-training on unlabeled data and transfer to downstream tasks (forecasting, classification, anomaly detection).\n7. **Positional Information Encoding**: DLinear implicitly encodes temporal order through the position-specific weights in linear layers (each weight corresponds to a specific input time step). PatchTST uses explicit learnable additive positional embeddings (W_pos ∈ ℝ^(D×N)) added to patch embeddings, providing more flexible positional encoding that can capture both local patch-level and global sequence-level ordering.\n8. **Normalization Layer Design**: DLinear uses no internal normalization layers beyond the input instance normalization. PatchTST's encoder incorporates BatchNorm layers within the Transformer architecture (implemented as `nn.Sequential(Transpose(1,2), nn.BatchNorm1d(configs.d_model), Transpose(1,2))`) to normalize across the batch dimension for each feature dimension, stabilizing training of the deeper network."
  },
  {
    "source": "FEDformer_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture**: Both papers adopt Transformer encoder architectures for time series forecasting, leveraging multi-head attention mechanisms to capture temporal dependencies.\n2. **Instance Normalization**: Both models apply instance normalization to handle distribution shifts between training and testing data. PatchTST explicitly normalizes each univariate series with zero mean and unit variance before processing, then denormalizes predictions. FEDformer's implementation also uses normalization (means are detached via .detach() to prevent gradient flow to statistics, similar to Non-stationary Transformer).\n3. **Encoder-only Design for Forecasting**: Both papers primarily use encoder-only architectures for the forecasting task, mapping historical sequences to predictions through linear projection heads rather than auto-regressive decoder structures.\n4. **Positional Encoding**: Both models incorporate positional information - FEDformer uses learnable position embeddings in its standard Transformer structure, while PatchTST uses additive positional encoding (PositionalEmbedding with sinusoidal encoding) to maintain patch ordering.\n5. **MSE Loss Function**: Both papers use Mean Squared Error (MSE) as the primary loss function for training, measuring discrepancy between predictions and ground truth.",
    "differences": "1. **Core Innovation - Frequency vs. Patching**: FEDformer's main contribution is frequency-enhanced decomposition using Fourier/Wavelet transforms (FEB-f/FEB-w blocks) that operate in frequency domain with mode selection (O(N) complexity after selecting M<<N modes). PatchTST's innovation is patching time series into subseries tokens (patch_len=16, stride=8) to reduce sequence length from L to ~L/S, enabling quadratic memory/computation reduction.\n2. **Attention Mechanism Design**: FEDformer replaces standard attention with Frequency Enhanced Blocks (FEB) that perform FFT→mode selection→learnable frequency mixing→IFFT, using complex-valued weight tensors (weights1, weights2) for frequency domain operations. PatchTST uses vanilla full attention (FullAttention) on patch tokens with standard Q-K-V scaled dot-product attention.\n3. **Decomposition Strategy**: FEDformer employs deep seasonal-trend decomposition architecture with Mixture of Experts Decomposition (MOEDecomp) blocks throughout encoder and decoder layers, explicitly separating and progressively refining seasonal and trend components. PatchTST does not use explicit decomposition; it relies on patching and attention to implicitly capture patterns.\n4. **Channel Handling Strategy**: FEDformer processes multivariate time series with channel mixing (input shape [B, L, M] where M channels interact). PatchTST adopts strict channel-independence: each univariate series is processed separately through shared Transformer backbone, splitting input into M independent forward passes.\n5. **Multi-scale Temporal Modeling**: FEDformer captures multiple scales through frequency domain mode selection (selecting specific Fourier/Wavelet modes at different frequencies) and hierarchical decomposition across layers. PatchTST achieves multi-scale modeling implicitly through patch length P (local semantics within patches) and stride S (overlap control), but operates primarily at patch granularity.\n6. **Encoder-Decoder vs. Encoder-only**: FEDformer uses encoder-decoder architecture with Frequency Enhanced Attention (FEA) for cross-attention between encoder output and decoder, accumulating trend components across decoder layers (T_de^l = T_de^(l-1) + weighted trends). PatchTST uses encoder-only with direct linear mapping from flattened patch representations to predictions (FlattenHead: [bs×nvars×d_model×patch_num] → [bs×nvars×pred_len]).\n7. **Computational Complexity**: FEDformer achieves O(N) complexity per layer through mode selection (keeping only M modes where M<<N), reducing from O(N²) attention. PatchTST achieves O((L/S)²) complexity through patching, reducing token count from L to N≈L/S, resulting in quadratic savings (e.g., S=8 gives ~64x memory reduction).\n8. **Input Token Representation**: FEDformer tokens are single time steps with full channel information (each token = one time point across all variables). PatchTST tokens are patches (subseries segments of length P), where each token represents local temporal patterns; patching is done via unfold operation with ReplicationPad1d for boundary handling.\n9. **Self-supervised Learning**: PatchTST explicitly includes masked autoencoder framework for representation learning (randomly masking patches and reconstructing them), enabling transfer learning to downstream tasks. FEDformer focuses solely on supervised forecasting without self-supervised pretraining mechanisms.\n10. **Normalization Placement**: FEDformer uses LayerNorm in standard Transformer layers. PatchTST uses BatchNorm1d in the encoder (nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))) as the normalization layer, which is unconventional for Transformers and may provide different training dynamics."
  },
  {
    "source": "Pyraformer_2021",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer Encoder Architecture**: Both papers use vanilla Transformer encoder layers as the core backbone, consisting of multi-head attention mechanisms followed by position-wise feed-forward networks with residual connections.\n2. **Positional Encoding**: Both employ learnable/fixed positional encodings to preserve temporal order information - Pyraformer uses additive position embeddings in its attention mechanism, while PatchTST uses PositionalEmbedding with sinusoidal encoding for patch positions.\n3. **Complexity Reduction Strategy**: Both papers aim to reduce the O(L²) complexity of standard Transformer attention - Pyraformer achieves O(L) through pyramidal sparse attention, while PatchTST reduces tokens from L to approximately L/S through patching.\n4. **Linear Projection for Embedding**: Both use linear transformations to project input features into the model's latent space (d_model dimension) - Pyraformer uses Linear layers in DataEmbedding, PatchTST uses nn.Linear in PatchEmbedding for projecting patch_len to d_model.\n5. **Batch Normalization**: Both implementations use BatchNorm layers within their architectures - Pyraformer's code shows BatchNorm1d in ConvLayer, while PatchTST's encoder uses nn.BatchNorm1d in the normalization layer (with Transpose operations).",
    "differences": "1. **Core Innovation**: Pyraformer introduces pyramidal attention with multi-resolution C-ary tree structure to capture multi-scale temporal dependencies through inter-scale and intra-scale connections, achieving O(L) complexity. PatchTST proposes patching strategy that segments time series into subseries-level patches (semantic local information) combined with channel-independence, reducing tokens to L/S and enabling longer lookback windows.\n2. **Attention Mechanism Design**: Pyraformer uses sparse pyramidal attention where each node attends to limited neighbors (adjacent A nodes, C children, 1 parent) defined by equation (2-3), with custom mask generation via get_mask() function. PatchTST uses standard full attention on patch tokens with FullAttention mechanism, but operates on drastically reduced sequence length (N patches instead of L timesteps).\n3. **Multi-scale Modeling Approach**: Pyraformer explicitly constructs multi-resolution hierarchy with S scales using Coarser-Scale Construction Module (CSCM) via Bottleneck_Construct with ConvLayer downsampling (stride=window_size), creating coarser representations at multiple scales. PatchTST achieves implicit multi-scale through patch length P, capturing local semantic patterns within patches, but does not build explicit hierarchical scales.\n4. **Channel Handling Strategy**: Pyraformer processes multivariate time series jointly through shared encoder with all channels feeding into the same pyramidal structure. PatchTST adopts strict channel-independence where each univariate series x^(i) is processed completely independently through separate forward passes, sharing only model weights but not computations.\n5. **Input Processing and Token Construction**: Pyraformer operates on raw timestep-level tokens with convolution-based downsampling (kernel_size=window_size, stride=window_size) to build pyramid levels, using refer_points() to gather features from pyramid sequences. PatchTST uses non-overlapping/overlapping patching via unfold(dimension=-1, size=patch_len, step=stride) with ReplicationPad1d for padding, treating each patch as a semantic token.\n6. **Normalization Technique**: Pyraformer uses BatchNorm1d within ConvLayer modules for pyramid construction. PatchTST explicitly applies instance normalization (zero mean, unit std) to each time series channel before patching, then denormalizes predictions by adding back mean and std - this helps mitigate distribution shift between train/test data.\n7. **Prediction Head Design**: Pyraformer uses hierarchical structure with refer_points() and indexes to gather features from all pyramid scales, concatenating multi-scale representations before final prediction. PatchTST uses FlattenHead that flattens patch representations (d_model × patch_num) and applies single linear layer to map to prediction horizon T, with head dimension nf = d_model × int((seq_len - patch_len)/stride + 2).\n8. **Computational Complexity**: Pyraformer achieves O(AL) ≈ O(L) time/space complexity through sparse pyramidal connections with maximum signal traversing path of O(log_C L). PatchTST achieves O((L/S)²) complexity on attention, which is O(L²/S²) - quadratically reduced by stride factor S, allowing longer input sequences within same memory budget."
  },
  {
    "source": "Informer_2020",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer Backbone Architecture**: Both use Transformer encoder architecture with multi-head self-attention as the core component for temporal modeling, though applied at different granularities (point-level vs. patch-level)\n2. **Instance Normalization for Distribution Shift**: Both employ instance normalization to handle distribution shift between training and testing data. PatchTST explicitly mentions normalizing each time series with zero mean and unit standard deviation before processing; Informer's code implementation shows `means = x_enc.mean(1, keepdim=True).detach()` and `stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)` with `.detach()` preventing gradient flow to statistics\n3. **Positional Encoding**: Both use learnable/sinusoidal positional embeddings to preserve temporal order information in the sequence (Informer uses DataEmbedding with positional encoding; PatchTST uses PositionalEmbedding module)\n4. **Encoder-based Design**: Both primarily rely on encoder architecture for feature extraction, though Informer includes an optional decoder for generative-style forecasting while PatchTST uses encoder-only with direct linear projection\n5. **MSE Loss Function**: Both use mean squared error as the primary objective function for forecasting tasks",
    "differences": "1. **Core Innovation - Attention Efficiency vs. Semantic Patching**: Informer introduces ProbSparse self-attention to reduce quadratic complexity O(L²) to O(L log L) by selecting top-u dominant queries based on sparsity measurement M(q,K), using KL-divergence approximation. PatchTST introduces patching to create semantic tokens from subseries (patch_len=16, stride=8), reducing token count from L to ~L/S and achieving quadratic memory reduction by factor of S², fundamentally changing what attention operates on (patches vs. points)\n2. **Attention Mechanism Design**: Informer uses ProbSparse attention with query sparsity measurement (max-mean measurement M̄(qi,K) = max{qikj/√d} - mean{qikj/√d}) and random sampling of U=LK ln LQ dot-products to identify important queries. PatchTST uses standard full attention (FullAttention with scaled dot-product) without sparsity, relying on reduced token count from patching for efficiency\n3. **Channel Handling Strategy**: Informer processes multivariate series jointly with cross-variate attention, mixing information across channels. PatchTST employs strict channel-independence where each univariate series x^(i) is fed independently through the same shared Transformer backbone, with final loss averaged over M channels: L = E[1/M Σ||x̂^(i) - x^(i)||²]\n4. **Multi-scale Temporal Modeling**: Informer includes distilling operation with ConvLayer (Conv1d with kernel_size=3, MaxPool1d with stride=2) between encoder layers to progressively halve sequence length and capture multi-scale features. PatchTST has no explicit multi-scale mechanism, relying solely on patch-level granularity\n5. **Input Representation and Embedding**: Informer embeds each time step individually through DataEmbedding (value embedding + temporal encoding + positional encoding), operating at point-level granularity. PatchTST uses PatchEmbedding that segments series into patches via unfold operation, applies linear projection (nn.Linear(patch_len, d_model)) to map each patch to d_model space, fundamentally changing input token semantics\n6. **Decoder Architecture**: Informer uses full encoder-decoder structure with DecoderLayer containing self-attention, cross-attention, and feed-forward networks for autoregressive-style generation. PatchTST is encoder-only with a simple FlattenHead (flattens d_model × patch_num and applies linear projection to pred_len), directly mapping representations to predictions\n7. **Complexity and Scalability**: Informer achieves O(L log L) complexity through sparse attention but still processes L individual time points. PatchTST achieves O((L/S)²) complexity through patching, with typical S=8 providing 64× reduction in attention computation, plus allows longer lookback windows within same memory budget\n8. **Prediction Head Design**: Informer uses projection layer after decoder (self.projection in Decoder) for sequence-to-sequence mapping. PatchTST uses FlattenHead that flattens [bs × nvars × d_model × patch_num] to [bs × nvars × (d_model*patch_num)] then linearly projects to [bs × nvars × pred_len], with head_nf = d_model × int((seq_len - patch_len)/stride + 2)"
  },
  {
    "source": "Pyraformer_2021",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Transformer Architecture**: Both papers utilize Transformer-based architectures for time series forecasting. Pyraformer implements an encoder with stacked attention layers (EncoderLayer class) and a projection module for prediction. Non-stationary Transformer also adopts the standard Encoder-Decoder structure. The source paper's EncoderLayer implementation (with self-attention + feed-forward network) can be adapted as the base structure, though the attention mechanism needs modification.\n\n2. **Embedding Layer Design**: Both papers use similar embedding strategies combining data embedding with positional and covariate information. Pyraformer's DataEmbedding class (which combines observation, covariate, and positional embeddings through addition) can be directly reused for Non-stationary Transformer's input processing before the normalization step.\n\n3. **Feed-Forward Network (FFN) Structure**: Both papers employ position-wise feed-forward networks with similar designs. Pyraformer's PositionwiseFeedForward class (two-layer FFN with GELU activation, dropout, and layer normalization) can be directly reused in Non-stationary Transformer's encoder/decoder layers without modification.\n\n4. **Multi-Head Attention Framework**: Both papers utilize multi-head attention mechanisms as the core component. The basic multi-head structure and query-key-value transformation logic from Pyraformer's AttentionLayer can serve as the foundation, though the specific attention calculation needs to be replaced with De-stationary Attention.\n\n5. **Batch Processing and Tensor Operations**: Both papers process time series in batches with shape [B, L, D]. The tensor manipulation patterns in Pyraformer's forward passes (batch dimension handling, sequence dimension operations) can guide the implementation of Non-stationary Transformer's data flow.",
    "differences": "1. **Core Innovation - Non-stationarity Handling vs. Complexity Reduction**: Pyraformer focuses on reducing computational complexity from O(L²) to O(L) through pyramidal attention with multi-scale C-ary tree structure. Non-stationary Transformer addresses the over-stationarization problem by introducing Series Stationarization (normalization/de-normalization wrapper) and De-stationary Attention to recover non-stationary information. **NEW IMPLEMENTATION NEEDED**: (a) Normalization module computing μ_x and σ_x from input series; (b) De-normalization module restoring statistics to output; (c) MLP projectors to learn de-stationary factors τ and Δ from statistics; (d) Modified attention calculation incorporating τ and Δ scaling/shifting.\n\n2. **Attention Mechanism Design**: Pyraformer implements Pyramidal Attention Module (PAM) with sparse connectivity based on a C-ary tree structure, where each node attends to neighbors at three scales (adjacent nodes, children, parent) defined by mathematical formulas in Equation 2-3. It uses custom CUDA kernels and mask-based attention with get_mask() and refer_points() functions. **NEW IMPLEMENTATION NEEDED**: Non-stationary Transformer requires De-stationary Attention (Equation 6) that performs full attention on stationarized Q', K', V' but rescales with learned factors: Softmax((τQ'K'^T + 1Δ^T)/√d_k)V'. This needs (a) removing all pyramidal structure code (CSCM, mask generation, multi-scale tree); (b) implementing standard full attention; (c) integrating τ scaling and Δ shifting into Softmax input.\n\n3. **Multi-Scale Modeling Strategy**: Pyraformer explicitly constructs multi-resolution representations through Coarser-Scale Construction Module (CSCM) using Bottleneck_Construct class with sequential ConvLayer operations (stride-C convolutions) to build a C-ary tree from fine to coarse scales. It concatenates features across scales and uses refer_points() to gather multi-scale information. **NON-EXISTENT IN TARGET**: Non-stationary Transformer does NOT use any multi-scale decomposition or hierarchical structure. It operates on the original single-scale temporal sequence after normalization. All CSCM-related code (ConvLayer, Bottleneck_Construct, multi-scale concatenation) should be completely removed.\n\n4. **Input/Output Processing Pipeline**: Pyraformer directly processes embedded sequences through pyramidal attention and projects concatenated multi-scale features to predictions. **NEW IMPLEMENTATION NEEDED**: Non-stationary Transformer requires a three-stage pipeline: (a) **Pre-processing**: Normalization module before encoder (Equation 1) computing mean/std along temporal dimension and normalizing input series; (b) **Statistics Preservation**: Storing μ_x and σ_x for both de-stationary factor learning and de-normalization; (c) **Post-processing**: De-normalization module after decoder (Equation 2) restoring original scale/location to predictions using σ_x⊙y'_i + μ_x.\n\n5. **Computational Complexity Trade-off**: Pyraformer achieves O(L) complexity through sparse pyramidal attention with limited connectivity (A+C+1 neighbors per node) and custom CUDA implementation for efficiency. **TARGET PAPER CHANGE**: Non-stationary Transformer uses standard full attention with O(L²) complexity but focuses on improving predictability through stationarity handling rather than complexity reduction. The entire complexity optimization infrastructure (custom masks, sparse attention kernels, pyramidal graph traversal) from Pyraformer is irrelevant and should be replaced with standard scaled dot-product attention.\n\n6. **Parameter Learning Focus**: Pyraformer learns parameters for pyramidal attention weights, convolution kernels in CSCM (window_size=[4,4] downsampling), and projection layers. **NEW IMPLEMENTATION NEEDED**: Non-stationary Transformer introduces additional learnable components: (a) MLP projectors (typically 2-layer networks) that map from (σ_x, x) → log τ and (μ_x, x) → Δ; (b) These projectors are shared across all attention layers; (c) No learnable affine parameters in normalization (unlike RevIN), making it parameter-free stationarization."
  },
  {
    "source": "Informer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both papers adopt the standard Transformer encoder-decoder structure for long-term forecasting. The source code's `Encoder`, `Decoder`, `EncoderLayer`, and `DecoderLayer` classes can be directly reused as the base architecture for Nonstationary Transformer, requiring only modifications to the attention mechanism.\n\n2. **Embedding Strategy**: Both use `DataEmbedding` for input representation, combining value embedding with positional encoding. The Informer's `enc_embedding` and `dec_embedding` modules can be directly reused without modification for the target paper.\n\n3. **Decoder Input Construction**: Both papers use a similar decoder input strategy with start tokens from historical observations and placeholder zeros for target sequence. Informer's decoder input construction in Eq.(6) `Concat(X_token, X_0)` matches Nonstationary Transformer's approach, so this code logic can be directly transferred.\n\n4. **Multi-Head Attention Framework**: Both implement multi-head attention with query/key/value projections. The `AttentionLayer` class structure (with `query_projection`, `key_projection`, `value_projection`, `out_projection`) can be reused, only replacing the inner attention computation.\n\n5. **Feed-Forward Network**: Both use standard FFN with Conv1d layers in decoder. The `conv1`, `conv2`, `activation`, `dropout` components in `DecoderLayer` can be directly reused.\n\n6. **Task Format**: Both target long-sequence time-series forecasting (LSTF) with the same input-output paradigm: encoding historical observations and predicting future values. The `long_forecast` function structure and MSE loss can be reused.\n\n7. **Normalization Layers**: Both use LayerNorm (`nn.LayerNorm`) in encoder/decoder layers for stabilizing training, which can be directly reused from the source implementation.",
    "differences": "1. **Core Innovation - Stationarity Handling**: Nonstationary Transformer introduces **Series Stationarization** (normalization/de-normalization wrapper) and **De-stationary Attention** to handle non-stationary series, which are completely absent in Informer. NEW IMPLEMENTATION NEEDED: (a) Normalization module computing μ_x, σ_x on temporal dimension (Eq.1), (b) De-normalization module restoring statistics to output (Eq.2), (c) MLP projectors to learn de-stationary factors τ and Δ from μ_x, σ_x.\n\n2. **Attention Mechanism**: Informer uses **ProbSparse Attention** with query sparsity measurement (Eq.2-4) and Top-u query selection for O(L log L) complexity. Nonstationary Transformer uses **De-stationary Attention** (Eq.6) that rescales attention scores with learned factors: `Softmax((τ·Q'K'^T + 1Δ^T)/√d_k)·V'`. NEW IMPLEMENTATION NEEDED: Replace `ProbAttention` class entirely with De-stationary Attention that incorporates τ (scaling) and Δ (shifting) factors into standard attention computation.\n\n3. **Encoder Distilling Strategy**: Informer employs **self-attention distilling** with Conv1d+MaxPooling (Eq.5) to progressively halve sequence length and build pyramid-like multi-scale encoder stacks (`ConvLayer` class). Nonstationary Transformer does NOT use distilling and maintains standard encoder without sequence length reduction. NEW IMPLEMENTATION NEEDED: Remove all `ConvLayer` components and distilling logic from encoder; use standard encoder layers only.\n\n4. **Statistical Information Flow**: Informer processes embeddings directly without explicit statistical tracking. Nonstationary Transformer requires computing and passing **μ_x, σ_x** throughout the model to: (a) normalize inputs, (b) generate de-stationary factors, (c) de-normalize outputs. NEW IMPLEMENTATION NEEDED: Add statistical computation pipeline that calculates μ_x, σ_x from raw input x, passes them to attention layers for τ/Δ generation, and stores them for final de-normalization.\n\n5. **Complexity Focus**: Informer focuses on reducing computational complexity from O(L²) to O(L log L) through sparse attention and distilling. Nonstationary Transformer focuses on handling non-stationarity while maintaining standard O(L²) attention complexity (or can be combined with efficient variants). The complexity optimization is NOT a primary concern for the target paper.\n\n6. **Parameter Sharing**: Nonstationary Transformer shares de-stationary factors τ and Δ across ALL attention layers (both encoder and decoder), computed once from input statistics. Informer has no such cross-layer parameter sharing mechanism. NEW IMPLEMENTATION NEEDED: Design a mechanism to compute τ, Δ once and broadcast them to all encoder/decoder attention layers.\n\n7. **Decoder Masking**: While both use masked attention in decoder, Nonstationary Transformer's De-stationary Attention applies masking to the rescaled scores `τ·Q'K'^T + 1Δ^T` rather than just `Q'K'^T`. NEW IMPLEMENTATION NEEDED: Modify masking logic to work with the de-stationarized attention scores."
  },
  {
    "source": "Reformer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer Encoder-Decoder Architecture**: Both papers adopt the standard Transformer encoder-decoder structure for sequence modeling. The Reformer implementation provides `Encoder`, `EncoderLayer` classes that can be directly reused as the base architecture for Nonstationary Transformer, with the encoder extracting information from past observations.\n\n2. **Multi-Head Attention Framework**: Both utilize multi-head attention mechanisms as the core component for capturing temporal dependencies. The Reformer's attention layer structure (with queries, keys, values) can serve as a template for implementing the De-stationary Attention, though the attention calculation formula needs modification.\n\n3. **Embedding Layer for Input Processing**: Both papers use embedding layers to project raw time series into higher-dimensional representations. The Reformer's `DataEmbedding` class (combining value embedding, positional encoding, and temporal encoding) can be directly reused for Nonstationary Transformer's input processing.\n\n4. **Layer Normalization**: Both architectures incorporate layer normalization for stabilizing training. The Reformer code's `norm_layer=torch.nn.LayerNorm(configs.d_model)` can be reused in Nonstationary Transformer's encoder/decoder layers.\n\n5. **Feed-Forward Networks**: Both papers use position-wise feed-forward networks within each layer. The Reformer's `EncoderLayer` structure with `d_ff` (feed-forward dimension) can be adapted for Nonstationary Transformer's architecture.\n\n6. **Projection Head for Forecasting**: Both use linear projection layers to map the model output to prediction space. Reformer's `self.projection = nn.Linear(configs.d_model, configs.c_out)` provides a direct template for the final forecasting projection.",
    "differences": "1. **Core Innovation - Attention Mechanism**: Reformer uses LSH (Locality-Sensitive Hashing) attention with O(L log L) complexity to reduce computational cost through hash bucketing and chunking (bucket_size, n_hashes parameters). Nonstationary Transformer proposes De-stationary Attention that incorporates non-stationary information through learned scaling factor τ and shifting vector Δ, maintaining O(L²) complexity but addressing over-stationarization. **NEW IMPLEMENTATION NEEDED**: De-stationary Attention module with MLP projectors to learn τ and Δ from input statistics (μ_x, σ_x), and modified attention calculation: `Softmax((τ * Q'K'^T + 1Δ^T) / √d_k) * V'`.\n\n2. **Series Stationarization Wrapper**: Reformer does not address non-stationarity in time series, while Nonstationary Transformer introduces a two-stage normalization-denormalization wrapper. **NEW IMPLEMENTATION NEEDED**: (a) Normalization module to compute mean μ_x and std σ_x along temporal dimension and normalize input: `x' = (x - μ_x) / σ_x`; (b) De-normalization module to restore statistics to output: `ŷ = σ_x ⊙ y' + μ_x`; (c) These statistics (μ_x, σ_x) must be preserved and passed through the model for both De-stationary Attention and final denormalization.\n\n3. **Non-Stationary Information Flow**: Reformer's attention only depends on stationarized queries/keys (Q=K design for LSH), while Nonstationary Transformer explicitly maintains and utilizes non-stationary statistics throughout the model. **NEW IMPLEMENTATION NEEDED**: A mechanism to compute and propagate μ_x, σ_x from raw input x through all layers, feeding them to MLP projectors in each De-stationary Attention layer to generate layer-specific or shared τ and Δ factors.\n\n4. **Memory Efficiency Strategy**: Reformer achieves memory efficiency through (a) LSH bucketing to reduce attention from O(L²) to O(L log L), (b) Reversible layers to eliminate activation storage across layers, and (c) Chunking feed-forward computations. Nonstationary Transformer does not focus on memory optimization and uses standard O(L²) attention. **NO REFORMER CODE REUSABLE**: The LSHSelfAttention, ReformerLayer with bucket_size/n_hashes, and reversible architecture are specific to Reformer's efficiency goals and cannot be used in Nonstationary Transformer.\n\n5. **Shared Q-K Design**: Reformer enforces Q=K (shared-QK) through using the same linear layer and normalizing key lengths, which is essential for LSH hashing. Nonstationary Transformer uses standard separate Q, K projections with independent parameters. **IMPLEMENTATION DIFFERENCE**: Nonstationary Transformer requires separate linear projections for Q and K, unlike Reformer's shared projection.\n\n6. **Attention Masking and Causality**: Reformer implements causal masking for autoregressive generation with special handling for shared-QK attention (preventing self-attention except when no other targets exist). Nonstationary Transformer uses standard encoder-decoder attention masks. **NEW IMPLEMENTATION NEEDED**: Standard causal masking for decoder self-attention and cross-attention masks, without Reformer's special shared-QK constraints.\n\n7. **Statistical Learning Components**: Nonstationary Transformer introduces learnable MLP projectors to estimate de-stationary factors from input statistics, which is absent in Reformer. **NEW IMPLEMENTATION NEEDED**: Two MLP networks (can be simple 2-layer MLPs) that take (σ_x, x) → log τ and (μ_x, x) → Δ, where τ ∈ ℝ⁺ is a scalar scaling factor and Δ ∈ ℝ^(S×1) is a shifting vector for each attention calculation."
  },
  {
    "source": "Informer_2020",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Pattern**: Both models use an encoder-decoder paradigm for long-term forecasting, though with fundamentally different implementations. The source code's `long_forecast()` method structure (encoder → decoder → projection) provides a template for organizing the forward pass, though TiDE's encoder/decoder are MLP-based rather than attention-based.\n2. **Embedding and Input Processing**: Both models employ embedding layers to process input sequences. Informer's `DataEmbedding` class (combining value embedding, positional encoding, and temporal encoding) can be partially adapted - TiDE needs similar temporal covariate encoding, though without positional encoding. The embedding dropout and normalization components are directly reusable.\n3. **Residual Connections**: Both architectures utilize residual connections. Informer's residual blocks in encoder/decoder layers (x + dropout(attention(x))) mirror TiDE's ResidualBlock design. The implementation pattern of skip connections with layer normalization can be directly adapted from Informer's code.\n4. **Global Training with MSE Loss**: Both models are trained globally across all time series using MSE loss and mini-batch gradient descent. The training loop, loss computation, and batch construction logic from Informer's implementation can be reused with minimal modification.\n5. **Channel-Independent Processing**: While not explicit in Informer's core architecture, both models can process channels independently. TiDE explicitly processes one time series at a time, and Informer's implementation can be adapted to this paradigm by reshaping inputs appropriately.\n6. **Multi-Layer Stacking**: Both use stacked layers (Informer: multiple EncoderLayer/DecoderLayer; TiDE: multiple ResidualBlocks in encoder/decoder). The ModuleList-based layer stacking pattern in Informer's Encoder/Decoder classes provides a reusable template for TiDE's dense encoder/decoder implementation.",
    "differences": "1. **Core Architecture Innovation - MLP vs. Attention**: TiDE replaces Informer's ProbSparse attention mechanism entirely with dense MLP layers (ResidualBlocks). NEW IMPLEMENTATION NEEDED: (a) ResidualBlock class with single hidden layer, ReLU activation, skip connection, dropout, and layer norm; (b) Dense encoder/decoder as stacked ResidualBlocks; (c) Feature projection layer for covariate dimensionality reduction; (d) Temporal decoder that concatenates decoded vectors with projected covariates. None of Informer's attention-related code (ProbAttention, AttentionLayer, masking) is applicable.\n2. **Feature Projection and Covariate Handling**: TiDE introduces a novel feature projection step that maps high-dimensional covariates  $(r)$  to lower dimensions  $(\\tilde{r})$  at each time-step before flattening, preventing prohibitively large input vectors. NEW IMPLEMENTATION NEEDED: (a) Per-timestep feature projection ResidualBlock; (b) Flattening and concatenation logic for projected covariates across look-back and horizon; (c) Static attribute concatenation mechanism. Informer's `DataEmbedding` handles temporal features differently (as additive encodings) and cannot be directly adapted.\n3. **Temporal Decoder Highway Connection**: TiDE adds a unique \"highway\" connection from future covariates at time-step  $L+t$  directly to predictions at  $L+t$  through the temporal decoder. NEW IMPLEMENTATION NEEDED: Temporal decoder ResidualBlock that takes concatenated  $(d_t^{(i)}; \\tilde{x}_{L+t}^{(i)})$  as input and outputs scalar predictions. This direct covariate-to-prediction pathway has no equivalent in Informer.\n4. **No Self-Attention Distilling or Hierarchical Processing**: Informer uses self-attention distilling with ConvLayer (Conv1d + ELU + MaxPool) to progressively reduce sequence length and build pyramid-like encoder stacks. TiDE has NO sequence length reduction or hierarchical processing - it maintains full temporal resolution throughout. The entire ConvLayer implementation and distilling logic is NOT needed for TiDE.\n5. **Complexity and Efficiency**: Informer achieves  $O(L \\log L)$  complexity through ProbSparse attention and distilling. TiDE achieves  $O(L)$  complexity through pure MLP operations on flattened sequences. NEW IMPLEMENTATION NEEDED: Efficient flattening/reshaping operations for  $(L+H) \\times \\tilde{r}$  covariate matrices and  $H \\times p$  decoder outputs. No query sparsity measurement, top-k selection, or sampling logic from Informer is needed.\n6. **Decoder Input Strategy**: Informer uses start tokens (earlier sequence slice) concatenated with zero placeholders for the target sequence. TiDE's decoder takes only the encoded representation  $e^{(i)}$  and does NOT use any form of target sequence initialization or masking. NEW IMPLEMENTATION NEEDED: Decoder that maps fixed-size encoding directly to  $H \\times p$  output vector without autoregressive masking. Informer's `TriangularCausalMask` and decoder masking logic are completely unnecessary.\n7. **Linear Residual Connection**: TiDE adds a global linear layer that maps look-back  $y_{1:L}^{(i)}$  directly to horizon predictions, ensuring a linear model is always a subclass. NEW IMPLEMENTATION NEEDED: nn.Linear(L, H) layer with direct addition to final predictions. While Informer has residual connections within layers, it lacks this global look-back-to-horizon linear skip connection.\n8. **Reshape Operations**: TiDE explicitly reshapes decoder output  $g^{(i)} \\in \\mathbb{R}^{p \\cdot H}$  to matrix  $D^{(i)} \\in \\mathbb{R}^{p \\times H}$  to separate temporal steps. NEW IMPLEMENTATION NEEDED: Reshape logic to convert flat decoder output into per-timestep vectors for temporal decoder processing. Informer maintains sequence structure throughout and doesn't require this operation."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing**: Both models process each time series independently. Pyraformer applies pyramidal attention to each sequence separately, while TiDE explicitly adopts a \"channel independent manner\" where input is (y_{1:L}^{(i)}, x_{1:L}^{(i)}, a^{(i)}) for each series i. The source code's per-sample processing logic in the encoder can be adapted for TiDE's channel-wise operations.\n\n2. **Embedding with Covariates**: Both incorporate temporal covariates (x_mark) alongside observations. Pyraformer uses DataEmbedding(x_enc, x_mark_enc) combining observation, covariate, and positional embeddings. TiDE similarly uses dynamic covariates x_t^{(i)} and static attributes a^{(i)}. The source code's DataEmbedding module can be partially reused for TiDE's feature projection step, particularly the covariate encoding logic.\n\n3. **Residual Connections**: Both employ residual connections for gradient flow. Pyraformer uses residual connections in EncoderLayer (enc_output = self.slf_attn(...) + residual) and PositionwiseFeedForward. TiDE uses ResidualBlock with skip connections and a global linear residual from look-back to horizon. The source code's residual connection patterns and layer normalization (nn.LayerNorm) can be directly adapted.\n\n4. **Normalization Techniques**: Both use LayerNorm for stabilizing training. Pyraformer applies it in PositionwiseFeedForward (self.layer_norm) and Bottleneck_Construct. TiDE uses layer norm at ResidualBlock outputs. The source code's normalization implementations are directly reusable.\n\n5. **Multi-Layer Architecture**: Both stack multiple processing layers. Pyraformer uses nn.ModuleList for encoder layers (configs.e_layers), and TiDE stacks n_e encoder layers and n_d decoder layers. The source code's ModuleList-based stacking pattern can be adapted for TiDE's dense encoder/decoder structure.\n\n6. **Direct Multi-Step Prediction**: Both output all future time steps in one forward pass rather than autoregressive generation. Pyraformer's projection layer outputs self.pred_len * configs.enc_in, reshaped to (batch, pred_len, features). TiDE similarly decodes to H×p then reshapes. The source code's batch prediction logic (self.projection(...).view(...)) can guide TiDE's decoder output reshaping.",
    "differences": "1. **Core Architecture Paradigm**: Pyraformer is a **Transformer-based model** with pyramidal attention mechanism (PAM) achieving O(L) complexity through sparse attention on multi-resolution graphs. TiDE is a **pure MLP-based model** with no attention mechanism, using stacked ResidualBlocks (MLPs with ReLU, dropout, layer norm). NEW IMPLEMENTATION NEEDED: Dense MLP encoder/decoder blocks with residual connections, feature projection MLP, temporal decoder MLP - none of these exist in Pyraformer's attention-centric architecture.\n\n2. **Temporal Modeling Strategy**: Pyraformer captures dependencies through **pyramidal attention** on a C-ary tree structure with inter-scale and intra-scale connections, using get_mask() and refer_points() for sparse attention patterns. TiDE uses **dense MLPs** that flatten and process entire sequences (y_{1:L}, x_{1:L+H}) without explicit attention. NEW IMPLEMENTATION NEEDED: Complete MLP-based temporal modeling with flattening operations, no attention mask generation, no pyramidal graph construction.\n\n3. **Multi-Scale Handling**: Pyraformer explicitly constructs **multi-resolution representations** via Bottleneck_Construct with ConvLayers (stride-C convolutions creating scales s=1..S), forming a pyramidal graph where coarser scales capture long-range dependencies. TiDE has **no explicit multi-scale decomposition**, treating the entire sequence uniformly through dense projections. NEW IMPLEMENTATION NEEDED: Remove all convolution-based coarse-scale construction; implement uniform sequence flattening instead.\n\n4. **Feature Processing**: Pyraformer uses **convolutional bottleneck** (Bottleneck_Construct with down-projection to d_inner, stride-C convolutions, up-projection) for hierarchical feature extraction. TiDE uses **feature projection** (Equation 3: ResidualBlock mapping x_t^{(i)} from dimension r to \\tilde{r} at each time-step independently), then flattens projected features. NEW IMPLEMENTATION NEEDED: Per-timestep feature projection MLP (not sequence-level convolution), dimension reduction from r to \\tilde{r}, concatenation with static attributes.\n\n5. **Decoder Architecture**: Pyraformer uses either a **single projection layer** (self.projection mapping concatenated pyramid features to pred_len×enc_in) or a **two-layer full-attention decoder** with prediction tokens. TiDE uses a **two-stage decoder**: (1) Dense decoder reshaping embedding to p×H matrix D^{(i)}, (2) Temporal decoder applying per-timestep MLP to [d_t^{(i)}; \\tilde{x}_{L+t}^{(i)}]. NEW IMPLEMENTATION NEEDED: Reshape operation from flat vector to (p, H) matrix, per-timestep temporal decoder combining decoded vectors with future covariates, highway connection from future covariates to predictions.\n\n6. **Global Residual Connection**: Pyraformer has **no global residual** from input to output (only local residuals within layers). TiDE adds a **global linear residual** mapping look-back y_{1:L}^{(i)} directly to horizon predictions \\hat{y}_{L+1:L+H}^{(i)}, ensuring linear models are a subclass. NEW IMPLEMENTATION NEEDED: Global linear layer from input sequence to output sequence, bypassing all intermediate processing.\n\n7. **Complexity and Efficiency**: Pyraformer achieves **O(L) complexity** through sparse pyramidal attention with custom CUDA kernels (mentioned but not in provided code), storing only A+C+1 attention pairs per node. TiDE has **O(L×hiddenSize) complexity** from dense MLP operations on flattened sequences, simpler to implement without custom kernels but potentially more parameters. NEW IMPLEMENTATION NEEDED: Standard dense layers without sparse attention optimizations.\n\n8. **Covariate Integration**: Pyraformer **embeds covariates once** at input (DataEmbedding combining x_mark_enc with observations), then processes through attention layers. TiDE uses covariates **at multiple stages**: (1) feature projection reduces covariate dimensionality, (2) projected covariates concatenated with look-back for encoding, (3) future projected covariates concatenated with decoded vectors in temporal decoder. NEW IMPLEMENTATION NEEDED: Multi-stage covariate processing with separate projection, encoding, and decoding pathways."
  },
  {
    "source": "FEDformer_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Global Training with Channel Independence**: Both models train globally across all time series but process each channel independently during inference. FEDformer's implementation processes channels through separate attention heads (configs.n_heads), while TiDE explicitly states \"channel independent manner\" where each time series is processed individually. The data loading and batching logic from FEDformer can be adapted for TiDE's channel-independent processing.\n\n2. **Embedding and Feature Projection**: Both models use embedding layers to project input features. FEDformer's DataEmbedding (combining value embedding, positional encoding, and temporal encoding) shares conceptual similarity with TiDE's Feature Projection step that maps covariates x_t to lower dimensions. The embedding infrastructure (nn.Linear layers, dropout) from FEDformer can be reused for TiDE's feature projection residual blocks.\n\n3. **Decomposition for Initialization**: FEDformer uses series_decomp (moving average based) to extract trend and seasonal components for decoder initialization. While TiDE doesn't explicitly use decomposition in its architecture, it initializes predictions using mean values similar to FEDformer's trend initialization. The decomposition utility (series_decomp class with AvgPool) could be adapted if TiDE benefits from decomposed features.\n\n4. **Residual Connections**: Both architectures employ residual connections. FEDformer uses residual connections in encoder/decoder layers (x + FEB(x), x + FeedForward(x)), and TiDE's core building block is the ResidualBlock with skip connections. FEDformer's residual implementation pattern (add & norm) can directly guide TiDE's ResidualBlock implementation.\n\n5. **Layer Normalization**: Both use layer normalization. FEDformer implements my_Layernorm throughout encoder/decoder, and TiDE specifies layer norm at ResidualBlock outputs. The normalization infrastructure from FEDformer can be reused.\n\n6. **Training Infrastructure**: Both use MSE loss for training and similar mini-batch gradient descent strategies. FEDformer's training loop, loss computation, and evaluation metrics (MSE, MAE) can be directly adapted for TiDE with minimal modifications.",
    "differences": "1. **Core Architecture Paradigm - NEW IMPLEMENTATION REQUIRED**: FEDformer uses Transformer encoder-decoder with frequency-domain attention (Fourier/Wavelet blocks), while TiDE uses pure MLP architecture with dense encoder-decoder. TiDE requires implementing: (a) Dense MLP encoder with multiple residual blocks stacking flattened past values, projected covariates, and static attributes, (b) Dense MLP decoder mapping encodings to H×p dimensional outputs, (c) Temporal decoder that processes each time step independently. None of FEDformer's attention mechanisms (FourierBlock, FourierCrossAttention, MultiWaveletTransform) are applicable.\n\n2. **Frequency Domain Processing vs. Time Domain - NOT APPLICABLE**: FEDformer's core innovation is frequency-enhanced attention using DFT/DWT (FourierBlock with rfft/irfft, MultiWaveletTransform with wavelet decomposition/reconstruction, mode selection). TiDE operates purely in time domain with no frequency transformations. All of FEDformer's frequency components (get_frequency_modes, compl_mul1d, wavelet_transform, sparseKernelFT1d) cannot be reused and should NOT be implemented for TiDE.\n\n3. **Covariate Handling - NEW IMPLEMENTATION REQUIRED**: TiDE explicitly handles dynamic covariates (x_t) and static attributes (a^(i)) as first-class inputs with dedicated feature projection and concatenation operations. FEDformer uses x_mark (temporal encodings) but doesn't have explicit static attribute handling. TiDE requires: (a) Feature projection residual block to map r-dimensional covariates to \\tilde{r} dimensions, (b) Concatenation logic for [y_{1:L}, \\tilde{x}_{1:L+H}, a] before encoder, (c) Temporal decoder highway combining d_t with \\tilde{x}_{L+t} for each prediction step. This covariate integration pathway is entirely new.\n\n4. **Attention Mechanism vs. Dense Layers - NOT APPLICABLE**: FEDformer uses sophisticated attention variants (self-attention in encoder, cross-attention in decoder) with O(L) complexity through mode selection. TiDE has NO attention mechanism whatsoever - it uses pure dense MLPs to map flattened sequences. All attention-related code (AutoCorrelationLayer, multi-head splitting, query-key-value projections) should not be implemented for TiDE.\n\n5. **Decoder Structure - NEW IMPLEMENTATION REQUIRED**: FEDformer's decoder uses autoregressive structure with trend accumulation (trend_part progressively updated through decoder layers) and seasonal refinement. TiDE's decoder is non-autoregressive: (a) Dense decoder outputs H×p tensor reshaped to p×H matrix, (b) Each column d_t is processed independently by temporal decoder, (c) All H predictions generated in parallel. The reshape operation and parallel temporal decoding are unique to TiDE.\n\n6. **Global Linear Residual - NEW IMPLEMENTATION REQUIRED**: TiDE adds a learnable linear layer mapping look-back y_{1:L} directly to horizon predictions \\hat{y}_{L+1:L+H}, added to final output. This ensures linear models are a subclass. FEDformer doesn't have this global skip connection from encoder input to final output (only local residuals within layers). This requires implementing a separate nn.Linear(L, H) layer bypassing the entire encoder-decoder.\n\n7. **Sequence Length Handling - DIFFERENT APPROACH**: FEDformer handles variable lengths through padding in wavelet transform (extra_x padding to nearest power of 2) and mode selection. TiDE flattens fixed-length sequences (L for lookback, H for horizon) with no special padding logic. TiDE's simpler approach doesn't need FEDformer's complex length handling.\n\n8. **Computational Complexity - FUNDAMENTALLY DIFFERENT**: FEDformer achieves O(L) complexity through sparse frequency mode selection (modes=64 fixed, regardless of sequence length). TiDE's complexity is O(L×hiddenSize) for dense layers, where the entire flattened sequence is processed. TiDE doesn't use any sparsity or mode selection strategies from FEDformer."
  },
  {
    "source": "DLinear_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing Strategy**: Both models process each channel/variate independently without modeling cross-channel dependencies. DLinear's implementation uses separate linear layers per channel (when `individual=True`), which can be directly adapted for TiDE's channel-independent architecture. The loop structure `for i in range(self.channels)` in DLinear can guide TiDE's per-channel processing implementation.\n\n2. **Series Decomposition for Trend Handling**: Both employ decomposition strategies to handle trend components. DLinear uses `series_decomp(configs.moving_avg)` from Autoformer with moving average kernels to separate trend and seasonal components. TiDE doesn't explicitly use decomposition in the main architecture but the decomposition module code from DLinear can be reused as a preprocessing option or integrated into TiDE's feature projection step.\n\n3. **Global Linear Residual Connection**: Both models incorporate linear skip connections from input to output. DLinear's core is linear mapping `W @ X`, and TiDE explicitly adds \"a global residual connection that linearly maps the look-back to a vector the size of the horizon.\" DLinear's `nn.Linear(self.seq_len, self.pred_len)` implementation can be directly adapted for TiDE's global residual pathway.\n\n4. **Direct Multi-Step Forecasting (DMS)**: Both use Direct Multi-Step forecasting strategy rather than autoregressive approaches, avoiding error accumulation. DLinear's `forecast()` method that directly outputs the full prediction horizon `[:, -self.pred_len:, :]` demonstrates the implementation pattern that TiDE follows.\n\n5. **Simplicity-First Design Philosophy**: Both papers challenge Transformer complexity by proposing simpler architectures. DLinear proves linear models can outperform Transformers, while TiDE extends this with MLPs. The weight initialization strategy in DLinear `(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])` provides guidance for initializing TiDE's linear components.\n\n6. **Temporal Dimension Processing**: Both operate along the temporal dimension with permutation operations. DLinear's `.permute(0, 2, 1)` pattern to reshape [Batch, Length, Channels] for temporal processing can be reused in TiDE's feature projection and temporal decoder implementations.",
    "differences": "1. **Core Architecture Paradigm**: DLinear uses pure linear layers (one-layer or two-layer with decomposition) with complexity O(L×T), while TiDE introduces a multi-layer MLP encoder-decoder architecture with residual blocks. NEW IMPLEMENTATION NEEDED: Multi-layer residual blocks with ReLU activation, dropout, and layer normalization (`ResidualBlock` class with hidden layers, skip connections, and normalization components not present in DLinear).\n\n2. **Covariate Integration**: DLinear does not use time covariates or static attributes (the paper notes covariates \"hurt performance\" for linear models), while TiDE's key innovation is sophisticated covariate handling through feature projection, dense encoding, and temporal decoding. NEW IMPLEMENTATION NEEDED: (a) Feature projection module to map high-dimensional covariates `x_t` to lower dimension `temporalWidth`, (b) Concatenation logic for dynamic covariates, static attributes, and lookback, (c) Temporal decoder that combines decoded vectors with projected future covariates.\n\n3. **Encoding-Decoding Structure**: DLinear has no explicit encoder-decoder separation (just linear transformation), while TiDE has distinct encoding (Feature Projection → Dense Encoder) and decoding (Dense Decoder → Temporal Decoder) phases. NEW IMPLEMENTATION NEEDED: (a) Dense encoder with `numEncoderLayers` stacked residual blocks mapping to embedding space, (b) Dense decoder with `numDecoderLayers` producing H×p dimensional output, (c) Reshape operation to convert flat decoder output to [decoderOutputDim, Horizon] matrix, (d) Temporal decoder processing each horizon timestep with its covariates.\n\n4. **Non-linearity and Depth**: DLinear is entirely linear (or piecewise linear with decomposition), while TiDE introduces deep non-linear transformations. NEW IMPLEMENTATION NEEDED: (a) ReLU activation functions between layers, (b) Configurable hidden layer sizes (`hiddenSize`, `temporalDecoderHidden`), (c) Deep stacking of multiple residual blocks (controlled by `numEncoderLayers`, `numDecoderLayers` hyperparameters), (d) Dropout layers for regularization.\n\n5. **Dimensionality Reduction Strategy**: DLinear works directly with raw input dimensions, while TiDE uses feature projection for dimensionality reduction of covariates from r to temporalWidth. NEW IMPLEMENTATION NEEDED: Feature projection residual block that reduces covariate dimension at each timestep before flattening, preventing input explosion when (L+H)×r is large.\n\n6. **Parameter Initialization**: DLinear uses explicit uniform initialization `(1/self.seq_len) * torch.ones()` for linear weights, while TiDE likely uses standard PyTorch initialization for MLP layers. NEW IMPLEMENTATION NEEDED: Proper initialization strategy for deep residual networks, potentially Xavier/Kaiming initialization for MLP weights.\n\n7. **Highway Connections for Covariates**: TiDE introduces a novel \"highway\" from future covariates directly to predictions through the temporal decoder, allowing strong direct effects (e.g., holidays). NEW IMPLEMENTATION NEEDED: Temporal decoder that concatenates decoded representation `d_t` with projected future covariate `x_tilde_{L+t}` for each horizon timestep, creating direct covariate-to-prediction pathways not present in DLinear.\n\n8. **Model Capacity and Flexibility**: DLinear has minimal hyperparameters (seq_len, pred_len, moving_avg window), while TiDE has extensive configurability (temporalWidth, hiddenSize, numEncoderLayers, numDecoderLayers, decoderOutputDim, temporalDecoderHidden). NEW IMPLEMENTATION NEEDED: Configuration management system and hyperparameter tuning framework for 6+ architectural hyperparameters."
  },
  {
    "source": "PatchTST_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing**: Both models process multivariate time series in a channel-independent manner, where each univariate series is fed independently through the model backbone with shared weights. PatchTST's implementation of splitting input `(x_1,...,x_L)` into M univariate series `x^(i)` and processing them separately can be directly reused for TiDE's channel-independent architecture.\n\n2. **Instance Normalization with Reversibility**: Both employ instance normalization to handle distribution shift - normalizing each time series instance with zero mean and unit variance before processing, then denormalizing outputs. PatchTST's normalization code (`means = x_enc.mean(1, keepdim=True).detach()` and `stdev = torch.sqrt(torch.var(...))`) can be directly adapted for TiDE's input preprocessing.\n\n3. **MSE Loss Function**: Both models use MSE (Mean Squared Error) as the training objective, averaging loss across all channels. PatchTST's loss computation `L = E[1/M * sum(||pred - true||^2)]` is identical to TiDE's training loss formulation.\n\n4. **Lookback-Horizon Framework**: Both follow the standard long-term forecasting setup with lookback window L and prediction horizon H (T in PatchTST). The data loading, batching, and rolling validation evaluation logic from PatchTST can be reused.\n\n5. **Global Residual Connection**: While not explicitly shown in PatchTST's main architecture, the codebase's normalization-denormalization pattern creates an implicit residual path. TiDE explicitly adds a linear residual from lookback to horizon, which can be implemented by adapting PatchTST's linear projection layers.\n\n6. **Dropout Regularization**: Both models employ dropout for regularization. PatchTST's dropout implementation in embedding layers (`self.dropout = nn.Dropout(dropout)`) and head (`head_dropout`) can be reused in TiDE's residual blocks.",
    "differences": "1. **Core Architecture Paradigm**: PatchTST uses Transformer encoder with self-attention mechanism (O(N²) complexity where N is number of patches), while TiDE uses pure MLP-based architecture with stacked residual blocks (O(L) complexity). TiDE requires implementing: (a) ResidualBlock module with skip connections, layer norm, and single hidden layer with ReLU, (b) stacked encoder/decoder blocks without any attention mechanism, (c) linear transformations instead of attention-based feature mixing.\n\n2. **Input Tokenization Strategy**: PatchTST segments time series into patches of length P with stride S, reducing sequence length from L to ~L/S tokens, then applies linear projection `W_p ∈ R^(D×P)` and positional encoding. TiDE does NOT use patching - instead it implements: (a) Feature Projection layer that maps each time-step's covariates `x_t^(i)` from dimension r to reduced dimension r̃ independently, (b) flattening of the entire lookback `y_{1:L}` concatenated with projected covariates `x̃_{1:L+H}` and static attributes `a^(i)` into a single vector for the dense encoder.\n\n3. **Temporal Information Encoding**: PatchTST uses learnable additive positional embeddings `W_pos ∈ R^(D×N)` to preserve temporal order of patches within the attention mechanism. TiDE requires implementing: (a) temporal information implicitly through the sequential structure of flattened inputs, (b) Temporal Decoder that operates per time-step, combining decoded vector `d_t^(i)` with projected future covariates `x̃_{L+t}^(i)` to generate prediction for each horizon step independently, creating explicit temporal highways.\n\n4. **Covariate Handling**: PatchTST does not explicitly model time-varying covariates in its architecture (only uses time series values). TiDE requires implementing: (a) separate processing pathway for dynamic covariates `x_t` at each time-step through feature projection, (b) static attribute handling `a^(i)` concatenated with encoded features, (c) integration of future covariates `x_{L+1:L+H}` in both encoder input and temporal decoder, (d) the temporal decoder's highway connection from future covariates to predictions.\n\n5. **Decoder Architecture**: PatchTST uses a simple FlattenHead that flattens patch representations and applies single linear layer `(d_model × patch_num) → pred_len`. TiDE requires implementing: (a) Dense Decoder with n_d residual block layers mapping encoding `e^(i)` to vector of size `H×p`, (b) Reshape operation converting decoder output to matrix `D^(i) ∈ R^(p×H)`, (c) Temporal Decoder as separate residual block processing each time-step `t ∈ [H]` independently by concatenating `d_t^(i)` with `x̃_{L+t}^(i)`, (d) explicit global linear residual mapping lookback directly to horizon predictions.\n\n6. **Multi-Head Attention vs Dense Layers**: PatchTST's core computation uses multi-head self-attention with query-key-value transformations (`Q_h, K_h, V_h`), scaled dot-product attention, and attention dropout. TiDE completely eliminates attention mechanism and requires implementing: (a) deep stacks of residual blocks with configurable layer counts (numEncoderLayers, numDecoderLayers), (b) different hidden sizes for encoder (hiddenSize), decoder (decoderOutputDim), and temporal decoder (temporalDecoderHidden), (c) layer normalization within residual blocks instead of batch normalization.\n\n7. **Sequence Length Handling**: PatchTST reduces effective sequence length through patching (L → L/S), enabling longer lookback windows within memory constraints. TiDE requires implementing: (a) feature projection to reduce covariate dimensionality (r → r̃) as the primary compression mechanism, (b) flattening entire lookback without segmentation, handling full sequence length `(L+H)×r̃ + L + s` where s is static attribute dimension, (c) decoder that generates H separate outputs through reshape rather than sequence-to-sequence mapping."
  },
  {
    "source": "PatchTST_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patching-based Input Processing**: Both models segment input time series into patches to reduce computational complexity and enable longer lookback windows. PatchTST uses configurable patch_len and stride parameters (default 16/8), creating N=⌊(L-P)/S⌋+2 patches. MultiPatchFormer extends this with multi-scale patching using 4 different patch sizes. **Code Reuse**: The `PatchEmbedding` module including `nn.ReplicationPad1d` padding and `unfold` operation can be directly adapted, requiring extension to handle multiple patch sizes simultaneously.\n\n2. **Instance Normalization for Distribution Shift**: Both apply instance normalization to each time series independently before processing, computing mean and standard deviation per sample, then denormalizing predictions. PatchTST implements this in the `forecast` method with `means = x_enc.mean(1, keepdim=True).detach()` and `stdev = torch.sqrt(torch.var(...))`. **Code Reuse**: The exact normalization/denormalization logic from PatchTST's forecast function can be directly copied to MultiPatchFormer.\n\n3. **Channel-Independent Processing**: Both models process each channel (variate) independently through the temporal encoder. PatchTST reshapes input to `(bs * nvars, patch_num, d_model)` treating each channel separately. MultiPatchFormer explicitly states channels are embedded independently by reshaping to `((B,C), L, 1)`. **Code Reuse**: The channel separation strategy `x_enc.permute(0, 2, 1)` and reshaping logic can be reused, though MultiPatchFormer adds a subsequent inter-series encoder.\n\n4. **Transformer Encoder Architecture**: Both use vanilla Transformer encoders with multi-head self-attention and feed-forward networks. PatchTST employs `FullAttention` with scaled dot-product attention and BatchNorm layers. **Code Reuse**: The entire `Encoder` and `EncoderLayer` modules from PatchTST, including `AttentionLayer` with `FullAttention`, can serve as the temporal encoder backbone for MultiPatchFormer.\n\n5. **Positional Encoding**: Both add positional information to patch embeddings. PatchTST uses learnable additive `PositionalEmbedding` with sinusoidal initialization. **Code Reuse**: The `PositionalEmbedding` class can be directly reused, though MultiPatchFormer may need to handle different numbers of patches per scale.\n\n6. **Linear Prediction Head Structure**: Both use linear layers to map from latent representations to forecast horizon. PatchTST flattens encoder output and applies a single linear layer. **Code Reuse**: The `FlattenHead` module provides a foundation, but MultiPatchFormer's multi-step decoder requires significant modification to break into sequential linear layers.",
    "differences": "1. **Multi-Scale Patching vs. Single-Scale**: PatchTST uses a single fixed patch size (patch_len=16, stride=8), while MultiPatchFormer's core innovation is multi-scale embedding with 4 different patch sizes processed in parallel. **New Implementation**: Must implement parallel 1D convolution layers with different kernel sizes and strides (each outputting d_model/4 channels), then concatenate outputs. This requires a new `MultiScaleEmbedding` module with 4 separate `nn.Conv1d` layers replacing PatchTST's single `value_embedding` linear projection.\n\n2. **Inter-Series Encoder for Channel Mixing**: PatchTST maintains strict channel independence throughout, while MultiPatchFormer adds an inter-series encoder module after the temporal encoder to capture cross-channel correlations. **New Implementation**: Requires a second Transformer encoder that operates on the channel dimension after reshaping from `(B*C, patch_num, d_model)` to `(B, C, d_model)` or similar. This involves a new encoder module with channel-wise attention, plus 1D convolution for reshaping between temporal and inter-series encoders.\n\n3. **Multi-Step Autoregressive Predictor**: PatchTST uses a single-step linear head (`FlattenHead` with one `nn.Linear(nf, target_window)`), while MultiPatchFormer employs a multi-step decoder with sequential linear layers that generate predictions progressively. **New Implementation**: Must implement a `MultiStepDecoder` module with multiple sequential `nn.Linear` layers, each generating partial predictions (e.g., if pred_len=96 and num_steps=4, each step predicts 24 timestamps), potentially with intermediate activations and residual connections.\n\n4. **Convolution-Based Patching vs. Linear Projection**: PatchTST uses `nn.Linear(patch_len, d_model)` for patch embedding, treating patches as fixed-length vectors. MultiPatchFormer explicitly uses 1D convolutions for patching to \"capture local temporal dynamics inside a patch (intra-patch relations)\". **New Implementation**: Replace linear projection with `nn.Conv1d(in_channels=1, out_channels=d_model/4, kernel_size=patch_size, stride=stride)` for each scale, requiring modification of the embedding pipeline.\n\n5. **Patch Number Alignment Across Scales**: MultiPatchFormer maintains fixed number of patches (PN1) across all 4 scales by adjusting strides appropriately, enabling direct concatenation. PatchTST has no such constraint. **New Implementation**: Requires careful calculation of stride values for each patch size to ensure equal patch numbers: stride_i = (L - patch_size_i) / (PN - 1), plus padding logic to handle edge cases.\n\n6. **Channel Dimension Handling Strategy**: PatchTST processes channels independently end-to-end with separate predictions per channel. MultiPatchFormer uses independent temporal encoding but adds cross-channel interaction via inter-series encoder before prediction. **New Implementation**: Requires reshaping operations between encoders and a mechanism to aggregate channel information (possibly attention or pooling) before the final predictor, fundamentally changing the information flow."
  },
  {
    "source": "DLinear_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition Strategy**: Both papers employ seasonal-trend decomposition using moving average kernels. DLinear uses `series_decomp` from Autoformer to split input into trend and seasonal components, which can be directly reused in MultiPatchFormer's preprocessing pipeline. The decomposition module (`self.decompsition = series_decomp(configs.moving_avg)`) is already implemented and tested.\n\n2. **Instance Normalization for Distribution Shift**: Both papers apply normalization techniques to handle distribution shifts between training and test sets. DLinear's NLinear variant uses subtraction-addition normalization (subtracting last value, processing, then adding back), while MultiPatchFormer explicitly mentions instance normalization. The normalization logic from DLinear can be adapted as a preprocessing step.\n\n3. **Channel-Independent Processing**: Both models process each channel (variate) independently. DLinear's `individual=True` mode uses separate linear layers per channel (`nn.ModuleList()` with per-channel parameters), and MultiPatchFormer reshapes series to `((B,C), L, 1)` for channel-wise processing. DLinear's channel iteration logic can guide MultiPatchFormer's channel handling implementation.\n\n4. **Direct Multi-Step Forecasting (DMS)**: Both employ Direct Multi-Step forecasting rather than autoregressive prediction. DLinear directly maps from `seq_len` to `pred_len` via linear layers, avoiding error accumulation. MultiPatchFormer's multi-step decoder follows similar philosophy but breaks it into multiple linear layers. The basic DMS framework from DLinear provides the foundation.\n\n5. **Linear Layer as Core Component**: Both use linear transformations as fundamental building blocks. DLinear's `nn.Linear(self.seq_len, self.pred_len)` for temporal mapping can inform MultiPatchFormer's predictor design, though MultiPatchFormer uses a sequence of linear layers rather than a single one.\n\n6. **Permutation Operations for Dimension Handling**: DLinear uses `.permute(0, 2, 1)` to swap channel and temporal dimensions for processing, then permutes back. This dimension manipulation pattern is essential for both architectures when handling `[B, L, C]` vs `[B, C, L]` formats.",
    "differences": "1. **Multi-Scale Patching vs. Single-Scale Processing**: MultiPatchFormer's core innovation is multi-scale embedding using 4 different patch sizes with 1D convolutions (filters of different kernel sizes and strides), merging outputs into `d_model` dimension. DLinear processes the entire sequence as-is without patching. NEW IMPLEMENTATION NEEDED: Multi-scale 1D convolution module with configurable patch sizes (e.g., [8, 16, 32, 64]), stride selection, and channel merging logic (`d_model/4` per scale).\n\n2. **Transformer Encoder Architecture vs. Pure Linear**: MultiPatchFormer employs temporal encoder and inter-series encoder modules with multi-head self-attention mechanisms, while DLinear is purely linear without any attention mechanism. NEW IMPLEMENTATION NEEDED: (a) Temporal encoder with self-attention over patches (inter-patch relations), (b) Channel-wise encoder with attention across variates, (c) Positional encoding for patches, (d) Multi-head attention layers with O(L²) or optimized complexity.\n\n3. **Multi-Step Decoder vs. Single Linear Projection**: MultiPatchFormer uses a sequence of linear layers to decode predictions in multiple steps (simulating auto-regressive decoding at patch level), generating partial predictions iteratively. DLinear uses single linear layers (`Linear_Seasonal` and `Linear_Trend`) for direct mapping. NEW IMPLEMENTATION NEEDED: Multi-step predictor architecture that breaks `d_model → pred_len` into multiple intermediate steps (e.g., `d_model → hidden → hidden → pred_len`).\n\n4. **Intra-Patch and Inter-Patch Modeling**: MultiPatchFormer explicitly captures local dynamics within patches (intra-patch) via 1D convolution and relationships between patches (inter-patch) via temporal encoder. DLinear treats all timestamps uniformly without hierarchical structure. NEW IMPLEMENTATION NEEDED: Patch-level feature extraction and hierarchical temporal modeling logic.\n\n5. **Complexity and Model Capacity**: DLinear has O(L) complexity with minimal parameters (just weight matrices), making it extremely lightweight. MultiPatchFormer has higher complexity due to multi-head attention (potentially O(P²) where P is number of patches), convolution layers, and deeper architecture. The implementation needs to balance model capacity with efficiency.\n\n6. **Embedding Strategy**: MultiPatchFormer uses learnable 1D convolutions to embed multi-scale patches into `d_model` space, with separate sub-spaces per scale. DLinear has no embedding layer, directly applying linear transformations to raw values. NEW IMPLEMENTATION NEEDED: Patch embedding module with 1D Conv layers, dimension projection, and scale merging.\n\n7. **Reshape and Dimension Transformation**: MultiPatchFormer uses 1D convolution after temporal encoder to reshape for channel-wise encoder (`(B, C, num_patches, d_model) → appropriate shape`). DLinear only uses simple permutations. NEW IMPLEMENTATION NEEDED: Intermediate reshape operations and dimension management between encoder stages.\n\n8. **Channel Interaction**: MultiPatchFormer has dedicated inter-series (channel-wise) encoder to model correlations between variates, while DLinear explicitly avoids modeling spatial correlations (\"does not model any spatial correlations\"). NEW IMPLEMENTATION NEEDED: Channel-wise attention mechanism to capture inter-variate dependencies."
  },
  {
    "source": "Crossformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Embedding Strategy**: Both papers segment time series into patches before processing. Crossformer uses Dimension-Segment-Wise (DSW) embedding with segment length L_seg=12, while MultiPatchFormer uses multi-scale patches. The PatchEmbedding module in Crossformer's code (using nn.Linear for projection + positional embedding) can be adapted for MultiPatchFormer's embedding layer, though MultiPatchFormer uses 1D convolutions instead of linear layers.\n\n2. **Hierarchical/Multi-scale Architecture**: Both employ multi-scale temporal modeling. Crossformer's Hierarchical Encoder-Decoder uses segment merging (SegMerging class) to create coarser scales at higher layers, while MultiPatchFormer uses parallel multi-scale patches. The scale_block and encoder structure in Crossformer can guide the implementation of multi-scale processing, though MultiPatchFormer processes scales in parallel rather than hierarchically.\n\n3. **Positional Embedding**: Both use learnable positional embeddings. Crossformer's PositionalEmbedding class (sinusoidal encoding) and learnable position parameters (enc_pos_embedding, dec_pos_embedding) can be directly reused or adapted for MultiPatchFormer's positional encoding needs.\n\n4. **Encoder-Decoder Framework**: Both adopt encoder-decoder architectures. Crossformer's Encoder and Decoder classes with attention-based processing provide a structural template. The encoder-decoder connection mechanism (cross-attention in DecoderLayer) can inform MultiPatchFormer's implementation, though the specific attention mechanisms differ.\n\n5. **Instance Normalization**: Both papers mention normalization techniques. Crossformer uses LayerNorm extensively (norm1, norm2, norm3, norm4 in TwoStageAttentionLayer), while MultiPatchFormer explicitly uses instance normalization. The normalization infrastructure in Crossformer's code can be adapted.\n\n6. **Multi-head Attention Foundation**: Both use attention mechanisms for temporal modeling. Crossformer's AttentionLayer and FullAttention classes provide a complete multi-head attention implementation with query/key/value projections that can serve as a baseline for MultiPatchFormer's temporal encoder.",
    "differences": "1. **Core Multi-scale Strategy**: Crossformer processes scales hierarchically through sequential segment merging (win_size=2 merging at each layer), while MultiPatchFormer processes multiple patch sizes (4 scales) in PARALLEL using separate 1D convolutions with different kernel sizes/strides. NEW IMPLEMENTATION NEEDED: Parallel multi-scale convolution module with 4 different patch sizes, each outputting d_model/4 channels, then concatenating to d_model.\n\n2. **Embedding Method**: Crossformer uses linear projection (nn.Linear) for patch embedding, while MultiPatchFormer uses 1D CONVOLUTIONS to capture intra-patch local dynamics. NEW IMPLEMENTATION NEEDED: Replace linear projection with nn.Conv1d layers (4 separate convolutions for 4 scales, each with output channels=d_model/4, different kernel sizes and strides).\n\n3. **Channel Handling Architecture**: Crossformer uses Two-Stage Attention (TSA) with router mechanism for cross-dimension dependency (complexity O(DL) via c routers), processing time and dimension separately. MultiPatchFormer uses channel-wise encoder with 1D convolution reshaping before channel attention. NEW IMPLEMENTATION NEEDED: (1) 1D convolution layer to reshape temporal encoder output for channel processing, (2) Channel-wise multi-head self-attention without the router mechanism, (3) Noise reduction strategy for large channel numbers.\n\n4. **Prediction/Decoder Strategy**: Crossformer uses multi-layer decoder with cross-attention and layer-wise predictions summed together (Eq. 8: final_predict = sum of all layer predictions). MultiPatchFormer uses a MULTI-STEP LINEAR PREDICTOR that breaks the final layer into multiple sequential linear layers for auto-regressive patch-level decoding. NEW IMPLEMENTATION NEEDED: Sequential multi-step decoder with multiple linear layers generating partial predictions iteratively, completely different from Crossformer's parallel layer summation.\n\n5. **Attention Mechanism Details**: Crossformer implements specific Two-Stage Attention with explicit cross-time stage (MSA on each dimension independently) and cross-dimension stage (router-based). MultiPatchFormer uses standard temporal encoder followed by channel-wise encoder. NEW IMPLEMENTATION NEEDED: Remove router mechanism, implement simpler channel-wise attention after temporal processing, add 1D conv reshaping between temporal and channel encoders.\n\n6. **Scale Processing Logic**: Crossformer's scales are created through iterative merging (scale_block with SegMerging), where each layer doubles the temporal receptive field. MultiPatchFormer's scales are FIXED at input by choosing appropriate patch sizes and strides to maintain consistent patch numbers (PN1) across scales. NEW IMPLEMENTATION NEEDED: Fixed multi-scale patch generation at input level with predetermined patch sizes/strides, no iterative merging.\n\n7. **Complexity and Efficiency**: Crossformer's TSA has O(DL²) complexity with router mechanism reducing cross-dimension to O(DL). MultiPatchFormer's parallel multi-scale with channel-wise attention has different complexity profile. NEW IMPLEMENTATION NEEDED: Implement noise reduction mechanism for channel-wise attention when dealing with large number of channels (not present in Crossformer)."
  },
  {
    "source": "FEDformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Distribution Shift**: Both papers employ instance normalization to handle distribution shifts between training and test sets. FEDformer uses it implicitly in the DataEmbedding module, while MultiPatchFormer explicitly mentions it as a key component. The normalization logic from FEDformer's embedding can be directly reused.\n\n2. **Encoder-Decoder Architecture**: Both adopt encoder-decoder frameworks for long-term forecasting. FEDformer's Encoder and Decoder classes with their layer-stacking mechanism can serve as structural templates for MultiPatchFormer's temporal and inter-series encoders.\n\n3. **Embedding Strategy**: Both use embedding modules to project input time series into model space. FEDformer's DataEmbedding (with positional and temporal encodings) provides a foundation that can be adapted for MultiPatchFormer's multi-scale embedding, particularly the positional encoding components.\n\n4. **Series Decomposition Philosophy**: Both leverage decomposition concepts - FEDformer explicitly uses series_decomp with moving average for trend-seasonal separation, while MultiPatchFormer implicitly decomposes through multi-scale patching. The MOEDecomp (Mixture of Experts Decomposition) from FEDformer with multiple average pooling filters could inspire MultiPatchFormer's multi-scale design.\n\n5. **Channel-Independent Processing**: Both process channels independently initially. FEDformer reshapes to (B*C, L, D) for frequency operations, MultiPatchFormer reshapes to ((B,C), L, 1) for convolution. This channel-wise processing pattern is directly transferable.\n\n6. **Feedforward Networks**: Both use feedforward layers in their architectures. FEDformer's EncoderLayer and DecoderLayer include feedforward networks that can be reused for MultiPatchFormer's linear predictor layers.\n\n7. **Multi-Head Mechanism**: Both employ multi-head designs - FEDformer in attention (n_heads parameter), MultiPatchFormer in channel-wise self-attention. The multi-head splitting/merging logic from FEDformer can be adapted.",
    "differences": "1. **Core Attention Mechanism - NEEDS NEW IMPLEMENTATION**: FEDformer uses frequency-domain attention (FourierBlock/FourierCrossAttention for Fourier version, MultiWaveletTransform/MultiWaveletCross for Wavelet version) operating in frequency space with O(L) complexity. MultiPatchFormer requires standard time-domain multi-head self-attention for both temporal and channel-wise encoders, which is fundamentally different and needs to be implemented from scratch using standard scaled dot-product attention.\n\n2. **Multi-Scale Patching Strategy - NEEDS NEW IMPLEMENTATION**: FEDformer does not use explicit patching; it operates on full sequences in frequency domain. MultiPatchFormer's core innovation is multi-scale patching using 4 different patch sizes with 1D convolutions (kernel sizes and strides), where each scale is embedded to d_model/4 dimension then concatenated. This requires implementing: (a) 4 separate Conv1d layers with different kernel/stride sizes, (b) patch number synchronization across scales via stride adjustment, (c) concatenation strategy to merge multi-scale embeddings.\n\n3. **Multi-Step Decoder Architecture - NEEDS NEW IMPLEMENTATION**: FEDformer uses a single linear projection layer (nn.Linear(d_model, c_out)) for final prediction. MultiPatchFormer proposes a novel multi-step predictor with multiple sequential linear layers that decode predictions progressively (generating parts of forecast horizon at each step) to reduce overfitting. This autoregressive-like patch-level decoding mechanism needs complete new implementation.\n\n4. **Channel Mixing Strategy - NEEDS NEW IMPLEMENTATION**: FEDformer processes channels independently throughout (no explicit channel interaction beyond final projection). MultiPatchFormer introduces a dedicated inter-series (channel-wise) encoder module that applies self-attention across channels after temporal encoding. This requires: (a) reshaping from (B, C, L, D) to enable channel-wise attention, (b) implementing channel-dimension self-attention (likely with reduced heads to avoid overfitting as mentioned), (c) a 1D convolution layer before channel encoder to reshape temporal encoder output.\n\n5. **Frequency vs. Time Domain Processing**: FEDformer's core operations (FFT/IFFT in FourierBlock, wavelet transform in MultiWaveletTransform) happen in frequency domain with mode selection (random/low modes). MultiPatchFormer operates entirely in time domain with standard attention, requiring no frequency transforms. All FEDformer's frequency-domain code (compl_mul1d, get_frequency_modes, wavelet filters) is irrelevant for MultiPatchFormer.\n\n6. **Complexity and Computational Focus**: FEDformer achieves O(L) complexity through frequency-domain operations with fixed mode selection (modes=64 default). MultiPatchFormer's complexity depends on patch numbers (PN1 across scales) and uses standard O(L²) attention but on reduced sequence length due to patching. The complexity optimization strategies are fundamentally different - frequency truncation vs. sequence length reduction.\n\n7. **Decomposition Mechanism**: FEDformer uses explicit trend-seasonal decomposition (series_decomp with moving average) applied recursively in encoder/decoder layers, with separate trend and seasonal pathways in decoder. MultiPatchFormer achieves implicit decomposition through multi-scale patching (different patch sizes capture different frequency components) without explicit trend extraction. MultiPatchFormer doesn't need FEDformer's MOEDecomp or trend accumulation logic."
  },
  {
    "source": "PatchTST_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Channel Independence Strategy**: Both papers adopt channel-independent (CI) processing where multivariate time series are split into M univariate series and processed separately through shared weights. PatchTST's implementation of splitting input (x_1,...,x_L) into M univariate series x^(i)∈ℝ^(1×L) can be directly reused. The forward pass iteration over channels and loss aggregation (L = E_x (1/M)Σ||ŷ^(i) - y^(i)||²) is identical in structure.\n\n2. **Instance Normalization for Distribution Shift**: Both employ instance normalization to handle distribution shift between train/test data. PatchTST normalizes each x^(i) with zero mean and unit variance before processing, then denormalizes predictions. SegRNN uses a simpler last-value subtraction (x^(i) = x^(i) - x_L^(i)). PatchTST's normalization module (means/stdev computation and reversal) can be adapted by replacing with SegRNN's subtraction-based approach.\n\n3. **Segment/Patch-based Input Processing**: Both partition input sequences into segments to reduce computational burden. PatchTST divides L-length series into N patches of length P with stride S (N = ⌊(L-P)/S⌋ + 2). SegRNN partitions into n segments of window length w (n = L/w). The patching logic from PatchTST (unfold operation: x.unfold(dimension=-1, size=patch_len, step=stride)) can be adapted for SegRNN's non-overlapping segmentation by setting stride=patch_len.\n\n4. **Linear Projection for Embedding**: Both use learnable linear projections to map segments to hidden dimensions. PatchTST's value_embedding (nn.Linear(patch_len, d_model)) projects patches to D-dimensional space. SegRNN uses W_prj∈ℝ^(w×d) to transform segments from w to d dimensions. The nn.Linear layer implementation can be directly reused, only adjusting input/output dimensions.\n\n5. **Prediction Head with Linear Mapping**: Both employ linear layers for final prediction. PatchTST's FlattenHead flattens encoder output and applies linear transformation to target_window. SegRNN uses W_prd∈ℝ^(d×w) to transform hidden states to segment predictions. The linear layer structure (nn.Linear + Dropout) from PatchTST can be adapted.\n\n6. **Batch Processing and Loss Computation**: Both reshape tensors for batch processing across channels (bs * nvars) and use similar loss functions. PatchTST's MSE loss can be replaced with SegRNN's MAE loss (L1Loss in PyTorch) while maintaining the same computational structure.",
    "differences": "1. **Core Architecture: Transformer vs. RNN**: PatchTST uses vanilla Transformer encoder with multi-head self-attention (O(N²) complexity where N=number of patches), while SegRNN employs GRU-based recurrent architecture with segment-wise iterations (O(n) complexity where n=L/w segments). NEW IMPLEMENTATION NEEDED: GRU cells with specific formulations (update gate z_t, reset gate r_t, candidate hidden state h̃_t) for both encoding and decoding phases. The entire Transformer encoder stack (EncoderLayer, AttentionLayer, FullAttention) from PatchTST is NOT used.\n\n2. **Encoding Strategy: Self-Attention vs. Recurrent Iterations**: PatchTST applies parallel self-attention across all patches simultaneously, computing Q, K, V matrices and attention scores. SegRNN performs sequential recurrent iterations over n segments, where each iteration updates hidden state h_t based on previous h_(t-1). NEW IMPLEMENTATION NEEDED: Sequential GRU iteration loop for encoding phase, managing hidden state propagation across segments.\n\n3. **Decoding Strategy: Direct Prediction vs. Parallel Multi-step Forecasting (PMF)**: PatchTST uses a single-pass flatten + linear head to directly predict entire horizon T from encoder output. SegRNN introduces PMF strategy that duplicates final hidden state h_n m times (m=H/w), combines with positional embeddings PE^(i)∈ℝ^(m×d), and processes in parallel through shared GRU cell to generate m output segments. NEW IMPLEMENTATION NEEDED: (1) Positional embedding generation combining relative position (rp∈ℝ^(d/2)) and channel position (cp∈ℝ^(d/2)) encodings, (2) Hidden state duplication mechanism, (3) Parallel GRU decoding that processes m segment predictions simultaneously without recursion.\n\n4. **Positional Encoding Design**: PatchTST uses standard learnable additive positional encoding W_pos∈ℝ^(D×N) or sinusoidal PositionalEmbedding for patch positions. SegRNN designs specialized positional embeddings PE^(i) that concatenate relative position encoding (segment position within sequence) and channel position encoding (channel index within multivariate data). NEW IMPLEMENTATION NEEDED: Custom positional embedding module that generates position-aware and channel-aware embeddings for decoder segments.\n\n5. **Attention Mechanism vs. No Attention**: PatchTST's core relies on scaled dot-product attention with Softmax(QK^T/√d_k)V computation, including multi-head attention, attention masks, and dropout. SegRNN completely eliminates attention mechanisms, relying solely on GRU's gating mechanisms (update/reset gates) for feature extraction. NEW IMPLEMENTATION NEEDED: None for attention (it's removed), but GRU gating logic with sigmoid and tanh activations must be implemented.\n\n6. **Temporal Complexity Reduction Approach**: PatchTST reduces from L to L/S tokens through patching with overlapping (stride S < patch_len P), maintaining some temporal overlap for continuity. SegRNN reduces from L to L/w iterations through non-overlapping segmentation AND further reduces decoding from H/w to 1 iteration via PMF's parallel processing. The decoding iteration reduction (H/w → 1) is SegRNN's unique contribution. NEW IMPLEMENTATION NEEDED: PMF's single-iteration parallel decoding logic that avoids recursive error accumulation.\n\n7. **Model Component Sharing**: PatchTST uses separate encoder and prediction head with no weight sharing between encoding/decoding. SegRNN explicitly shares the same GRU cell between encoding and decoding phases, using h_n from encoding as initial state for decoding. NEW IMPLEMENTATION NEEDED: Weight-shared GRU architecture where the same GRU parameters are used for both encoding segments and decoding with positional embeddings.\n\n8. **Normalization Simplicity**: PatchTST computes mean and standard deviation for normalization (x_enc = (x_enc - means)/stdev), requiring storage and reversal of both statistics. SegRNN uses simpler last-value subtraction (x = x - x_L), requiring only one value storage. Implementation difference: Replace PatchTST's mean/stdev computation with simple last-value extraction and subtraction."
  },
  {
    "source": "TimesNet_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization Strategy**: Both papers employ instance normalization to handle distribution shifts. TimesNet uses mean-std normalization (subtract mean, divide by std) in its forecast method, while SegRNN uses last-value normalization (subtract last value). The normalization framework from TimesNet's code (lines computing means/stdev and de-normalization) can be adapted by replacing mean-std operations with last-value operations.\n2. **Channel-Independent (CI) Processing**: Both models process each channel independently. TimesNet's embedding layer (DataEmbedding) processes channels separately, and SegRNN explicitly uses CI strategy. The channel-wise iteration structure from TimesNet can be reused, though SegRNN adds channel position encoding to partially compensate for CI limitations.\n3. **Encoder-Decoder Architecture Pattern**: Both follow an encoder-decoder paradigm where encoding captures historical patterns and decoding generates predictions. TimesNet's sequential layer structure (ModuleList of TimesBlocks) provides a template for organizing encoding layers, though SegRNN uses GRU instead of TimesBlocks.\n4. **Linear Projection Layers**: Both use linear layers for dimension transformation and final prediction. TimesNet's predict_linear and projection layers demonstrate the pattern of using nn.Linear for temporal alignment and output generation, which directly maps to SegRNN's projection (W_prj) and prediction (W_prd) layers.\n5. **Residual Connections**: TimesNet uses residual connections in TimesBlock (res = res + x), which is a general deep learning practice. While SegRNN doesn't explicitly mention residuals in its main architecture, the modular design pattern from TimesNet can inform SegRNN's implementation structure.",
    "differences": "1. **Core Temporal Modeling Mechanism**: TimesNet uses 2D convolution on reshaped time series (FFT-based period detection → reshape to 2D → Inception block convolution), while SegRNN uses GRU-based recurrent processing on segments. NEW IMPLEMENTATION NEEDED: GRU cells with update gate (z_t), reset gate (r_t), and candidate hidden state (h_tilde) as specified in SegRNN's equations - none of this exists in TimesNet's CNN-based code.\n2. **Segment-wise vs Point-wise Processing**: TimesNet processes full sequences with multi-periodicity detection (FFT_for_Period function), while SegRNN partitions sequences into fixed-length segments (L/w segments of length w) and processes segment-wise. NEW IMPLEMENTATION NEEDED: Segment partitioning logic to split input into windows, and segment-wise iteration through GRU (reducing iterations from L to L/w).\n3. **Parallel Multi-step Forecasting (PMF) vs Sequential Decoding**: TimesNet uses direct prediction through linear layers (predict_linear aligns temporal dimension, then projection outputs), while SegRNN introduces PMF strategy where the final hidden state h_n is duplicated m times and combined with positional embeddings for parallel decoding. NEW IMPLEMENTATION NEEDED: (1) Positional embedding generation (relative position + channel position concatenation), (2) Parallel GRU processing where m output vectors are computed simultaneously from duplicated h_n, (3) Reshape logic to convert m×w predictions back to H-length sequence.\n4. **Multi-Periodicity vs Single-Scale Segmentation**: TimesNet explicitly models multiple periodicities (top-k frequencies from FFT, adaptive aggregation with softmax weights), creating k different 2D representations. SegRNN uses uniform fixed-length segments without frequency analysis. NEW IMPLEMENTATION NEEDED: Fixed window segmentation (simpler than TimesNet's FFT approach) but requires careful handling of segment boundaries and position tracking.\n5. **2D Convolution with Inception Block vs 1D Recurrent Processing**: TimesNet's core is the Inception_Block_V1 with multiple 2D convolutional kernels (kernel sizes 1×1, 3×3, 5×5, etc.) processing reshaped tensors. SegRNN uses standard GRU operations on 1D segment embeddings. NEW IMPLEMENTATION NEEDED: Complete GRU implementation with proper state management across segment iterations, sharing the same GRU cell between encoding and decoding phases.\n6. **Complexity and Iteration Count**: TimesNet has O(L) complexity with fixed number of layers (e_layers), while SegRNN explicitly reduces recurrent iterations from L to L/w in encoding and from H/w to 1 in decoding (via PMF). NEW IMPLEMENTATION NEEDED: Iteration control logic that limits GRU passes to n=L/w steps in encoding, and parallel single-step processing in decoding (vs traditional H/w recursive steps).\n7. **Frequency Domain vs Time Domain**: TimesNet heavily relies on FFT for period detection and uses frequency amplitudes for adaptive aggregation (FFT_for_Period, softmax over amplitudes). SegRNN operates purely in time domain with segment-based processing. NO frequency domain operations needed for SegRNN - this is a simplification from TimesNet.\n8. **State Propagation Strategy**: TimesNet uses layer-wise residual connections without explicit state carrying between blocks, while SegRNN requires explicit hidden state propagation (h_t carries information across segment iterations, h_n is the final encoded state passed to decoder). NEW IMPLEMENTATION NEEDED: Hidden state management system to track and propagate GRU states across encoding segments and into parallel decoding."
  },
  {
    "source": "DLinear_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization Strategy**: Both papers employ reversible instance normalization to handle distribution shift. DLinear's NLinear variant subtracts the last value of the input sequence before processing and adds it back after prediction (x - x_L, then restore). SegRNN uses the exact same normalization: x_{1:L}^{(i)} = x_{1:L}^{(i)} - x_L^{(i)} before encoding and adds back after decoding. **CODE REUSE**: The normalization logic from DLinear can be directly adapted - the subtraction/addition operations are identical.\n\n2. **Channel-Independent (CI) Strategy**: Both models process each channel/variate independently without modeling cross-channel dependencies. DLinear explicitly states \"LTSF-Linear shares weights across different variates and does not model any spatial correlations\" and offers an 'individual' mode for channel-specific parameters. SegRNN processes each X^{(i)} separately through the same architecture. **CODE REUSE**: The channel iteration loop structure from DLinear's individual mode can be adapted for SegRNN's per-channel processing.\n\n3. **Direct Multi-Step Forecasting (DMS)**: Both papers adopt DMS strategy rather than iterative/autoregressive prediction to avoid error accumulation. DLinear directly maps L input steps to H output steps via linear layers. SegRNN's PMF (Parallel Multi-step Forecasting) generates all H predictions simultaneously in parallel, also following DMS philosophy. **CODE REUSE**: The non-autoregressive prediction framework and loss computation over the entire horizon can be reused.\n\n4. **Simple Architecture Philosophy**: Both papers advocate for simplicity over complex Transformer architectures. DLinear demonstrates that simple linear layers outperform Transformers. SegRNN shows simple RNN with proper design (segment-wise iteration, PMF) achieves strong performance. Both challenge the necessity of complex attention mechanisms for LTSF.\n\n5. **Encoder-Only Style with Direct Prediction**: Both use encoder-style processing followed by direct prediction layers. DLinear: decomposition → linear encoding → direct output. SegRNN: segment encoding → parallel decoding → direct output. Neither uses traditional autoregressive decoders. **CODE REUSE**: The overall forward pass structure (encode → predict) and the direct prediction head design pattern can be adapted.\n\n6. **MAE Loss Function**: Both papers can use MAE as the primary loss function for training. DLinear's implementation supports MSE/MAE, and SegRNN explicitly uses MAE: L = (1/HC)Σ|ŷ_t - y_t|. **CODE REUSE**: The loss computation code is directly transferable.",
    "differences": "1. **Core Temporal Modeling Mechanism**: DLinear uses pure linear layers (weighted sum W∈R^{T×L}) operating on the temporal dimension - no recurrence, no memory. SegRNN employs GRU cells with recurrent hidden states (h_t) to capture temporal dependencies through gating mechanisms (update gate z_t, reset gate r_t). **NEW IMPLEMENTATION NEEDED**: Full GRU cell implementation with gates (z_t = σ(W_z·[h_{t-1}, x_t]), r_t = σ(W_r·[h_{t-1}, x_t]), h̃_t, h_t updates), hidden state management, and recurrent iterations.\n\n2. **Segment-Based Processing**: DLinear operates on point-wise temporal data directly (entire sequence as input). SegRNN introduces segment partitioning: input L is divided into n=L/w segments of length w, reducing recurrent iterations from L to L/w. Each segment X_w^{(i)}∈R^{n×w} is projected to d-dimensional space before GRU processing. **NEW IMPLEMENTATION NEEDED**: Segment partition logic (reshape L → n×w), learnable segment projection layer W_{prj}∈R^{w×d} with ReLU activation, and segment-wise iteration loop for GRU.\n\n3. **Decomposition Strategy**: DLinear uses explicit series decomposition (from Autoformer) with moving average kernel to separate trend and seasonal components, then applies separate linear layers to each component and sums them (seasonal_output + trend_output). SegRNN does NOT use decomposition - it processes raw normalized sequences directly through GRU. **NEW IMPLEMENTATION NEEDED**: None for decomposition (SegRNN doesn't use it), but need to REMOVE/BYPASS DLinear's decomposition module when adapting code.\n\n4. **Positional Embeddings for Decoding**: DLinear has no positional embeddings (linear layers are position-aware by weight indices). SegRNN requires sophisticated positional embeddings PE^{(i)}∈R^{m×d} where m=H/w segments, each pe^{(i)} concatenates relative position encoding rp∈R^{d/2} (segment position in sequence) and channel position encoding cp∈R^{d/2} (channel index i). **NEW IMPLEMENTATION NEEDED**: Positional embedding generation logic with both relative and channel-aware components, concatenation mechanism, and integration with hidden states.\n\n5. **Parallel Multi-step Forecasting (PMF) Mechanism**: DLinear directly projects from L to H in one shot (single linear transformation). SegRNN's PMF duplicates the final hidden state h_n m times, combines with m positional embeddings, processes all pairs SIMULTANEOUSLY through shared GRU cell (parallel computation), generating m outputs Ȳ_d^{(i)}∈R^{m×d} which are then projected to segments and reshaped to H. **NEW IMPLEMENTATION NEEDED**: Hidden state duplication logic, parallel GRU cell processing (batch processing of m pairs), output aggregation, and segment-to-sequence reshaping (m×w → H).\n\n6. **Prediction Head Design**: DLinear uses simple linear layers (Linear_Seasonal, Linear_Trend) initialized with uniform weights (1/seq_len). SegRNN uses: (1) Dropout layer for regularization on Ȳ_d^{(i)}, (2) learnable linear prediction layer W_{prd}∈R^{d×w} to transform from hidden dimension d to segment length w, (3) reshape operation to recover full sequence. **NEW IMPLEMENTATION NEEDED**: Dropout integration, segment-wise prediction layer (d→w transformation), and reshape logic from (m×w) to H.\n\n7. **Model Complexity and Iteration Count**: DLinear has O(1) iterations (single forward pass through linear layers), complexity O(L×H×C). SegRNN has O(L/w) encoding iterations + O(1) decoding iteration (due to PMF), with GRU complexity O((L/w)×d²) for hidden state updates. The segment-wise approach and PMF dramatically reduce iterations compared to traditional RNN's O(L+H). **NEW IMPLEMENTATION NEEDED**: Iteration control logic for segment-wise encoding, GRU state persistence across segments, and parallel decoding implementation.\n\n8. **Weight Initialization and Architecture Components**: DLinear initializes linear weights uniformly as (1/seq_len)×ones([pred_len, seq_len]) for averaging effect. SegRNN requires: (1) GRU weight matrices (W_z, W_r, W) with proper initialization, (2) projection layer W_{prj} initialization, (3) prediction layer W_{prd} initialization, (4) positional embedding initialization. **NEW IMPLEMENTATION NEEDED**: GRU-specific weight initialization schemes (typically Xavier/orthogonal for recurrent weights), and management of multiple weight matrices for gates."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Seasonal-Trend Decomposition Strategy**: Both papers employ seasonal-trend decomposition as a core component. FEDformer uses the MOEDecomp (Mixture of Experts Decomposition) with multiple average pooling filters, while DLinear uses a simpler moving average kernel. **Code Reuse**: FEDformer's `series_decomp` class from `layers.Autoformer_EncDec` can be directly adapted for DLinear's decomposition - the moving average logic is already implemented and can be simplified by using a single kernel size instead of the mixture of experts approach.\n\n2. **Direct Multi-Step (DMS) Forecasting**: Both models adopt the DMS strategy where the entire prediction horizon is generated in one forward pass, avoiding autoregressive error accumulation. FEDformer's decoder generates predictions through `dec_out[:, -self.pred_len:, :]`, and this same slicing approach can be used in DLinear. **Code Reuse**: The training loop structure, loss computation, and prediction extraction logic from FEDformer can be directly transferred.\n\n3. **Normalization with Zero-Mean**: Both papers acknowledge the importance of normalization in time series forecasting. FEDformer applies standard normalization in its embedding layers (`DataEmbedding`), while DLinear's NLinear variant uses last-value subtraction. **Code Reuse**: The normalization utilities and data preprocessing pipeline from FEDformer's implementation can serve as a foundation.\n\n4. **Channel-Independent Processing**: Both models process each variate independently without explicit cross-channel attention or mixing. FEDformer processes channels through separate heads in attention mechanisms, and DLinear explicitly shares weights across variates but processes them independently. **Code Reuse**: The channel iteration and batch processing structure from FEDformer can be adapted.\n\n5. **Task Configuration and Data Handling**: Both use similar experimental setups with seq_len (lookback window), pred_len (forecast horizon), and handle multivariate time series. **Code Reuse**: FEDformer's configuration class structure (`configs.seq_len`, `configs.pred_len`, `configs.enc_in`, `configs.dec_in`) and data loading pipeline can be directly reused for DLinear with minimal modifications.",
    "differences": "1. **Core Architecture - Transformer vs. Pure Linear**: FEDformer employs a complex encoder-decoder Transformer architecture with frequency-domain attention (Fourier/Wavelet blocks), multi-head attention, and multiple layers. DLinear uses an extremely simple architecture with just one or two linear layers (temporal dimension mapping). **New Implementation Needed**: A minimal linear model class with a single `nn.Linear(seq_len, pred_len)` layer that operates on the temporal dimension, completely replacing the encoder-decoder structure.\n\n2. **Attention Mechanism Elimination**: FEDformer's core innovation is frequency-enhanced attention (FEB-f/FEB-w for self-attention, FEA-f/FEA-w for cross-attention) operating in Fourier or Wavelet domains with O(L) complexity. DLinear completely abandons any attention mechanism. **New Implementation Needed**: Remove all attention-related components (FourierBlock, FourierCrossAttention, MultiWaveletTransform, MultiWaveletCross) and replace with simple linear transformations.\n\n3. **Embedding Strategy**: FEDformer uses rich input embeddings including positional encoding, temporal embeddings (DataEmbedding with value embedding, positional encoding, and temporal encoding). DLinear uses raw input values directly without any embedding transformation. **New Implementation Needed**: Bypass or remove the embedding layers entirely - DLinear operates directly on normalized raw time series values.\n\n4. **Decomposition Complexity**: FEDformer uses MOEDecomp with a mixture of experts approach (multiple average pooling filters with different kernel sizes combined via learned weights: `Softmax(L(x)) * F(x)`). DLinear uses a single fixed-size moving average kernel for decomposition. **New Implementation Needed**: Simplify to a basic moving average implementation with a single kernel size (e.g., kernel_size=25), removing the mixture of experts mechanism.\n\n5. **Model Complexity and Parameter Count**: FEDformer has O(L) time complexity but involves multiple encoder/decoder layers, Fourier/Wavelet transformations, and numerous parameters. DLinear has O(L) complexity with minimal parameters (only the linear layer weights W ∈ R^(T×L)). **New Implementation Needed**: A lightweight model class with drastically reduced parameter count - essentially just weight matrices for linear projection.\n\n6. **Normalization Variants**: FEDformer uses standard zero-mean normalization. DLinear introduces NLinear variant with a specific last-value subtraction and addition scheme: subtract the last value before linear transformation, then add it back. **New Implementation Needed**: Implement the reversible normalization strategy where `normalized_input = input - input[:, -1:, :]` and `output = linear(normalized_input) + input[:, -1:, :]`.\n\n7. **Decoder Design**: FEDformer uses a sophisticated decoder with trend initialization (`trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)`), seasonal padding, and progressive refinement through decoder layers. DLinear has no decoder - it's a single-pass feedforward operation. **New Implementation Needed**: Replace the entire decoder structure with direct linear mapping from input to output.\n\n8. **Frequency Domain Processing**: FEDformer operates extensively in frequency domain using FFT (`torch.fft.rfft`, `torch.fft.irfft`) and Wavelet transforms with learnable parameters in spectral space. DLinear operates purely in time domain with no frequency transformations. **New Implementation Needed**: None of the frequency-domain code (get_frequency_modes, compl_mul1d, FFT operations, Wavelet filters) is needed for DLinear."
  },
  {
    "source": "Autoformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Strategy**: Both papers employ the same moving average-based series decomposition to separate time series into trend and seasonal components. DLinear directly reuses Autoformer's `series_decomp` module (as seen in the code import `from layers.Autoformer_EncDec import series_decomp`). The decomposition uses AvgPool with padding to extract trend-cyclical components, then obtains seasonal components via subtraction.\n2. **Decomposition as Preprocessing**: Both apply decomposition before the main modeling components. Autoformer uses it as an inner block throughout encoder-decoder layers, while DLinear uses it once at the input stage. Both process trend and seasonal components separately through different pathways.\n3. **Direct Multi-Step (DMS) Forecasting**: Both models adopt DMS strategy, predicting all future time steps simultaneously rather than iteratively. Autoformer's decoder outputs length-O predictions directly, and DLinear's linear layers map from seq_len to pred_len in one step.\n4. **Temporal Embeddings**: Both papers acknowledge the importance of temporal context. Autoformer uses comprehensive temporal embeddings (hour, day, month, weekday) combined with positional encoding, while DLinear implicitly handles temporal patterns through the decomposition and linear projection.",
    "differences": "1. **Core Architecture Philosophy**: Autoformer is a complex Transformer-based architecture with encoder-decoder structure, auto-correlation mechanism, and stacked layers (N encoder + M decoder layers), while DLinear is an embarrassingly simple model consisting of only two single-layer linear transformations (one for trend, one for seasonal) without any attention mechanism or deep structure.\n2. **Attention vs. Linear Mapping**: Autoformer introduces the novel Auto-Correlation mechanism to replace self-attention, discovering period-based dependencies through time delay aggregation with O(L log L) complexity. It uses FFT for correlation computation and aggregates similar sub-series from underlying periods. DLinear completely abandons attention mechanisms, using only temporal linear layers (W ∈ R^(T×L)) for direct regression from historical to future values.\n3. **Complexity and Computational Cost**: Autoformer has O(L log L) time complexity due to FFT-based auto-correlation and multiple encoder-decoder layers with feed-forward networks. DLinear has O(L) complexity with minimal parameters (two linear layers), making it orders of magnitude faster and more parameter-efficient.\n4. **Channel Modeling Strategy**: Autoformer processes all channels together through multi-head attention mechanisms, allowing cross-channel information exchange. DLinear offers two modes: shared weights across channels (default) or individual linear layers per channel, explicitly avoiding spatial correlation modeling to prevent overfitting.\n5. **Progressive Refinement vs. One-Shot Prediction**: Autoformer uses an accumulation structure in the decoder, progressively refining trend predictions across layers (equations show trend accumulation T_de^(l,1), T_de^(l,2), T_de^(l,3)). DLinear performs one-shot decomposition and prediction without iterative refinement.\n6. **Embedding and Feature Engineering**: Autoformer employs rich input embeddings including TokenEmbedding (1D convolution with kernel size 3), PositionalEmbedding (sinusoidal), and TemporalEmbedding (learnable embeddings for time features). DLinear eliminates all embedding layers, directly operating on raw normalized time series values.\n7. **Motivation and Research Goal**: Autoformer aims to improve Transformer efficiency while maintaining modeling capability for long-term dependencies through auto-correlation. DLinear challenges the necessity of Transformers for LTSF, demonstrating that simple linear models with proper decomposition can outperform complex Transformer variants, questioning whether the semantic correlation mechanism (self-attention) is appropriate for temporal numerical data."
  },
  {
    "source": "Informer_2020",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt a DMS prediction approach where the entire future sequence is predicted in one forward pass, avoiding the error accumulation problem of autoregressive iterative methods. Informer uses a generative-style decoder for DMS, while DLinear directly outputs the full prediction horizon.\n2. **Time Series Decomposition**: Both methods incorporate decomposition strategies. Informer's implementation includes ConvLayer with downsampling for hierarchical feature extraction, while DLinear explicitly adopts the series_decomp module (moving average kernel) from Autoformer to separate trend and seasonal components before applying linear transformations.\n3. **Encoder-Decoder Architecture Paradigm**: Both follow an encoder-decoder framework conceptually. Informer uses explicit Transformer-based encoder-decoder with ProbSparse attention and distilling layers, while DLinear's encoder method performs decomposition and linear projection, effectively serving as a simplified encoder-decoder where decomposition acts as encoding and linear layers as decoding.\n4. **Batch Normalization for Stability**: Informer's implementation uses BatchNorm1d in ConvLayer for feature normalization during downsampling, while DLinear initializes linear weights uniformly (1/seq_len) for stable training dynamics, both addressing numerical stability concerns.",
    "differences": "1. **Core Innovation and Philosophy**: Informer focuses on making Transformers efficient for LTSF through ProbSparse self-attention (reducing O(L²) to O(L log L) complexity) and self-attention distilling, believing semantic correlations captured by attention are valuable. DLinear challenges this premise, arguing that simple linear layers outperform complex Transformers for time series because temporal ordering matters more than permutation-invariant attention mechanisms, demonstrating an \"embarrassingly simple\" baseline.\n2. **Temporal Modeling Mechanism**: Informer uses ProbSparse self-attention with query sparsity measurement (max-mean KL divergence approximation) to identify dominant query-key pairs and capture long-range dependencies through attention weights. DLinear uses direct temporal linear projection (weighted sum) along the time axis without any attention mechanism, treating forecasting as pure regression on historical values.\n3. **Model Complexity and Components**: Informer employs multi-head attention, feed-forward networks (Conv1d layers), layer normalization, dropout, positional/temporal embeddings, and hierarchical distilling (ConvLayer with MaxPool1d for sequence length reduction). DLinear uses only two single-layer linear transformations (one for trend, one for seasonal) with no attention, no embeddings, no normalization layers, and no complex architectural components.\n4. **Channel Handling Strategy**: Informer processes all variates jointly through multi-head attention where queries, keys, and values span all channels, enabling cross-variate interaction. DLinear explicitly decouples channels with individual=False mode sharing weights across variates (no spatial correlation modeling) or individual=True mode using separate linear layers per channel, fundamentally treating each variate independently.\n5. **Input Processing and Embedding**: Informer uses DataEmbedding combining value embedding (linear projection), positional encoding (fixed or learnable), and temporal embeddings (hour, day, week, month) to inject rich contextual information. DLinear operates directly on raw time series values after decomposition with no embedding layers, positional encoding, or temporal feature engineering.\n6. **Computational Efficiency**: Informer achieves O(L log L) complexity through sparse attention sampling (randomly selecting U=L_K ln L_Q dot-products) and distilling layers that progressively halve sequence length. DLinear achieves O(L) complexity with simple matrix multiplication, being orders of magnitude faster with minimal memory footprint (no attention matrices stored).\n7. **Prediction Mechanism**: Informer's decoder uses masked self-attention on target sequences with cross-attention to encoder outputs, requiring start tokens and placeholder sequences for future steps. DLinear directly maps input sequence length L to prediction length T through weight matrix W∈R^(T×L), with no decoder structure, no masking, and no iterative generation.\n8. **Feature Extraction Philosophy**: Informer relies on learned attention patterns to automatically discover important temporal dependencies and distilling operations to extract multi-scale features hierarchically. DLinear explicitly decomposes into trend (via moving average) and seasonal (residual) components, applying separate linear transformations to each, making the feature extraction interpretable and deterministic."
  },
  {
    "source": "Pyraformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt DMS forecasting where the model directly predicts the entire future horizon T in one forward pass, avoiding autoregressive error accumulation. This is a key design choice that DLinear explicitly validates as critical for LTSF performance.\n2. **Time Series Decomposition**: Both utilize decomposition strategies. Pyraformer's implementation includes bottleneck convolution layers (CSCM) for hierarchical feature extraction, while DLinear explicitly adopts the series_decomp module from Autoformer to separate trend and seasonal components using moving average kernels.\n3. **Embedding and Positional Encoding**: Both papers recognize the importance of temporal context. Pyraformer uses DataEmbedding with positional and temporal embeddings to inject ordering information, while DLinear implicitly acknowledges this need but demonstrates that simpler approaches without complex embeddings can be effective.\n4. **Batch Normalization Usage**: Pyraformer's implementation uses BatchNorm1d in ConvLayer modules for stabilizing training. While DLinear doesn't explicitly use normalization in its core linear layers, both papers recognize the importance of proper normalization for time series modeling.",
    "differences": "1. **Core Architecture Philosophy**: Pyraformer is a complex Transformer-based model with pyramidal attention mechanism (PAM) achieving O(L) complexity through multi-scale hierarchical modeling with C-ary tree structures and inter/intra-scale connections. DLinear is an embarrassingly simple linear baseline that directly challenges the necessity of Transformers for LTSF, using only one-layer linear projections on decomposed components.\n2. **Temporal Dependency Modeling**: Pyraformer uses sophisticated attention mechanisms with pyramidal graphs to capture multi-resolution temporal dependencies (hourly→daily→weekly→monthly patterns) through query-key-value attention with constrained receptive fields defined by neighboring nodes. DLinear uses simple weighted sum operations (linear layers) along the temporal axis without any attention mechanism, treating forecasting as direct linear regression.\n3. **Multi-Scale Modeling Approach**: Pyraformer explicitly constructs a C-ary tree with S scales using Coarser-Scale Construction Module (CSCM) with bottleneck convolutions, creating hierarchical representations where coarser scales capture long-range dependencies. DLinear has no multi-scale modeling; it operates on a single temporal resolution with decomposition only separating trend from seasonal components.\n4. **Complexity and Computational Cost**: Pyraformer achieves O(L) time/space complexity through sparse pyramidal attention but requires multiple encoder layers (N layers), attention heads, feed-forward networks, and complex mask construction (get_mask, refer_points functions). DLinear has trivial O(L) complexity with minimal parameters (two linear layers for trend/seasonal), making it orders of magnitude more efficient.\n5. **Channel Handling Strategy**: Pyraformer processes all channels jointly through shared attention mechanisms and embedding layers (enc_in dimension in DataEmbedding), mixing spatial and temporal information. DLinear offers both shared and individual modes, where individual=True creates separate linear layers per channel, explicitly avoiding cross-channel contamination.\n6. **Research Contribution**: Pyraformer's contribution is proposing an efficient Transformer variant for long-range time series modeling with theoretical guarantees (Lemma 1, Propositions 1-2) on receptive field and complexity. DLinear's contribution is a critical re-examination showing that simple linear models outperform complex Transformers on LTSF tasks, questioning whether self-attention's permutation-invariance is fundamentally misaligned with temporal order-sensitive time series.\n7. **Decoder Design**: Pyraformer uses a complex decoder with fully-connected layers concatenating spatio-temporal features and gather operations (torch.gather) to extract predictions from pyramidal sequences. DLinear has no decoder; it directly outputs predictions by summing trend and seasonal linear projections, with optional per-channel processing.\n8. **Input Processing**: Pyraformer employs sophisticated input processing with conv_layers (Bottleneck_Construct) that downsample sequences through multiple ConvLayer modules with kernel_size=window_size, creating pyramidal representations before attention. DLinear performs minimal preprocessing, only applying moving average decomposition before feeding to linear layers."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "",
    "differences": ""
  },
  {
    "source": "DLinear_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition**: Both papers utilize the seasonal-trend decomposition scheme. DLinear explicitly uses a moving average kernel (series_decomp from Autoformer) to separate trend and seasonal components, applying separate linear layers to each. Crossformer also references decomposition as a standard preprocessing technique in time series analysis, though it's not the core architectural component.\n\n2. **Encoder-Decoder Architecture**: Both adopt encoder-decoder frameworks for multivariate time series forecasting. DLinear uses a simplified encoder (decomposition + linear layers) with direct multi-step (DMS) forecasting strategy. Crossformer uses a hierarchical encoder-decoder with Two-Stage Attention layers.\n\n3. **Layer Normalization**: Both papers employ LayerNorm for stabilizing training. DLinear's implementation doesn't show explicit LayerNorm in the provided code, but Crossformer explicitly uses nn.LayerNorm in both TSA layers and decoder layers.\n\n4. **Multi-Step Direct Forecasting**: Both models predict the entire future horizon directly rather than autoregressively, avoiding error accumulation in long-term forecasting.",
    "differences": "1. **Core Architecture Philosophy**: DLinear advocates for extreme simplicity, arguing that Transformers are ineffective for time series due to permutation-invariance and lack of temporal awareness. It uses only linear layers (one-layer MLPs) on decomposed components with O(1) complexity per channel. Crossformer, conversely, is a full Transformer variant that explicitly captures both cross-time and cross-dimension dependencies through specialized attention mechanisms with O(DL²) complexity in cross-time stage.\n\n2. **Channel Handling Strategy**: DLinear treats each channel (variate) independently with optional weight sharing (individual=True/False flag), explicitly avoiding spatial correlation modeling. Crossformer explicitly models cross-dimension dependency through its Two-Stage Attention, where the cross-dimension stage uses a router mechanism to capture inter-variate relationships with O(cD) complexity, where c is the number of routers.\n\n3. **Attention Mechanism**: DLinear completely eliminates attention mechanisms, using only weighted sums via linear transformations (W ∈ R^(T×L)). Crossformer employs a novel Two-Stage Attention (TSA) with separate cross-time MSA (applied per dimension) and cross-dimension attention (using router mechanism), plus hierarchical multi-scale processing through SegMerging layers.\n\n4. **Input Embedding and Segmentation**: DLinear operates on raw time series values with simple permutation for temporal axis processing. Crossformer introduces Dimension-Segment-Wise (DSW) embedding, dividing each dimension into segments of length L_seg, embedding each segment into a vector with learnable position encoding, creating a 2D array structure (time × dimension) for processing.\n\n5. **Multi-Scale Hierarchical Processing**: DLinear has no multi-scale mechanism, operating at a single temporal resolution. Crossformer implements a hierarchical encoder with multiple scale_blocks, where SegMerging layers progressively reduce temporal resolution by merging adjacent segments (win_size parameter), enabling coarse-to-fine information flow across scales.\n\n6. **Computational Complexity**: DLinear achieves O(L) complexity per channel with minimal parameters (just weight matrices for seasonal and trend linear layers). Crossformer has O(DL² + cD) complexity per TSA layer, where the cross-time stage is O(DL²) and cross-dimension stage is O(cD) through the router mechanism, significantly higher but designed to capture richer dependencies.\n\n7. **Decoder Design**: DLinear has no explicit decoder, directly outputting predictions through linear transformation of decomposed components. Crossformer uses a multi-layer decoder with self-attention and cross-attention mechanisms, where each DecoderLayer performs cross-attention between decoder queries and encoder outputs at different scales, aggregating layer predictions for final output.\n\n8. **Initialization Strategy**: DLinear initializes linear layer weights uniformly as (1/seq_len) * ones, creating an initial moving average effect. Crossformer uses standard PyTorch initialization for its various components (linear projections, attention weights, position embeddings)."
  },
  {
    "source": "FEDformer_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both adopt a hierarchical encoder-decoder framework with multi-layer structures. FEDformer uses N encoder layers and M decoder layers with seasonal-trend decomposition at each layer, while Crossformer uses multi-scale encoder blocks with corresponding decoder layers.\n\n2. **Frequency Domain Processing**: Both leverage frequency domain transformations for enhanced temporal modeling. FEDformer uses Discrete Fourier Transform (DFT) with mode selection to reduce complexity from O(N²) to O(N), while Crossformer implicitly benefits from frequency properties through its segment-based attention mechanism.\n\n3. **Decomposition Strategy**: Both employ decomposition mechanisms to separate time series components. FEDformer uses Mixture of Experts Decomposition (MODEcomp) to extract seasonal and trend components at each layer, accumulating trends progressively. Crossformer uses SegMerging to hierarchically aggregate temporal segments across scales.\n\n4. **Multi-Head Attention**: Both utilize multi-head attention mechanisms for capturing dependencies. FEDformer's FourierBlock uses multi-head structure with parameterized kernels R ∈ ℂ^(D×D×M) for frequency domain operations, while Crossformer's Two-Stage Attention applies MSA separately for cross-time and cross-dimension stages.\n\n5. **Positional Encoding**: Both incorporate positional information into embeddings. FEDformer uses standard positional embeddings added to input sequences, while Crossformer uses learnable position embeddings E^(pos)_(i,d) for each segment position (i,d) in the 2D array.\n\n6. **LayerNorm and Residual Connections**: Both employ layer normalization and residual connections following the standard Transformer design pattern to stabilize training and enable deep architectures.",
    "differences": "1. **Core Innovation Focus**: FEDformer focuses on frequency-enhanced operations through Fourier/Wavelet transforms with mode selection, performing representation learning in frequency domain to capture global temporal patterns. Crossformer innovates on explicitly modeling cross-dimension dependency through Dimension-Segment-Wise (DSW) embedding, treating MTS as a 2D array where each vector represents a univariate segment rather than multi-dimensional points at single timesteps.\n\n2. **Embedding Strategy**: FEDformer embeds each timestep across all dimensions into a single vector (x_t → h_t, where x_t ∈ ℝ^D), resulting in T embedded vectors, following traditional Transformer approaches. Crossformer proposes DSW embedding that divides each dimension into segments of length L_seg and embeds them separately (x^(s)_(i,d) → h_(i,d)), creating a 2D array of T/L_seg × D vectors, explicitly preserving dimension structure.\n\n3. **Attention Mechanism Design**: FEDformer replaces self-attention with Frequency Enhanced Block (FEB-f) that operates in frequency domain: performs FFT, selects M modes randomly, applies learnable complex-valued kernels, and performs inverse FFT with O(N) complexity. Crossformer uses Two-Stage Attention (TSA) with sequential processing: first applies MSA along time dimension for each dimension separately (cross-time stage), then uses router mechanism with c routers for cross-dimension dependency, achieving O(DL²) + O(2cD) = O(DL²) complexity.\n\n4. **Cross-Attention Strategy**: FEDformer's FourierCrossAttention (FEA) performs frequency domain cross-attention between encoder and decoder, using different mode indices for queries (index_q) and keys/values (index_kv), with complex multiplication in frequency space. Crossformer's decoder uses standard spatial attention between decoder queries and encoder outputs after rearranging dimensions, without frequency domain operations.\n\n5. **Multi-Scale Modeling Approach**: FEDformer achieves multi-scale through frequency mode selection, where different frequency components naturally represent different temporal scales, but processes all scales simultaneously in frequency domain. Crossformer explicitly constructs hierarchical scales through SegMerging layers that progressively merge win_size segments, creating a pyramid structure with multiple scale blocks processing different temporal resolutions sequentially.\n\n6. **Decomposition Mechanism**: FEDformer uses MODEcomp (Mixture of Experts Decomposition) with multiple moving average kernels of different sizes, extracting both seasonal and trend components at each encoder/decoder layer, with trends accumulated via weighted sum (T^l_de = T^(l-1)_de + W_(l,1)·T^(l,1)_de + ...). Crossformer does not employ explicit seasonal-trend decomposition; instead relies on hierarchical segment merging for implicit multi-resolution representation.\n\n7. **Channel Handling Philosophy**: FEDformer treats all dimensions jointly at each timestep through its embedding (implicitly mixing channels in the embedding space), with frequency operations applied uniformly across the embedded representation. Crossformer explicitly separates dimensions through DSW embedding and processes them with dimension-specific operations in cross-time stage, then aggregates cross-dimension information through router mechanism, maintaining clearer dimension boundaries.\n\n8. **Prediction Output Strategy**: FEDformer outputs final prediction as sum of refined seasonal component (W_S · X^M_de) and accumulated trend component (T^M_de), explicitly combining decomposed components. Crossformer accumulates layer-wise predictions from each decoder layer (final_predict = Σ layer_predict), where each layer contributes a prediction through linear projection of segments, without explicit component decomposition in the output."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Multi-Resolution/Multi-Scale Architecture**: Both papers employ hierarchical structures to capture temporal dependencies at different scales. Pyraformer uses a C-ary tree pyramid with coarser-scale construction module (CSCM) via convolution layers, while Crossformer uses segment merging (SegMerging) to progressively aggregate segments. Both achieve O(L) complexity through hierarchical processing rather than full attention.\n\n2. **Encoder-Decoder Framework**: Both adopt encoder-decoder architectures for forecasting. Pyraformer's encoder builds pyramidal representations and decoder generates predictions, while Crossformer's hierarchical encoder-decoder (HED) processes multiple scales with cross-attention between encoder and decoder layers.\n\n3. **Segmentation-Based Input Processing**: Both divide time series into segments before attention computation. Pyraformer constructs multi-resolution nodes where coarser scales summarize C nodes from finer scales. Crossformer uses Dimension-Segment-Wise (DSW) embedding with fixed segment length L_seg. This segmentation reduces computational complexity and captures local patterns.\n\n4. **Position Embedding**: Both use learnable position embeddings added to input representations. Pyraformer embeds positions separately and adds to data embeddings. Crossformer's DSW embedding includes position embedding E^(pos)_{i,d} for each segment position (i,d).\n\n5. **Layer Normalization and Residual Connections**: Both employ standard Transformer components with LayerNorm and residual connections in their attention layers for training stability.",
    "differences": "1. **Core Innovation Focus**: Pyraformer addresses long-range temporal dependency through pyramidal attention with O(L) complexity via a C-ary tree structure connecting nodes across scales (inter-scale) and within scales (intra-scale). Crossformer innovates by explicitly capturing cross-dimension dependency through Two-Stage Attention (TSA), separating cross-time and cross-dimension modeling, arguing that previous Transformers only implicitly handle dimension relationships.\n\n2. **Attention Mechanism Design**: Pyraformer uses Pyramidal Attention Module (PAM) where each node attends to neighbors at three scales: adjacent A nodes at same scale (A_ℓ^(s)), C children (C_ℓ^(s)), and parent (P_ℓ^(s)) in the tree, with complexity O(AL). Crossformer employs Two-Stage Attention: first applies MSA independently to each dimension for cross-time dependency (complexity O(DL²)), then uses router mechanism for cross-dimension stage (complexity O(cD) where c is fixed number of routers), achieving total O(DL²+cDL).\n\n3. **Dimension Handling Strategy**: Pyraformer embeds all dimensions at the same time step into a single vector (x_t → h_t), treating MTS as univariate-like with dimension information embedded implicitly. Crossformer explicitly preserves dimension structure through DSW embedding where each segment x^(s)_{i,d} represents a univariate segment, creating a 2D array (time × dimension) to explicitly model cross-dimension dependency.\n\n4. **Hierarchical Construction Method**: Pyraformer builds hierarchy through Bottleneck_Construct with ConvLayer applying stride convolution (kernel_size=window_size, stride=window_size) with BatchNorm and ELU activation, using bottleneck dimension d_model/4 for efficiency. Crossformer uses SegMerging that concatenates win_size consecutive segments and applies linear transformation with LayerNorm, merging segments without dimensionality reduction during merging.\n\n5. **Decoder Prediction Strategy**: Pyraformer's decoder generates predictions through attention layers followed by projection. Crossformer's decoder uses cross-attention between decoder queries and multi-scale encoder outputs, with each decoder layer producing layer_predict via linear_pred, and final prediction is the sum of all layer predictions, enabling multi-scale fusion.\n\n6. **Complexity Trade-offs**: Pyraformer achieves O(L) space and time complexity with maximum signal traversing path O(log_C L) through sparse pyramidal connections. Crossformer has O(DL²) complexity for cross-time stage but argues this is acceptable for capturing full temporal dependency per dimension, while reducing cross-dimension complexity to O(cD) through router mechanism, prioritizing explicit dimension modeling over pure efficiency."
  },
  {
    "source": "Autoformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both adopt Transformer-based encoder-decoder frameworks for long-term time series forecasting, where encoders process historical sequences and decoders generate future predictions.\n\n2. **Embedding Strategy**: Both use specialized embedding modules including positional embeddings (PositionalEmbedding class) and value embeddings. The source implementation uses DataEmbedding_wo_pos with TokenEmbedding (1D convolution with kernel_size=3), while Crossformer uses PatchEmbedding with linear projection.\n\n3. **Layer Normalization**: Both apply LayerNorm after attention and feedforward operations following standard Transformer design. The source implementation shows LayerNorm is applied in the decomposition blocks (SeriesDecomp) and after each sub-layer.\n\n4. **Multi-Head Attention**: Both utilize multi-head attention mechanisms to capture dependencies, though applied differently (Autoformer uses Auto-Correlation while Crossformer uses standard MSA in two stages).\n\n5. **Residual Connections**: Both employ residual connections around attention and feedforward layers, as evidenced in the source code's encoder/decoder layer implementations and Crossformer's TSA layer design.\n\n6. **Progressive Refinement**: Both use multi-layer stacking (N encoder layers, M decoder layers) to progressively refine representations and predictions through hierarchical processing.",
    "differences": "1. **Core Innovation**: Autoformer introduces Auto-Correlation mechanism for period-based dependency discovery using FFT-based correlation and series decomposition as an inner block; Crossformer proposes Dimension-Segment-Wise (DSW) embedding and Two-Stage Attention (TSA) to explicitly capture cross-dimension dependency.\n\n2. **Decomposition Strategy**: Autoformer uses series decomposition (moving average with AvgPool) as a built-in block throughout encoder/decoder to separate trend and seasonal components progressively; Crossformer does not use decomposition, instead focusing on segment-wise processing.\n\n3. **Attention Mechanism**: Autoformer replaces self-attention with Auto-Correlation that performs time delay aggregation based on autocorrelation in frequency domain (O(L log L) via FFT); Crossformer uses standard multi-head self-attention but applies it in two stages (cross-time then cross-dimension) with router mechanism for efficiency.\n\n4. **Input Embedding Approach**: Autoformer embeds all dimensions at each time step into a single vector (time-wise embedding: x_t → h_t where x_t ∈ R^D), implicitly handling cross-dimension dependency; Crossformer uses DSW embedding that segments each dimension separately (x_{i,d}^(s) → h_{i,d}), creating a 2D array to explicitly model both time and dimension axes.\n\n5. **Multi-scale Modeling**: Autoformer does not explicitly use multi-scale processing; Crossformer implements hierarchical encoder-decoder with SegMerging layers that progressively merge segments (scale_block with different win_sizes) to capture patterns at multiple temporal scales.\n\n6. **Channel Strategy**: Autoformer treats all dimensions together at each time step with channel-mixed embedding; Crossformer treats dimensions independently in cross-time stage (MSA applied per dimension with shared weights) then captures cross-dimension dependency in separate stage with router mechanism (O(cD) complexity where c is fixed number of routers).\n\n7. **Decoder Initialization**: Autoformer initializes decoder with decomposed seasonal part (zeros) and trend part (mean values) from the latter half of encoder input; Crossformer uses learned queries and progressively accumulates predictions from multiple decoder layers.\n\n8. **Complexity Trade-offs**: Autoformer achieves O(L log L) complexity through FFT-based Auto-Correlation; Crossformer achieves O(DL² + cDL) = O(DL²) complexity but with explicit cross-dimension modeling through two-stage design and router mechanism to avoid O(D²L²) full cross-attention."
  },
  {
    "source": "Informer_2020",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both papers adopt the Transformer-based encoder-decoder framework for long sequence time-series forecasting, with multi-layer stacking to capture hierarchical temporal dependencies.\n2. **Multi-Head Attention Mechanism**: Both utilize multi-head self-attention as the core component for dependency modeling, though applied differently (Informer uses ProbSparse attention; Crossformer uses Two-Stage Attention).\n3. **Layer Normalization and Residual Connections**: Both employ standard Transformer components including LayerNorm and residual connections in their attention layers and feed-forward networks.\n4. **Position Embedding**: Both incorporate positional information into their embeddings, though Crossformer uses learnable 2D position embeddings while Informer uses standard sinusoidal positional encoding.\n5. **Feed-Forward Networks**: Both use MLP/FFN layers after attention mechanisms with similar structure (linear-activation-linear with residual connections and normalization).",
    "differences": "1. **Core Innovation - Attention Mechanism**: Informer proposes ProbSparse self-attention with O(L log L) complexity by selecting top-u dominant queries based on sparsity measurement M(q,K), using random sampling for efficiency. Crossformer introduces Two-Stage Attention (TSA) that separates cross-time and cross-dimension stages, applying attention independently along each axis with O(DL²) complexity for cross-time and O(cD) for cross-dimension (using router mechanism).\n2. **Embedding Strategy**: Informer embeds all dimensions at each time step into a single vector (point-wise: x_t → h_t, where x_t ∈ R^D), implicitly handling cross-dimension dependency. Crossformer proposes Dimension-Segment-Wise (DSW) embedding that segments each dimension independently and embeds each segment (x_{i,d}^(s) → h_{i,d}), explicitly preserving dimension information in a 2D array structure.\n3. **Cross-Dimension Dependency Modeling**: Informer does not explicitly model cross-dimension dependency, relying on implicit mixing through embedding. Crossformer explicitly captures cross-dimension dependency through the router mechanism in the cross-dimension stage, where c fixed routers (c << D) gather and distribute information across D dimensions, reducing complexity from O(D²) to O(cD).\n4. **Hierarchical Multi-Scale Processing**: Informer uses a distilling operation with ConvLayer (1D convolution + max pooling) to progressively reduce sequence length in the encoder by half at each layer, focusing on dominant features. Crossformer employs SegMerging that concatenates win_size consecutive segments and projects them to maintain d_model dimension, creating a hierarchical encoder-decoder structure with multiple scales.\n5. **Decoder Design**: Informer uses a standard Transformer decoder with masked self-attention on decoder inputs and cross-attention with encoder outputs, generating predictions autoregressively. Crossformer's decoder processes 2D arrays with self-attention on decoder segments, cross-attention with multi-scale encoder outputs, and each decoder layer produces predictions via linear projection (linear_pred) that are accumulated across layers for final output.\n6. **Computational Complexity Focus**: Informer targets reducing the quadratic complexity O(L²) of standard self-attention to O(L log L) for long sequences through sparse query selection. Crossformer addresses the challenge of large dimension D in multivariate series, reducing cross-dimension complexity from O(D²) to O(D) while maintaining O(L²) complexity per dimension in cross-time stage.\n7. **Input Sequence Handling**: Informer processes the full input sequence with gradually reduced temporal resolution through distilling layers. Crossformer segments input into fixed-length patches (L_seg) creating a 2D structure (time × dimension), treating each segment as an independent token for attention computation."
  },
  {
    "source": "Autoformer_2021",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture with Decomposition**: Both papers adopt an encoder-decoder Transformer architecture with built-in decomposition blocks. Autoformer uses series_decomp (moving average-based trend-seasonal decomposition) while ETSformer extracts level-growth-seasonal components. The basic encoder-decoder structure from Autoformer (Encoder/Decoder classes, layer stacking, residual connections) can be directly reused as the backbone.\n\n2. **Progressive Component Extraction**: Both models progressively refine components across multiple layers. Autoformer's encoder extracts seasonal parts via SeriesDecomp at each layer (Equation 3), and decoder accumulates trend components (Equation 4). ETSformer similarly extracts growth/seasonal representations layer-by-layer and accumulates them for final prediction (Equation 3). The multi-layer progressive extraction pattern and residual learning framework can be adapted.\n\n3. **FFT-based Efficient Computation**: Both leverage Fast Fourier Transform for O(L log L) complexity. Autoformer uses FFT in Auto-Correlation mechanism (Equation 8, Wiener-Khinchin theorem) to compute autocorrelation efficiently. ETSformer uses FFT in Frequency Attention (Equation 4) for periodicity detection and seasonal extraction. The FFT computation infrastructure (torch.fft.rfft/irfft) can be directly reused.\n\n4. **Series-wise Rather Than Point-wise Operations**: Both move away from point-wise dot-product attention. Autoformer's Auto-Correlation aggregates similar sub-series from underlying periods via time delay aggregation. ETSformer's ESA assigns weights based on time lag (not content), and FA operates on frequency components. This series-level processing philosophy is shared.\n\n5. **Input Embedding without Positional Encoding**: Both use convolutional token embedding (TokenEmbedding with Conv1d kernel_size=3) and do NOT rely on traditional positional encoding. Autoformer uses DataEmbedding_wo_pos, ETSformer explicitly states no manually designed time-dependent covariates. The Conv1d-based embedding module can be reused.\n\n6. **Layer Normalization and Feed-Forward Networks**: Both use LayerNorm and position-wise feed-forward networks (two linear layers with activation) in their encoder/decoder layers. Autoformer's EncoderLayer/DecoderLayer structure with LN and FF blocks can serve as templates for ETSformer's layer design.\n\n7. **Multi-Head Mechanism**: Both employ multi-head versions of their attention mechanisms. Autoformer's MultiHead Auto-Correlation (Equation 7) splits channels across heads and concatenates outputs. ETSformer's MH-ESA follows similar multi-head design. The multi-head splitting/concatenation logic can be adapted.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Autoformer uses Auto-Correlation (period-based dependencies via autocorrelation + time delay aggregation with Roll operation), while ETSformer introduces two completely new mechanisms: (a) **Exponential Smoothing Attention (ESA)**: Non-adaptive attention with exponential decay weights based on time lag (Algorithm 1, O(L log L) via cross-correlation), requires implementing the efficient ESA algorithm with FFT-based cross-correlation; (b) **Frequency Attention (FA)**: Selects top-K frequency components by amplitude (Equation 4), extrapolates seasonal patterns to forecast horizon, requires implementing DFT-based amplitude selection and inverse DFT reconstruction. Neither mechanism exists in Autoformer.\n\n2. **Decomposition Philosophy and Components - NEW IMPLEMENTATION REQUIRED**: Autoformer decomposes into trend-seasonal (2 components) using moving average (series_decomp with AvgPool). ETSformer decomposes into level-growth-seasonal (3 components) inspired by exponential smoothing theory. Need to implement: (a) **Level Module**: Exponential smoothing-based level extraction (similar to Equation 1 in paper) with learnable smoothing parameter α, processes adjusted level as weighted average of de-seasonalized current level and previous level-growth forecast; (b) **Growth Damping (GD)**: Trend damping with learnable damping parameter γ (Equation in 4.1.3), multi-head version with multiple γ parameters; (c) Remove Autoformer's moving average decomposition, replace with ESA-based growth extraction and FA-based seasonal extraction.\n\n3. **Decoder Input and Initialization Strategy - DIFFERENT APPROACH**: Autoformer initializes decoder with decomposed components from latter half of encoder input (I/2 length) plus placeholders (zeros for seasonal, mean for trend) - see Equation 2. ETSformer's decoder does NOT take input sequences; instead, it: (a) Repeats the last level E_t^(N) across forecast horizon; (b) Uses Growth Damping to project last growth B_t^(n) to forecast horizon; (c) Uses FA to extrapolate seasonal patterns from lookback to forecast horizon. Need to implement this fundamentally different decoder initialization and forecasting strategy.\n\n4. **Component Aggregation and Final Prediction - NEW IMPLEMENTATION REQUIRED**: Autoformer's final prediction is simple addition of refined seasonal and accumulated trend (W_S * X_de^M + T_de^M). ETSformer's final forecast (Equation 3) is: level + Linear(sum of all layers' growth and seasonal representations), where Linear projects from latent space (d) to observation space (m). Need to implement: (a) Summation across all N encoder layers of growth/seasonal representations; (b) Linear projection from latent to observation space; (c) Addition of level forecast (which is simply repeated last level).\n\n5. **Encoder Layer Structure - MODIFIED IMPLEMENTATION**: Autoformer's encoder: Auto-Correlation → SeriesDecomp → FeedForward → SeriesDecomp (Equation 3). ETSformer's encoder: FA (seasonal extraction) → MH-ESA (growth extraction) → residual updates with LN → FeedForward with LN. Key differences requiring new implementation: (a) Sequential seasonal then growth extraction (not parallel); (b) Explicit residual subtraction (Z := Z - S, then Z := LN(Z - B)) before feedforward; (c) Different decomposition timing and normalization placement.\n\n6. **Decoder Layer Structure - COMPLETELY DIFFERENT**: Autoformer decoder has: self Auto-Correlation → SeriesDecomp → cross Auto-Correlation → SeriesDecomp → FeedForward → SeriesDecomp, with trend accumulation from all three decomp blocks (Equation 4). ETSformer decoder has NO self/cross-attention layers, only: N stacks of (Growth Damping + Frequency Attention) + one Level Stack. This is a fundamentally different architecture requiring complete reimplementation.\n\n7. **Frequency Domain Operations - DIFFERENT USAGE**: While both use FFT, purposes differ: Autoformer uses FFT for computing autocorrelation in time domain (Wiener-Khinchin theorem, Equation 8) to find period-based dependencies. ETSformer uses FFT for: (a) Frequency Attention: decompose to frequency domain, select top-K amplitudes, reconstruct seasonal patterns (Equation 4); (b) ESA: FFT-based cross-correlation for efficient exponential smoothing (Algorithm 1). Need to implement amplitude-based selection and reconstruction logic not present in Autoformer.\n\n8. **Learnable Parameters - NEW PARAMETER TYPES**: Autoformer learns: projection matrices (W), moving average kernel (implicitly via AvgPool). ETSformer introduces new learnable parameters: (a) Smoothing parameter α (0<α<1) in Level Module and ESA; (b) Damping parameter γ (0<γ<1) in Growth Damping, with multi-head version; (c) Initial state v_0 in ESA mechanism. Need to implement proper initialization and constraints (0-1 range) for these parameters.\n\n9. **No Cross-Attention Between Encoder-Decoder - ARCHITECTURAL DIFFERENCE**: Autoformer has encoder-decoder Auto-Correlation where decoder queries attend to encoder outputs (Equation 6, K,V from encoder). ETSformer has NO cross-attention; encoder and decoder are decoupled - encoder extracts components from lookback, decoder independently projects them to forecast horizon using GD and FA extrapolation. This eliminates the need for cross-attention implementation."
  },
  {
    "source": "Informer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both papers adopt the encoder-decoder paradigm for time series forecasting. The Informer's encoder-decoder structure (with multi-layer stacking and attention mechanisms) provides a foundational template that can be adapted for ETSformer. The basic encoder/decoder layer structure with residual connections and layer normalization from Informer's code can be reused.\n\n2. **Multi-Layer Stacking with Residual Learning**: Both models use multiple encoder layers to progressively extract features from input sequences. Informer's EncoderLayer implementation with residual connections and layer normalization (LN) can be directly adapted for ETSformer's cascaded growth and seasonal extraction, as both follow the pattern: extract features → update residual → apply normalization.\n\n3. **Embedding Module**: Both use embedding modules to map raw input sequences to latent space. Informer's DataEmbedding class (combining value embedding with temporal encodings) can serve as a starting point, though ETSformer simplifies this by using only Conv1d without additional temporal features. The convolutional embedding approach in Informer's ConvLayer can be adapted for ETSformer's input embedding.\n\n4. **Feedforward Networks**: Both architectures employ position-wise feedforward networks (FFN) after attention mechanisms. Informer's Conv1d-based FFN implementation in EncoderLayer (conv1, conv2 with activation) can be directly reused for ETSformer's FF module, though ETSformer uses sigmoid activation instead of ELU.\n\n5. **Decoder Input Strategy**: Both use a generative-style decoder that takes a concatenation of known sequence (start token/lookback) and placeholder for prediction horizon. Informer's decoder input construction (Concat(X_token, X_0)) closely mirrors ETSformer's approach of using lookback window information to generate forecasts.\n\n6. **Layer Normalization**: Both extensively use LayerNorm for stabilizing training. Informer's norm_layer implementations in encoder/decoder can be directly reused in ETSformer's residual update steps (LN operations after seasonal/growth extraction).\n\n7. **Multi-Head Mechanism**: Both employ multi-head attention variants. Informer's multi-head structure (query/key/value projections, head splitting, output projection) provides the architectural template for ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA), though the attention computation itself differs.\n\n8. **O(L log L) Complexity Goal**: Both papers aim to reduce computational complexity from O(L²) to O(L log L). Informer achieves this through ProbSparse attention with selective query sampling, while ETSformer uses FFT-based operations. The efficiency-focused design philosophy is shared, though implementation differs.",
    "differences": "1. **Attention Mechanism - CORE INNOVATION**: Informer uses ProbSparse self-attention with query sparsity measurement (KL-divergence based, selecting Top-u queries via max-mean measurement M(q,K)) to achieve O(L log L) complexity. ETSformer introduces two completely NEW attention types: (a) Exponential Smoothing Attention (ESA) - non-adaptive, time-lag based attention with exponential decay weights (α(1-α)^j), implemented via FFT cross-correlation; (b) Frequency Attention (FA) - non-learnable, DFT-based attention selecting Top-K frequency components. **NEW IMPLEMENTATION REQUIRED**: ESA algorithm with FFT-based efficient computation (Algorithm 1), FA module with DFT/inverse DFT and Top-K frequency selection (Equation 4), neither of which exists in Informer.\n\n2. **Time Series Decomposition Strategy**: Informer does NOT explicitly decompose time series into components - it uses distilling operations (MaxPool + Conv1d) to progressively reduce sequence length and extract hierarchical features. ETSformer explicitly decomposes into Level-Growth-Seasonal components inspired by exponential smoothing methods, with dedicated modules for each: Level module (exponential smoothing equation with learnable α), Growth extraction (via MH-ESA on successive differences), Seasonal extraction (via FA on Fourier domain). **NEW IMPLEMENTATION REQUIRED**: Level smoothing module with recurrent exponential smoothing equation, Growth extraction via differencing, Seasonal pattern extraction via Fourier analysis - none of these decomposition components exist in Informer.\n\n3. **Encoder Design Philosophy**: Informer's encoder uses self-attention distilling with ConvLayer (Conv1d + MaxPool) to create a pyramid structure that progressively halves sequence length across layers, focusing on capturing long-range dependencies efficiently. ETSformer's encoder iteratively extracts growth/seasonal from residuals without length reduction, using residual learning (Z := Z - S, Z := Z - B) to progressively decompose signals. **NEW IMPLEMENTATION REQUIRED**: Residual-based decomposition logic where seasonal and growth are sequentially removed from residual at each layer, maintaining full sequence length throughout encoder.\n\n4. **Decoder Forecasting Strategy**: Informer's decoder uses masked multi-head attention with cross-attention to encoder outputs, generating predictions via standard Transformer decoder with attention to both past tokens and encoder memory. ETSformer's decoder has specialized stacks: (a) Growth+Seasonal (G+S) Stacks with Growth Damping (TD) using learnable damping parameter γ for multi-step extrapolation; (b) Level Stack simply repeating last level; (c) Final forecast as additive composition: X̂ = E + Linear(Σ(B + S)). **NEW IMPLEMENTATION REQUIRED**: Growth Damping module (TD) with learnable γ parameter and cumulative damping formula, Level Stack with repeat operation, compositional forecast aggregation logic.\n\n5. **Input Feature Requirements**: Informer relies on rich temporal covariates (positional encodings, time features like month-of-year, day-of-week) embedded via DataEmbedding module, using both value and temporal information. ETSformer deliberately avoids manual temporal features, using only raw values with simple Conv1d embedding, as FA automatically uncovers seasonal patterns through Fourier analysis. **NEW IMPLEMENTATION REQUIRED**: Remove temporal covariate dependencies, implement pure value-based embedding with Conv1d (kernel=3), ensure FA module can operate without external time features.\n\n6. **Frequency Domain Processing**: Informer operates entirely in time domain with no frequency analysis. ETSformer extensively uses frequency domain via DFT for seasonal pattern extraction in FA module: computing amplitude/phase (A_k,i, Φ_k,i), selecting Top-K frequencies by amplitude, reconstructing seasonal signals via inverse Fourier synthesis with conjugate pairs. **NEW IMPLEMENTATION REQUIRED**: Full frequency domain pipeline including DFT computation (torch.fft), amplitude/phase extraction, Top-K frequency selection logic, inverse DFT reconstruction for both lookback and forecast horizons.\n\n7. **Multi-Scale Temporal Modeling**: Informer uses distilling layers with progressive downsampling (stride-2 MaxPool) creating a multi-scale pyramid, with replica stacks processing halved inputs at different scales, concatenating outputs. ETSformer does NOT use multi-scale processing - all encoder layers operate at full resolution, with multi-scale patterns captured implicitly through stacked growth/seasonal extraction and frequency domain analysis. **NEW IMPLEMENTATION REQUIRED**: Remove distilling/downsampling logic, implement full-resolution processing throughout all layers.\n\n8. **Interpretability and Component Transparency**: Informer produces end-to-end learned representations without explicit component interpretation - the model is a black box mapping inputs to forecasts. ETSformer provides human-interpretable decomposition with explicit level (E), growth (B), and seasonal (S) components at each layer, making the forecast formation transparent via compositional equation. **NEW IMPLEMENTATION REQUIRED**: Component tracking and storage across layers, compositional forecast assembly with explicit level/growth/seasonal contributions, potential visualization of decomposed components."
  },
  {
    "source": "Reformer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention via Frequency Domain Operations**: Both papers leverage frequency-domain transformations to achieve O(L log L) complexity. Reformer uses LSH with bucketing/sorting operations that can be optimized via FFT-like operations, while ETSformer explicitly uses FFT in its Frequency Attention (FA) module. The source code's FFT-based attention computation patterns (particularly in handling sequence transformations) can be adapted for ETSformer's FA implementation, especially the batching and dimension handling logic.\n\n2. **Multi-Head Attention Architecture**: Both employ multi-head attention mechanisms to capture diverse patterns. Reformer's `LSHSelfAttention` with configurable `heads` parameter and ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA) share the fundamental structure of splitting representations across multiple heads. The source code's head splitting/concatenation logic in `ReformerLayer` can be directly reused for implementing ETSformer's multi-head mechanisms.\n\n3. **Encoder-Based Architecture with Layer Stacking**: Both use stacked encoder layers for progressive feature extraction. Reformer's `Encoder` with multiple `EncoderLayer` instances and ETSformer's N-stack encoder architecture follow similar hierarchical processing. The source code's `Encoder` class structure, including layer normalization and residual connections, provides a reusable template for ETSformer's encoder implementation.\n\n4. **Embedding Layer for Input Processing**: Both papers use embedding layers to project raw inputs into latent space. Reformer's `DataEmbedding` (with temporal/positional encoding) and ETSformer's Conv-based input embedding share the goal of creating rich initial representations. The source code's embedding infrastructure can be adapted, though ETSformer uses Conv instead of linear projection.\n\n5. **Normalization and Residual Connections**: Both architectures employ layer normalization (LN) and residual connections for stable training. The source code's `norm_layer=torch.nn.LayerNorm(configs.d_model)` and residual addition patterns in `EncoderLayer` can be directly reused in ETSformer's encoder/decoder layers.\n\n6. **Sequence Length Handling with Padding**: Reformer's `fit_length` method pads sequences to match bucket size requirements (must be divisible by bucket_size * 2). ETSformer may need similar padding logic for FFT operations (which work best with power-of-2 lengths). The source code's padding implementation can be adapted for ETSformer's frequency-domain operations.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses Locality-Sensitive Hashing (LSH) attention with random projections, hash bucketing, and sorted chunk-based attention to approximate full attention. ETSformer introduces two completely novel attention types: (a) **Exponential Smoothing Attention (ESA)** - a non-adaptive attention with learnable exponential decay weights based on time lag (α parameter), implemented via efficient cross-correlation/FFT (Algorithm 1), and (b) **Frequency Attention (FA)** - selects top-K frequency components via DFT, extracts amplitude/phase, and reconstructs seasonal patterns via inverse DFT. Both ESA and FA require complete new implementations with no direct code reuse from Reformer's LSH attention.\n\n2. **Time Series Decomposition Philosophy - NEW COMPONENTS NEEDED**: Reformer has no explicit decomposition strategy, treating sequences as generic tokens. ETSformer fundamentally decomposes time series into **Level**, **Growth (Trend)**, and **Seasonal** components inspired by exponential smoothing methods. This requires implementing: (a) **Level Module** with learnable smoothing parameter α and recurrent exponential smoothing equation, (b) **Growth Damping (GD)** module with learnable damping parameter γ for trend extrapolation, (c) **Residual learning** where seasonal and growth are iteratively extracted and subtracted from residuals across layers. These decomposition modules have no counterpart in Reformer.\n\n3. **Decoder Architecture and Forecasting Strategy - NEW STRUCTURE REQUIRED**: Reformer uses a simple linear projection layer for forecasting (`self.projection = nn.Linear(configs.d_model, configs.c_out)`), treating it as a sequence-to-sequence mapping. ETSformer employs a sophisticated **encoder-decoder architecture** with: (a) **N Growth+Seasonal (G+S) Stacks** in decoder using GD and FA modules, (b) **Level Stack** that repeats last level across forecast horizon, (c) **Compositional forecast** combining level, growth, and seasonal components (Equation 3). The decoder requires implementing G+S stacks, level repetition logic, and component composition, none of which exist in Reformer.\n\n4. **Input Feature Engineering Philosophy - DIFFERENT APPROACH**: Reformer's implementation uses `DataEmbedding` which typically incorporates temporal features (month, day-of-week via `configs.freq`), positional encodings, and value embeddings. ETSformer explicitly avoids manual temporal covariates, relying solely on a temporal convolutional filter (kernel size 3) for input embedding, as FA automatically uncovers seasonal patterns. This requires removing temporal feature engineering and implementing a simple Conv1d-based embedding layer.\n\n5. **Attention Weight Computation - FUNDAMENTALLY DIFFERENT**: Reformer's LSH attention weights are **content-based and adaptive** - computed via dot-product similarity between queries/keys after hashing (Q·K^T in same hash bucket). ETSformer's ESA weights are **non-adaptive and purely time-lag based** - determined solely by relative temporal distance with exponential decay, independent of input content. FA is **non-learnable and frequency-based** - weights determined by amplitude magnitudes in frequency domain. This fundamental difference requires completely different attention weight computation logic.\n\n6. **Complexity and Efficiency Trade-offs - DIFFERENT MECHANISMS**: While both achieve O(L log L), the mechanisms differ: Reformer achieves this via LSH bucketing (reducing attention to local chunks, with bucket_size and n_hashes hyperparameters controlling approximation quality). ETSformer achieves this via: (a) FFT-based cross-correlation for ESA (exact computation, not approximation), (b) DFT with top-K frequency selection for FA (K hyperparameter controls seasonal complexity). Reformer's `bucket_size=4, n_hashes=4` parameters have no equivalent in ETSformer, which instead uses `K` (number of frequency components) and damping parameters.\n\n7. **Reversibility and Memory Optimization - NOT APPLICABLE**: Reformer extensively discusses reversible layers (RevNet) and chunking strategies to reduce memory from O(b·l·d_ff·n_l) to O(b·l·d_model), critical for very long sequences (64K tokens). ETSformer does not employ reversible architectures or explicit chunking, as typical forecasting tasks use shorter lookback windows (96-720 steps). The reversible transformer components in Reformer are not needed for ETSformer implementation.\n\n8. **Shared QK Constraint - DIFFERENT REQUIREMENTS**: Reformer requires Q=K (shared-QK Transformer) for LSH to work, using the same linear layer for both and normalizing key lengths. ETSformer's ESA and FA do not have this constraint - ESA operates directly on value sequences without explicit Q/K separation, and FA operates on Fourier-transformed signals. This eliminates the need for shared-QK implementation logic."
  },
  {
    "source": "Autoformer_2021",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Decomposition Architecture**: Both papers adopt a deep decomposition architecture that progressively separates seasonal and trend-cyclical components throughout the encoder-decoder structure. The encoder focuses on seasonal pattern modeling while the decoder accumulates trend components across layers.\n2. **Series Decomposition Block**: Both use series decomposition blocks as inner operations after each attention/transformation layer. Autoformer uses moving average (AvgPool with padding) while FEDformer uses MOEcomp (Mixture of Experts decomposition), but both follow the pattern: extract trend, compute seasonal = input - trend.\n3. **Decoder Initialization Strategy**: Both papers initialize the decoder with the same two-part structure: (1) decomposed components from the latter half of encoder input (length I/2) to provide recent information, (2) placeholders for future predictions (length O) - zeros for seasonal and mean values for trend.\n4. **Progressive Trend Accumulation**: Both decoders accumulate trend components extracted from hidden variables progressively across layers using weighted projections: T_de^l = T_de^(l-1) + W_l,1·T_de^l,1 + W_l,2·T_de^l,2 + W_l,3·T_de^l,3.\n5. **Encoder-Decoder Architecture**: Both use multi-layer encoder-decoder Transformer-style architectures with residual connections and feed-forward networks after each attention/transformation block, followed by decomposition.\n6. **Final Prediction Strategy**: Both combine refined seasonal and trend components for final prediction: W_S·X_de^M + T_de^M, where seasonal component is projected to target dimension.",
    "differences": "1. **Core Innovation**: Autoformer introduces Auto-Correlation mechanism for period-based dependency discovery using time-delay aggregation in time domain. FEDformer proposes Frequency Enhanced structures (FEB/FEA) that operate in frequency domain using Fourier/Wavelet transforms for global view and computational efficiency.\n2. **Attention Mechanism**: Autoformer uses Auto-Correlation with (1) period-based dependencies discovery via autocorrelation computation and (2) time delay aggregation by rolling and aggregating top-k correlated sub-series. FEDformer uses Fourier/Wavelet-based attention that transforms to frequency domain, applies learnable complex-valued weights to selected frequency modes, then transforms back to time domain.\n3. **Computational Complexity**: Autoformer's Auto-Correlation has O(L log L) complexity due to FFT-based autocorrelation computation but processes all time points. FEDformer achieves O(L) complexity by selecting only M<<L frequency modes before transformation, significantly reducing computation through mode selection.\n4. **Decomposition Method**: Autoformer uses simple moving average (AvgPool) with circular padding for trend extraction, which is a fixed time-domain operation. FEDformer introduces MOEcomp (Mixture of Experts Decomposition) with multiple decomposition kernels and learnable gating mechanism for adaptive decomposition.\n5. **Frequency Domain vs Time Domain**: Autoformer operates primarily in time domain - even though it uses FFT for autocorrelation computation, the actual representation learning and aggregation happen in time domain through rolling and weighted averaging. FEDformer performs representation learning directly in frequency domain with learnable complex-valued kernels applied to frequency modes.\n6. **Mode Selection Strategy**: Autoformer selects top-k time delays based on autocorrelation values dynamically during training/inference. FEDformer pre-selects frequency modes (either random or lowest modes) before training and keeps them fixed, using parameterized complex weights for these modes.\n7. **Cross-Attention Design**: Autoformer uses Auto-Correlation for encoder-decoder interaction, computing correlation between decoder queries and encoder outputs in time domain. FEDformer uses FEA (Frequency Enhanced Attention) that transforms queries and keys to frequency domain, applies attention mechanism on selected frequency modes, enabling more efficient cross-sequence interaction.\n8. **Multi-Resolution Capability**: Autoformer focuses on discovering multiple periodicities through top-k time delay selection but operates at single temporal resolution. FEDformer offers two versions (Fourier-based and Wavelet-based), where the Wavelet version naturally provides multi-resolution analysis through wavelet decomposition at different scales."
  },
  {
    "source": "Informer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both adopt the encoder-decoder framework with multi-layer stacking for long-term time series forecasting, where the encoder processes historical sequences and the decoder generates future predictions.\n2. **Data Embedding Strategy**: Both use DataEmbedding for input processing, combining value embedding with positional encoding to capture temporal patterns and sequential information.\n3. **Attention-Based Temporal Modeling**: Both employ attention mechanisms as the core component for capturing temporal dependencies, replacing traditional RNN structures to avoid gradient vanishing/exploding issues.\n4. **Multi-Head Attention Design**: Both implement multi-head attention to capture different aspects of temporal relationships, with queries, keys, and values projected into multiple subspaces.\n5. **Feed-Forward Networks**: Both incorporate position-wise feed-forward networks (typically Conv1d-based) after attention layers with residual connections and layer normalization for feature transformation.\n6. **Decoder Input Strategy**: Both decoders use a combination of historical tokens and zero-padded future tokens as input, where historical part provides context and future part is gradually refined through layers.",
    "differences": "1. **Core Innovation - Attention Mechanism**: Informer introduces ProbSparse self-attention with query sparsity measurement (selecting top-u queries based on KL divergence approximation) to achieve O(L log L) complexity, while FEDformer proposes Frequency Enhanced structures operating in frequency domain using Fourier/Wavelet transforms for O(L) or O(L log L) complexity with better global pattern capture.\n2. **Frequency Domain Processing**: Informer operates entirely in time domain with sparse attention patterns, whereas FEDformer explicitly transforms sequences to frequency domain (via FFT/DWT), performs linear operations on selected frequency modes, and transforms back to time domain, enabling more efficient global dependency modeling.\n3. **Decomposition Strategy**: Informer uses a simple distilling operation (Conv1d + MaxPool1d) in the encoder to progressively reduce sequence length, while FEDformer employs a sophisticated Mixture-of-Experts Decomposition (MOEDecomp) throughout both encoder and decoder to explicitly separate seasonal and trend components at each layer, with trend components accumulated across decoder layers.\n4. **Decoder Architecture Complexity**: Informer's decoder has standard self-attention and cross-attention blocks with single output stream, while FEDformer's decoder maintains dual streams (seasonal and trend), applies decomposition after each sub-block, and progressively refines trend through weighted accumulation (T_de^l = T_de^(l-1) + W_l,1·T_de^l,1 + W_l,2·T_de^l,2 + W_l,3·T_de^l,3).\n5. **Mode Selection and Parameterization**: Informer's ProbSparse attention dynamically selects important query-key pairs based on data statistics during forward pass, while FEDformer pre-selects fixed frequency modes (random or lowest modes) and learns parameterized kernels (R ∈ C^(D×D×M)) for frequency domain transformations, making the selection strategy data-independent but learnable.\n6. **Computational Focus**: Informer focuses on reducing quadratic attention complexity through sparse query selection while maintaining time-domain operations, whereas FEDformer achieves efficiency by operating on compressed frequency representations (selecting M << N modes) and leveraging FFT's O(N log N) complexity, fundamentally changing where computation occurs (time vs. frequency domain)."
  },
  {
    "source": "Reformer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both adopt Transformer-based encoder-decoder frameworks with multi-layer stacking, using residual connections and layer normalization after each sub-module.\n2. **Efficiency Motivation**: Both papers are motivated by reducing computational complexity for long sequences - Reformer achieves O(L log L) through LSH attention, while FEDformer achieves O(L) through frequency-domain operations with mode selection.\n3. **Embedding Strategy**: Both use positional and value embeddings to encode input sequences before feeding into the encoder, maintaining the standard Transformer input processing pipeline.\n4. **Multi-Head Mechanism**: Both retain the multi-head design from vanilla Transformer to capture different representation subspaces, though implemented differently (LSH bucketing vs. frequency-domain linear transforms).",
    "differences": "1. **Core Attention Mechanism**: Reformer uses Locality-Sensitive Hashing (LSH) to approximate full attention by hashing queries and keys into buckets, attending only within buckets to achieve O(L log L) complexity. FEDformer replaces self-attention entirely with Fourier/Wavelet transforms in frequency domain, performing linear operations on randomly selected frequency modes to achieve O(L) complexity.\n2. **Decomposition Strategy**: Reformer has no explicit decomposition mechanism and processes sequences holistically. FEDformer employs deep seasonal-trend decomposition throughout the architecture using Mixture of Experts Decomposition (MODEcomp) after each block, progressively refining seasonal and trend components separately.\n3. **Frequency-Domain Processing**: Reformer operates purely in time domain with hash-based attention approximation. FEDformer explicitly transforms sequences to frequency domain via DFT/DWT, performs parameterized linear transforms on selected modes (FEB-f/FEB-w blocks), and transforms back to time domain, fundamentally operating on frequency representations.\n4. **Shared-QK Design**: Reformer requires Q=K (shared-QK Transformer) and normalizes key lengths for LSH to work effectively, constraining the attention mechanism. FEDformer uses separate Q, K, V projections in its cross-attention (FEA) but processes them through frequency transforms rather than dot-product attention.\n5. **Trend Accumulation**: Reformer has no explicit trend handling. FEDformer accumulates trend components across decoder layers with learnable projection weights (W_{l,i}) for each decomposition step, enabling progressive trend refinement: T_de^l = T_de^{l-1} + Σ W_{l,i}·T_de^{l,i}.\n6. **Mode Selection Strategy**: Reformer uses hash bucket size and number of hashes as hyperparameters to control attention sparsity. FEDformer uses random or lowest-frequency mode selection with a fixed number of modes M << N, directly controlling which frequency components to model and achieving more aggressive dimension reduction.\n7. **Complexity and Scalability**: Reformer achieves O(L log L) time and O(L log L) space complexity through LSH bucketing and chunking. FEDformer achieves O(L) complexity for both time and space by operating only on M selected frequency modes, making it more scalable for extremely long sequences.\n8. **Cross-Attention Design**: Reformer applies the same LSH attention mechanism for both self-attention and cross-attention. FEDformer uses specialized Frequency Enhanced Attention (FEA-f/FEA-w) for cross-attention, which applies different frequency mode selections for queries (seq_len_q) and keys/values (seq_len_kv), with activation functions on attention weights in frequency domain."
  },
  {
    "source": "Informer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "Informer_2020",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers use the Transformer encoder-decoder framework for sequence modeling. The source code's `Encoder` and `EncoderLayer` structure can be directly reused, with the attention mechanism being the primary component to replace. The basic layer stacking, residual connections, and feed-forward networks remain compatible.\n\n2. **Efficient Attention as Core Innovation**: Both papers aim to reduce the O(L²) complexity of standard dot-product attention. Reformer uses LSH (Locality-Sensitive Hashing) attention achieving O(L log L), while Informer uses ProbSparse attention also targeting O(L log L). The attention interface pattern in the source code (taking Q, K, V as inputs) can be maintained, only swapping the attention computation logic.\n\n3. **Multi-Head Attention Mechanism**: Both retain the multi-head attention structure from vanilla Transformer. The source code's multi-head wrapper and projection layers can be reused. Both papers apply their sparse attention variants within each head independently.\n\n4. **Shared Query-Key Design Philosophy**: Reformer explicitly uses shared-QK (setting K = Q/||Q||) for LSH hashing compatibility. Informer's ProbSparse attention also computes query sparsity by comparing queries against keys, implying a similar Q-K relationship analysis. The source code's linear projection layers for Q, K, V generation can be adapted.\n\n5. **Positional and Temporal Embeddings**: Both papers embed input sequences with positional information before feeding into the encoder. The source code's `DataEmbedding` class (combining value embedding, positional encoding, and temporal encoding) can be directly reused for Informer.\n\n6. **Layer Normalization and Residual Connections**: Both use standard pre-norm or post-norm residual blocks. The source code's `EncoderLayer` structure with LayerNorm can be maintained without modification.\n\n7. **Feed-Forward Networks**: Both use identical position-wise feed-forward networks (FFN) with two linear transformations and activation. The FFN implementation in source code is fully reusable.\n\n8. **Batch Processing and Memory Efficiency Focus**: Both papers emphasize memory-efficient implementations for long sequences. The source code's batching strategy and tensor shape management (batch_size, length, d_model) align with Informer's requirements.",
    "differences": "1. **Attention Selection Strategy - NEW IMPLEMENTATION REQUIRED**: Reformer uses LSH (random projections, hash bucketing, sorting) to group similar queries/keys, attending within hash buckets. Informer uses **ProbSparse attention** that selects Top-u queries based on sparsity measurement M(q_i, K) = LSE - mean, requiring implementation of: (a) Query sparsity score computation via Log-Sum-Exp and arithmetic mean, (b) Top-u query selection mechanism (u = c·ln(L_Q)), (c) Random sampling of U = L_K·ln(L_Q) dot-products for efficient measurement. This is fundamentally different from LSH's hashing approach and requires completely new attention logic.\n\n2. **Self-Attention Distilling in Encoder - NEW IMPLEMENTATION REQUIRED**: Informer introduces a **distilling operation** that progressively reduces sequence length through Conv1d + MaxPooling layers between attention blocks (Equation 5), creating a pyramid structure with halving inputs at each layer. This requires implementing: (a) 1D convolutional layers with kernel width=3 after each attention block, (b) MaxPooling with stride 2 for downsampling, (c) Pyramid stacking with multiple encoder replicas processing different input lengths, (d) Concatenation of multi-scale features. Reformer has no such multi-scale distilling mechanism.\n\n3. **Decoder Generative Inference Strategy - NEW IMPLEMENTATION REQUIRED**: Informer uses a **generative-style decoder** that takes concatenated input [X_token, X_0] where X_token is a sampled historical slice and X_0 is a zero-placeholder for target positions (Equation 6), predicting all future steps in one forward pass. Reformer uses standard autoregressive decoding. This requires implementing: (a) Start token sampling from input history, (b) Zero-placeholder generation for target sequence, (c) Single-pass prediction logic, (d) Modified masking for this generative paradigm.\n\n4. **Reversible Layers vs. Standard Layers**: Reformer implements **RevNet-style reversible layers** (Equations 7-8) to save memory by recomputing activations during backpropagation, eliminating the need to store intermediate activations. The source code uses standard non-reversible layers. Informer does not mention reversible layers and uses standard Transformer layers, so this Reformer feature should NOT be carried over.\n\n5. **Chunking Strategy**: Reformer applies **chunking** to feed-forward layers (Equation 10) to reduce memory from O(b·l·d_ff·n_l) to O(b·l·d_model), processing positions in chunks. Informer does not employ chunking but relies on distilling to reduce sequence length. The chunking logic in Reformer is unnecessary for Informer.\n\n6. **Hash Bucketing and Sorting - NOT NEEDED**: Reformer's core mechanism involves: (a) Random matrix R for hash computation h(x) = argmax([xR; -xR]), (b) Sorting queries/keys by hash bucket, (c) Chunked attention within sorted buckets (Equation 5). Informer does not use hashing at all, making the entire `LSHSelfAttention` module and its `fit_length` bucket padding logic irrelevant.\n\n7. **Multi-Round Hashing - NOT NEEDED**: Reformer uses n_rounds (typically 4-8) of hashing with different hash functions to reduce collision probability (Equation 6). Informer's ProbSparse attention is deterministic based on sparsity measurement, requiring no multi-round mechanism.\n\n8. **Complexity and Sparsity Philosophy**: Reformer achieves O(L log L) by grouping similar items via LSH, assuming locality in the embedding space. Informer achieves O(L log L) by **selecting dominant queries** that contribute most to attention diversity, based on KL-divergence from uniform distribution. The sparsity assumptions differ: Reformer assumes spatial locality, Informer assumes query importance heterogeneity.\n\n9. **Causal Masking Implementation**: Reformer implements causal masking by forbidding self-attention (token attending to itself) except when no other targets exist, due to shared-QK causing self-dot-products to dominate. Informer uses standard causal masking in decoder without this restriction, as its attention mechanism differs.\n\n10. **Experimental Domain and Task**: Reformer targets NLP tasks (text generation, duplication task in experiments) with discrete tokens. Informer specifically targets **long sequence time-series forecasting (LSTF)** with continuous multivariate data, requiring different data preprocessing, evaluation metrics (MSE, MAE), and dataset handling (ETTh1, ETTh2, Weather, ECL datasets)."
  },
  {
    "source": "PatchTST_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization with Non-stationary Handling**: Both papers apply instance normalization (zero mean, unit variance) to each time series before processing and denormalize predictions afterward. The source implementation uses `means.detach()` and computes `stdev` with `unbiased=False` to prevent gradient flow to normalization statistics, which is preserved in iTransformer's code.\n2. **Encoder-Only Architecture**: Both adopt encoder-only Transformer structures without decoder components, focusing on representation learning rather than autoregressive generation. They use vanilla Transformer encoder blocks with multi-head self-attention and feed-forward networks.\n3. **Linear Projection for Forecasting**: Both models use simple linear layers (MLP) to map learned representations to prediction horizons, avoiding complex decoder architectures. PatchTST uses FlattenHead with linear projection, while iTransformer uses `nn.Linear(d_model, pred_len)`.\n4. **Shared Transformer Components**: Both leverage the same modular components from the codebase including `Encoder`, `EncoderLayer`, `AttentionLayer`, and `FullAttention` with identical attention mechanisms (scaled dot-product attention with softmax).\n5. **Task Formulation**: Both frame forecasting as a supervised learning problem with MSE loss, directly predicting future values from historical observations without iterative generation.",
    "differences": "1. **Core Innovation - Token Representation**: PatchTST segments time series into patches (temporal subsequences of length P with stride S) and treats each patch as a token, reducing sequence length from L to ~L/S. iTransformer inverts the dimension by treating each entire variate series as a token, embedding the full lookback window (T timesteps) into d_model dimensions, fundamentally changing what attention operates on.\n2. **Channel Strategy - Independence vs. Correlation**: PatchTST enforces strict channel independence where each variate is processed separately through the same Transformer backbone (`x.shape[0] * x.shape[1]` reshaping splits variates), with attention operating within each series. iTransformer applies attention across variates to capture multivariate correlations, with the attention mechanism explicitly modeling dependencies between different variables.\n3. **Attention Semantics**: PatchTST's attention learns dependencies between temporal patches within a single series (temporal self-attention on patches). iTransformer's attention learns correlations between different variates across the entire dataset (variate-wise attention), making attention maps directly interpretable as multivariate correlation structures.\n4. **Embedding Strategy**: PatchTST uses `PatchEmbedding` with learnable linear projection `nn.Linear(patch_len, d_model)` plus positional embeddings to encode patch positions. iTransformer uses `DataEmbedding_inverted` that directly projects the entire time dimension via `nn.Linear(seq_len, d_model)` without positional encoding, as temporal order is implicitly captured in neuron permutations.\n5. **Layer Normalization Scope**: PatchTST applies BatchNorm across the feature dimension of temporal tokens (`nn.BatchNorm1d(d_model)` with transpose operations). iTransformer applies LayerNorm to each variate token independently (`torch.nn.LayerNorm(d_model)`), normalizing series representations rather than temporal features, which helps handle non-stationary series and reduces cross-variate measurement discrepancies.\n6. **Scalability Trade-offs**: PatchTST's complexity scales with patch number O((L/S)²) and handles long sequences efficiently through patching, but processes each variate independently. iTransformer's complexity scales with variate number O(N²), making it efficient for high-frequency series with many timesteps but potentially costly for datasets with many variables.\n7. **Position Encoding**: PatchTST explicitly uses `PositionalEmbedding` with sinusoidal encodings to mark patch positions in the sequence. iTransformer completely removes positional encoding, relying on the feed-forward network's neuron arrangement to implicitly encode temporal information."
  },
  {
    "source": "Crossformer_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Only Architecture**: Both papers adopt encoder-only Transformer architectures for time series forecasting, departing from the traditional encoder-decoder generation paradigm. Crossformer uses a hierarchical encoder with decoder layers for multi-scale prediction, while iTransformer uses a pure encoder with final projection layers.\n\n2. **Layer Normalization on Series Dimension**: Both papers apply layer normalization along the series/variate dimension rather than the temporal dimension. Crossformer's implementation normalizes across the d_model dimension for each segment, while iTransformer explicitly normalizes each variate token independently (Equation 2), which helps handle non-stationary data and reduces inter-variate measurement discrepancies.\n\n3. **Multi-Head Self-Attention (MSA) as Core Module**: Both utilize standard multi-head self-attention mechanisms as the primary dependency modeling component. Crossformer applies MSA in both cross-time and cross-dimension stages within its Two-Stage Attention layer, while iTransformer applies MSA on variate tokens to capture multivariate correlations.\n\n4. **Feed-Forward Network for Representation Learning**: Both employ feed-forward networks (MLPs) after attention layers for representation extraction. Crossformer uses MLPs in both TSA stages, while iTransformer leverages FFN to encode series representations of each variate token, treating it as a universal approximator for time series patterns.\n\n5. **Linear Projection for Final Prediction**: Both papers use linear layers for final forecasting output. Crossformer's DecoderLayer includes a linear_pred layer that projects d_model to seg_len, while iTransformer uses a projection layer mapping d_model to pred_len, reflecting the shared insight that simple linear mappings are effective for numerical time series prediction.",
    "differences": "1. **Core Innovation - Dimension Treatment**: Crossformer's main contribution is Dimension-Segment-Wise (DSW) embedding that explicitly captures cross-dimension dependency through a 2D vector array (time × dimension), treating time and dimension as two separate axes. iTransformer inverts the traditional approach by treating each entire time series as a token, fundamentally shifting from temporal tokens to variate tokens, making the attention mechanism directly model multivariate correlations.\n\n2. **Attention Mechanism Design**: Crossformer employs a Two-Stage Attention (TSA) layer with separate cross-time and cross-dimension stages, using a router mechanism in the cross-dimension stage to reduce complexity from O(D²) to O(D). iTransformer uses standard full attention on variate tokens with complexity O(N²) where N is the number of variates, but allows for plug-in efficient attention variants (FlashAttention, etc.) when N is large.\n\n3. **Segmentation vs. Whole Series Embedding**: Crossformer divides each time series into segments of length L_seg and embeds each segment independently (x_{i,d}^(s) ∈ ℝ^{L_seg}), creating a 2D array of segment vectors. The implementation uses PatchEmbedding with unfold operations for segmentation. iTransformer embeds the entire lookback window (length T) of each variate as a single token via MLP: ℝ^T → ℝ^D, without any segmentation, treating the whole series as the atomic unit.\n\n4. **Multi-Scale Modeling**: Crossformer implements hierarchical multi-scale modeling through SegMerging layers that progressively merge segments by concatenating win_size consecutive segments and projecting them (win_size × d_model → d_model), with multiple scale_blocks at different resolutions. iTransformer does not incorporate explicit multi-scale mechanisms, operating at a single temporal resolution.\n\n5. **Positional Encoding Strategy**: Crossformer uses learnable 2D positional embeddings E_{i,d}^(pos) for each segment position (i, d) in the time-dimension grid, explicitly encoding both temporal and dimensional positions. iTransformer eliminates positional embeddings entirely, arguing that the order of the sequence is implicitly stored in the neuron permutation of the feed-forward network when processing whole series.\n\n6. **Normalization Implementation Details**: Crossformer's implementation uses standard LayerNorm on concatenated segment features within the SegMerging module. iTransformer adopts instance normalization from Non-stationary Transformer, explicitly computing mean and std for each variate's lookback window and applying normalization before embedding: x_enc = (x_enc - means) / stdev, with de-normalization applied after projection. The implementation includes detach() on means to prevent gradient flow to statistics.\n\n7. **Channel Mixing Strategy**: Crossformer explicitly mixes information across dimensions through the cross-dimension stage of TSA with router mechanisms, enabling D-to-D interactions. iTransformer maintains channel independence in the feed-forward network (each variate token processed identically but independently by shared FFN), with cross-variate interaction occurring only through self-attention, aligning with recent Channel Independence findings.\n\n8. **Decoder Architecture**: Crossformer uses a hierarchical encoder-decoder (HED) structure with DecoderLayers that perform cross-attention between decoder queries and multi-scale encoder outputs, aggregating predictions from multiple scales (final_predict = Σ layer_predict). iTransformer uses a decoder-free approach with direct projection from encoder output to predictions, significantly simplifying the architecture.\n\n9. **Computational Complexity**: Crossformer's TSA layer has complexity O(DL² + 2cD) where L = T/L_seg is the number of segments, c is the number of routers, and D is the number of dimensions. iTransformer has complexity O(N²D + NDF) where N is the number of variates, with quadratic dependency on variate count rather than temporal length, making it more suitable for scenarios with many time steps but fewer variates."
  },
  {
    "source": "TiDE_2023",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization with Reversible De-normalization**: Both papers apply instance normalization on the temporal dimension (normalizing each time series independently across time steps) before processing, and reverse it after prediction. In TiDE's implementation, the normalization uses `means.detach()` and computes `stdev` with `unbiased=False` and adds `1e-5` for numerical stability; iTransformer follows the exact same implementation pattern with `means.detach()`, `unbiased=False`, and `1e-5` epsilon, preventing gradient flow to normalization statistics.\n2. **Channel Independence Strategy**: Both models process each variate independently rather than mixing channel information in early layers. TiDE explicitly uses channel-independent processing where each time series is fed separately through the model (\"applied in a channel independent manner\"), then stacks results. iTransformer inverts the dimension to treat each variate as a separate token, achieving similar channel independence through architectural design.\n3. **Linear Residual Connection**: Both architectures incorporate global linear mappings as residual connections. TiDE uses `self.residual_proj = nn.Linear(self.seq_len, self.pred_len)` to directly map lookback to horizon and adds it to the main prediction path. iTransformer's projection layer `nn.Linear(d_model, pred_len)` serves a similar purpose of providing a direct linear pathway from input to output, acknowledging the effectiveness of simple linear models.\n4. **Encoder-Only Architecture for Forecasting**: Both papers move away from the traditional encoder-decoder Transformer structure for forecasting. TiDE uses stacked MLP encoders and decoders without attention mechanisms. iTransformer adopts an encoder-only Transformer architecture, eliminating the generative decoder and focusing on representation learning rather than autoregressive generation.\n5. **Handling of Temporal Covariates**: Both models incorporate exogenous dynamic features (time-related covariates). TiDE has explicit feature projection for dynamic covariates with dimensionality reduction. iTransformer's `DataEmbedding_inverted` concatenates time marks with the input series when available: `torch.cat([x, x_mark.permute(0, 2, 1)], 1)`, integrating temporal features into the embedding space.",
    "differences": "1. **Core Architecture Paradigm**: TiDE is purely MLP-based with no attention mechanism, using stacked residual blocks for encoding/decoding with explicit feature projection, dense encoder (n_e layers), dense decoder (n_d layers), and temporal decoder. iTransformer inverts the Transformer architecture to apply self-attention across variates rather than time steps, treating each entire time series as a token, fundamentally repurposing attention for multivariate correlation modeling rather than temporal dependency.\n2. **Temporal Dependency Modeling**: TiDE captures temporal patterns implicitly through the flattened lookback window processed by MLPs, relying on dense connections to learn temporal relationships without explicit sequential modeling. iTransformer encodes temporal information in the neuron permutation of the feed-forward network after embedding the entire series, with self-attention operating on variate tokens to model cross-series dependencies while FFN extracts series-level representations.\n3. **Attention Mechanism and Complexity**: TiDE has O(1) complexity with respect to sequence length and number of variates since it processes each series independently through fixed-size MLPs. iTransformer uses full self-attention with O(N²) complexity where N is the number of variates, but is independent of sequence length L since attention operates on variate tokens rather than time steps, making it efficient for long sequences but potentially costly for high-dimensional multivariate data.\n4. **Feature Engineering Approach**: TiDE employs explicit feature projection using a residual block to reduce dynamic covariate dimensionality from r to r̃ (temporalWidth), concatenates projected features with lookback, and uses a separate temporal decoder that combines decoded vectors with projected future covariates at each horizon step. iTransformer embeds the entire series directly with optional time marks concatenated in the embedding layer, without explicit feature dimensionality reduction or separate temporal decoding stages.\n5. **Scalability and Flexibility**: TiDE's architecture is fixed to the number of variates during training (processes each channel separately then stacks), requiring retraining for different variate counts. iTransformer's attention-based design allows flexible token numbers, enabling training on arbitrary numbers of variates and potential transfer across datasets with different dimensionalities, as the attention mechanism can naturally handle varying numbers of variate tokens.\n6. **Layer Normalization Application**: TiDE applies LayerNorm to the output of each residual block across the feature dimension of individual processing paths. iTransformer applies LayerNorm to normalize each variate token's series representation independently (Equation 2 in paper), explicitly addressing non-stationarity and measurement discrepancies across variates, whereas vanilla Transformers normalize across time steps which can cause oversmoothing."
  },
  {
    "source": "TimesNet_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Instance Normalization for Non-stationarity**: Both papers adopt instance normalization from Non-stationary Transformer to handle distribution shifts. In the source implementation, normalization statistics (means and stdev) use `.detach()` to prevent gradient flow during backpropagation, treating them as constants. Both normalize along the temporal dimension (dim=1) and apply reversible denormalization to predictions.\n2. **Encoder-Only Architecture**: Both use encoder-only architectures rather than encoder-decoder structures. TimesNet stacks TimesBlocks residually, while iTransformer stacks standard Transformer encoder layers, both avoiding the complexity of autoregressive decoding.\n3. **Direct Multi-Step Prediction**: Both employ direct forecasting strategies using linear projection layers to map encoded representations to future horizons (pred_len), avoiding iterative generation. TimesNet uses `predict_linear` to align temporal dimensions before processing, while iTransformer projects from d_model to pred_len after encoding.\n4. **Layer Normalization**: Both incorporate LayerNorm for training stability. TimesNet applies it after each TimesBlock (`self.layer_norm(self.model[i](enc_out))`), while iTransformer uses it within standard Transformer encoder layers.\n5. **Embedding Layer**: Both use embedding layers to project raw inputs into higher-dimensional feature spaces (d_model). TimesNet uses `DataEmbedding` for temporal tokens, while iTransformer uses `DataEmbedding_inverted` for variate tokens.",
    "differences": "1. **Core Innovation - Token Representation**: TimesNet transforms 1D time series into 2D tensors based on discovered periodicities (via FFT) to capture intraperiod and interperiod variations using 2D convolutions. iTransformer inverts the token dimension, treating each variate as a token (embedding entire time series) rather than each timestamp, fundamentally changing attention from temporal to multivariate correlations.\n2. **Temporal Modeling Mechanism**: TimesNet uses parameter-efficient Inception blocks with multiple 2D convolutional kernels (kernel sizes 1×1, 3×3, 5×5, etc.) to capture multi-scale 2D variations in reshaped tensors. iTransformer uses standard self-attention (FullAttention) on variate tokens, with feed-forward networks (FFN) encoding temporal representations within each series.\n3. **Multi-Periodicity vs. Multivariate Correlation**: TimesNet explicitly discovers top-k periods through FFT amplitude analysis, reshapes series into k different 2D tensors, processes them separately, and adaptively aggregates results using softmax-weighted period amplitudes. iTransformer focuses on learning multivariate dependencies through attention across variate tokens, without explicit periodicity extraction.\n4. **Channel Handling Strategy**: TimesNet mixes channels within temporal tokens and processes all variates together through 2D convolutions (in_channels=d_model, processing N variates simultaneously in the channel dimension). iTransformer enforces channel independence by embedding each variate separately and applying identical FFN to each token, with attention only capturing cross-variate correlations.\n5. **Complexity and Scalability**: TimesNet has O(k × p × f) complexity for 2D convolutions over k periods with dimensions p×f, plus FFT overhead O(T log T). iTransformer has O(N² × D) attention complexity where N is the number of variates, making it more suitable for datasets with moderate variate counts but potentially unlimited temporal length (as temporal dimension is in token embedding).\n6. **Embedding Dimension Inversion**: TimesNet embeds temporal dimension (seq_len → d_model via DataEmbedding), producing tokens of shape [B, T, d_model]. iTransformer inverts this by embedding variate dimension (seq_len → d_model via DataEmbedding_inverted with input permutation), producing tokens of shape [B, N, d_model], fundamentally changing the attention target from time steps to variates."
  },
  {
    "source": "DLinear_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Normalization Strategy**: Both employ instance normalization (normalizing across the temporal dimension for each variate independently). DLinear's code implementation uses zero-mean normalization implicitly through decomposition, while iTransformer explicitly implements non-stationary normalization with `means = x_enc.mean(1, keepdim=True).detach()` and `stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)`. The source implementation uses `.detach()` to prevent gradient flow to statistics, treating normalization parameters as constants during backpropagation.\n\n2. **Series Decomposition Philosophy**: Both leverage temporal decomposition concepts from Autoformer. DLinear explicitly uses `series_decomp` with moving average kernels to separate trend and seasonal components, applying separate linear layers to each. iTransformer implicitly adopts decomposition thinking through its normalization (removing mean/trend) and uses the feed-forward network to capture residual patterns, though not as explicitly structured as DLinear's dual-branch design.\n\n3. **Channel Independence**: Both fundamentally treat each variate independently for temporal feature extraction. DLinear's code shows `individual=False` mode applies the same linear weights across variates (shared temporal modeling), while `individual=True` creates separate linear layers per channel. iTransformer inverts this by embedding each series as independent tokens, then applies attention across variates - but the temporal encoding within each token via FFN maintains channel-wise independence for series representation.\n\n4. **Direct Multi-Step Forecasting**: Both use Direct Multi-Step (DMS) strategy, avoiding autoregressive error accumulation. DLinear directly maps `seq_len → pred_len` through linear layers. iTransformer uses `projection = nn.Linear(d_model, pred_len)` to directly decode all future steps from learned representations, sharing the same non-autoregressive philosophy.\n\n5. **Linear Projection as Core Decoder**: Both ultimately rely on linear layers for final predictions. DLinear uses `nn.Linear(seq_len, pred_len)` as the primary forecasting mechanism. iTransformer uses `nn.Linear(d_model, pred_len)` as the projection head after Transformer encoding, acknowledging linear layers' effectiveness for time series forecasting as demonstrated by DLinear's success.",
    "differences": "1. **Core Architecture Philosophy**: DLinear challenges Transformer necessity, proposing an embarrassingly simple linear baseline that operates directly on raw temporal sequences with O(1) complexity per variate. iTransformer rehabilitates Transformers by inverting the architecture - treating entire time series as tokens and applying attention across variates rather than time steps, arguing that multivariate correlations (not temporal point-wise semantics) are where attention excels.\n\n2. **Attention Mechanism vs. No Attention**: DLinear completely eliminates attention mechanisms, using only two linear layers (one for trend, one for seasonal component) with weights initialized as `(1/seq_len) * torch.ones([pred_len, seq_len])` for uniform temporal averaging. iTransformer employs full self-attention (`FullAttention`) but inverts its application - attention operates on N variate tokens (capturing multivariate correlations) rather than T temporal tokens, with complexity O(N²) instead of O(T²).\n\n3. **Temporal Representation Learning**: DLinear uses explicit decomposition (`series_decomp` with moving average kernel) followed by direct linear mapping of temporal patterns - trend and seasonal components are processed separately then summed. iTransformer embeds entire time series into d_model dimensional space via `DataEmbedding_inverted`, then uses stacked feed-forward networks (MLP with d_ff hidden dimensions) to encode complex temporal representations implicitly through multiple non-linear transformations.\n\n4. **Token Granularity and Embedding**: DLinear operates on raw time points without tokenization - input shape is [Batch, Time, Variate], directly permuted to [Batch, Variate, Time] for channel-wise linear operations. iTransformer inverts to [Batch, Variate, Time] then embeds via `value_embedding = nn.Linear(c_in, d_model)` where c_in=seq_len, creating variate tokens of shape [Batch, N, d_model] where each token represents an entire series, not a time point.\n\n5. **Multivariate Interaction Modeling**: DLinear enforces strict channel independence (no cross-variate information flow) - the `individual` flag only controls whether weights are shared or separate per variate, but never mixes variate information. iTransformer explicitly models multivariate correlations through inverted attention mechanism, where self-attention computes dependencies between different variates' series representations, enabling the model to learn which variates influence each other.\n\n6. **Decomposition Implementation**: DLinear's code uses `series_decomp(moving_avg)` with configurable kernel size (e.g., 25), applying `AvgPool1d` for trend extraction and computing seasonal as residual, then separate `Linear_Trend` and `Linear_Seasonal` layers process each component. iTransformer forgoes explicit decomposition modules, instead relying on normalization (removing mean/variance) and letting the Transformer encoder's feed-forward networks learn to decompose patterns implicitly through representation learning.\n\n7. **Model Capacity and Complexity**: DLinear has minimal parameters: 2 × (seq_len × pred_len) per variate in individual mode, or shared across all variates otherwise, with O(T) time complexity. iTransformer uses multi-layer Transformer encoder with e_layers blocks, each containing multi-head attention (n_heads, d_model dimensions) and FFN (d_ff expansion), resulting in O(N² × d_model + N × d_ff) complexity per layer, substantially higher capacity for learning complex patterns.\n\n8. **Positional Information Encoding**: DLinear implicitly encodes temporal order through the neuron positions in linear weight matrices - the i-th weight connects the i-th historical time step to outputs. iTransformer explicitly states 'position embedding in vanilla Transformer is no longer needed' because temporal order is encoded in the neuron permutation of the feed-forward network operating on series representations, and optional timestamp features can be concatenated in `DataEmbedding_inverted` if available."
  },
  {
    "source": "TimesNet_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Informer_2020",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition**: Both papers utilize series decomposition to separate trend and seasonal components. Autoformer uses moving average-based decomposition (series_decomp with AvgPool), while LightTS implicitly handles temporal patterns through sampling. The moving_avg and series_decomp modules from Autoformer can be adapted as preprocessing components for LightTS.\n\n2. **Encoder-Decoder Architecture Philosophy**: Both employ a two-stage processing approach. Autoformer uses explicit encoder-decoder with decomposition blocks, while LightTS has Part I (feature extraction per variable) and Part II (cross-variable learning). The staged processing concept and residual connections from Autoformer's implementation can guide LightTS's two-part structure.\n\n3. **Embedding and Input Processing**: Both use data embedding layers to transform raw inputs. Autoformer's DataEmbedding_wo_pos (with TokenEmbedding via Conv1d and temporal embeddings) can be partially reused for LightTS's input preprocessing, especially the temporal feature extraction components.\n\n4. **Layer Normalization**: Both apply normalization techniques. Autoformer's my_Layernorm (specialized for seasonal parts with bias removal) can be adapted for LightTS's feature normalization between IEBlocks to stabilize training.\n\n5. **Multi-Layer Stacking**: Both stack multiple processing layers. Autoformer's EncoderLayer and DecoderLayer stacking pattern with residual connections can inform LightTS's multiple IEBlock stacking strategy, reusing the layer iteration and gradient flow patterns.\n\n6. **Forecasting Task Setup**: Both address multivariate long-term time series forecasting with input-I-predict-O paradigm. Autoformer's forecast() method structure (handling x_enc, x_mark_enc, x_dec, x_mark_dec) provides a template for LightTS's data flow organization.",
    "differences": "1. **Core Architecture Paradigm**: Autoformer uses Transformer-based auto-correlation mechanism with O(L log L) complexity via FFT for period-based dependencies discovery. LightTS uses pure MLP-based architecture with sampling-oriented design, requiring NEW implementation of: (a) Continuous sampling and interval sampling modules (Eq. 1-2), (b) IEBlock with bottleneck design (temporal/channel/output projections), (c) Weight-shared MLPs across columns/rows, (d) Feature concatenation and down-projection layers.\n\n2. **Attention vs. Sampling Mechanism**: Autoformer employs AutoCorrelation mechanism (FFT-based autocorrelation computation, time delay aggregation with Roll operations, top-k period selection). LightTS completely eliminates attention mechanisms, requiring NEW implementation of: (a) Non-overlapping sub-sequence generation, (b) Multi-scale feature extraction through different sampling strategies, (c) Direct MLP-based information exchange without any correlation computation.\n\n3. **Temporal Dependency Modeling**: Autoformer discovers period-based dependencies through autocorrelation R(τ) and aggregates similar sub-series via time delay (Roll operations). LightTS captures dependencies through NEW components: (a) Continuous sampling for short-term local patterns (consecutive C tokens), (b) Interval sampling for long-term global patterns (fixed interval sampling), (c) Parallel processing of both sampling results without explicit period detection.\n\n4. **Channel/Variable Handling Strategy**: Autoformer treats all variables jointly through shared encoder-decoder with d_model dimensional representations. LightTS requires NEW implementation of: (a) Part I: Independent processing of each time series variable (N separate pathways), (b) Part II: Cross-variable information exchange via channel projection in IEBlock-C, (c) Feature concatenation strategy (2F×N matrix formation), (d) Variable-specific feature extraction before cross-variable learning.\n\n5. **Decomposition Integration**: Autoformer has decomposition as built-in blocks throughout encoder-decoder (progressive trend extraction in each layer with accumulation structure). LightTS requires NEW implementation of: (a) Implicit decomposition through sampling strategies (no explicit trend-seasonal separation), (b) Bottleneck architecture in IEBlock (F' << F for efficiency), (c) Linear down-projection from T/C to 1 dimension per variable.\n\n6. **Prediction Head Design**: Autoformer accumulates trend components across decoder layers (trend = trend + W*extracted_trends) and adds final seasonal projection. LightTS requires NEW implementation of: (a) IEBlock-C as prediction head (2F×N → L×N mapping), (b) Direct multi-step output without iterative refinement, (c) No separate trend-seasonal reconstruction, unified feature-to-prediction mapping.\n\n7. **Computational Efficiency Strategy**: Autoformer achieves efficiency through FFT-based autocorrelation (O(L log L)) and sparse top-k period selection. LightTS requires NEW implementation of: (a) Down-sampling to C×(T/C) matrices (reducing sequence length by factor C), (b) Bottleneck design with F' << H, F for channel projection efficiency, (c) Weight sharing across all columns/rows in projections, (d) No FFT or complex frequency domain operations."
  },
  {
    "source": "Informer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers use an encoder-decoder paradigm for long sequence time series forecasting (LSTF). The target paper can reuse the basic data loading pipeline, dataset preprocessing utilities, and the general training loop structure from Informer's implementation.\n\n2. **Embedding Layer Components**: Both models use temporal embeddings to encode positional/time information. LightTS can directly adapt Informer's DataEmbedding class which includes positional encoding, temporal encoding (hour, day, week, month), and value embedding - particularly the temporal feature extraction logic for handling time stamps.\n\n3. **Multi-step Forecasting Setup**: Both tackle the same problem formulation - given historical window T, predict future horizon L. The decoder input construction strategy in Informer (concatenating start token with target placeholders) can be adapted for LightTS's prediction part, especially the data batching and output formatting logic.\n\n4. **Training Infrastructure**: Both use MSE loss for forecasting and share similar training procedures. LightTS can directly reuse Informer's loss computation, metric calculation (MSE, MAE), experiment logging, model checkpointing, and evaluation pipeline code.\n\n5. **Normalization Strategy**: Both apply instance normalization on input sequences. Informer's normalization utilities (mean-std normalization per instance) can be directly integrated into LightTS's preprocessing pipeline to stabilize training.",
    "differences": "1. **Core Architecture Paradigm Shift**: Informer uses Transformer-based architecture with ProbSparse self-attention (O(L log L) complexity), while LightTS is a pure MLP-based model without any attention mechanism. NEW IMPLEMENTATION NEEDED: (a) Continuous and interval sampling modules to downsample sequences into sub-sequences; (b) Information Exchange Block (IEBlock) with bottleneck design featuring temporal projection, channel projection, and output projection; (c) Weight-sharing MLP layers across temporal and channel dimensions.\n\n2. **Temporal Dependency Modeling**: Informer captures dependencies through multi-head ProbSparse attention with query sparsity measurement (KL divergence-based Top-u selection), self-attention distilling with Conv1D and MaxPooling for pyramid encoder. LightTS uses NO attention - instead relies on two-stage sampling (continuous for local patterns, interval for global patterns) combined with IEBlocks. NEW IMPLEMENTATION: Sampling transformation logic (Eq. 1-2 in paper) to convert 1D sequences to 2D matrices, and the entire IEBlock architecture with its three projection types.\n\n3. **Multi-scale Feature Extraction**: Informer uses self-attention distilling with convolutional layers and max-pooling to progressively halve sequence length across encoder layers, creating a pyramid structure. LightTS achieves multi-scale through dual sampling strategies applied in parallel (continuous for short-term, interval for long-term) without hierarchical layer stacking. NEW IMPLEMENTATION: Parallel sampling pathways, feature concatenation logic across sampling types, and the specific linear down-projection from (F × T/C) to F dimensions.\n\n4. **Channel/Variable Interaction Strategy**: Informer treats all variables jointly through attention mechanisms where queries/keys/values include all channels simultaneously. LightTS uses a two-part strategy: Part I processes each time series INDEPENDENTLY (no inter-variable communication), Part II concatenates all features and uses IEBlock-C to learn inter-dependencies. NEW IMPLEMENTATION: Independent processing loops for each variable in Part I, feature concatenation creating (2F × N) matrix, and IEBlock-C specifically designed for channel mixing with shape transformation from (2F × N) to (L × N).\n\n5. **Decoder Design and Prediction Strategy**: Informer uses a standard Transformer decoder with masked self-attention and encoder-decoder cross-attention, employing generative inference with start tokens. LightTS has NO explicit decoder - the final IEBlock-C directly outputs predictions (L × N) in one forward pass without autoregressive generation or cross-attention. NEW IMPLEMENTATION: Direct prediction head that maps from concatenated features to final forecast without intermediate decoder layers or attention mechanisms.\n\n6. **Complexity and Efficiency Focus**: Informer achieves O(L log L) through sparse attention and distilling, targeting memory efficiency for long sequences. LightTS achieves O(L) through pure MLPs with bottleneck design (F' << F, H) and sampling-based dimension reduction. NEW IMPLEMENTATION: Bottleneck architecture where temporal projection reduces rows from H to F' before expensive channel projections, and the specific hyperparameter configuration for bottleneck ratio to balance efficiency and expressiveness."
  },
  {
    "source": "Reformer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Sequential Processing Architecture**: Both papers process time series as sequences and use encoder-based architectures. Reformer's encoder structure with LayerNorm and residual connections can be reused as the backbone for LightTS's feature extraction blocks. The DataEmbedding module from Reformer's implementation can be adapted for initial input processing.\n\n2. **Multi-Layer Feature Extraction**: Both employ stacked layers to progressively extract features. Reformer's EncoderLayer structure (attention + feed-forward + normalization) shares conceptual similarity with LightTS's stacked IEBlocks. The layer stacking mechanism and normalization strategies from Reformer's implementation can be adapted.\n\n3. **Projection-Based Output**: Both use linear projection layers to map features to predictions. Reformer's final projection layer (nn.Linear(configs.d_model, configs.c_out)) can be directly adapted for LightTS's output projection from feature dimension to prediction horizon.\n\n4. **Batch Processing and Tensor Operations**: Both handle batched inputs with shape [B, L, D]. The tensor manipulation utilities and batch processing logic from Reformer's implementation can be reused for LightTS's matrix operations.\n\n5. **Training Infrastructure**: Standard components like loss functions, optimizers, and training loops from Reformer's implementation can be directly reused since both are supervised forecasting tasks with similar input-output structures.",
    "differences": "1. **Core Architecture Paradigm**: Reformer uses LSH-based sparse attention mechanism (O(L log L) complexity) to handle long sequences by bucketing similar queries/keys, while LightTS uses pure MLP-based architecture with NO attention mechanism whatsoever. LightTS requires implementing: (a) Information Exchange Blocks with temporal/channel projections, (b) weight-shared MLPs across dimensions, (c) bottleneck architecture with F' << F dimension reduction. None of these MLP-based components exist in Reformer.\n\n2. **Temporal Modeling Strategy**: Reformer captures dependencies through attention over all positions (with LSH approximation), while LightTS uses two distinct sampling strategies: (a) continuous sampling (consecutive C tokens), (b) interval sampling (C tokens with fixed intervals). Must implement: sampling functions that transform [B, T, N] → [B, C, T/C, N] for both strategies, and logic to process them in parallel through separate IEBlocks.\n\n3. **Multi-Scale Feature Extraction**: Reformer processes the full sequence uniformly, while LightTS explicitly separates short-term (continuous sampling) and long-term (interval sampling) patterns. Requires implementing: (a) dual-path architecture with IEBlock-A and IEBlock-B, (b) feature concatenation along temporal dimension [2F × N], (c) down-projection from [F, T/C] to [F] using linear mapping.\n\n4. **Channel Handling Philosophy**: Reformer treats multivariate time series as a single sequence with d_model dimensions (channel-mixed from start via embedding), while LightTS has a two-stage approach: (a) Part I: independent processing of each variable (N separate paths), (b) Part II: explicit channel mixing through IEBlock-C. Must implement variable-independent feature extraction followed by cross-variable information exchange.\n\n5. **Information Flow Design**: Reformer uses reversible layers with residual connections (Y1 = X1 + Attention(X2), Y2 = X2 + FeedForward(Y1)), while LightTS uses sequential projections without residual connections: temporal projection → channel projection → output projection. The IEBlock requires implementing three distinct MLP stages with different input/output dimensions and weight sharing patterns.\n\n6. **Computational Efficiency Mechanism**: Reformer achieves efficiency through LSH bucketing (bucket_size=4, n_hashes=4) and chunking for memory, while LightTS achieves efficiency through: (a) downsampling to C×(T/C) matrices, (b) bottleneck design with F' << F intermediate dimension, (c) weight sharing across all rows/columns in projections. These dimension reduction and sharing strategies are completely absent in Reformer.\n\n7. **Input Transformation**: Reformer pads sequences to multiples of (bucket_size × 2) for LSH bucketing (fit_length function), while LightTS transforms inputs into non-overlapping sub-sequences through sampling without any padding. Must implement sampling logic that preserves all tokens while reorganizing them into 2D matrices for different temporal scales."
  },
  {
    "source": "FEDformer_2022",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Seasonal-Trend Decomposition Strategy**: Both papers adopt series decomposition to separate seasonal and trend-cyclical components. FEDformer uses MOEDecomp with multiple average pooling kernels combined via learned weights, while MICN uses MHDecomp with mean aggregation. The source code's `series_decomp` class and moving average pooling implementation can be directly adapted for MICN's decomposition block, only requiring modification of the aggregation method from learned weights to simple mean.\n\n2. **Multi-scale Temporal Modeling**: Both models employ multi-scale approaches. FEDformer's Wavelet Enhanced Structure (FEB-w/FEA-w) performs recursive decomposition at multiple scales (L levels), and MICN's MIC layer uses branches with different kernel sizes (I/4, I/8, etc.). The multi-scale decomposition logic in FEDformer's `MWT_CZ1d` and `MultiWaveletCross` classes provides reference architecture for implementing MICN's multi-branch structure.\n\n3. **Encoder-Decoder Architecture with Embedding**: Both use encoder-decoder style architectures with comprehensive embedding (value embedding, positional encoding, temporal features). FEDformer's `DataEmbedding` class combining value, position, and temporal encodings can be directly reused for MICN's embedding module, requiring only minor adjustments to accommodate MICN's zero-padding strategy.\n\n4. **Normalization and Residual Connections**: Both employ layer normalization and residual connections throughout their architectures. FEDformer's `my_Layernorm` and residual connection patterns in `EncoderLayer`/`DecoderLayer` can be directly adapted for MICN's MIC layers.\n\n5. **Final Prediction Assembly**: Both combine seasonal and trend predictions additively to produce final output. FEDformer's final assembly logic (`trend_part + seasonal_part`) provides direct template for MICN's `Y_pred = Y_s + Y_t` implementation.",
    "differences": "1. **Core Innovation - Frequency Domain vs. Convolution-Based**: FEDformer's main contribution is frequency-enhanced attention using Fourier/Wavelet transforms (FEB-f, FEA-f, FEB-w, FEA-w) operating in frequency domain with O(L) complexity. MICN innovates with **isometric convolution** for global correlation modeling in time domain. **NEW IMPLEMENTATION NEEDED**: Isometric convolution layer (padding S-1 zeros, kernel=S), transposed convolution for upsampling, and Conv2d-based branch merging - none of these exist in FEDformer's codebase.\n\n2. **Attention Mechanism vs. Convolution**: FEDformer uses frequency-domain attention mechanisms (Fourier cross-attention, wavelet cross-attention) with mode selection and complex multiplication operations. MICN **completely abandons attention**, using 1D convolution for local feature extraction and isometric convolution for global correlation. **NEW IMPLEMENTATION NEEDED**: The entire Local-Global module with Conv1d downsampling (stride=kernel) and isometric convolution architecture must be built from scratch.\n\n3. **Trend Modeling Approach**: FEDformer accumulates trend components through decoder layers with learned projections (`W_l,i * T_de^l,i`). MICN uses **simple linear regression or mean** for trend prediction without iterative refinement. **NEW IMPLEMENTATION NEEDED**: Linear regression module for trend-cyclical prediction (MICN-regre variant) - FEDformer has no equivalent simple regression strategy.\n\n4. **Multi-scale Implementation**: FEDformer's multi-scale is hierarchical via recursive wavelet decomposition (decompose → process → reconstruct). MICN's multi-scale is **parallel branches** with different kernel sizes processed simultaneously then merged via Conv2d. **NEW IMPLEMENTATION NEEDED**: Parallel multi-branch architecture with Conv2d-based weighted merging (Equation 9) - fundamentally different from FEDformer's sequential decomposition.\n\n5. **Input Strategy**: FEDformer uses traditional encoder-decoder input (encoder: past I, decoder: I/2 + O with label_len overlap). MICN uses **simplified complementary zero strategy** (Concat(X_s, X_zero)) without encoder-decoder separation. **NEW IMPLEMENTATION NEEDED**: Single-pass architecture with zero-padding instead of encoder-decoder structure.\n\n6. **Complexity and Efficiency**: FEDformer achieves O(L) via mode selection in frequency domain (selecting M<<L modes). MICN achieves efficiency through **downsampling via strided convolution** then isometric convolution on compressed sequences. Different efficiency strategies requiring separate implementation.\n\n7. **Channel Handling**: FEDformer processes channels through hidden dimension D with multi-head attention. MICN maintains explicit channel dimension d throughout, with convolutions operating on temporal dimension while preserving channel structure. **NEW IMPLEMENTATION NEEDED**: Channel-preserving convolution architecture distinct from FEDformer's attention-based channel mixing."
  },
  {
    "source": "Autoformer_2021",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Architecture**: Both papers employ time series decomposition to separate seasonal and trend-cyclical components. Autoformer uses `series_decomp` with moving average (AvgPool with padding), and MICN uses a similar decomposition strategy. The `series_decomp` module from Autoformer can be directly reused or adapted for MICN's decomposition needs, particularly the moving average implementation with padding to maintain sequence length.\n\n2. **Embedding Strategy**: Both models use similar embedding approaches combining value embedding, positional encoding, and temporal features. Autoformer's `DataEmbedding_wo_pos` (with `TokenEmbedding`, `PositionalEmbedding`, `TemporalEmbedding`/`TimeFeatureEmbedding`) can be largely reused for MICN's embedding layer. The Conv1d-based `TokenEmbedding` for value encoding is particularly relevant.\n\n3. **Normalization Techniques**: Both employ custom normalization. Autoformer uses `my_Layernorm` (LayerNorm with bias removal for seasonal parts), and MICN uses Norm operations in its MIC layers. The normalization modules from Autoformer can be adapted for MICN's seasonal prediction block.\n\n4. **Encoder-Decoder-like Structure**: Both models follow a decomposition-based prediction framework where seasonal and trend components are processed separately and then combined for final prediction. The overall architecture pattern of separating component modeling is shared.\n\n5. **Multi-scale Temporal Patterns**: Both papers recognize the importance of capturing patterns at different temporal scales. Autoformer's Auto-Correlation discovers period-based dependencies at multiple delays (top-k selection), while MICN uses multi-scale convolutions. The concept of handling multiple temporal resolutions is common.\n\n6. **Placeholder Strategy**: Both use zero-padding placeholders for future prediction horizons. Autoformer concatenates zeros for seasonal initialization, and MICN concatenates `X_zero` for embedding input. This pattern can be reused.\n\n7. **Progressive Refinement**: Both models employ layer-wise progressive processing. Autoformer's encoder/decoder layers progressively refine representations, and MICN's stacked MIC layers iteratively process features. The stacking pattern with residual connections is similar.",
    "differences": "1. **Core Temporal Modeling Mechanism**: Autoformer uses **Auto-Correlation mechanism** with FFT-based autocorrelation computation and time delay aggregation (Roll operations on values based on discovered periods), achieving O(L log L) complexity. MICN uses **Multi-scale Isometric Convolution (MIC)** with local Conv1d downsampling and global isometric convolution for correlation modeling, avoiding attention mechanisms entirely. NEW IMPLEMENTATION NEEDED: The entire MIC layer architecture including (a) multi-branch local Conv1d with different kernel sizes for downsampling, (b) isometric convolution (Conv1d with kernel=sequence_length and padding=sequence_length-1), (c) Conv1dTranspose for upsampling, and (d) Conv2d-based merge operation for multi-scale fusion.\n\n2. **Multi-scale Decomposition Strategy**: Autoformer uses a single-kernel moving average for decomposition. MICN introduces **Multi-scale Hybrid Decomposition (MHDecomp)** that applies multiple moving average kernels simultaneously and averages the results to capture different temporal patterns. NEW IMPLEMENTATION NEEDED: MHDecomp block that computes decomposition with multiple kernels (e.g., kernel_1, kernel_2, ..., kernel_n) and takes their mean for both trend and seasonal components.\n\n3. **Trend-Cyclical Prediction**: Autoformer uses an **accumulation structure** where trend components are progressively accumulated through decoder layers with learnable projections (W_l,1, W_l,2, W_l,3) and initialized with the mean of input. MICN uses either **simple linear regression** (MICN-regre) or **mean strategy** (MICN-mean) for trend prediction. NEW IMPLEMENTATION NEEDED: Linear regression module for trend forecasting that directly maps input trend to output trend without accumulation.\n\n4. **Encoder-Decoder vs. Single-Path**: Autoformer has a full **encoder-decoder architecture** with cross-attention between encoder output and decoder (encoder-decoder Auto-Correlation). MICN uses a **simplified single-path architecture** without explicit encoder-decoder separation—it directly processes the concatenated input+placeholder sequence through stacked MIC layers. NEW IMPLEMENTATION NEEDED: Remove encoder-decoder structure; implement direct sequential processing with complementary zero strategy.\n\n5. **Attention vs. Convolution**: Autoformer relies on **attention-based aggregation** (Auto-Correlation as attention replacement with Q, K, V projections and multi-head design). MICN completely replaces attention with **pure convolution operations** (local Conv1d, isometric Conv1d, Conv1dTranspose, Conv2d). NEW IMPLEMENTATION NEEDED: Isometric convolution implementation (large kernel convolution with specific padding strategy) to replace all attention mechanisms.\n\n6. **Channel Mixing Strategy**: Autoformer processes all channels together through its attention mechanism and feedforward networks. MICN's Conv2d merge operation explicitly models **inter-pattern relationships** across different scale branches. NEW IMPLEMENTATION NEEDED: Conv2d-based merging module that takes multi-scale outputs as input channels and produces weighted fusion.\n\n7. **Global Correlation Modeling**: Autoformer uses **series-wise Auto-Correlation** with FFT to find period-based dependencies globally. MICN uses **isometric convolution** (Conv1d with kernel equal to compressed sequence length) as an alternative to self-attention for global modeling, introducing translation equivariance as inductive bias. NEW IMPLEMENTATION NEEDED: Isometric convolution layer that pads sequence to 2S-1 length and applies Conv1d with kernel=S for global receptive field.\n\n8. **Complexity and Efficiency**: Autoformer achieves O(L log L) complexity through FFT-based computation. MICN achieves **O(L) complexity** through pure convolution operations with downsampling-upsampling, making it more efficient for very long sequences. The convolution-based approach avoids quadratic attention costs entirely.\n\n9. **Input Initialization**: Autoformer's decoder uses **latter half of encoder input** (length I/2) plus placeholders. MICN uses **full input sequence** (length I) concatenated with placeholders (length O), simplifying the input strategy and avoiding redundant computation. NEW IMPLEMENTATION NEEDED: Modified embedding that takes full sequence + O placeholders directly.\n\n10. **Layer-wise Processing**: Autoformer's decoder extracts trend at each layer and accumulates (trend = trend + W*extracted_trend). MICN's MIC layers do **not extract trend progressively**; trend is predicted separately and only seasonal part flows through MIC layers. NEW IMPLEMENTATION NEEDED: Separate trend and seasonal prediction paths without layer-wise trend extraction and accumulation."
  },
  {
    "source": "Informer_2020",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt an encoder-decoder paradigm for long-term sequence forecasting. Informer's basic encoder structure (EncoderLayer with attention + feedforward) can be adapted as a template. The DataEmbedding module (combining value embedding, positional encoding, and temporal features) from Informer can be directly reused for MICN's embedding component, requiring only minor modifications to accommodate MICN's concatenation of input with zero placeholders.\n\n2. **Time Series Decomposition Strategy**: Both employ decomposition to separate complex patterns - Informer uses moving average in its distilling operation (AvgPool in ConvLayer), while MICN uses multi-scale hybrid decomposition (MHDecomp). The core AvgPool-based decomposition logic from Informer's ConvLayer can be extended to implement MICN's multi-scale decomposition by applying multiple kernel sizes and averaging results. The basic decomposition framework (X_trend = AvgPool(Padding(X)), X_seasonal = X - X_trend) is shared.\n\n3. **Normalization and Regularization**: Both use LayerNorm and Dropout extensively for stabilization. Informer's normalization patterns in EncoderLayer/DecoderLayer (norm after attention, norm after feedforward) can be reused in MICN's MIC layers. The dropout application strategy (after activation functions) is identical.\n\n4. **Convolutional Operations for Feature Extraction**: Informer's ConvLayer uses Conv1d with ELU activation for distilling, which shares conceptual similarity with MICN's local module using Conv1d for downsampling. The Conv1d implementation pattern (kernel_size, padding, stride settings) from Informer can guide MICN's local feature extraction, though MICN requires additional Conv1dTranspose for upsampling.\n\n5. **Projection Layer for Output**: Both use a final linear projection to map hidden states to output dimensions. Informer's projection in Decoder (nn.Linear(d_model, c_out)) can be directly adapted for MICN's Projection operation before truncation.\n\n6. **Training Paradigm**: Both use MSE loss for forecasting and follow similar forward pass patterns (encode input, decode/predict, return final output). The training loop structure and loss calculation from Informer can be reused with minimal changes.",
    "differences": "1. **Core Attention Mechanism vs. Isometric Convolution**: Informer's main innovation is ProbSparse self-attention with O(L log L) complexity, using query sparsity measurement (KL divergence-based) and top-u query selection. This requires implementing _prob_QK, _get_initial_context, _update_context methods with sparse attention masks. MICN completely abandons self-attention in favor of isometric convolution (padding sequence with S-1 zeros, kernel=S) for global correlation modeling. NEW IMPLEMENTATION NEEDED: Isometric convolution layer with specific padding strategy (length S-1) and kernel size equal to sequence length, plus the mathematical justification that it provides translation equivariance as global inductive bias.\n\n2. **Multi-Scale Architecture Design**: Informer uses self-attention distilling with progressive halving (MaxPool with stride 2) to build a pyramid structure with concatenated outputs from multiple stacks. MICN employs multi-scale isometric convolution (MIC) with parallel branches using different scale sizes (I/4, I/8, etc.) and Conv2d-based weighted merging. NEW IMPLEMENTATION NEEDED: (a) Multi-branch architecture where each branch has different downsampling rates (kernel=i, stride=i for i∈{I/4, I/8,...}), (b) Conv2d merging layer to weight different scale outputs instead of simple concatenation, (c) Local-global module structure combining Conv1d downsampling + isometric convolution + Conv1dTranspose upsampling in each branch.\n\n3. **Decomposition Complexity**: Informer uses single-kernel moving average decomposition implicitly in distilling. MICN uses multi-scale hybrid decomposition (MHDecomp) with multiple AvgPool kernels and mean aggregation across scales. NEW IMPLEMENTATION NEEDED: Multi-kernel decomposition block that applies AvgPool with kernel_1, ..., kernel_n, then averages results: mean(AvgPool(X)_k1, ..., AvgPool(X)_kn). The kernel sizes must align with MIC branch scales.\n\n4. **Trend-Cyclical Prediction Strategy**: Informer doesn't explicitly model trend-cyclical components separately; trend information is implicitly captured through encoder representations. MICN uses explicit trend-cyclical prediction with two variants: (a) MICN-regre: linear regression on X_t, (b) MICN-mean: mean of X_t. NEW IMPLEMENTATION NEEDED: Separate trend prediction block with regression module (can be simple linear layer or more sophisticated regression) that operates on decomposed trend-cyclical component X_t.\n\n5. **Input Processing and Placeholder Strategy**: Informer's decoder uses start token (L_token length from input) + zero placeholders for target sequence, processed through masked attention. MICN concatenates input X_s with zero placeholders X_zero of length O directly in embedding stage, avoiding traditional encoder-decoder token strategy. NEW IMPLEMENTATION NEEDED: Modified embedding that concatenates Concat(X_s, X_zero) before applying TFE+PE+VE, eliminating the need for start tokens and masked attention.\n\n6. **Sequential Inference Capability**: Informer uses masked multi-head attention in decoder to prevent future information leakage, enabling auto-regressive generation if needed. MICN's isometric convolution naturally supports sequential inference through its padding structure, where the kernel encompasses entire sequence but convolution slides to generate predictions. NEW IMPLEMENTATION NEEDED: Isometric convolution's sequential inference mechanism where Conv1d with kernel=S and padding=S-1 allows position-wise prediction without explicit masking.\n\n7. **Global Correlation Modeling Philosophy**: Informer models global dependencies through query-key-value attention with learned probability distributions p(k_j|q_i). MICN argues that for shorter compressed sequences (after local downsampling), isometric convolution with large kernel provides better global correlation modeling through translation equivariance and temporal inductive bias from training data. NEW IMPLEMENTATION NEEDED: Theoretical justification and empirical validation that isometric convolution outperforms self-attention on compressed sequences, plus implementation of Tanh activation after isometric convolution (vs. Softmax in attention)."
  },
  {
    "source": "Reformer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanisms**: Both papers address the O(L²) complexity problem of standard Transformer attention. Reformer uses LSH (Locality-Sensitive Hashing) attention to reduce complexity to O(L log L), while Pyraformer uses pyramidal attention to achieve O(L). The source code's `LSHSelfAttention` module demonstrates sparse attention implementation patterns that can guide custom attention kernel development in Pyraformer.\n\n2. **Shared-QK Architecture**: Both models employ shared query-key mechanisms. Reformer explicitly sets Q=K through shared linear projections and normalizes keys (k_j = q_j/||q_j||). Pyraformer's pyramidal attention also operates on shared Q-K pairs within the multi-resolution structure. The source code's `ReformerLayer` with shared-QK can be adapted for Pyraformer's attention computation framework.\n\n3. **Multi-Head Attention Framework**: Both retain the multi-head attention mechanism from vanilla Transformer. The source implementation shows how to integrate custom attention (LSH) within standard multi-head structure. This architectural pattern (splitting heads, parallel attention, concatenation) can be directly reused when implementing Pyraformer's PAM with multiple heads.\n\n4. **Encoder-Only Architecture for Forecasting**: Both use encoder-based architectures for time series forecasting. The source code's `Model` class with `Encoder` and `EncoderLayer` stack provides a template for Pyraformer's encoder structure, including layer normalization and feed-forward networks.\n\n5. **Memory-Efficient Design Philosophy**: Both papers prioritize memory efficiency for long sequences. Reformer's reversible layers and chunking strategies (though not fully shown in the provided code) inspire similar memory-conscious implementations. The source code's `fit_length` function shows practical handling of sequence length constraints, which is relevant for Pyraformer's scale-based length requirements (L divisible by C^(S-1)).\n\n6. **Embedding Layer Reusability**: Both use standard embedding approaches combining data embedding, positional encoding, and temporal features. The source code's `DataEmbedding` module can be directly reused or minimally adapted for Pyraformer's embedding layer before the CSCM module.\n\n7. **Layer Stacking and Normalization**: Both employ stacked encoder layers with layer normalization. The source code's `Encoder` structure with `norm_layer=torch.nn.LayerNorm` provides a direct template for Pyraformer's N-layer encoder stack.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses hash-based bucketing (LSH) to group similar queries/keys, while Pyraformer uses pyramidal graph-based sparse attention with inter-scale and intra-scale connections. Pyraformer requires implementing: (a) Custom CUDA kernel for pyramidal attention pattern where each node attends to A adjacent nodes, C children, and 1 parent (Equation 3), (b) Graph-based attention masking for the specific connectivity pattern in Equation 2, (c) Multi-scale attention aggregation across S scales. The source code's LSH bucketing logic cannot be reused as the sparsity patterns are fundamentally different.\n\n2. **Multi-Resolution Architecture - NEW CSCM MODULE REQUIRED**: Pyraformer introduces a hierarchical C-ary tree structure with S scales, requiring implementation of the Coarser-Scale Construction Module (CSCM). This needs: (a) Sequential convolution layers with kernel size C and stride C to create coarser scales (Figure 3), (b) Bottleneck structure with dimension reduction/restoration, (c) Concatenation of multi-scale sequences before PAM input. Reformer has no equivalent multi-resolution construction - it operates on a single-scale sequence with hash-based grouping.\n\n3. **Complexity and Path Length Guarantees**: Reformer achieves O(L log L) complexity with O(log L) maximum path length through hash bucketing, while Pyraformer achieves O(L) complexity with O(1) maximum path length through pyramidal structure (Propositions 1-2). Pyraformer's complexity depends on constants A (adjacent nodes), C (tree branching), and S (scales), requiring careful hyperparameter selection to satisfy Equations 4-5 for global receptive field. Implementation must track and validate these constraints.\n\n4. **Prediction Module Design - NEW DECODER OPTIONS REQUIRED**: Pyraformer offers two prediction strategies: (a) Single-step: gather features from last nodes at all scales, concatenate, and map through FC layer, (b) Multi-step with two-layer decoder: uses prediction tokens F_p with full attention layers combining encoder output F_e. Reformer's source code only shows simple linear projection (`self.projection`). Pyraformer requires implementing: scale-wise feature gathering, multi-scale concatenation, and the optional two-layer attention decoder with specific Q-K-V routing.\n\n5. **Sequence Length Handling**: Reformer requires sequence length to be divisible by (bucket_size × 2) and uses padding (`fit_length` function). Pyraformer requires L divisible by C^(S-1) for proper C-ary tree construction. The padding/truncation logic needs to be redesigned based on the tree structure constraints rather than hash bucket constraints.\n\n6. **Attention Scope and Receptive Field**: Reformer uses hash-based probabilistic attention where similar items may fall in different buckets (addressed by n_hashes rounds), while Pyraformer uses deterministic graph-based attention with explicit parent-child-sibling relationships. Pyraformer's receptive field grows deterministically through the tree structure, requiring implementation of the specific neighbor set N_ℓ^(s) computation (Equation 2) with adjacent, children, and parent sets.\n\n7. **Parameter Efficiency Trade-offs**: Reformer's parameters are O(N(HD_KD_K + DD_F)) same as vanilla Transformer, while Pyraformer adds O((S-1)CD_K²) parameters for CSCM convolutions. The source code doesn't include reversible layers or chunking (mentioned in Reformer paper), while Pyraformer requires implementing the CSCM bottleneck structure to control parameter growth across scales.\n\n8. **Custom Kernel Requirements**: Pyraformer explicitly requires custom TVM-based CUDA kernel for pyramidal attention to avoid O(L²) naive masked attention implementation. Reformer's source code uses the `reformer_pytorch` library's `LSHSelfAttention`, which handles LSH internally. Pyraformer needs a completely new sparse attention kernel specialized for the pyramidal graph pattern, as existing libraries don't support this specific connectivity structure."
  },
  {
    "source": "Informer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture**: Both use encoder-decoder frameworks for long-range time series forecasting. The source code's `Encoder`, `Decoder`, and `EncoderLayer`/`DecoderLayer` base structures can be adapted, particularly the decoder's cross-attention mechanism that attends to encoder outputs.\n\n2. **Embedding Strategy**: Both employ identical input embedding approaches combining data embedding, temporal encoding, and positional encoding. The source's `DataEmbedding` class (combining token, positional, and temporal embeddings) can be directly reused for Pyraformer.\n\n3. **Multi-Head Attention Framework**: Both use multi-head attention as the core mechanism. The source's `AttentionLayer` wrapper (with query/key/value projections and multi-head splitting) provides a reusable template, though the inner attention computation differs.\n\n4. **Feed-Forward Networks**: Both employ identical position-wise feed-forward networks after attention layers. The source's Conv1d-based FFN implementation in encoder/decoder layers (with dropout, activation, and layer normalization) can be directly reused.\n\n5. **Layer Normalization and Residual Connections**: Both use pre-norm or post-norm architectures with residual connections. The source's normalization strategy in `EncoderLayer` and `DecoderLayer` can be adapted.\n\n6. **Generative Decoder Input**: Both use start tokens with zero placeholders for future predictions. The source's decoder input construction `Concat(X_token, X_0)` (Eq. 6) matches Pyraformer's prediction token approach.\n\n7. **Single Forward Pass Prediction**: Both avoid autoregressive decoding by predicting all future steps in one forward pass, using masked attention in the decoder to prevent information leakage from future time steps.\n\n8. **MSE Loss Function**: Both use mean squared error as the training objective, directly applicable from source implementation.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Informer uses ProbSparse attention (O(L log L)) with query sparsity measurement and top-u query selection, while Pyraformer introduces Pyramidal Attention Module (PAM) with O(L) complexity. Pyraformer requires implementing: (a) Multi-resolution pyramidal graph structure with inter-scale (parent-child) and intra-scale (neighboring nodes) connections, (b) Sparse attention where each node attends only to neighbors A_ℓ^(s) ∪ C_ℓ^(s) ∪ P_ℓ^(s) (Eq. 2-3), (c) Custom CUDA kernel using TVM for efficient sparse attention computation, which is completely absent in Informer's ProbSparse implementation.\n\n2. **Multi-Scale Hierarchical Structure - NEW IMPLEMENTATION REQUIRED**: Informer uses single-scale processing with distilling (halving sequence length via max-pooling), while Pyraformer constructs explicit multi-resolution C-ary tree. Requires implementing: (a) Coarser-Scale Construction Module (CSCM) with sequential Conv1d layers (kernel=C, stride=C) to build scales from fine to coarse, (b) Bottleneck structure (dimension reduction before convolution, restoration after) to reduce parameters, (c) Concatenation of all scale representations before PAM, (d) Scale-specific node initialization and management across S scales.\n\n3. **Encoder Architecture**: Informer uses distilling with ConvLayer (Conv1d + BatchNorm + ELU + MaxPool) to progressively halve sequence length, creating pyramid-like replicas with different input lengths. Pyraformer builds explicit multi-scale nodes via CSCM without distilling. The source's distilling mechanism cannot be reused; need to implement CSCM's scale construction logic.\n\n4. **Attention Complexity and Sparsity Pattern**: Informer's ProbSparse selects top-u queries based on KL-divergence measurement M(q_i, K) = LSE - mean (Eq. 2), with random sampling for efficiency. Pyraformer's PAM has fixed sparse pattern based on graph topology (parent-child-sibling connections), independent of attention scores. Completely different sparsity strategies requiring separate implementations.\n\n5. **Decoder Prediction Module - PARTIAL NEW IMPLEMENTATION**: Informer uses single decoder with ProbSparse self-attention and cross-attention. Pyraformer offers two options: (a) Single-step style: gather last nodes from all scales + FC layer (NEW), (b) Multi-step style: decoder with two full attention layers (can adapt from source but needs modification to attend to all PAM nodes across scales, not just encoder output).\n\n6. **Maximum Path Length Guarantee**: Informer maintains O(log L) maximum path length through hierarchical distilling. Pyraformer achieves O(1) maximum path length via pyramidal graph design (Proposition 2) when C satisfies Eq. 5, enabling more direct long-range dependency modeling. This is a theoretical difference affecting hyperparameter selection (A, C, S).\n\n7. **Memory Management**: Informer uses attention masking (ProbMask, TriangularCausalMask) and selective query computation to achieve O(L log L) memory. Pyraformer's custom CUDA kernel exploits sparse graph structure for O(L) memory with fixed A+C+1 neighbors per node. Requires implementing specialized sparse attention kernel rather than using standard PyTorch operations.\n\n8. **Scale-Specific Feature Aggregation - NEW IMPLEMENTATION**: Pyraformer concatenates features from last nodes at all S scales before prediction, leveraging multi-resolution information explicitly. Informer concatenates outputs from pyramid-like encoder replicas but doesn't have explicit scale-wise feature extraction. Need to implement scale-specific gathering and concatenation logic."
  },
  {
    "source": "DLinear_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition**: Both papers employ decomposition strategies to handle trend and seasonal components. DLinear uses moving average kernels from Autoformer for trend-seasonal decomposition (series_decomp), while TimesNet implicitly handles periodicity through FFT-based period detection and 2D reshaping.\n2. **Normalization Technique**: Both implementations use instance normalization (zero-mean normalization). DLinear's code doesn't explicitly show normalization in the provided snippet, but TimesNet's implementation includes mean-std normalization with detach() on statistics to prevent gradient flow: `means = x_enc.mean(1, keepdim=True).detach()` and `stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)`, following Non-stationary Transformer's approach.\n3. **Direct Multi-Step Forecasting**: Both adopt Direct Multi-Step (DMS) forecasting strategy rather than autoregressive approaches, avoiding error accumulation in long-term predictions. Both predict the entire future horizon in one forward pass.\n4. **Residual Connections**: Both architectures incorporate residual/skip connections. DLinear sums seasonal and trend outputs, while TimesNet uses explicit residual connections in TimesBlock: `res = res + x`.\n5. **Linear Projection for Output**: Both use linear layers for final predictions. DLinear applies separate linear layers to seasonal and trend components, while TimesNet uses `self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)` for output projection.",
    "differences": "1. **Core Innovation**: DLinear challenges Transformer-based methods by demonstrating that simple linear models with decomposition outperform complex architectures for LTSF, emphasizing that temporal ordering matters more than semantic correlations. TimesNet proposes transforming 1D time series into 2D tensors based on multi-periodicity, enabling simultaneous modeling of intraperiod- and interperiod-variations through 2D convolutions.\n2. **Temporal Modeling Approach**: DLinear uses pure linear mappings along the temporal axis (weight matrix W ∈ R^(T×L)) without any convolution or attention mechanisms. TimesNet employs 2D convolutional inception blocks to capture spatial-temporal patterns in reshaped 2D tensors, combining FFT-based period detection with parameter-efficient 2D convolutions.\n3. **Multi-Scale and Multi-Period Handling**: DLinear operates on a single temporal scale with fixed input length, handling only trend-seasonal decomposition. TimesNet explicitly models multiple periodicities by selecting top-k frequencies from FFT analysis, creating k different 2D representations (one per period), and adaptively aggregating them using softmax-weighted combination based on amplitude values.\n4. **Architecture Complexity**: DLinear has O(L) complexity with minimal parameters (two linear layers of size seq_len × pred_len). TimesNet has higher complexity with multi-layer architecture (e_layers TimesBlocks), each containing inception blocks with multiple 2D convolution kernels (num_kernels, typically 6), FFT operations, and adaptive aggregation mechanisms.\n5. **Channel Strategy**: DLinear supports both channel-independent (individual=True, separate linear layers per channel) and channel-mixed modes (individual=False, shared weights). DLinear explicitly initializes weights as (1/seq_len) * ones for both seasonal and trend components. TimesNet processes all channels together through embedding layers and shared 2D convolutions, with channel dimension maintained throughout: `out.reshape(B, length // period, period, N).permute(0, 3, 1, 2)`.\n6. **Decomposition Philosophy**: DLinear uses explicit moving average-based decomposition at the input level, separating trend and seasonal components before applying linear transformations. TimesNet uses implicit frequency-domain decomposition through FFT to discover dominant periods, then reshapes time series into 2D structures where rows represent interperiod-variation and columns represent intraperiod-variation.\n7. **Embedding and Feature Processing**: DLinear operates directly on raw input values without embedding layers, maintaining interpretability. TimesNet uses DataEmbedding with positional and temporal encodings, projects inputs to d_model dimensions, and includes a predict_linear layer to align temporal dimensions before processing: `enc_out = self.predict_linear(enc_out.permute(0, 2, 1)).permute(0, 2, 1)`."
  },
  {
    "source": "FEDformer_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Frequency Domain Analysis for Period Discovery**: Both papers utilize Fast Fourier Transform (FFT) to analyze time series in the frequency domain. FEDformer uses FFT for frequency-enhanced attention and feature extraction, while TimesNet uses FFT specifically to discover dominant periods by selecting top-k frequencies based on amplitude values. Both employ `torch.fft.rfft` for real-valued FFT computation.\n2. **Mode/Frequency Selection Strategy**: Both implement selective frequency processing rather than using all frequencies. FEDformer's implementation uses `get_frequency_modes()` with random or lowest-mode selection to choose a subset of frequencies (default modes=64), while TimesNet uses `FFT_for_Period()` to select top-k dominant frequencies (default k=2) based on amplitude ranking. Both zero out the DC component (frequency[0]=0) to avoid bias.\n3. **Encoder-Decoder Architecture with Residual Connections**: Both adopt multi-layer architectures with residual connections. FEDformer uses explicit encoder-decoder structure with seasonal-trend decomposition at each layer, while TimesNet stacks TimesBlocks in a residual manner (Equation 4: X^l = TimesBlock(X^(l-1)) + X^(l-1)).\n4. **Embedding Layer for Input Processing**: Both use DataEmbedding for initial feature projection. FEDformer embeds historical series to X_en^0 ∈ R^(I×D), while TimesNet projects raw inputs to deep features X_1D^0 ∈ R^(T×d_model) via Embed() operation.\n5. **Normalization Strategy**: Both papers implement instance normalization techniques. TimesNet explicitly uses Non-stationary Transformer's normalization (subtracting mean and dividing by standard deviation) before processing and reverses it after prediction. FEDformer's implementation code shows similar normalization patterns in the decomposition blocks.",
    "differences": "1. **Core Innovation - Frequency vs. Period-based Modeling**: FEDformer's core innovation is Frequency Enhanced Decomposition, operating entirely in frequency domain with learnable complex-valued kernels (R ∈ C^(D×D×M)) for Fourier/Wavelet basis manipulation. TimesNet's innovation is transforming 1D time series into 2D tensors based on discovered periods, then applying 2D convolutions to capture intraperiod and interperiod variations simultaneously. FEDformer stays in frequency domain for computation, while TimesNet converts back to time domain after reshaping.\n2. **Attention Mechanism vs. Convolutional Processing**: FEDformer implements Frequency Enhanced Attention (FEA) with complex multiplication in frequency domain (compl_mul1d operation with real/imaginary component handling) for both self-attention (FEB) and cross-attention (FEA-f/FEA-w). TimesNet completely abandons attention mechanisms, instead using parameter-efficient Inception blocks with multiple 2D convolutional kernels (kernel sizes from 1×1 to 2k+1×2k+1, default num_kernels=6) to process reshaped 2D tensors.\n3. **Decomposition Strategy - Explicit vs. Implicit**: FEDformer employs explicit Mixture of Experts Decomposition (MOEDecomp) at every layer to separate seasonal and trend components, with trend accumulation across decoder layers (T_de^l = T_de^(l-1) + W·T_de^(l,i)). TimesNet has no explicit decomposition mechanism; it relies on the 2D transformation and convolutional processing to implicitly capture temporal patterns, though it borrows normalization from Non-stationary Transformer.\n4. **Multi-scale Temporal Modeling Approach**: FEDformer handles multiple scales through random frequency mode selection (selecting M modes from N//2 available frequencies) with a single set of learnable weights applied across selected modes. TimesNet explicitly models k different periods (default k=2) by creating k separate 2D tensors (X_2D^i ∈ R^(p_i×f_i×C) for i=1..k), processing each with shared Inception blocks, then adaptively aggregating results using softmax-normalized amplitude weights as attention scores.\n5. **Computational Complexity and Padding Strategy**: FEDformer achieves O(L) complexity through sparse frequency selection and FFT operations (O(N log N) reduced to O(N) with mode selection). TimesNet's complexity depends on period lengths and involves reshaping operations requiring zero-padding to make sequence length divisible by each period (length = ((seq_len+pred_len)//period + 1)*period), followed by 2D convolutions on multiple reshaped tensors, resulting in higher memory overhead but enabling explicit 2D variation modeling.\n6. **Channel Handling and Output Strategy**: FEDformer processes all channels together through frequency domain operations with channel dimension preserved throughout (D or d_model channels), using final projection W_S for seasonal component. TimesNet's Inception blocks operate on all channels simultaneously in 2D space (permute to [B, N, length//period, period] format), and uses a predict_linear layer to align temporal dimensions before applying TimesBlocks, followed by a projection layer for final output dimension matching."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Multi-resolution Temporal Modeling**: Both papers employ multi-scale/multi-resolution approaches to capture temporal patterns. Pyraformer uses a pyramidal structure with C-ary tree hierarchy (daily→weekly→monthly scales), while TimesNet discovers multiple periods via FFT and creates multiple 2D representations.\n2. **Embedding Strategy**: Both use similar embedding approaches combining value embedding with temporal covariates. Pyraformer's implementation uses DataEmbedding(enc_in, d_model, dropout) for position and covariate encoding, and TimesNet uses DataEmbedding(enc_in, d_model, embed, freq, dropout) with the same architectural pattern.\n3. **Hierarchical Feature Extraction**: Both employ hierarchical processing - Pyraformer through pyramidal attention across scales, TimesNet through period-based 2D tensor transformations. Both aggregate information from multiple resolutions/periods.\n4. **Residual Connections**: Both architectures use residual connections in their core blocks. Pyraformer's EncoderLayer and TimesNet's TimesBlock both implement skip connections to facilitate gradient flow.\n5. **Batch Normalization in Convolution**: Both use normalization in convolutional operations. Pyraformer's ConvLayer includes BatchNorm1d after 1D convolution, while TimesNet's Inception_Block_V1 applies 2D convolutions with proper initialization.",
    "differences": "1. **Core Innovation**: Pyraformer introduces pyramidal attention with C-ary tree structure to achieve O(L) complexity for long-range dependencies via inter-scale and intra-scale connections. TimesNet proposes transforming 1D time series into 2D space based on discovered periods to simultaneously model intraperiod- and interperiod-variations.\n2. **Attention Mechanism**: Pyraformer uses sparse pyramidal attention where each node attends to neighbors at same scale (A nodes), children (C nodes), and parent (1 node), with explicit attention mask construction. TimesNet completely abandons attention mechanisms, instead using 2D convolutional Inception blocks to capture spatial-temporal patterns in reshaped tensors.\n3. **Frequency Domain Usage**: Pyraformer does not explicitly use frequency analysis. TimesNet extensively uses FFT to discover dominant periods (top-k frequencies), uses amplitude values for adaptive aggregation weights (softmax normalization), and bases its entire 2D transformation on periodicity detection.\n4. **Complexity and Architecture**: Pyraformer achieves O(AL) ≈ O(L) time/space complexity through sparse pyramidal graph with maximum path length O(log_C L). TimesNet has O(kL) complexity where k is number of selected periods, using parameter-efficient Inception blocks with multiple kernel sizes (1×1, 3×3, 5×5, etc.).\n5. **Temporal Structure Representation**: Pyraformer maintains 1D sequential structure throughout, building multi-scale hierarchy via downsampling convolutions (window_size stride) and tree connections. TimesNet explicitly reshapes 1D sequences into 2D tensors (period × frequency) for each discovered period, enabling 2D convolution to capture both intra-period and inter-period variations.\n6. **Prediction Strategy**: Pyraformer uses standard encoder architecture with linear projection for forecasting. TimesNet employs predict_linear layer to align temporal dimensions (seq_len → pred_len+seq_len) before TimesBlocks, and applies Non-stationary Transformer's normalization (mean subtraction, standard deviation division with detach() and denormalization).\n7. **Aggregation Mechanism**: Pyraformer aggregates multi-scale features by gathering from pyramid using refer_points indexes to extract finest-scale representations. TimesNet uses adaptive aggregation with FFT amplitude-based weights (softmax normalized) to combine representations from different periods: res = Σ(res_i × period_weight_i)."
  }
]
