{
  "id": "FEDformer_2022",
  "paper_title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting",
  "alias": "FEDformer",
  "year": 2022,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "FEDformer introduces Frequency Enhanced Decomposition for long-term time series forecasting by replacing self-attention and cross-attention mechanisms with frequency domain operations. The core innovation includes: (1) Frequency Enhanced Blocks (FEB) and Frequency Enhanced Attention (FEA) that operate in frequency domain using either Fourier or Wavelet transforms, achieving O(L) complexity; (2) Random mode selection strategy that selects a small subset of frequency modes rather than all modes or just low-frequency modes; (3) Mixture of Experts Decomposition (MOEDecomp) that uses multiple average pooling filters with different window sizes for better seasonal-trend decomposition. Unlike Autoformer which uses sub-sequence level correlation in time domain, FEDformer performs global feature extraction in frequency domain across all modes.",
  "introduction": "# 1. Introduction\n\nLong-term time series forecasting is a long-standing challenge in various applications (e.g., energy, weather, traffic, economics). Despite the impressive results achieved by RNN-type methods (Rangapuram et al., 2018; Flunkert et al., 2017), they often suffer from the problem of gradi-\n\n*Equal contribution  Machine Intelligence Technology, Alibaba Group.. Correspondence to: Tian Zhou <tian.zt@alibaba-inc.com>, Rong Jin <jinrong.jr@alibaba-inc.com>.\n\nProceedings of the  $39^{th}$  International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\n\nent vanishing or exploding (Pascanu et al., 2013), significantly limiting their performance. Following the recent success in NLP and CV community (Vaswani et al., 2017; Devlin et al., 2019; Dosovitskiy et al., 2021; Rao et al., 2021), Transformer (Vaswani et al., 2017) has been introduced to capture long-term dependencies in time series forecasting and shows promising results (Zhou et al., 2021; Wu et al., 2021). Since high computational complexity and memory requirement make it difficult for Transformer to be applied to long sequence modeling, numerous studies are devoted to reduce the computational cost of Transformer (Li et al., 2019; Kitaev et al., 2020; Zhou et al., 2021; Wang et al., 2020; Xiong et al., 2021; Ma et al., 2021). A through overview of this line of works can be found in Appendix A.\n\nDespite the progress made by Transformer-based methods for time series forecasting, they tend to fail in capturing the overall characteristics/distribution of time series in some cases. In Figure 1, we compare the time series of ground truth with that predicted by the vanilla Transformer method (Vaswani et al., 2017) in a real-world ETTm1 dataset (Zhou et al., 2021). It is clear that the predicted time series shared a different distribution from that of ground truth. The discrepancy between ground truth and prediction could be explained by the point-wise attention and prediction in Transformer. Since prediction for each timestep is made individually and independently, it is likely that the model fails to maintain the global property and statistics of time series as a whole. To address this problem, we exploit two ideas in this work. The first idea is to incorporate a seasonal-trend decomposition approach (Cleveland et al., 1990; Wen et al., 2019), which is widely used in time series analysis, into the Transformer-based method. Although this idea has been exploited before (Oreshkin et al., 2019; Wu et al., 2021), we present a special design of network that is effective in bringing the distribution of prediction close to that of ground truth, according to Kologrov-Smirnov distribution test. Our second idea is to combine Fourier analysis with the Transformer-based method. Instead of applying Transformer to the time domain, we apply it to the frequency domain which helps Transformer better capture global properties of time series. Combining both ideas, we propose a Frequency Enhanced\n\nDecomposition Transformer, or, FEDformer for short, for long-term time series forecasting.\n\nOne critical question with FEDformer is which subset of frequency components should be used by Fourier analysis to represent time series. A common wisdom is to keep low-frequency components and throw away the high-frequency ones. This may not be appropriate for time series forecasting as some of trend changes in time series are related to important events, and this piece of information could be lost if we simply remove all high-frequency components. We address this problem by effectively exploiting the fact that time series tend to have (unknown) sparse representations on a basis like Fourier basis. According to our theoretical analysis, a randomly selected subset of frequency components, including both low and high ones, will give a better representation for time series, which is further verified by extensive empirical studies. Besides being more effective for long term forecasting, combining Transformer with frequency analysis allows us to reduce the computational cost of Transformer from quadratic to linear complexity. We note that this is different from previous efforts on speeding up Transformer, which often leads to a performance drop.\n\nIn short, we summarize the key contributions of this work as follows:\n\n1. We propose a frequency enhanced decomposed Transformer architecture with mixture of experts for seasonal-trend decomposition in order to better capture global properties of time series.  \n2. We propose Fourier enhanced blocks and Wavelet enhanced blocks in the Transformer structure that allows us to capture important structures in time series through frequency domain mapping. They serve as substitutions for both self-attention and cross-attention blocks.  \n3. By randomly selecting a fixed number of Fourier components, the proposed model achieves linear computational complexity and memory cost. The effectiveness of this selection method is verified both theoretically and empirically.  \n4. We conduct extensive experiments over 6 benchmark datasets across multiple domains (energy, traffic, economics, weather and disease). Our empirical studies show that the proposed model improves the performance of state-of-the-art methods by  $14.8\\%$  and  $22.6\\%$  for multivariate and univariate forecasting, respectively.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/4a4c760e4c0098ca29ea6fa4890e5d148828ec62a967e551c4eb59d6c41f9be1.jpg)  \nFigure 1. Different distribution between ground truth and forecasting output from vanilla Transformer in a real-world ETTm1 dataset. Left: frequency mode and trend shift. Right: trend shift.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/ce800c81a26fa81fcbb81c8aab11161c3661df7470874ffca000eb8056ab927c.jpg)\n",
  "method": "# 3. Model Structure\n\nIn this section, we will introduce (1) the overall structure of FEDformer, as shown in Figure 2, (2) two subversion structures for signal process: one uses Fourier basis and the other uses Wavelet basis, (3) the mixture of experts mechanism for seasonal-trend decomposition, and (4) the complexity analysis of the proposed model.\n\n# 3.1.FEDformer Framework\n\nPreliminary Long-term time series forecasting is a sequence to sequence problem. We denote the input length as  $I$  and output length as  $O$ . We denote  $D$  as the hidden states of the series. The input of the encoder is a  $I \\times D$  matrix and the decoder has  $(I / 2 + O) \\times D$  input.\n\nFEDformer Structure Inspired by the seasonal-trend decomposition and distribution analysis as discussed in Section 1, we renovate Transformer as a deep decomposition architecture as shown in Figure 2, including Frequency Enhanced Block (FEB), Frequency Enhanced Attention\n\n(FEA) connecting encoder and decoder, and the Mixture Of Experts Decomposition block (MODEcomp). The detailed description of FEB, FEA, and MODEcomp blocks will be given in the following Section 3.2, 3.3, and 3.4 respectively.\n\nThe encoder adopts a multilayer structure as:  $\\mathcal{X}_{\\mathrm{en}}^{l} =$  Encoder  $(\\mathcal{X}_{\\mathrm{en}}^{l - 1})$  , where  $l\\in \\{1,\\dots ,N\\}$  denotes the output of  $l$  -th encoder layer and  $\\mathcal{X}_{\\mathrm{en}}^0\\in \\mathbb{R}^{I\\times D}$  is the embedded historical series. The Encoder  $(\\cdot)$  is formalized as\n\n$$\n\\begin{array}{l} \\mathcal {S} _ {\\mathrm {e n}, -} ^ {l, 1} = \\operatorname {M O E D e c o m p} \\left(\\operatorname {F E B} \\left(\\mathcal {X} _ {\\mathrm {e n}} ^ {l - 1}\\right) + \\mathcal {X} _ {\\mathrm {e n}} ^ {l - 1}\\right), \\\\ \\mathcal {S} _ {\\mathrm {e n}, -} ^ {l, 2} = \\operatorname {M O E D e c o m p} (\\text {F e e d F o r w a r d} \\left(\\mathcal {S} _ {\\mathrm {e n}} ^ {l, 1}\\right) + \\mathcal {S} _ {\\mathrm {e n}} ^ {l, 1}), \\tag {1} \\\\ \\mathcal {X} _ {\\mathrm {e n}} ^ {l} = \\mathcal {S} _ {\\mathrm {e n}} ^ {l, 2}, \\\\ \\end{array}\n$$\n\nwhere  $S_{\\mathrm{en}}^{l,i}, i \\in \\{1,2\\}$  represents the seasonal component after the  $i$ -th decomposition block in the  $l$ -th layer respectively. For FEB module, it has two different versions (FEB-f & FEB-w) which are implemented through Discrete Fourier transform (DFT) and Discrete Wavelet transform (DWT) mechanism respectively and can seamlessly replace the self-attention block.\n\nThe decoder also adopts a multilayer structure as:  $\\mathcal{X}_{\\mathrm{de}}^{l},\\mathcal{T}_{\\mathrm{de}}^{l} = \\mathrm{Decoder}(\\mathcal{X}_{\\mathrm{de}}^{l - 1},\\mathcal{T}_{\\mathrm{de}}^{l - 1})$  , where  $l\\in \\{1,\\dots ,M\\}$  denotes the output of  $l$  -th decoder layer. The Decoder(·) is formalized as\n\n$$\n\\begin{array}{l} \\mathcal {S} _ {\\mathrm {d e}} ^ {l, 1}, \\mathcal {T} _ {\\mathrm {d e}} ^ {l, 1} = \\operatorname {M O E D e c o m p} \\left(\\operatorname {F E B} \\left(\\mathcal {X} _ {\\mathrm {d e}} ^ {l - 1}\\right) + \\mathcal {X} _ {\\mathrm {d e}} ^ {l - 1}\\right), \\\\ \\mathcal {S} _ {\\mathrm {d e}} ^ {l, 2}, \\mathcal {T} _ {\\mathrm {d e}} ^ {l, 2} = \\operatorname {M O E D e c o m p} \\left(\\operatorname {F E A} \\left(\\mathcal {S} _ {\\mathrm {d e}} ^ {l, 1}, \\mathcal {X} _ {\\mathrm {e n}} ^ {N}\\right) + \\mathcal {S} _ {\\mathrm {d e}} ^ {l, 1}\\right), \\\\ \\mathcal {S} _ {\\mathrm {d e}} ^ {l, 3}, \\mathcal {T} _ {\\mathrm {d e}} ^ {l, 3} = \\operatorname {M O E D e c o m p} \\left(\\operatorname {F e e d F o r w a r d} \\left(\\mathcal {S} _ {\\mathrm {d e}} ^ {l, 2}\\right) + \\mathcal {S} _ {\\mathrm {d e}} ^ {l, 2}\\right), \\\\ \\mathcal {X} _ {\\mathrm {d e}} ^ {l} = \\mathcal {S} _ {\\mathrm {d e}} ^ {l, 3}, \\\\ \\mathcal {T} _ {\\mathrm {d e}} ^ {l} = \\mathcal {T} _ {\\mathrm {d e}} ^ {l - 1} + \\mathcal {W} _ {l, 1} \\cdot \\mathcal {T} _ {\\mathrm {d e}} ^ {l, 1} + \\mathcal {W} _ {l, 2} \\cdot \\mathcal {T} _ {\\mathrm {d e}} ^ {l, 2} + \\mathcal {W} _ {l, 3} \\cdot \\mathcal {T} _ {\\mathrm {d e}} ^ {l, 3}, \\tag {2} \\\\ \\end{array}\n$$\n\nwhere  $S_{\\mathrm{de}}^{l,i}, T_{\\mathrm{de}}^{l,i}, i \\in \\{1,2,3\\}$  represent the seasonal and trend component after the  $i$ -th decomposition block in the  $l$ -th layer respectively.  $W_{l,i}, i \\in \\{1,2,3\\}$  represents the projector for the  $i$ -th extracted trend  $T_{\\mathrm{de}}^{l,i}$ . Similar to FEB, FEA has two different versions (FEA-f & FEA-w) which are implemented through DFT and DWT projection respectively with attention design, and can replace the cross-attention block. The detailed description of  $\\mathrm{FEA}(\\cdot)$  will be given in the following Section 3.3.\n\nThe final prediction is the sum of the two refined decomposed components as  $\\mathcal{W}_S \\cdot \\mathcal{X}_{\\mathrm{de}}^M + \\mathcal{T}_{\\mathrm{de}}^M$ , where  $\\mathcal{W}_S$  is to project the deep transformed seasonal component  $\\mathcal{X}_{\\mathrm{de}}^M$  to the target dimension.\n\n# 3.2. Fourier Enhanced Structure\n\nDiscrete Fourier Transform (DFT) The proposed Fourier Enhanced Structures use discrete Fourier transform (DFT). Let  $\\mathcal{F}$  denotes the Fourier transform and  $\\mathcal{F}^{-1}$  de\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/54203324ae193dc01609d825f7e05753c42f203e4db917505d2f082ce58836b6.jpg)  \nFigure 3. Frequency Enhanced Block with Fourier transform (FEB-f) structure.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/35386d832ca848173302fb5e265217301c03258a63a4aadd57516c4e9f3f8707.jpg)  \nFigure 4. Frequency Enhanced Attention with Fourier transform (FEA-f) structure,  $\\sigma (\\cdot)$  is the activation function.\n\nnotes the inverse Fourier transform. Given a sequence of real numbers  $x_{n}$  in time domain, where  $n = 1,2\\dots N$ . DFT is defined as  $X_{l} = \\sum_{n = 0}^{N - 1}x_{n}e^{-i\\omega ln}$ , where  $i$  is the imaginary unit and  $X_{l}, l = 1,2\\dots L$  is a sequence of complex numbers in the frequency domain. Similarly, the inverse DFT is defined as  $x_{n} = \\sum_{l = 0}^{L - 1}X_{l}e^{i\\omega ln}$ . The complexity of DFT is  $O(N^2)$ . With fast Fourier transform (FFT), the computation complexity can be reduced to  $O(N\\log N)$ . Here a random subset of the Fourier basis is used and the scale of the subset is bounded by a scalar. When we choose the mode index before DFT and reverse DFT operations, the computation complexity can be further reduced to  $O(N)$ .\n\nFrequency Enhanced Block with Fourier Transform (FEB-f) The FEB-f is used in both encoder and decoder as shown in Figure 2. The input  $(\\boldsymbol{x} \\in \\mathbb{R}^{N \\times D})$  of the FEB-f block is first linearly projected with  $\\boldsymbol{w} \\in \\mathbb{R}^{D \\times D}$ , so  $\\boldsymbol{q} = \\boldsymbol{x} \\cdot \\boldsymbol{w}$ . Then  $\\boldsymbol{q}$  is converted from the time domain to the frequency domain. The Fourier transform of  $\\boldsymbol{q}$  is denoted as  $\\boldsymbol{Q} \\in \\mathbb{C}^{N \\times D}$ . In frequency domain, only the randomly selected  $M$  modes are kept so we use a select operator as\n\n$$\n\\tilde {\\boldsymbol {Q}} = \\operatorname {S e l e c t} (\\boldsymbol {Q}) = \\operatorname {S e l e c t} (\\mathcal {F} (\\boldsymbol {q})), \\tag {3}\n$$\n\nwhere  $\\tilde{\\pmb{Q}}\\in \\mathbb{C}^{M\\times D}$  and  $M <   <   N$  . Then, the FEB-f is defined as\n\n$$\n\\operatorname {F E B} - \\mathrm {f} (\\boldsymbol {q}) = \\mathcal {F} ^ {- 1} \\left(\\text {P a d d i n g} (\\tilde {\\boldsymbol {Q}} \\odot \\boldsymbol {R})\\right), \\tag {4}\n$$\n\nwhere  $\\pmb{R} \\in \\mathbb{C}^{D \\times D \\times M}$  is a parameterized kernel initialized randomly. Let  $\\pmb{Y} = \\pmb{Q} \\odot \\pmb{C}$ , with  $\\pmb{Y} \\in \\mathbb{C}^{M \\times D}$ . The production operator  $\\odot$  is defined as:  $Y_{m, d_o} = \\sum_{d_i=0}^{D} Q_{m, d_i} \\cdot R_{d_i, d_o, m}$ , where  $d_i = 1, 2\\dots D$  is the input channel and  $d_o = 1, 2\\dots D$  is the output channel. The result of  $\\pmb{Q} \\odot \\pmb{R}$  is then zero-padded to  $\\mathbb{C}^{N \\times D}$  before performing inverse Fourier transform back to the time domain. The structure is shown in Figure 3.\n\nFrequency Enhanced Attention with Fourier Transform (FEA-f) We use the expression of the canonical transformer. The input: queries, keys, values are denoted as  $\\pmb{q} \\in \\mathbb{R}^{L \\times D}$ ,  $\\pmb{k} \\in \\mathbb{R}^{L \\times D}$ ,  $\\pmb{v} \\in \\mathbb{R}^{L \\times D}$ . In cross-attention, the queries come from the decoder and can be obtained by  $\\pmb{q} = \\pmb{x}_{en} \\cdot \\pmb{w}_q$ , where  $\\pmb{w}_q \\in \\mathbb{R}^{D \\times D}$ . The keys and values are from the encoder and can be obtained by  $\\pmb{k} = \\pmb{x}_{de} \\cdot \\pmb{w}_k$  and  $\\pmb{v} = \\pmb{x}_{de} \\cdot \\pmb{w}_v$ , where  $\\pmb{w}_k, \\pmb{w}_v \\in \\mathbb{R}^{D \\times D}$ . Formally, the canonical attention can be written as\n\n$$\n\\operatorname {A t t e n} (\\boldsymbol {q}, \\boldsymbol {k}, \\boldsymbol {v}) = \\operatorname {S o f t m a x} \\left(\\frac {\\boldsymbol {q} \\boldsymbol {k} ^ {\\top}}{\\sqrt {d _ {q}}}\\right) \\boldsymbol {v}. \\tag {5}\n$$\n\nIn FEA-f, we convert the queries, keys, and values with Fourier Transform and perform a similar attention mechanism in the frequency domain, by randomly selecting M modes. We denote the selected version after Fourier Transform as  $\\vec{Q} \\in \\mathbb{C}^{M \\times D}$ ,  $\\vec{K} \\in \\mathbb{C}^{M \\times D}$ ,  $\\vec{V} \\in \\mathbb{C}^{M \\times D}$ . The FEA-f is defined as\n\n$$\n\\tilde {\\boldsymbol {Q}} = \\operatorname {S e l e c t} (\\boldsymbol {\\mathcal {F}} (\\boldsymbol {q}))\n$$\n\n$$\n\\tilde {\\boldsymbol {K}} = \\operatorname {S e l e c t} (\\mathcal {F} (\\boldsymbol {k})) \\tag {6}\n$$\n\n$$\n\\tilde {V} = \\operatorname {S e l e c t} (\\mathcal {F} (v))\n$$\n\n$$\n\\operatorname {F E A - f} (\\boldsymbol {q}, \\boldsymbol {k}, \\boldsymbol {v}) = \\mathcal {F} ^ {- 1} \\left(\\text {P a d d i n g} \\left(\\sigma \\left(\\tilde {\\boldsymbol {Q}} \\cdot \\tilde {\\boldsymbol {K}} ^ {\\top}\\right) \\cdot \\tilde {\\boldsymbol {V}}\\right)\\right), \\tag {7}\n$$\n\nwhere  $\\sigma$  is the activation function. We use softmax or tanh for activation, since their converging performance differs in different data sets. Let  $\\mathbf{Y} = \\sigma (\\tilde{\\mathbf{Q}}\\cdot \\tilde{\\mathbf{K}}^{\\top})\\cdot \\tilde{\\mathbf{V}}$ , and  $\\mathbf{Y}\\in \\mathbb{C}^{M\\times D}$  needs to be zero-padded to  $\\mathbb{C}^{L\\times D}$  before performing inverse Fourier transform. The FEA-f structure is shown in Figure 4.\n\n# 3.3. Wavelet Enhanced Structure\n\nDiscrete Wavelet Transform (DWT) While the Fourier transform creates a representation of the signal in the frequency domain, the Wavelet transform creates a representation in both the frequency and time domain, allowing efficient access of localized information of the signal. The multiwavelet transform synergizes the advantages of orthogonal polynomials as well as wavelets. For a given  $f(x)$ , the multiwavelet coefficients at the scale  $n$  can be defined as  $\\mathbf{s}_l^n = \\left[\\langle f,\\phi_{il}^n\\rangle_{\\mu_n}\\right]_{i = 0}^{k - 1},\\mathbf{d}_l^n = \\left[\\langle f,\\psi_{il}^n\\rangle_{\\mu_n}\\right]_{i = 0}^{k - 1}$ , respectively, w.r.t. measure  $\\mu_{n}$  with  $\\mathbf{s}_l^n,\\mathbf{d}_l^n\\in \\mathbb{R}^{k\\times 2^n}$ .  $\\phi_{il}^{n}$  are wavelet orthonormal basis of piecewise polynomials. The decomposition/reconstruction across scales is defined as\n\n$$\n\\mathbf {s} _ {l} ^ {n} = H ^ {(0)} \\mathbf {s} _ {2 l} ^ {n + 1} + H ^ {(1)} \\mathbf {s} _ {2 l + 1} ^ {n + 1},\n$$\n\n$$\n\\mathbf {s} _ {2 l} ^ {n + 1} = \\Sigma^ {(0)} \\left(H ^ {(0) T} \\mathbf {s} _ {l} ^ {n} + G ^ {(0) T} \\mathbf {d} _ {l} ^ {n}\\right), \\tag {8}\n$$\n\n$$\n\\mathbf {d} _ {l} ^ {n} = G ^ {(0)} \\mathbf {s} _ {2 l} ^ {n + 1} + H ^ {(1)} \\mathbf {s} _ {2 l + 1} ^ {n + 1},\n$$\n\n$$\n\\mathbf {s} _ {2 l + 1} ^ {n + 1} = \\Sigma^ {(1)} \\left(H ^ {(1) T} \\mathbf {s} _ {l} ^ {n} + G ^ {(1) T} \\mathbf {d} _ {l} ^ {n}\\right),\n$$\n\nwhere  $\\left(H^{(0)},H^{(1)},G^{(0)},G^{(1)}\\right)$  are linear coefficients for multiwavelet decomposition filters. They are fixed matrices\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/3258bb879bb97b3ffc5d64b9e8708256b9fad5822c38bf213bb63c94079d3ba4.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/c042de60a79f6cb15beeec538aae7484c609f28eb7c6912408dbdce2189d47c3.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/d227ea1b1430eb5b9fb6b0ac6e7397f0bda415e33df7b0ac8e825d88083cd6f2.jpg)  \nFigure 5. Top Left: Wavelet frequency enhanced block decomposition stage. Top Right: Wavelet block reconstruction stage shared by FEB-w and FEA-w. Bottom: Wavelet frequency enhanced cross attention decomposition stage.\n\nused for wavelet decomposition. The multiwavelet representation of a signal can be obtained by the tensor product of multiscale and multiwavelet basis. Note that the basis at various scales are coupled by the tensor product, so we need to untangle it. Inspired by (Gupta et al., 2021), we adapt a non-standard wavelet representation to reduce the model complexity. For a map function  $F(x) = x'$ , the map under multiwavelet domain can be written as\n\n$$\nU _ {d l} ^ {n} = A _ {n} d _ {l} ^ {n} + B _ {n} s _ {l} ^ {n}, \\quad U _ {\\bar {s} l} ^ {n} = C _ {n} d _ {l} ^ {n}, \\quad U _ {s l} ^ {L} = \\bar {F} s _ {l} ^ {L}, \\tag {9}\n$$\n\nwhere  $(U_{sl}^{n}, U_{dl}^{n}, s_{l}^{n}, d_{l}^{n})$  are the multiscale, multiwavelet coefficients,  $L$  is the coarsest scale under recursive decomposition, and  $A_{n}$ ,  $B_{n}$ ,  $C_{n}$  are three independent FEB-f blocks modules used for processing different signal during decomposition and reconstruction. Here  $\\bar{F}$  is a single-layer of perceptrons which processes the remaining coarsest signal after  $L$  decomposed steps. More designed detail is described in Appendix D.\n\nFrequency Enhanced Block with Wavelet Transform (FEB-w) The overall FEB-w architecture is shown in Figure 5. It differs from FEB-f in the recursive mechanism: the input is decomposed into 3 parts recursively and operates individually. For the wavelet decomposition part, we implement the fixed Legendre wavelets basis decomposition matrix. Three FEB-f modules are used to process the resulting high-frequency part, low-frequency part, and remaining part from wavelet decomposition respectively. For each cycle  $L$ , it produces a processed high-frequency tensor  $Ud(L)$ , a processed low-frequency frequency tensor\n\n$Us(L)$ , and the raw low-frequency tensor  $X(L + 1)$ . This is a ladder-down approach, and the decomposition stage performs the decimation of the signal by a factor of  $1 / 2$ , running for a maximum of  $L$  cycles, where  $L < \\log_2(M)$  for a given input sequence of size  $M$ . In practice,  $L$  is set as a fixed argument parameter. The three sets of FEB-f blocks are shared during different decomposition cycles  $L$ . For the wavelet reconstruction part, we recursively build up our output tensor as well. For each cycle  $L$ , we combine  $X(L + 1)$ ,  $Us(L)$ , and  $Ud(L)$  produced from the decomposition part and produce  $X(L)$  for the next reconstruction cycle. For each cycle, the length dimension of the signal tensor is increased by 2 times.\n\nFrequency Enhanced Attention with Wavelet Transform (FEA-w) FEA-w contains the decomposition stage and reconstruction stage like FEB-w. Here we keep the reconstruction stage unchanged. The only difference lies in the decomposition stage. The same decomposed matrix is used to decompose  $q, k, v$  signal separately, and  $q, k, v$  share the same sets of module to process them as well. As shown above, a frequency enhanced block with wavelet decomposition block (FEB-w) contains three FEB-f blocks for the signal process. We can view the FEB-f as a substitution of self-attention mechanism. We use a straightforward way to build the frequency enhanced cross attention with wavelet decomposition, substituting each FEB-f with a FEA-f module. Besides, another FEA-f module is added to process the coarsest remaining  $q(L), k(L), v(L)$  signal.\n\n# 3.4. Mixture of Experts for Seasonal-Trend Decomposition\n\nBecause of the commonly observed complex periodic pattern coupled with the trend component on real-world data, extracting the trend can be hard with fixed window average pooling. To overcome such a problem, we design a Mixture Of Experts Decomposition block (MOEDecomp). It contains a set of average filters with different sizes to extract multiple trend components from the input signal and a set of data-dependent weights for combining them as the final trend. Formally, we have\n\n$$\n\\mathbf {X} _ {\\text {t r e n d}} = \\operatorname {S o f t m a x} (L (x)) * (F (x)), \\tag {10}\n$$\n\nwhere  $F(\\cdot)$  is a set of average pooling filters and  $\\operatorname{Softmax}(L(x))$  is the weights for mixing these extracted trends.\n\n# 3.5. Complexity Analysis\n\nFor FEDformer-f, the computational complexity for time and memory is  $O(L)$  with a fixed number of randomly selected modes in FEB & FEA blocks. We set modes number  $M = 64$  as default value. Though the complexity\n\nTable 1. Complexity analysis of different forecasting models.  \n\n<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Training</td><td>Testing</td></tr><tr><td>Time</td><td>Memory</td><td>Steps</td></tr><tr><td>FEDformer</td><td>O(L)</td><td>O(L)</td><td>1</td></tr><tr><td>Autoformer</td><td>O(L log L)</td><td>O(L log L)</td><td>1</td></tr><tr><td>Informer</td><td>O(L log L)</td><td>O(L log L)</td><td>1</td></tr><tr><td>Transformer</td><td>O(L2)</td><td>O(L2)</td><td>L</td></tr><tr><td>LogTrans</td><td>O(L log L)</td><td>O(L2)</td><td>1</td></tr><tr><td>Reformer</td><td>O(L log L)</td><td>O(L log L)</td><td>L</td></tr><tr><td>LSTM</td><td>O(L)</td><td>O(L)</td><td>L</td></tr></table>\n\nof full DFT transformation by FFT is  $(O(L\\log (L))$ , our model only needs  $O(L)$  cost and memory complexity with the pre-selected set of Fourier basis for quick implementation. For FEDformer-w, when we set the recursive decompose step to a fixed number  $L$  and use a fixed number of randomly selected modes the same as FEDformer-f, the time complexity and memory usage are  $O(L)$  as well. In practice, we choose  $L = 3$  and modes number  $M = 64$  as default value. The comparisons of the time complexity and memory usage in training and the inference steps in testing are summarized in Table 1. It can be seen that the proposed FEDformer achieves the best overall complexity among Transformer-based forecasting models.\n",
  "experiments": "# 4. Experiments\n\nTo evaluate the proposed FEDformer, we conduct extensive experiments on six popular real-world datasets, including energy, economics, traffic, weather, and disease. Since classic models like ARIMA and basic RNN/CNN models perform relatively inferior as shown in (Zhou et al., 2021) and (Wu et al., 2021), we mainly include four state-of-the-art transformer-based models for comparison, i.e., Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021), Log-Trans (Li et al., 2019) and Reformer (Kitaev et al., 2020) as baseline models. Note that since Autoformer holds the best performance in all the six benchmarks, it is used as the main baseline model for comparison. More details about baseline models, datasets, and implementation are described in Appendix A.2, F.1, and F.2, respectively.\n\n# 4.1. Main Results\n\nFor better comparison, we follow the experiment settings of Autoformer in (Wu et al., 2021) where the input length is fixed to 96, and the prediction lengths for both training and evaluation are fixed to be 96, 192, 336, and 720, respectively.\n\nMultivariate Results For the multivariate forecasting, FEDformer achieves the best performance on all six benchmark datasets at all horizons as shown in Table 2. Compared with Autoformer, the proposed FEDformer yields an overall  $14.8\\%$  relative MSE reduction. It is worth noting\n\nTable 2. Multivariate long-term series forecasting results on six datasets with input length  $I = 96$  and prediction length  $O \\in \\{96,192,336,720\\}$  (For ILI dataset, we use input length  $I = 36$  and prediction length  $O \\in \\{24,36,48,60\\}$ ). A lower MSE indicates better performance, and the best results are highlighted in bold.  \n\n<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Metric</td><td colspan=\"4\">ETTm2</td><td colspan=\"3\">Electricity</td><td colspan=\"4\">Exchange</td><td colspan=\"3\">Traffic</td><td colspan=\"4\">Weather</td><td colspan=\"4\">ILI</td><td></td><td></td></tr><tr><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>24</td><td>36</td><td>48</td><td>60</td></tr><tr><td rowspan=\"2\">FEDformer-f</td><td>MSE</td><td>0.203</td><td>0.269</td><td>0.325</td><td>0.421</td><td>0.193</td><td>0.201</td><td>0.214</td><td>0.246</td><td>0.148</td><td>0.271</td><td>0.460</td><td>1.195</td><td>0.587</td><td>0.604</td><td>0.621</td><td>0.626</td><td>0.217</td><td>0.276</td><td>0.339</td><td>0.403</td><td>3.228</td><td>2.679</td><td>2.622</td><td>2.857</td></tr><tr><td>MAE</td><td>0.287</td><td>0.328</td><td>0.366</td><td>0.415</td><td>0.308</td><td>0.315</td><td>0.329</td><td>0.355</td><td>0.278</td><td>0.380</td><td>0.500</td><td>0.841</td><td>0.366</td><td>0.373</td><td>0.383</td><td>0.382</td><td>0.296</td><td>0.336</td><td>0.380</td><td>0.428</td><td>1.260</td><td>1.080</td><td>1.078</td><td>1.157</td></tr><tr><td rowspan=\"2\">FEDformer-w</td><td>MSE</td><td>0.204</td><td>0.316</td><td>0.359</td><td>0.433</td><td>0.183</td><td>0.195</td><td>0.212</td><td>0.231</td><td>0.139</td><td>0.256</td><td>0.426</td><td>1.090</td><td>0.562</td><td>0.562</td><td>0.570</td><td>0.596</td><td>0.227</td><td>0.295</td><td>0.381</td><td>0.424</td><td>2.203</td><td>2.272</td><td>2.209</td><td>2.545</td></tr><tr><td>MAE</td><td>0.288</td><td>0.363</td><td>0.387</td><td>0.432</td><td>0.297</td><td>0.308</td><td>0.313</td><td>0.343</td><td>0.276</td><td>0.369</td><td>0.464</td><td>0.800</td><td>0.349</td><td>0.346</td><td>0.323</td><td>0.368</td><td>0.304</td><td>0.363</td><td>0.416</td><td>0.434</td><td>0.963</td><td>0.976</td><td>0.981</td><td>1.061</td></tr><tr><td rowspan=\"2\">Autoformer</td><td>MSE</td><td>0.255</td><td>0.281</td><td>0.339</td><td>0.422</td><td>0.201</td><td>0.222</td><td>0.231</td><td>0.254</td><td>0.197</td><td>0.300</td><td>0.509</td><td>1.447</td><td>0.613</td><td>0.616</td><td>0.622</td><td>0.660</td><td>0.266</td><td>0.307</td><td>0.359</td><td>0.419</td><td>3.483</td><td>3.103</td><td>2.669</td><td>2.770</td></tr><tr><td>MAE</td><td>0.339</td><td>0.340</td><td>0.372</td><td>0.419</td><td>0.317</td><td>0.334</td><td>0.338</td><td>0.361</td><td>0.323</td><td>0.369</td><td>0.524</td><td>0.941</td><td>0.388</td><td>0.382</td><td>0.337</td><td>0.408</td><td>0.336</td><td>0.367</td><td>0.395</td><td>0.428</td><td>1.287</td><td>1.148</td><td>1.085</td><td>1.125</td></tr><tr><td rowspan=\"2\">Informer</td><td>MSE</td><td>0.365</td><td>0.533</td><td>1.363</td><td>3.379</td><td>0.274</td><td>0.296</td><td>0.300</td><td>0.373</td><td>0.847</td><td>1.204</td><td>1.672</td><td>2.478</td><td>0.719</td><td>0.696</td><td>0.777</td><td>0.864</td><td>0.300</td><td>0.598</td><td>0.578</td><td>1.059</td><td>5.764</td><td>4.755</td><td>4.763</td><td>5.264</td></tr><tr><td>MAE</td><td>0.453</td><td>0.563</td><td>0.887</td><td>1.338</td><td>0.368</td><td>0.386</td><td>0.394</td><td>0.439</td><td>0.752</td><td>0.895</td><td>1.036</td><td>1.310</td><td>0.391</td><td>0.379</td><td>0.420</td><td>0.472</td><td>0.384</td><td>0.544</td><td>0.523</td><td>0.741</td><td>1.677</td><td>1.467</td><td>1.469</td><td>1.564</td></tr><tr><td rowspan=\"2\">LogTrans</td><td>MSE</td><td>0.768</td><td>0.989</td><td>1.334</td><td>3.048</td><td>0.258</td><td>0.266</td><td>0.280</td><td>0.283</td><td>0.968</td><td>1.040</td><td>1.659</td><td>1.941</td><td>0.684</td><td>0.685</td><td>0.7337</td><td>0.717</td><td>0.458</td><td>0.658</td><td>0.797</td><td>0.869</td><td>4.480</td><td>4.799</td><td>4.800</td><td>5.278</td></tr><tr><td>MAE</td><td>0.642</td><td>0.757</td><td>0.872</td><td>1.328</td><td>0.357</td><td>0.368</td><td>0.380</td><td>0.376</td><td>0.812</td><td>0.851</td><td>1.081</td><td>1.127</td><td>0.384</td><td>0.390</td><td>0.408</td><td>0.396</td><td>0.490</td><td>0.589</td><td>0.652</td><td>0.675</td><td>1.444</td><td>1.467</td><td>1.468</td><td>1.560</td></tr><tr><td rowspan=\"2\">Reformer</td><td>MSE</td><td>0.658</td><td>1.078</td><td>1.549</td><td>2.631</td><td>0.312</td><td>0.348</td><td>0.350</td><td>0.340</td><td>1.065</td><td>1.188</td><td>1.357</td><td>1.510</td><td>0.732</td><td>0.733</td><td>0.742</td><td>0.755</td><td>0.689</td><td>0.752</td><td>0.639</td><td>1.130</td><td>4.400</td><td>4.783</td><td>4.832</td><td>4.882</td></tr><tr><td>MAE</td><td>0.619</td><td>0.827</td><td>0.972</td><td>1.242</td><td>0.402</td><td>0.433</td><td>0.433</td><td>0.420</td><td>0.829</td><td>0.906</td><td>0.976</td><td>1.016</td><td>0.423</td><td>0.420</td><td>0.420</td><td>423</td><td>0.596</td><td>0.638</td><td>0.596</td><td>0.792</td><td>1.382</td><td>1.448</td><td>1.465</td><td>1.483</td></tr></table>\n\nTable 3. Univariate long-term series forecasting results on six datasets with input length  $I = 96$  and prediction length  $O \\in \\{96,192,336,720\\}$  (For ILI dataset, we use input length  $I = 36$  and prediction length  $O \\in \\{24,36,48,60\\}$ ). A lower MSE indicates better performance, and the best results are highlighted in bold.  \n\n<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Metric</td><td colspan=\"4\">ETTm2</td><td colspan=\"4\">Electricity</td><td colspan=\"4\">Exchange</td><td colspan=\"4\">Traffic</td><td colspan=\"4\">Weather</td><td colspan=\"4\">ILI</td></tr><tr><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>24</td><td>36</td><td>48</td><td>60</td></tr><tr><td rowspan=\"2\">FEDformer-f</td><td>MSE</td><td>0.072</td><td>0.102</td><td>0.130</td><td>0.178</td><td>0.253</td><td>0.282</td><td>0.346</td><td>0.422</td><td>0.154</td><td>0.286</td><td>0.511</td><td>1.301</td><td>0.207</td><td>0.205</td><td>0.219</td><td>0.244</td><td>0.0062</td><td>0.0060</td><td>0.0041</td><td>0.0055</td><td>0.708</td><td>0.584</td><td>0.717</td><td>0.855</td></tr><tr><td>MAE</td><td>0.206</td><td>0.245</td><td>0.279</td><td>0.325</td><td>0.370</td><td>0.386</td><td>0.431</td><td>0.484</td><td>0.304</td><td>0.420</td><td>0.555</td><td>0.879</td><td>0.312</td><td>0.312</td><td>0.323</td><td>0.344</td><td>0.062</td><td>0.062</td><td>0.050</td><td>0.059</td><td>0.627</td><td>0.617</td><td>0.697</td><td>0.774</td></tr><tr><td rowspan=\"2\">FEDformer-w</td><td>MSE</td><td>0.063</td><td>0.110</td><td>0.147</td><td>0.219</td><td>0.262</td><td>0.316</td><td>0.361</td><td>0.448</td><td>0.131</td><td>0.277</td><td>0.426</td><td>1.162</td><td>0.170</td><td>0.173</td><td>0.178</td><td>0.187</td><td>0.0035</td><td>0.0054</td><td>0.008</td><td>0.015</td><td>0.693</td><td>0.554</td><td>0.699</td><td>0.828</td></tr><tr><td>MAE</td><td>0.189</td><td>0.252</td><td>0.301</td><td>0.368</td><td>0.378</td><td>0.410</td><td>0.445</td><td>0.501</td><td>0.284</td><td>0.420</td><td>0.511</td><td>0.832</td><td>0.263</td><td>0.265</td><td>0.266</td><td>0.286</td><td>0.046</td><td>0.059</td><td>0.072</td><td>0.091</td><td>0.629</td><td>0.604</td><td>0.696</td><td>0.770</td></tr><tr><td rowspan=\"2\">Autoformer</td><td>MSE</td><td>0.065</td><td>0.118</td><td>0.154</td><td>0.182</td><td>0.341</td><td>0.345</td><td>0.406</td><td>0.565</td><td>0.241</td><td>0.300</td><td>0.509</td><td>1.260</td><td>0.246</td><td>0.266</td><td>0.263</td><td>0.269</td><td>0.011</td><td>0.0075</td><td>0.0063</td><td>0.0085</td><td>0.948</td><td>0.634</td><td>0.791</td><td>0.874</td></tr><tr><td>MAE</td><td>0.189</td><td>0.256</td><td>0.305</td><td>0.335</td><td>0.438</td><td>0.428</td><td>0.470</td><td>0.581</td><td>0.387</td><td>0.369</td><td>0.524</td><td>0.867</td><td>0.346</td><td>0.370</td><td>0.371</td><td>0.372</td><td>0.081</td><td>0.067</td><td>0.062</td><td>0.070</td><td>0.732</td><td>0.650</td><td>0.752</td><td>0.797</td></tr><tr><td rowspan=\"2\">Informer</td><td>MSE</td><td>0.080</td><td>0.112</td><td>0.166</td><td>0.228</td><td>0.258</td><td>0.285</td><td>0.336</td><td>0.607</td><td>1.327</td><td>1.258</td><td>2.179</td><td>1.280</td><td>0.257</td><td>0.299</td><td>0.312</td><td>0.366</td><td>0.004</td><td>0.002</td><td>0.004</td><td>0.003</td><td>5.282</td><td>4.554</td><td>4.273</td><td>5.214</td></tr><tr><td>MAE</td><td>0.217</td><td>0.259</td><td>0.314</td><td>0.380</td><td>0.367</td><td>0.388</td><td>0.423</td><td>0.599</td><td>0.944</td><td>0.924</td><td>1.296</td><td>0.953</td><td>0.353</td><td>0.376</td><td>0.387</td><td>0.436</td><td>0.044</td><td>0.040</td><td>0.049</td><td>0.042</td><td>2.050</td><td>1.916</td><td>1.846</td><td>2.057</td></tr><tr><td rowspan=\"2\">LogTrans</td><td>MSE</td><td>0.075</td><td>0.129</td><td>0.154</td><td>0.160</td><td>0.288</td><td>0.432</td><td>0.430</td><td>0.491</td><td>0.237</td><td>0.738</td><td>2.018</td><td>2.405</td><td>0.226</td><td>0.314</td><td>0.387</td><td>0.437</td><td>0.0046</td><td>0.0060</td><td>0.0060</td><td>0.007</td><td>3.607</td><td>2.407</td><td>3.106</td><td>3.698</td></tr><tr><td>MAE</td><td>0.208</td><td>0.275</td><td>0.302</td><td>0.322</td><td>0.393</td><td>0.483</td><td>0.483</td><td>0.531</td><td>0.377</td><td>0.619</td><td>1.070</td><td>1.175</td><td>0.317</td><td>0.408</td><td>0.453</td><td>0.491</td><td>0.052</td><td>0.060</td><td>0.054</td><td>0.059</td><td>1.662</td><td>1.363</td><td>1.575</td><td>1.733</td></tr><tr><td rowspan=\"2\">Reformer</td><td>MSE</td><td>0.077</td><td>0.138</td><td>0.160</td><td>0.168</td><td>0.275</td><td>0.304</td><td>0.370</td><td>0.460</td><td>0.298</td><td>0.777</td><td>1.833</td><td>1.203</td><td>0.313</td><td>0.386</td><td>0.423</td><td>0.378</td><td>0.012</td><td>0.0098</td><td>0.013</td><td>0.011</td><td>3.838</td><td>2.934</td><td>3.755</td><td>4.162</td></tr><tr><td>MAE</td><td>0.214</td><td>0.290</td><td>0.313</td><td>0.334</td><td>0.379</td><td>0.402</td><td>0.448</td><td>0.511</td><td>0.444</td><td>0.719</td><td>1.128</td><td>0.956</td><td>0.383</td><td>0.453</td><td>0.468</td><td>0.433</td><td>0.087</td><td>0.044</td><td>0.100</td><td>0.083</td><td>1.720</td><td>1.520</td><td>1.749</td><td>1.847</td></tr></table>\n\nthat for some of the datasets, such as Exchange and ILI, the improvement is even more significant (over  $20\\%$ ). Note that the Exchange dataset does not exhibit clear periodicity in its time series, but FEDformer can still achieve superior performance. Overall, the improvement made by FEDformer is consistent with varying horizons, implying its strength in long term forecasting. More detailed results on ETT full benchmark are provided in Appendix F.3.\n\nUnivariate Results The results for univariate time series forecasting are summarized in Table 3. Compared with Autoformer, FEDformer yields an overall  $22.6\\%$  relative MSE reduction, and on some datasets, such as traffic and weather, the improvement can be more than  $30\\%$ . It again verifies that FEDformer is more effective in long-term forecasting. Note that due to the difference between Fourier and wavelet basis, FEDformer-f and FEDformer-w perform well on different datasets, making them complementary choice for long term forecasting. More detailed results on ETT full benchmark are provided in Appendix F.3.\n\n# 4.2. Ablation Studies\n\nIn this section, the ablation experiments are conducted, aiming at comparing the performance of frequency enhanced block and its alternatives. The current SOTA results of Aut\n\nofformer which uses the autocorrelation mechanism serve as the baseline. Three ablation variants of FEDformer are tested: 1) FEDformer V1: we use FEB to substitute self-attention only; 2) FEDformer V2: we use FEA to substitute cross attention only; 3) FEDFormer V3: we use FEA to substitute both self and cross attention. The ablated versions of FEDformer-f as well as the SOTA models are compared in Table 4, and we use a bold number if the ablated version brings improvements compared with Autoformer. We omit the similar results in FEDformer-w due to space limit. It can be seen in Table 4 that FEDformer V1 brings improvement in 10/16 cases, while FEDformer V2 improves in 12/16 cases. The best performance is achieved in our FEDformer with FEB and FEA blocks which improves performance in all 16/16 cases. This verifies the effectiveness of the designed FEB, FEA for substituting self and cross attention. Furthermore, experiments on ETT and Weather datasets show that the adopted MOEDecomp (mixture of experts decomposition) scheme can bring an average of  $2.96\\%$  improvement compared with the single decomposition scheme. More details are provided in Appendix F.5.\n\nTable 4. Ablation studies: multivariate long-term series forecasting results on ETTm1 and ETTm2 with input length  $I = {96}$  and prediction length  $O \\in  \\{ {96},{192},{336},{720}\\}$  . Three variants of FEDformer-f are compared with baselines. The best results are highlighted in bold.  \n\n<table><tr><td colspan=\"2\">Methods</td><td colspan=\"2\">Transformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">FEDformer V1</td><td colspan=\"2\">FEDformer V2</td><td colspan=\"2\">FEDformer V3</td><td colspan=\"2\">FEDformer-f</td></tr><tr><td colspan=\"2\">Self-att Cross-att</td><td colspan=\"2\">FullAtt FullAtt</td><td colspan=\"2\">ProbAtt ProbAtt</td><td colspan=\"2\">AutoCorr AutoCorr</td><td colspan=\"2\">FEB-f(Eq. 4) AutoCorr</td><td colspan=\"2\">AutoCorr FEA-f(Eq. 7)</td><td colspan=\"2\">FEA-f(Eq. 7) FEA-f(Eq. 7)</td><td colspan=\"2\">FEB-f(Eq. 4) FEA-f(Eq. 7)</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">ETTm1</td><td>96</td><td>0.525</td><td>0.486</td><td>0.458</td><td>0.465</td><td>0.481</td><td>0.463</td><td>0.378</td><td>0.419</td><td>0.539</td><td>0.490</td><td>0.534</td><td>0.482</td><td>0.379</td><td>0.419</td></tr><tr><td>192</td><td>0.526</td><td>0.502</td><td>0.564</td><td>0.521</td><td>0.628</td><td>0.526</td><td>0.417</td><td>0.442</td><td>0.556</td><td>0.499</td><td>0.552</td><td>0.493</td><td>0.426</td><td>0.441</td></tr><tr><td>336</td><td>0.514</td><td>0.502</td><td>0.672</td><td>0.559</td><td>0.728</td><td>0.567</td><td>0.480</td><td>0.477</td><td>0.541</td><td>0.498</td><td>0.565</td><td>0.503</td><td>0.445</td><td>0.459</td></tr><tr><td>720</td><td>0.564</td><td>0.529</td><td>0.714</td><td>0.596</td><td>0.658</td><td>0.548</td><td>0.543</td><td>0.517</td><td>0.558</td><td>0.507</td><td>0.585</td><td>0.515</td><td>0.543</td><td>0.490</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.268</td><td>0.346</td><td>0.227</td><td>0.305</td><td>0.255</td><td>0.339</td><td>0.259</td><td>0.337</td><td>0.216</td><td>0.297</td><td>0.211</td><td>0.292</td><td>0.203</td><td>0.287</td></tr><tr><td>192</td><td>0.304</td><td>0.355</td><td>0.300</td><td>0.360</td><td>0.281</td><td>0.340</td><td>0.285</td><td>0.344</td><td>0.274</td><td>0.331</td><td>0.272</td><td>0.329</td><td>0.269</td><td>0.328</td></tr><tr><td>336</td><td>0.365</td><td>0.400</td><td>0.382</td><td>0.410</td><td>0.339</td><td>0.372</td><td>0.320</td><td>0.373</td><td>0.334</td><td>0.369</td><td>0.327</td><td>0.363</td><td>0.325</td><td>0.366</td></tr><tr><td>720</td><td>0.475</td><td>0.466</td><td>1.637</td><td>0.794</td><td>0.422</td><td>0.419</td><td>0.761</td><td>0.628</td><td>0.427</td><td>0.420</td><td>0.418</td><td>0.415</td><td>0.421</td><td>0.415</td></tr><tr><td colspan=\"2\">Count</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>5</td><td>5</td><td>6</td><td>6</td><td>7</td><td>7</td><td>8</td><td>8</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/a66f3c97f086de45ddf8e5bbb1764fed803f32d10e9611e237a055fe8465763d.jpg)  \nFigure 6. Comparison of two base-modes selection method (FixRand). Rand policy means randomly selecting a subset of modes, Fix policy means selecting the lowest frequency modes. Two policies are compared on a variety of base-modes number  $M \\in \\{2,4,8\\dots 256\\}$  on ETT full-benchmark (h1, m1, h2, m2).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-11-19/1a245e44-a2d3-459b-84ae-06ae19879e21/948378c92a4e1ba7a903927b6062bdc829b42a8af2ce1d8e086c3aa15fdd703e.jpg)\n\n# 4.3. Mode Selection Policy\n\nThe selection of discrete Fourier basis is the key to effectively representing the signal and maintaining the model's linear complexity. As we discussed in Section 2, random Fourier mode selection is a better policy in forecasting tasks. more importantly, random policy requires no prior knowledge of the input and generalizes easily in new tasks. Here we empirically compare the random selection policy with fixed selection policy, and summarize the experimental results in Figure 6. It can be observed that the adopted random policy achieves better performance than the common fixed policy which only keeps the low frequency modes. Meanwhile, the random policy exhibits some mode saturation effect, indicating an appropriate random number of modes instead of all modes would bring better performance, which is also consistent with the theoretical analysis in Section 2.\n\n# 4.4. Distribution Analysis of Forecasting Output\n\nIn this section, we evaluate the distribution similarity between the input sequence and forecasting output of different transformer models quantitatively. In Table 5, we applied the Kolmogrov-Smirnov test to check if the forecasting results of different models made on ETTm1 and\n\nTable 5. P-values of Kolmogrov-Smirnov test of different transformer models for long-term forecasting output on ETTm1 and ETTm2 dataset. Larger value indicates the hypothesis (the input sequence and forecasting output come from the same distribution) is less likely to be rejected. The best results are highlighted.  \n\n<table><tr><td colspan=\"2\">Methods</td><td>Transformer</td><td>Informer</td><td>Autoformer</td><td>FEDformer</td><td>True</td></tr><tr><td rowspan=\"4\">ETTm1</td><td>96</td><td>0.0090</td><td>0.0055</td><td>0.020</td><td>0.048</td><td>0.023</td></tr><tr><td>192</td><td>0.0052</td><td>0.0029</td><td>0.015</td><td>0.028</td><td>0.013</td></tr><tr><td>336</td><td>0.0022</td><td>0.0019</td><td>0.012</td><td>0.015</td><td>0.010</td></tr><tr><td>720</td><td>0.0023</td><td>0.0016</td><td>0.008</td><td>0.014</td><td>0.004</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.0012</td><td>0.0008</td><td>0.079</td><td>0.071</td><td>0.087</td></tr><tr><td>192</td><td>0.0011</td><td>0.0006</td><td>0.047</td><td>0.045</td><td>0.060</td></tr><tr><td>336</td><td>0.0005</td><td>0.00009</td><td>0.027</td><td>0.028</td><td>0.042</td></tr><tr><td>720</td><td>0.0008</td><td>0.0002</td><td>0.023</td><td>0.021</td><td>0.023</td></tr><tr><td colspan=\"2\">Count</td><td>0</td><td>0</td><td>3</td><td>5</td><td>NA</td></tr></table>\n\nETTm2 are consistent with the input sequences. In particular, we test if the input sequence of fixed 96-time steps come from the same distribution as the predicted sequence, with the null hypothesis that both sequences come from the same distribution. On both datasets, by setting the common P-value as 0.01, various existing Transformer baseline models have much less values than 0.01 except Autoformer, which indicates their forecasting output have a higher probability to be sampled from the different distributions compared to the input sequence. In contrast, Autoformer and FEDformer have much larger P-value compared to others, which mainly contributes to their seasonal-trend decomposition mechanism. Though we get close results from ETTm2 by both models, the proposed FEDformer has much larger P-value in ETTm1. And it's the only model whose null hypothesis can not be rejected with P-value larger than 0.01 in all cases of the two datasets, implying that the output sequence generated by FEDformer shares a more similar distribution as the input sequence than others and thus justifies the our design motivation of FEDformer as discussed in Section 1. More detailed analysis are provided in Appendix E.\n\n# 4.5. Differences Compared to Autoformer baseline\n\nSince we use the decomposed encoder-decoder overall architecture as Autoformer, we think it is critical to emphasize the differences. In Autoformer, the authors consider a nice idea to use the top-k sub-sequence correlation (auto-correlation) module instead of point-wise attention, and the Fourier method is applied to improve the efficiency for sub-sequence level similarity computation. In general, Autoformer can be considered as decomposing the sequence into multiple time domain sub-sequences for feature extraction. In contrast, We use frequency transform to decompose the sequence into multiple frequency domain modes to extract the feature. In particular, we do not use a selective approach in sub-sequence selection. Instead, all frequency features are computed from the whole sequence, and this global property makes our model engage better performance for long sequence.\n",
  "hyperparameter": "Input length I=96 (I=36 for ILI dataset); Output/prediction length O∈{96,192,336,720} (O∈{24,36,48,60} for ILI); Number of selected Fourier modes M=64 (default); Hidden dimension D (not specified exact value); Number of encoder layers N (not specified); Number of decoder layers M (not specified); Wavelet decomposition recursive steps L=3 (default, where L<log₂(M)); Activation function: softmax or tanh for FEA (dataset-dependent); Mode selection: random subset of M modes; Decoder input length: I/2+O"
}