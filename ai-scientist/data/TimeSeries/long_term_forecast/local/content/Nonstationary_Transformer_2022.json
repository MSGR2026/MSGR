{
  "id": "Nonstationary_Transformer_2022",
  "paper_title": "Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting",
  "alias": "Nonstationary_Transformer",
  "year": 2022,
  "domain": "TimeSeries",
  "task": "imputation",
  "idea": "Non-stationary Transformers addresses the over-stationarization problem in time series forecasting through two complementary mechanisms: (1) Series Stationarization, which normalizes input series by their temporal statistics (mean and variance) and de-normalizes outputs to restore original statistics, and (2) De-stationary Attention, which re-incorporates non-stationary information from raw series into the attention mechanism by learning scaling factor τ and shifting vector Δ from the statistics of unstationarized data. This framework enables models to benefit from the predictability of stationarized series while maintaining the inherent temporal dependencies of non-stationary raw data.",
  "introduction": "# 1 Introduction\n\nTime series forecasting has become increasingly ubiquitous in real-world applications, such as weather forecasting, energy consumption planning, and financial risk assessment. Recently, Transformers [34] have achieved progressive breakthrough on extensive areas [12, 13, 10, 24]. Especially in time series forecasting, credited to their stacked structure and the capability of attention mechanisms, Transformers can naturally capture the temporal dependencies from deep multi-level features [39, 19, 22, 37], thereby fitting the series forecasting task perfectly.\n\nDespite the remarkable architectural design, it is still challenging for Transformers to predict real-world time series because of the non-stationarity of data. Non-stationary time series is characterized by the continuous change of statistical properties and joint distribution over time, which makes the time series less predictable [6, 16]. Besides, it is a fundamental problem to make deep models generalize well on a varying distribution [28, 21, 5]. In previous work, it is generally acknowledged to pre-process the time series by stationarization [26, 29, 17], which can attenuate the non-stationarity of raw time series for better predictability and provide more stable data distribution for deep models.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/9df0572890476e5fcd4b59402186584e326916a6c6cbd896fc1e248680adfd07.jpg)  \nFigure 1: Visualization of learned temporal attentions for different series with varied mean  $\\mu$  and standard deviation  $\\sigma$ . (a) is from the vanilla Transformer [34] trained on raw series. (b) is from the Transformer trained on stationarized series, which presents similar attentions. (c) is from Non-stationary Transformers, which involves De-stationary Attention to avoid over-stationarization.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/8eb0e960db75a0c837082b60166e4f6fb8e2256251129b82aa5ea689fb11406c.jpg)\n\nHowever, non-stationarity is the inherent property of real-world time series and also good guidance for discovering temporal dependencies for forecasting. Experimentally, we observe that training on the stationarized series will undermine the distinction of attentions learned by Transformers. While vanilla Transformers [34] can capture distinct temporal dependencies from different series in Figure 1(a), Transformers trained on the stationarized series tend to generate indistinguishable attentions in Figure 1(b). This problem, named by the over-stationarization, will bring unexpected side-effect that makes Transformers fail to capture eventful temporal dependencies, limit the model's predictive ability, and even induce the model to generate outputs with huge non-stationarity deviation from the ground truth. Thus, how to attenuate time series non-stationarity towards better predictability and mitigate the over-stationarization problem for model capability simultaneously is the key problem to further improve the performance of forecasting.\n\nIn this paper, we explore the effect of stationarization in time series forecasting and propose Non-stationary Transformers as a general framework, which empowers Transformer [34] and its efficient variants [19, 39, 37] with great predictive ability for real-world time series. The proposed framework involves two interdependent modules: Series Stationarization to increase the predictability of non-stationary series and De-stationary Attention to alleviate over-stationarization. Technically, Series Stationarization adopts a simple but effective normalization strategy to unify the key statistics of each series without extra parameters. And De-stationary Attention approximates the attention of unstationarized data and compensates the intrinsic non-stationarity of raw series. Benefiting from the above designs, Non-stationary Transformers can take advantage of the great predictability of stationarized series and crucial temporal dependencies discovered from original non-stationary data. Our method achieves state-of-the-art performance on six real-world benchmarks and can generalize to various Transformers for further improvement. The contributions lie in three folds:\n\n- We refine that the predictive capability of non-stationary series is essential in real-world forecasting. By detailed analysis, we find out that current stationarization approaches will lead to the over-stationarization problem, limiting the predictive capability of Transformers.  \n- We propose Non-stationary Transformers as a generic framework, including Series Stationarization to make the series more predictable and De-stationary Attention to avoid the over-stationarization problem by re-incorporating the non-stationarity of original series.  \n- Non-stationary Transformers consistently boosts four mainstream Transformers by a large margin and achieves state-of-the-art performance on six real-world benchmarks.\n",
  "method": "# 3 Non-stationary Transformers\n\nAs aforementioned, stationarity is an important element of time series predictability. Previous \"direct stationarization\" designs can attenuate non-stationarity of series for better predictability, but they obviously neglect inherent properties of real-world series, which will result in the over-stationarization problem as stated in Figure 1. To deal with the dilemma, we go beyond previous works and propose Non-stationary Transformers as a generic framework. Our model involves two complementary parts: Series Stationarization to attenuate time series non-stationarity and De-stationary Attention to re-incorporate non-stationary information of raw series. Empowered by these designs, Non-stationary Transformers can improve data predictability and maintain model capability simultaneously.\n\n# 3.1 Series Stationarization\n\nNon-stationary time series make the forecasting task intractable for deep models because it is hard for them to generalize well on series with changed statistics during inference, typically varied mean and standard deviation. The pilot work, RevIN [17] applies instance normalization with learnable affine parameters to each input and restores the statistics to the corresponding output, which makes each series follow a similar distribution. Experimentally, we find that this design also works well without learnable parameters. Thus, we propose a more straightforward but effective design to wrap Transformers as the base model without extra parameters, naming by Series Stationarization. As is shown in Figure 2, it contains two corresponding operations: Normalization module at first to deal with the non-stationary series caused by varied mean and standard deviation, and De-normalization module at the end to transform the model outputs back with original statistics. Here are the details.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/b5c31b0942c7f243e3e6ac8e43b9f2aa675c7a34e14575b67fc008b44f1d4d83.jpg)  \nFigure 2: Non-stationary Transformers. Series Stationarization is adopted as a wrapper on the base model to normalize each incoming series and de-normalize the output. De-stationary Attention replaces the original Attention mechanism to approximate attention learned from unstationarized series, which rescales current temporal dependency weights with learned de-stationary factors  $\\tau$ ,  $\\Delta$ .\n\nNormalization module To attenuate the non-stationarity of each input series, we conduct normalization on the temporal dimension by a sliding window over time. For each input series  $\\mathbf{x} = [x_1,x_2,\\dots,x_S]^\\top \\in \\mathbb{R}^{S\\times C}$ , we transform it by translation and scaling operations and obtain  $\\mathbf{x}' = [x_1',x_2',\\dots,x_S']^\\top \\in \\mathbb{R}^{S\\times C}$ , where  $S$  and  $C$  denote the sequence length and variable number respectively. The Normalization module can be formulated as follows:\n\n$$\n\\mu_ {\\mathbf {x}} = \\frac {1}{S} \\sum_ {i = 1} ^ {S} x _ {i}, \\sigma_ {\\mathbf {x}} ^ {2} = \\frac {1}{S} \\sum_ {i = 1} ^ {S} \\left(x _ {i} - \\mu_ {\\mathbf {x}}\\right) ^ {2}, x _ {i} ^ {\\prime} = \\frac {1}{\\sigma_ {\\mathbf {x}}} \\odot \\left(x _ {i} - \\mu_ {\\mathbf {x}}\\right), \\tag {1}\n$$\n\nwhere  $\\mu_{\\mathbf{x}},\\sigma_{\\mathbf{x}}\\in \\mathbb{R}^{C\\times 1},\\frac{1}{\\sigma_{\\mathbf{x}}}$  means the element-wise division and  $\\odot$  is the element-wise product. Note that Normalization module decreases the distributional discrepancy among each input time series, making the distribution of the model input more stable.\n\nDe-normalization module As shown in Figure 2, after the base model  $\\mathcal{H}$  predicting the future value with length- $O$ , we adopt De-normalization to transform the model output  $\\mathbf{y}' = [y_1', y_2', \\dots, y_O']^\\top \\in \\mathbb{R}^{O \\times C}$  with  $\\sigma_{\\mathbf{x}}$  and  $\\mu_{\\mathbf{x}}$  and obtain  $\\hat{\\mathbf{y}} = [\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_O]^\\top$  as the eventual forecasting results. The De-normalization module can be formulated as follows:\n\n$$\n\\mathbf {y} ^ {\\prime} = \\mathcal {H} \\left(\\mathbf {x} ^ {\\prime}\\right), \\hat {y} _ {i} = \\sigma_ {\\mathbf {x}} \\odot y _ {i} ^ {\\prime} + \\mu_ {\\mathbf {x}}. \\tag {2}\n$$\n\nBy means of the two-stage transformation, the base models will receive stationarized inputs, which follow a stable distribution and are easier to generalize. This design also makes the model equivariant to translational and scaling perturbance of time series, thereby benefiting real-world series forecasting.\n\n# 3.2 De-stationary Attention\n\nWhile the statistics of each time series are explicitly restored to the corresponding prediction, the non-stationarity of the original series cannot be fully recovered only by De-normalization. For instance, Series Stationarization can generate the same stationarized input  $\\mathbf{x}^{\\prime}$  from distinct time series  $\\mathbf{x}_1,\\mathbf{x}_2$  (i.e.  $\\mathbf{x}_2 = \\alpha \\mathbf{x}_1 + \\beta$ ), and the base model will get identical attention that fails to capture crucial temporal dependencies entangled with non-stationarity (Figure 1). In other words, the undermined effects caused by over-stationarization happen inside the deep model, especially in the calculation of attention. Furthermore, non-stationary time series are fragmented and normalized into several series chunks with the same mean and variance, which follow more similar distributions than the raw data before stationarization. Thus, the model is more likely to generate over-stationary and uneventful outputs, which is irreconcilable with the natural non-stationarity of the original series.\n\nTo tackle the over-stationarization problem caused by Series Stationarization, we propose a novel De-stationary Attention mechanism, which can approximate the attention that is obtained without stationarization and discover the particular temporal dependencies from original non-stationary data.\n\nAnalysis of the plain model As mentioned above, the over-stationarization problem is caused by the vanishment of inherent non-stationarity information, which will make the base model fail to capture eventful temporal dependencies for forecasting. Therefore, we try to approximate the attention learned from the original non-stationary series. We start from the formula of Self-Attention [34]:\n\n$$\n\\operatorname {A t t n} (\\mathbf {Q}, \\mathbf {K}, \\mathbf {V}) = \\operatorname {S o f t m a x} \\left(\\frac {\\mathbf {Q} \\mathbf {K} ^ {\\top}}{\\sqrt {d _ {k}}}\\right) \\mathbf {V}, \\tag {3}\n$$\n\nwhere  $\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in \\mathbb{R}^{S\\times d_k}$  are length-  $S$  queries, keys and values of  $d_{k}$ -dimension respectively, and Softmax(·) is conducted row by row. To simplify the analysis, we assume the embedding and feed-forward layers  $f$  to hold the linear properties² and  $f$  is conducted separately on each time point, that is, each query token in  $\\mathbf{Q} = [q_1,q_2,\\dots,q_S]^\\top$  can be calculated as  $q_{i} = f(x_{i})$  with respect to the input series  $\\mathbf{x} = [x_{1},x_{2},\\dots,x_{S}]^{\\top}$ . Since it is a convention to conduct normalization on each time series variable to avoid certain variables that dominates the scale, we can further assume each variable of series  $\\mathbf{x}$  shares the same variance, and thus original  $\\sigma_{\\mathbf{x}}\\in \\mathbb{R}^{C\\times 1}$  is reduced to a scalar. After Normalization module, the model receives the stationarized input  $\\mathbf{x}' = (\\mathbf{x} - \\mathbf{1}\\mu_{\\mathbf{x}}^{\\top}) / \\sigma_{\\mathbf{x}}$ , where  $\\mathbf{1}\\in \\mathbb{R}^{S\\times 1}$  is an all-ones vector. Based on the linear property assumption, it can be proved that the Attention layer will receive  $\\mathbf{Q}' = [f(x_1'),\\dots,f(x_S')]^{\\top} = (\\mathbf{Q} - \\mathbf{1}\\mu_{\\mathbf{Q}}^{\\top}) / \\sigma_{\\mathbf{x}}$ , where  $\\mu_{\\mathbf{Q}}\\in \\mathbb{R}^{d_k\\times 1}$  is the mean of  $\\mathbf{Q}$  along the temporal dimension (See Appendix A for a detailed proof). And so is the corresponding transformed  $\\mathbf{K}'$ ,  $\\mathbf{V}'$ . Without Series Stationarization, the input of Softmax(·) in Self-Attention should be  $\\mathbf{Q}\\mathbf{K}^{\\top} / \\sqrt{d_k}$ , while now the attention is calculated based on  $\\mathbf{Q}'$ ,  $\\mathbf{K}'$ :\n\n$$\n\\mathbf {Q} ^ {\\prime} \\mathbf {K} ^ {\\prime \\top} = \\frac {1}{\\sigma_ {\\mathbf {x}} ^ {2}} \\left(\\mathbf {Q} \\mathbf {K} ^ {\\top} - \\mathbf {1} (\\mu_ {\\mathbf {Q}} ^ {\\top} \\mathbf {K} ^ {\\top}) - (\\mathbf {Q} \\mu_ {\\mathbf {K}}) \\mathbf {1} ^ {\\top} + \\mathbf {1} (\\mu_ {\\mathbf {Q}} ^ {\\top} \\mu_ {\\mathbf {K}}) \\mathbf {1} ^ {\\top}\\right),\n$$\n\n$$\n\\mathrm {S o f t m a x} \\left(\\frac {\\mathbf {Q} \\mathbf {K} ^ {\\top}}{\\sqrt {d _ {k}}}\\right) = \\mathrm {S o f t m a x} \\left(\\frac {\\sigma_ {\\mathbf {x}} ^ {2} \\mathbf {Q} ^ {\\prime} \\mathbf {K} ^ {\\prime \\top} + \\mathbf {1} (\\mu_ {\\mathbf {Q}} ^ {\\top} \\mathbf {K} ^ {\\top}) + (\\mathbf {Q} \\mu_ {\\mathbf {K}}) \\mathbf {1} ^ {\\top} - \\mathbf {1} (\\mu_ {\\mathbf {Q}} ^ {\\top} \\mu_ {\\mathbf {K}}) \\mathbf {1} ^ {\\top}}{\\sqrt {d _ {k}}}\\right).\n$$\n\nWe find that  $\\mathbf{Q}\\mu_{\\mathbf{K}}\\in \\mathbb{R}^{S\\times 1}$  and  $\\mu_{\\mathbf{Q}}^{\\top}\\mu_{\\mathbf{K}}\\in \\mathbb{R}$ , and they are repeatedly operated on each column and element of  $\\sigma_{\\mathbf{x}}^{2}\\mathbf{Q}'\\mathbf{K}'^{\\top}\\in \\mathbb{R}^{S\\times S}$  respectively. Since  $\\mathrm{Softmax}(\\cdot)$  is invariant to the same translation on the row dimension of input, we have the following equation:\n\n$$\n\\operatorname {S o f t m a x} \\left(\\frac {\\mathbf {Q} \\mathbf {K} ^ {\\top}}{\\sqrt {d _ {k}}}\\right) = \\operatorname {S o f t m a x} \\left(\\frac {\\sigma_ {\\mathbf {x}} ^ {2} \\mathbf {Q} ^ {\\prime} \\mathbf {K} ^ {\\prime \\top} + \\mathbf {1} \\mu_ {\\mathbf {Q}} ^ {\\top} \\mathbf {K} ^ {\\top}}{\\sqrt {d _ {k}}}\\right). \\tag {5}\n$$\n\nEquation 5 deduces a direct expression of the attention Softmax  $\\left(\\mathbf{Q}\\mathbf{K}^{\\top} / \\sqrt{d_k}\\right)$  learned from raw series  $\\mathbf{x}$ . Except for the current  $\\mathbf{Q}'$ ,  $\\mathbf{K}'$  from stationarized series  $\\mathbf{x}'$ , this expression also requires the non-stationary information  $\\sigma_{\\mathbf{x}}, \\mu_{\\mathbf{Q}}, \\mathbf{K}$  that are eliminated by Series Stationarization.\n\nDe-stationary Attention To recover the original attention on non-stationary series, we attempt to bring the vanished non-stationary information back to its calculation. Based on Equation 5, the key is to approximate the positive scaling scalar  $\\tau = \\sigma_{\\mathbf{x}}^{2} \\in \\mathbb{R}^{+}$  and shifting vector  $\\Delta = \\mathbf{K} \\mu_{\\mathbf{Q}} \\in \\mathbb{R}^{S \\times 1}$ , which are defined as de-stationary factors. Since the strict linear property hardly holds for a deep model, other than estimating and utilizing real factors with great effort, we try to learn de-stationary factors directly from the statistics of unstationarized  $\\mathbf{x}$ ,  $\\mathbf{Q}$  and  $\\mathbf{K}$  by a simple but effective multilayer perceptron layer. As we can only discover limited non-stationary information from current  $\\mathbf{Q}'$ ,  $\\mathbf{K}'$ , the unique and reasonable source to compensate non-stationarity is the original  $\\mathbf{x}$  without being normalized. Thus, as a direct deep learning implementation of Equation 5, we apply a multilayer perceptron as the projector to learn de-stationary factors  $\\tau, \\Delta$  from the statistics  $\\mu_{\\mathbf{x}}, \\sigma_{\\mathbf{x}}$  of unstationarized  $\\mathbf{x}$  individually. And the De-stationary Attention is calculated as follows:\n\n$$\n\\log \\tau = \\operatorname {M L P} \\left(\\sigma_ {\\mathbf {x}}, \\mathbf {x}\\right), \\boldsymbol {\\Delta} = \\operatorname {M L P} \\left(\\mu_ {\\mathbf {x}}, \\mathbf {x}\\right),\n$$\n\n$$\n\\operatorname {A t t n} \\left(\\mathbf {Q} ^ {\\prime}, \\mathbf {K} ^ {\\prime}, \\mathbf {V} ^ {\\prime}, \\tau , \\boldsymbol {\\Delta}\\right) = \\operatorname {S o f t m a x} \\left(\\frac {\\tau \\mathbf {Q} ^ {\\prime} \\mathbf {K} ^ {\\prime} {} ^ {\\top} + \\mathbf {1} \\boldsymbol {\\Delta} {} ^ {\\top}}{\\sqrt {d _ {k}}}\\right) \\mathbf {V} ^ {\\prime}, \\tag {6}\n$$\n\nwhere the de-stationary factors  $\\tau$  and  $\\Delta$  are shared by De-stationary Attention of all layers (Figure 2). De-stationary Attention mechanism learns the temporal dependencies from both stationarized series\n\n$\\mathbf{Q}'$ ,  $\\mathbf{K}'$  and non-stationary series  $\\mathbf{x}$ ,  $\\mu_{\\mathbf{x}}$ ,  $\\sigma_{\\mathbf{x}}$ , and multiplies by the stationarized values  $\\mathbf{V}'$ . Therefore, it can benefit from the predictability of stationarized series and maintain the inherent temporal dependencies of raw series simultaneously.\n\nOverall architecture Following the prior use of Transformers [39, 37] in time series forecasting, we adopt the standard Encoder-Decoder structure (Figure 2), where the encoder is to extract information from past observations, and the decoder is to aggregate past information and refine the prediction from simple initialization. The canonical Non-stationary Transformer is wrapped by Series Stationarization to both the input and output of vanilla Transformer [34], and replacing the Self-Attention by our proposed De-stationary Attention, which can boost the non-stationary series predictive capability of the base model. For the Transformer variants [19, 39, 37], we transform the terms inside Softmax( $\\cdot$ ) with the de-stationary factors  $\\tau$ ,  $\\Delta$  to re-integrate the non-stationary information (See Appendix E.2 for the implementation details).\n",
  "experiments": "# 4 Experiments\n\nWe conduct extensive experiments to evaluate the performance of Non-stationary Transformers on six real-world time series forecasting benchmarks and further validate the generality of the proposed framework on various mainstream Transformer variants.\n\nDatasets Here are the descriptions of the datasets: (1) Electricity [1] records the hourly electricity consumption of 321 clients from 2012 to 2014. (2) ETT [39] contains the time series of oil temperature and power load collected by electricity transformers from July 2016 to July 2018. ETTm1 /ETTm2 are recorded every 15 minutes, and ETTh1/ETTh2 are recorded every hour. (3) Exchange [20] collects the panel data of daily exchange rates from 8 countries from 1990 to 2016. (4) ILI [2] collects the ratio of influenza-like illness patients versus the total patients in one week, which is reported weekly by Centers for Disease Control and Prevention of the United States from 2002 and 2021. (5) Traffic [3] contains hourly road occupancy rates measured by 862 sensors on San Francisco Bay area freeways from January 2015 to December 2016. (6) Weather [4] includes meteorological time series with 21 weather indicators collected every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in 2020.\n\nEspecially, in this paper, we adopt the Augmented Dick-Fuller (ADF) test statistic [14] as the metric to quantitatively measure the degree of stationarity. A smaller ADF test statistic indicates a higher degree of stationarity, which means the distribution is more stable. Table 1 summarizes the overall statistics of the datasets and lists them in ascending order by degree of stationarity. We follow the standard protocol that divides each dataset into the training, validation, and testing subsets according to the chronological order. The split ratio is 6:2:2 for the ETT dataset and 7:1:2 for others.\n\nTable 1: Summary of datasets. Smaller ADF test statistic indicates more stationary dataset.  \n\n<table><tr><td>Dataset</td><td>Variable Number</td><td>Sampling Frequency</td><td>Total Observations</td><td>ADF Test Statistic</td></tr><tr><td>Exchange</td><td>8</td><td>1 Day</td><td>7,588</td><td>-1.889</td></tr><tr><td>ILI</td><td>7</td><td>1 Week</td><td>966</td><td>-5.406</td></tr><tr><td>ETTm2</td><td>7</td><td>15 Minutes</td><td>69,680</td><td>-6.225</td></tr><tr><td>Electricity</td><td>321</td><td>1 Hour</td><td>26,304</td><td>-8.483</td></tr><tr><td>Traffic</td><td>862</td><td>1 Hour</td><td>17,544</td><td>-15.046</td></tr><tr><td>Weather</td><td>21</td><td>10 Minutes</td><td>52,695</td><td>-26.661</td></tr></table>\n\nBaselines We evaluate the vanilla Transformer [34] equipped by the Non-stationary Transformers framework in both multivariate and univariate settings to demonstrate its effectiveness. For multivariate forecasting, we include six state-of-the-art deep forecasting models: Autoformer [37], Pyraformer [23], Informer [39], LogTrans [22], Reformer [19] and LSTM [20]. For univariate forecasting, we include seven competitive baselines: N-HiTS [9], N-BEATS [27], Autoformer [37], Pyraformer [23], Informer [39], Reformer [19] and ARIMA [7]. In addition, we adopt the proposed framework on both the canonical and efficient variants of Transformers: Transformer [34], Informer [39], Reformer [19] and Autoformer [37] to validate the generality of our framework.\n\nImplementation details All the experiments are implemented with PyTorch [30] and conducted on a single NVIDIA Titan V 12GB GPU. Each model is trained by ADAM [18] using L2 loss with\n\nthe initial learning rate of  $10^{-4}$  and batch size of 32. Each Transformer-based model contains two encoder layers and one decoder layer. Considering the efficiency of hyperparameters search, we use two-layer perceptron projector with the hidden dimension varying in  $\\{64, 128, 256\\}$  in De-stationary Attention. We repeat each experiment three times with different random seeds and report the test MSE/MAE under different prediction lengths, and the standard deviations are also provided in the Appendix C.2. A lower MSE/MAE indicates better performance.\n\n# 4.1 Main Results\n\nForecasting results As for multivariate forecasting results, the vanilla Transformer equipped with our framework consistently achieves state-of-the-art performance in all benchmarks and prediction lengths (Table 2). Notably, Non-stationary Transformer outperforms other deep models impressively on datasets characterized by high non-stationarity: under the prediction length of 336, we achieve  $17\\%$  MSE reduction  $(0.509 \\to 0.421)$  on Exchange and  $25\\%$ $(2.669 \\to 2.010)$  on ILI compared to previous state-of-the-art results, which indicates that the potential of deep model is still constrained on non-stationary data. We also list the univariate results of two typical datasets with different stationarity in Table 3. Non-stationary Transformer still realizes remarkable forecasting performance.\n\nTable 2: Forecasting results comparison under different prediction lengths  $O \\in \\{96,192,336,720\\}$ . The input sequence length is set to 36 for ILI and 96 for the others. Additional results (ETTm1, ETTh1, ETTh2) can be found in Appendix C.1.  \n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">Ours</td><td colspan=\"2\">Autoformer [37]</td><td colspan=\"2\">Pyraformer [23]</td><td colspan=\"2\">Informer [39]</td><td colspan=\"2\">LogTrans [22]</td><td colspan=\"2\">Reformer [19]</td><td colspan=\"2\">LSTNet [20]</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.111</td><td>0.237</td><td>0.197</td><td>0.323</td><td>0.852</td><td>0.780</td><td>0.847</td><td>0.752</td><td>0.968</td><td>0.812</td><td>1.065</td><td>0.829</td><td>1.551</td><td>1.058</td></tr><tr><td>192</td><td>0.219</td><td>0.335</td><td>0.300</td><td>0.369</td><td>0.993</td><td>0.858</td><td>1.204</td><td>0.895</td><td>1.040</td><td>0.851</td><td>1.188</td><td>0.906</td><td>1.477</td><td>1.028</td></tr><tr><td>336</td><td>0.421</td><td>0.476</td><td>0.509</td><td>0.524</td><td>1.240</td><td>0.958</td><td>1.672</td><td>1.036</td><td>1.659</td><td>1.081</td><td>1.357</td><td>0.976</td><td>1.507</td><td>1.031</td></tr><tr><td>720</td><td>1.092</td><td>0.769</td><td>1.447</td><td>0.941</td><td>1.711</td><td>1.093</td><td>2.478</td><td>1.310</td><td>1.941</td><td>1.127</td><td>1.510</td><td>1.016</td><td>2.285</td><td>1.243</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>2.294</td><td>0.945</td><td>3.483</td><td>1.287</td><td>5.800</td><td>1.693</td><td>5.764</td><td>1.677</td><td>4.480</td><td>1.444</td><td>4.400</td><td>1.382</td><td>6.026</td><td>1.770</td></tr><tr><td>36</td><td>1.825</td><td>0.848</td><td>3.103</td><td>1.148</td><td>6.043</td><td>1.733</td><td>4.755</td><td>1.467</td><td>4.799</td><td>1.467</td><td>4.783</td><td>1.448</td><td>5.340</td><td>1.668</td></tr><tr><td>48</td><td>2.010</td><td>0.900</td><td>2.669</td><td>1.085</td><td>6.213</td><td>1.763</td><td>4.763</td><td>1.469</td><td>4.800</td><td>1.468</td><td>4.832</td><td>1.465</td><td>6.080</td><td>1.787</td></tr><tr><td>60</td><td>2.178</td><td>0.963</td><td>2.770</td><td>1.125</td><td>6.531</td><td>1.814</td><td>5.264</td><td>1.564</td><td>5.278</td><td>1.560</td><td>4.882</td><td>1.483</td><td>5.548</td><td>1.720</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.192</td><td>0.274</td><td>0.255</td><td>0.339</td><td>0.409</td><td>0.488</td><td>0.365</td><td>0.453</td><td>0.768</td><td>0.642</td><td>0.658</td><td>0.619</td><td>3.142</td><td>1.365</td></tr><tr><td>192</td><td>0.280</td><td>0.339</td><td>0.281</td><td>0.340</td><td>0.673</td><td>0.641</td><td>0.533</td><td>0.563</td><td>0.989</td><td>0.757</td><td>1.078</td><td>0.827</td><td>3.154</td><td>1.369</td></tr><tr><td>336</td><td>0.334</td><td>0.361</td><td>0.339</td><td>0.372</td><td>1.210</td><td>0.846</td><td>1.363</td><td>0.887</td><td>1.334</td><td>0.872</td><td>1.549</td><td>0.972</td><td>3.160</td><td>1.369</td></tr><tr><td>720</td><td>0.417</td><td>0.413</td><td>0.422</td><td>0.419</td><td>4.044</td><td>1.526</td><td>3.379</td><td>1.388</td><td>3.048</td><td>1.328</td><td>2.631</td><td>1.242</td><td>3.171</td><td>1.368</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.169</td><td>0.273</td><td>0.201</td><td>0.317</td><td>0.498</td><td>0.299</td><td>0.274</td><td>0.368</td><td>0.258</td><td>0.357</td><td>0.312</td><td>0.402</td><td>0.680</td><td>0.645</td></tr><tr><td>192</td><td>0.182</td><td>0.286</td><td>0.222</td><td>0.334</td><td>0.828</td><td>0.312</td><td>0.296</td><td>0.386</td><td>0.266</td><td>0.368</td><td>0.348</td><td>0.433</td><td>0.725</td><td>0.676</td></tr><tr><td>336</td><td>0.200</td><td>0.304</td><td>0.231</td><td>0.338</td><td>1.476</td><td>0.326</td><td>0.300</td><td>0.394</td><td>0.280</td><td>0.380</td><td>0.350</td><td>0.433</td><td>0.828</td><td>0.727</td></tr><tr><td>720</td><td>0.222</td><td>0.321</td><td>0.254</td><td>0.361</td><td>4.090</td><td>0.372</td><td>0.373</td><td>0.439</td><td>0.283</td><td>0.376</td><td>0.340</td><td>0.420</td><td>0.957</td><td>0.811</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.612</td><td>0.338</td><td>0.613</td><td>0.388</td><td>0.684</td><td>0.393</td><td>0.719</td><td>0.391</td><td>0.684</td><td>0.384</td><td>0.732</td><td>0.423</td><td>1.107</td><td>0.685</td></tr><tr><td>192</td><td>0.613</td><td>0.340</td><td>0.616</td><td>0.382</td><td>0.692</td><td>0.394</td><td>0.696</td><td>0.379</td><td>0.685</td><td>0.390</td><td>0.733</td><td>0.420</td><td>1.157</td><td>0.706</td></tr><tr><td>336</td><td>0.618</td><td>0.328</td><td>0.622</td><td>0.337</td><td>0.699</td><td>0.396</td><td>0.777</td><td>0.420</td><td>0.733</td><td>0.408</td><td>0.742</td><td>0.420</td><td>1.216</td><td>0.730</td></tr><tr><td>720</td><td>0.653</td><td>0.355</td><td>0.660</td><td>0.408</td><td>0.712</td><td>0.404</td><td>0.864</td><td>0.472</td><td>0.717</td><td>0.396</td><td>0.755</td><td>0.423</td><td>1.481</td><td>0.805</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.173</td><td>0.223</td><td>0.266</td><td>0.336</td><td>0.354</td><td>0.392</td><td>0.300</td><td>0.384</td><td>0.458</td><td>0.490</td><td>0.689</td><td>0.596</td><td>0.594</td><td>0.587</td></tr><tr><td>192</td><td>0.245</td><td>0.285</td><td>0.307</td><td>0.367</td><td>0.673</td><td>0.597</td><td>0.598</td><td>0.544</td><td>0.658</td><td>0.589</td><td>0.752</td><td>0.638</td><td>0.560</td><td>0.565</td></tr><tr><td>336</td><td>0.321</td><td>0.338</td><td>0.359</td><td>0.395</td><td>0.634</td><td>0.592</td><td>0.578</td><td>0.523</td><td>0.797</td><td>0.652</td><td>0.639</td><td>0.596</td><td>0.597</td><td>0.587</td></tr><tr><td>720</td><td>0.414</td><td>0.410</td><td>0.419</td><td>0.428</td><td>0.942</td><td>0.723</td><td>1.059</td><td>0.741</td><td>0.869</td><td>0.675</td><td>1.130</td><td>0.792</td><td>0.618</td><td>0.599</td></tr></table>\n\n**Framework generality** We apply our framework to four mainstream Transformers and report the performance promotion of each model (Table 4). Our method consistently improves the forecasting ability of different models. Overall, it achieves averaged  $49.43\\%$  promotion on Transformer,  $47.34\\%$  on Informer,  $46.89\\%$  on Reformer and  $10.57\\%$  on Autoformer, making each of them surpass previous state-of-the-art. Compared to native blocks of the models, there is hardly any parameter and computation increase by applying our framework (See Appendix C.5 for details), and thereby their computational complexities can be preserved. It validates that Non-stationary Transformer is an effective and lightweight framework that can be widely applied to Transformer-based models and enhances their non-stationary predictability to achieve state-of-the-art performance.\n\nTable 3: Univariate results under different prediction lengths  $O \\in \\{96,192,336,720\\}$  on two typical datasets with strong non-stationary. The input sequence length is set to 96.  \n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">Ours</td><td colspan=\"2\">N-HiTS [9]</td><td colspan=\"2\">N-BEATS [27]</td><td colspan=\"2\">Autoformer [37]</td><td colspan=\"2\">Pyraformer [23]</td><td colspan=\"2\">Informer [39]</td><td colspan=\"2\">Reformer [19]</td><td colspan=\"2\">ARIMA [6]</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.104</td><td>0.235</td><td>0.114</td><td>0.248</td><td>0.156</td><td>0.299</td><td>0.241</td><td>0.387</td><td>0.290</td><td>0.439</td><td>0.591</td><td>0.615</td><td>1.327</td><td>0.944</td><td>0.112</td><td>0.245</td></tr><tr><td>192</td><td>0.230</td><td>0.375</td><td>0.250</td><td>0.387</td><td>0.669</td><td>0.665</td><td>0.273</td><td>0.403</td><td>0.594</td><td>0.644</td><td>1.183</td><td>0.912</td><td>1.258</td><td>0.924</td><td>0.304</td><td>0.404</td></tr><tr><td>336</td><td>0.432</td><td>0.509</td><td>0.434</td><td>0.516</td><td>0.611</td><td>0.605</td><td>0.508</td><td>0.539</td><td>0.962</td><td>0.824</td><td>1.367</td><td>0.984</td><td>2.179</td><td>1.296</td><td>0.736</td><td>0.598</td></tr><tr><td>720</td><td>0.782</td><td>0.682</td><td>1.061</td><td>0.773</td><td>1.111</td><td>0.860</td><td>0.991</td><td>0.768</td><td>1.285</td><td>0.958</td><td>1.872</td><td>1.072</td><td>1.280</td><td>0.953</td><td>1.871</td><td>0.935</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.069</td><td>0.193</td><td>0.092</td><td>0.232</td><td>0.082</td><td>0.219</td><td>0.065</td><td>0.189</td><td>0.074</td><td>0.208</td><td>0.088</td><td>0.225</td><td>0.131</td><td>0.288</td><td>0.211</td><td>0.362</td></tr><tr><td>192</td><td>0.109</td><td>0.249</td><td>0.128</td><td>0.276</td><td>0.120</td><td>0.268</td><td>0.118</td><td>0.256</td><td>0.116</td><td>0.252</td><td>0.132</td><td>0.283</td><td>0.186</td><td>0.354</td><td>0.261</td><td>0.406</td></tr><tr><td>336</td><td>0.139</td><td>0.286</td><td>0.165</td><td>0.314</td><td>0.226</td><td>0.370</td><td>0.154</td><td>0.305</td><td>0.143</td><td>0.295</td><td>0.180</td><td>0.336</td><td>0.220</td><td>0.381</td><td>0.317</td><td>0.448</td></tr><tr><td>720</td><td>0.180</td><td>0.331</td><td>0.243</td><td>0.397</td><td>0.188</td><td>0.338</td><td>0.182</td><td>0.335</td><td>0.197</td><td>0.338</td><td>0.300</td><td>0.435</td><td>0.267</td><td>0.430</td><td>0.366</td><td>0.487</td></tr></table>\n\nTable 4: Performance promotion by applying the proposed framework to Transformer and its variants. We report the averaged MSE/MAE of all prediction lengths (stated in Table 2) and the relative MSE reduction ratios (Promotion) by our framework. Full results (under all prediction lengths and promotion on ETSformer [36], FEDformer [40]) can be found in Appendix C.2.  \n\n<table><tr><td>Dataset</td><td colspan=\"2\">Exchange</td><td colspan=\"2\">ILI</td><td colspan=\"2\">ETTm2</td><td colspan=\"2\">Electricity</td><td colspan=\"2\">Traffic</td><td colspan=\"2\">Weather</td></tr><tr><td>Model</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"2\">Transformer + Ours</td><td>1.425</td><td>0.915</td><td>4.864</td><td>1.460</td><td>1.501</td><td>0.869</td><td>0.277</td><td>0.372</td><td>0.665</td><td>0.363</td><td>0.657</td><td>0.573</td></tr><tr><td>0.457</td><td>0.449</td><td>2.077</td><td>0.914</td><td>0.306</td><td>0.347</td><td>0.193</td><td>0.296</td><td>0.628</td><td>0.345</td><td>0.288</td><td>0.314</td></tr><tr><td>Promotion</td><td colspan=\"2\">67.93%</td><td colspan=\"2\">57.30%</td><td colspan=\"2\">79.61%</td><td colspan=\"2\">30.32%</td><td colspan=\"2\">5.56%</td><td colspan=\"2\">56.16%</td></tr><tr><td rowspan=\"2\">Informer + Ours</td><td>1.550</td><td>0.998</td><td>5.137</td><td>1.544</td><td>1.410</td><td>0.823</td><td>0.311</td><td>0.397</td><td>0.764</td><td>0.416</td><td>0.634</td><td>0.548</td></tr><tr><td>0.496</td><td>0.460</td><td>2.125</td><td>0.928</td><td>0.460</td><td>0.434</td><td>0.226</td><td>0.330</td><td>0.719</td><td>0.409</td><td>0.275</td><td>0.302</td></tr><tr><td>Promotion</td><td colspan=\"2\">68.00%</td><td colspan=\"2\">58.63%</td><td colspan=\"2\">67.38%</td><td colspan=\"2\">27.33%</td><td colspan=\"2\">5.89%</td><td colspan=\"2\">56.78%</td></tr><tr><td rowspan=\"2\">Reformer + Ours</td><td>1.280</td><td>0.932</td><td>4.724</td><td>1.443</td><td>1.479</td><td>0.915</td><td>0.338</td><td>0.429</td><td>0.741</td><td>0.423</td><td>0.803</td><td>0.656</td></tr><tr><td>0.462</td><td>0.468</td><td>2.865</td><td>1.065</td><td>0.493</td><td>0.441</td><td>0.206</td><td>0.308</td><td>0.682</td><td>0.372</td><td>0.286</td><td>0.308</td></tr><tr><td>Promotion</td><td colspan=\"2\">63.91%</td><td colspan=\"2\">39.35%</td><td colspan=\"2\">66.67%</td><td colspan=\"2\">39.05%</td><td colspan=\"2\">7.96%</td><td colspan=\"2\">64.38%</td></tr><tr><td rowspan=\"2\">Autoformer + Ours</td><td>0.613</td><td>0.539</td><td>3.006</td><td>1.161</td><td>0.324</td><td>0.368</td><td>0.227</td><td>0.338</td><td>0.628</td><td>0.379</td><td>0.338</td><td>0.382</td></tr><tr><td>0.487</td><td>0.491</td><td>2.545</td><td>1.039</td><td>0.305</td><td>0.345</td><td>0.216</td><td>0.315</td><td>0.619</td><td>0.364</td><td>0.286</td><td>0.310</td></tr><tr><td>Promotion</td><td colspan=\"2\">20.55%</td><td colspan=\"2\">15.34%</td><td colspan=\"2\">5.86%</td><td colspan=\"2\">4.85%</td><td colspan=\"2\">1.43%</td><td colspan=\"2\">15.38%</td></tr></table>\n\n# 4.2 Ablation Study\n\nQuality evaluation To explore the role of each module in our proposed framework, we compare the prediction results on ETTm2 obtained by three models: vanilla Transformer, Transformer with only Series Stationarization, and our Non-stationary Transformer. In Figure 3, we find out that the two modules strengthen the non-stationary forecasting ability of Transformer from different perspectives. Series Stationarization focuses on the alignment of statistical properties among each series input that benefits Transformer a lot to generalize on out-of-distribution data. However, as is shown in Figure 3(b), the over-stationarized circumstance for training makes the deep model more likely to output uneventful series with significant high stationarity and neglect the nature of non-stationary real-world data. With the aid of De-stationary Attention, the model gives concern back to the inherent non-stationarity of real-world time series. It is beneficial for an accurate prediction of the detailed series variation, which is vital in real-world time series forecasting.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/8109fd6d613ff9602edb2e9eb4987406784fad2127d925668a56b2ff76e21ae5.jpg)  \nFigure 3: Visualization of ETTm2 predictions given by different models.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/63b249f430aef3232f8a7427705f9c89856d79938112d16d651b5536497d0281.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/008816cdc1284d98847bf21311e9153e076fb8631e6779094109def2ebba9ed3.jpg)\n\nTable 5: Forecasting results obtained by applying different methods to Transformer and Reformer. We report the averaged MSE/MAE of all prediction lengths (stated in Table 2) for comparison. Complete results can be found in Appendix C.3.  \n\n<table><tr><td>Base Models</td><td colspan=\"6\">Transformer</td><td colspan=\"6\">Reformer</td></tr><tr><td>Methods</td><td colspan=\"2\">+ RevIN [17]</td><td colspan=\"2\">+ Series Stationarization</td><td colspan=\"2\">+ Ours</td><td colspan=\"2\">+ RevIN [17]</td><td colspan=\"2\">+ Series Stationarization</td><td colspan=\"2\">+ Ours</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>Exchange</td><td>0.567</td><td>0.487</td><td>0.569</td><td>0.488</td><td>0.461</td><td>0.454</td><td>0.469</td><td>0.472</td><td>0.470</td><td>0.473</td><td>0.462</td><td>0.468</td></tr><tr><td>ILI</td><td>2.205</td><td>0.934</td><td>2.206</td><td>0.934</td><td>2.077</td><td>0.914</td><td>3.024</td><td>1.096</td><td>3.023</td><td>1.096</td><td>2.865</td><td>1.065</td></tr><tr><td>ETTm2</td><td>0.460</td><td>0.416</td><td>0.461</td><td>0.416</td><td>0.306</td><td>0.347</td><td>0.542</td><td>0.459</td><td>0.537</td><td>0.459</td><td>0.493</td><td>0.441</td></tr><tr><td>Electricity</td><td>0.197</td><td>0.298</td><td>0.197</td><td>0.298</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.309</td><td>0.207</td><td>0.309</td><td>0.206</td><td>0.308</td></tr><tr><td>Traffic</td><td>0.643</td><td>0.352</td><td>0.641</td><td>0.352</td><td>0.628</td><td>0.345</td><td>0.687</td><td>0.378</td><td>0.691</td><td>0.380</td><td>0.682</td><td>0.372</td></tr><tr><td>Weather</td><td>0.301</td><td>0.316</td><td>0.304</td><td>0.317</td><td>0.288</td><td>0.314</td><td>0.291</td><td>0.309</td><td>0.292</td><td>0.309</td><td>0.286</td><td>0.308</td></tr></table>\n\nQuantitative performance In addition to the above case study, we also provide quantitative forecasting performance comparison with stationarization methods: a deep method RevIN [17] and Series Stationarization (Section 3.1). As is shown in Table 5, the forecasting results assisted by RevIN and Series Stationarization are basically the same, which indicates that the parameter-free version of normalization in our framework performs sufficiently to stationarize time series. Besides, the proposed De-stationary Attention in Non-stationary Transformers further boosts the performance and achieves the best in all six benchmarks. The MSE reduction brought by De-stationary Attention becomes significant, especially when the dataset is highly non-stationary (Exchange:  $0.569 \\to 0.461$ , ETTm2:  $0.461 \\to 0.306$ ). The comparison reveals that simply stationarizing time series still limits the predictive capability of Transformers, and the complementary mechanisms in Non-stationary Transformers can properly release the models' potential for non-stationary series forecasting.\n\n# 4.3 Model Analysis\n\nOver-stationarization problem To verify the over-stationarization problem from a statistical view, we train Transformers with the aforementioned methods respectively, arrange all predicted time series in chronological order and compare the degree of stationarity with the ground truth (Figure 4). While models solely equipped with stationarization methods tend to output series with unexpected high degree of stationarity, the results assisted by De-stationary Attention are close to the actual value (relative stationarity  $\\in [97\\%, 103\\%]$ ). Besides, as the degree of series stationarity increases, the over-stationarization problem becomes more significant. The huge discrepancy of the degree of stationarity can account for the inferior performance of Transformer with only stationarization. And it also demonstrates that De-stationary Attention as an internal renovation alleviates over-stationarization.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/4ae2a76c-3efa-4cd9-b561-40158c3bb6a6/ff991f9709ab636f45da2537debee1bde53fce43ab56cf004256a9cb20ac06da.jpg)  \nFigure 4: Relative stationarity is calculated as the ratio of ADF test statistics between the model predictions and ground truth. From left to right, the dataset is increasingly non-stationary. While models equipped with only stationarization tend to output highly stationary series, our method gives predictions with stationarity closer to ground truth.\n\nExploring of Non-stationary Information Re-incorporation It is notable that by specifying over-stationarization as less distinguishable attention, we narrow down our design space into the attention calculation mechanism. To explore other approaches to retrieve non-stationary information, we conduct experiments by re-incorporating the  $\\mu$  and  $\\sigma$  into feed-forward layers (DeFF), which is the left part of the Transformer architecture. In detail, we feed learned  $\\mu$  and  $\\sigma$  into each feed-forward layer iteratively. As is shown in Table 6, re-incorporating non-stationarity is necessary only when the inputs are stationarized (Stationary), which is beneficial for forecasting but will lead to stationarity discrepancy of model outputs. And our proposed design (Stat + DeAttn) makes further promotion and achieves the best in most cases (77%). In addition to the theoretical analysis, experimental results further validate the effectiveness of our design in re-incorporating non-stationarity on attention.\n\nTable 6: Ablation of framework design. Baseline means vanilla Transformer, Stationary means adding Series Stationarization, DeFF means re-incorporating non-stationarity on feed-forward layers, DeAttn means re-incorporating by De-stationary Attention, Stat + DeFF means adding Series Stationarization and re-incorporating on feed-forward layers. Stat + DeAttn means our proposed framework.  \n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">Baseline</td><td colspan=\"2\">Stationary</td><td colspan=\"2\">DeFF</td><td colspan=\"2\">DeAttn</td><td colspan=\"2\">Stat + DeFF</td><td colspan=\"2\">Stat + DeAttn</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.567</td><td>0.591</td><td>0.136</td><td>0.258</td><td>0.784</td><td>0.696</td><td>0.611</td><td>0.613</td><td>0.116</td><td>0.243</td><td>0.111</td><td>0.237</td></tr><tr><td>192</td><td>1.150</td><td>0.825</td><td>0.239</td><td>0.348</td><td>1.162</td><td>0.866</td><td>1.202</td><td>0.840</td><td>0.280</td><td>0.383</td><td>0.219</td><td>0.335</td></tr><tr><td>336</td><td>1.792</td><td>1.084</td><td>0.425</td><td>0.479</td><td>1.346</td><td>0.963</td><td>1.516</td><td>0.981</td><td>0.371</td><td>0.452</td><td>0.421</td><td>0.476</td></tr><tr><td>720</td><td>2.191</td><td>1.159</td><td>1.475</td><td>0.865</td><td>2.042</td><td>1.163</td><td>2.894</td><td>1.377</td><td>0.934</td><td>0.704</td><td>1.092</td><td>0.769</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>4.748</td><td>1.430</td><td>2.573</td><td>0.980</td><td>4.850</td><td>1.445</td><td>4.734</td><td>1.424</td><td>2.404</td><td>0.985</td><td>2.294</td><td>0.945</td></tr><tr><td>36</td><td>4.671</td><td>1.430</td><td>1.955</td><td>0.870</td><td>4.848</td><td>1.452</td><td>4.927</td><td>1.482</td><td>2.585</td><td>0.983</td><td>1.825</td><td>0.848</td></tr><tr><td>48</td><td>4.994</td><td>1.482</td><td>2.057</td><td>0.902</td><td>4.903</td><td>1.466</td><td>4.996</td><td>1.483</td><td>2.496</td><td>0.991</td><td>2.010</td><td>0.900</td></tr><tr><td>60</td><td>5.041</td><td>1.499</td><td>2.238</td><td>0.982</td><td>5.196</td><td>1.524</td><td>5.184</td><td>1.519</td><td>2.667</td><td>1.059</td><td>2.178</td><td>0.963</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.572</td><td>0.552</td><td>0.253</td><td>0.311</td><td>0.767</td><td>0.635</td><td>0.304</td><td>0.406</td><td>0.275</td><td>0.329</td><td>0.192</td><td>0.274</td></tr><tr><td>192</td><td>1.161</td><td>0.793</td><td>0.453</td><td>0.404</td><td>0.960</td><td>0.717</td><td>0.820</td><td>0.652</td><td>0.406</td><td>0.403</td><td>0.280</td><td>0.339</td></tr><tr><td>336</td><td>1.209</td><td>0.842</td><td>0.546</td><td>0.461</td><td>1.159</td><td>0.811</td><td>1.406</td><td>0.883</td><td>0.502</td><td>0.465</td><td>0.334</td><td>0.361</td></tr><tr><td>720</td><td>3.061</td><td>1.289</td><td>0.593</td><td>0.489</td><td>3.187</td><td>1.308</td><td>2.858</td><td>1.108</td><td>0.694</td><td>0.575</td><td>0.417</td><td>0.413</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.260</td><td>0.358</td><td>0.171</td><td>0.275</td><td>0.260</td><td>0.356</td><td>0.253</td><td>0.351</td><td>0.170</td><td>0.274</td><td>0.169</td><td>0.273</td></tr><tr><td>192</td><td>0.266</td><td>0.367</td><td>0.192</td><td>0.296</td><td>0.264</td><td>0.365</td><td>0.257</td><td>0.358</td><td>0.188</td><td>0.293</td><td>0.182</td><td>0.286</td></tr><tr><td>336</td><td>0.280</td><td>0.375</td><td>0.208</td><td>0.306</td><td>0.277</td><td>0.374</td><td>0.270</td><td>0.365</td><td>0.206</td><td>0.309</td><td>0.200</td><td>0.304</td></tr><tr><td>720</td><td>0.302</td><td>0.386</td><td>0.216</td><td>0.315</td><td>0.299</td><td>0.384</td><td>0.295</td><td>0.380</td><td>0.223</td><td>0.323</td><td>0.222</td><td>0.321</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.647</td><td>0.357</td><td>0.614</td><td>0.337</td><td>0.646</td><td>0.353</td><td>0.650</td><td>0.358</td><td>0.605</td><td>0.333</td><td>0.612</td><td>0.338</td></tr><tr><td>192</td><td>0.649</td><td>0.356</td><td>0.637</td><td>0.351</td><td>0.645</td><td>0.352</td><td>0.655</td><td>0.358</td><td>0.617</td><td>0.342</td><td>0.613</td><td>0.340</td></tr><tr><td>336</td><td>0.667</td><td>0.364</td><td>0.653</td><td>0.359</td><td>0.672</td><td>0.360</td><td>0.656</td><td>0.355</td><td>0.635</td><td>0.349</td><td>0.618</td><td>0.328</td></tr><tr><td>720</td><td>0.697</td><td>0.376</td><td>0.661</td><td>0.360</td><td>0.695</td><td>0.376</td><td>0.681</td><td>0.366</td><td>0.649</td><td>0.351</td><td>0.653</td><td>0.355</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.395</td><td>0.427</td><td>0.175</td><td>0.225</td><td>0.417</td><td>0.445</td><td>0.296</td><td>0.364</td><td>0.178</td><td>0.226</td><td>0.173</td><td>0.223</td></tr><tr><td>192</td><td>0.619</td><td>0.560</td><td>0.273</td><td>0.297</td><td>0.699</td><td>0.604</td><td>0.480</td><td>0.464</td><td>0.256</td><td>0.295</td><td>0.245</td><td>0.285</td></tr><tr><td>336</td><td>0.689</td><td>0.594</td><td>0.333</td><td>0.325</td><td>0.773</td><td>0.620</td><td>0.581</td><td>0.519</td><td>0.338</td><td>0.351</td><td>0.321</td><td>0.338</td></tr><tr><td>720</td><td>0.926</td><td>0.710</td><td>0.436</td><td>0.420</td><td>1.008</td><td>0.718</td><td>0.795</td><td>0.642</td><td>0.417</td><td>0.412</td><td>0.414</td><td>0.410</td></tr></table>\n",
  "hyperparameter": "Learning rate: 1e-4; Batch size: 32; Optimizer: ADAM with L2 loss; Model architecture: 2 encoder layers and 1 decoder layer for Transformer-based models; MLP projector in De-stationary Attention: 2-layer perceptron with hidden dimension varying in {64, 128, 256}; Input sequence length: 36 for ILI dataset, 96 for other datasets; Prediction lengths tested: {96, 192, 336, 720}; Data split ratio: 6:2:2 for ETT dataset, 7:1:2 for other datasets"
}