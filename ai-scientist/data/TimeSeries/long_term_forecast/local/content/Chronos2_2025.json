{
  "id": "Chronos2_2025",
  "paper_title": "Chronos-2: From Univariate to Universal Forecasting",
  "alias": "Chronos2",
  "year": 2025,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "Chronos-2 introduces a universal forecasting model that extends from univariate to multivariate forecasting with covariate support through a novel group attention mechanism. The key innovation is the group attention layer that enables flexible in-context learning by aggregating information across related time series within groups, allowing the model to handle diverse forecasting tasks (univariate, multivariate, and covariate-informed) within a unified architecture. The model uses robust scaling with sinh^(-1) transformation, patch-based tokenization with meta features (time index and mask), and direct multi-step quantile forecasting with 21 quantiles for richer probabilistic predictions.",
  "introduction": "# 1 Introduction\n\nThe advent of pretrained models (also referred to as foundation models) has led to a paradigm shift in time series forecasting. Instead of training a model for each time series (local models) (Hyndman & Athanasopoulos, 2018) or dataset (task-specific models) (Lim et al., 2021; Challu et al., 2023), a single model can be trained once on large-scale time series data and then applied across different forecasting problems (Ansari et al., 2024; Das et al., 2024b). Pretrained models greatly simplify the forecasting pipeline by eliminating the need for training from scratch for each use case. More remarkably, they often match or exceed the forecast accuracy of task-specific models (Aksu et al., 2024).\n\nDespite these advances, a fundamental limitation persists: most pretrained models operate only on univariate data, considering solely the historical observations of a single time series to generate forecasts. Although univariate forecasting is important, the class of real-world forecasting tasks spans far beyond it. In practice, one may encounter tasks where multiple co-evolving time series need to be predicted simultaneously (multivariate forecasting) (Banbura et al., 2010; Cohen et al., 2025) or where forecasts depend on various external factors (covariate-informed forecasting). For example, cloud infrastructure metrics such as CPU usage, memory consumption, and storage I/O evolve together and benefit from joint modeling (Cohen et al., 2025). Likewise, retail demand is heavily influenced by promotional\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/678f967c432202d485571f774a64c1dced936eb854bd6ec4122f2919c3bf4ba1.jpg)  \nFigure 1: The complete Chronos-2 pipeline. Input time series (targets and covariates) are first normalized using a robust scaling scheme, after which time index and mask meta features are added. The resulting sequences are split into non-overlapping patches and mapped to high-dimensional embeddings via a residual network. The core transformer stack operates on these patch embeddings and produces multi-patch quantile outputs corresponding to the masked future patches provided as input. Each transformer block alternates between time and group attention layers: the time attention layer aggregates information across patches within a single time series, while the group attention layer aggregates information across all series within a group at each patch index. A group is a flexible notion of relatedness and may correspond to a single time series, multiple series sharing a source or metadata, variates of a multivariate series, or targets along with associated covariates. The figure illustrates two multivariate time series with one known covariate each, with corresponding groups highlighted in blue and red. This example is for illustration only; Chronos-2 supports arbitrary numbers of targets and optional covariates.\n\nactivities, while energy consumption patterns are driven by weather conditions (Petropoulos et al., 2022). The lack of multivariate and covariate-informed forecasting capabilities hinders the widespread adoption of pretrained models in real-world production systems.\n\nDeveloping universal pretrained models that can handle both multivariate dependencies and covariates remains challenging due to two factors. First, the heterogeneity of forecasting problems requires rethinking the model architecture. Each downstream task differs in the number of dimensions and their semantics. Since it is impossible to know a priori how the variables will interact in an unseen task, the model must infer these interactions from the available context. Second, high-quality pretraining data with multivariate dependencies and informative covariates is scarce.\n\nIn this work, we present Chronos-2, a pretrained model designed to handle arbitrary forecasting tasks — univariate, multivariate, and covariate-informed — in a zero-shot manner. Chronos-2 leverages in-context learning (ICL) to support multivariate forecasting and arbitrary covariates, whether past-only or with known future values, real-valued or categorical. Its enhanced ICL capabilities also improve univariate forecasting by enabling cross learning, where the model shares information across univariate time series in the batch, leading to more accurate predictions.\n\nAt the core of Chronos-2's ICL capabilities is the group attention mechanism. It enables information exchange within groups of time series, which may represent arbitrary sets of related series, variates of a multivariate series, or targets and covariates (both past-only and known) in a forecasting task. Rather than extending the context by concatenating targets and covariates, the group attention layer shares information within groups across the batch axis, allowing it to scale gracefully with the number of variates. A key innovation of Chronos-2 lies in our training approach: to enable its ICL capabilities, we rely on synthetic time series data generated by imposing multivariate structure on time series sampled from base univariate generators. The complete inference pipeline of Chronos-2 including tokenization and modeling is shown in Figure 1.\n\nEmpirical evaluation on comprehensive forecasting benchmarks, including fev-bench (Shchur et al., 2025), GIFT-Eval (Aksu et al., 2024), and Chronos Benchmark II (Ansari et al., 2024), shows that Chronos-2 achieves state-of-the-art performance. On fev-bench, which spans a wide range of forecasting tasks — univariate, multivariate,\n\n<table><tr><td>Model</td><td>Univariate Forecasting</td><td>Multivariate Forecasting</td><td>Past-Only Covariates</td><td>Known Covariates</td><td>Categorical Covariates</td><td>Cross Learning</td><td>Memory Scaling</td></tr><tr><td>Chronos-2</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>O(V)</td></tr><tr><td>Toto-1.0</td><td>✓</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>X</td><td>O(V)</td></tr><tr><td>TabPFN-TS</td><td>✓</td><td>X</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>O(V)</td></tr><tr><td>COSMIC</td><td>✓</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>O(V2)</td></tr><tr><td>Moirai-1.0</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>O(V2)</td></tr><tr><td>Chronos-Bolt</td><td>✓</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>-</td></tr><tr><td>Moirai-2.0</td><td>✓</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>-</td></tr><tr><td>Sundial</td><td>✓</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>-</td></tr><tr><td>TimesFM-2.5</td><td>✓</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>-</td></tr><tr><td>TiRex</td><td>✓</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>-</td></tr></table>\n\nTable 1: Comparison of capabilities of pretrained forecasting models. Past-Only Covariates: support for covariates only observed in the past; Known Covariates: support for covariates whose future values are known; Categorical Covariates: support for nominal features in the covariates; Cross Learning: support for in-context learning across related time series; Memory Scaling: inference memory requirements with respect to the total number of variates  $V$  (including both targets and covariates).\n\nand covariate-informed — Chronos-2 outperforms baselines across all categories. The largest gains are observed on covariate-informed tasks, demonstrating Chronos-2's strength in this practically important setting. Chronos-2 offers these new capabilities while maintaining high computational efficiency, running on a single mid-range GPU (NVIDIA A10G) with a throughput of 300 time series per second. $^{1}$\n\nThe rest of the technical report is organized as follows. Section 2 introduces the background on time series forecasting and existing forecasting methods with a special focus on pretrained models. In Section 3, we describe the architecture of Chronos-2 and discuss its training and inference pipelines. Section 4 briefly discusses the training corpus of Chronos-2. In Section 5, we present our main results on three forecasting benchmarks, case studies on energy and retail domains, and ablations. We conclude the report and discuss potential future work in Section 6.\n",
  "method": "# 3 The Chronos-2 Model\n\nIn this section, we introduce the Chronos-2 model. We begin with scaling and tokenization, followed by the model's architecture including the group attention mechanism which enables Chronos-2's in-context learning capabilities. Subsequently, we discuss the training and inference pipelines of Chronos-2. The complete inference pipeline of Chronos-2 is visualized in Figure 1.\n\n# 3.1 Scaling and Tokenization\n\nInput Construction. The model operates on two inputs derived from the target  $\\mathbf{Y}_{1:T}$  and covariates  $\\mathbf{X}_{1:T + H}$ . We concatenate all historical values into  $\\mathbf{V} = [\\mathbf{v}_1, \\dots, \\mathbf{v}_T]$ , where each  $\\mathbf{v}_t \\in \\mathbb{R}^{D + M}$  consists of the target observation  $\\mathbf{y}_t$  and the corresponding covariate vector  $\\mathbf{x}_t$ . Similarly, we define the future values as  $\\mathbf{W} = [\\mathbf{w}_{T + 1}, \\dots, \\mathbf{w}_{T + H}]$ , where  $\\mathbf{w}_t \\in \\mathbb{R}^{D + M}$  contains known future covariate values  $\\mathbf{x}_t$  when available, while the entries corresponding to targets and past-only covariates are set to missing values.\n\nCategorical covariates in  $X_{1:T + H}$  are transformed into real-valued representations before being concatenated into  $V$  and  $W$ . For univariate targets, we apply target encoding (Pedregosa et al., 2011; Micci-Barreca, 2001), which maps each category to a numerical value based on its relationship with the target. For multivariate targets, the model falls back to ordinal encoding, assigning a unique integer to each category.\n\nRobust Scaling. The input values,  $V$  and  $W$ , may be at an arbitrary scale, so our tokenization pipeline begins by normalizing the series. We adopt standardization, a widely used normalization method in the literature, and introduce an additional step: applying the  $\\sinh^{-1}$  transformation to the standardized values. This log-like transformation further stabilizes variance and reduces the influence of outliers on the objective function. It has been used in econometrics (Burbidge et al., 1988) and energy price forecasting (Uniejewski & Weron, 2018) literature for handling extreme values. Formally, each historical value  $v_{t,d}$  and the future value  $w_{t,d}$  are normalized as\n\n$$\n\\tilde {v} _ {t, d} = \\sinh^ {- 1} \\left(\\frac {v _ {t , d} - \\mu_ {d}}{\\sigma_ {d}}\\right) \\quad \\text {f o r} t \\in \\{1, \\dots , T \\}, \\tag {1}\n$$\n\n$$\n\\tilde {w} _ {t, d} = \\sinh^ {- 1} \\left(\\frac {w _ {t , d} - \\mu_ {d}}{\\sigma_ {d}}\\right) \\quad \\text {f o r} t \\in \\{T + 1, \\dots , T + H \\}, \\tag {2}\n$$\n\nwhere  $\\mu_d$  and  $\\sigma_d$  are the mean and standard deviation of the historical values  $[v_{1,d},\\dots,v_{T,d}]$ , respectively. Any missing values in  $\\mathbf{V}$  are excluded when computing  $\\mu_d$  and  $\\sigma_d$ . The normalized historical values  $\\tilde{\\mathbf{V}}$  and future values  $\\tilde{\\mathbf{W}}$  are concatenated to construct the input matrix  $\\mathbf{U} = [\\tilde{\\mathbf{V}},\\tilde{\\mathbf{W}}] \\in \\mathbb{R}^{(T + H)\\times (D + M)}$ .\n\nMeta Features. During tokenization, each dimension of  $\\mathbf{U}$  is processed independently by the model. To describe the tokenization procedure, consider a single column  $\\mathbf{u}_d = [u_{1,d},\\dots,u_{T + H,d}]^\\top$  corresponding to one target or covariate dimension  $d$ . Two additional meta features are appended to each column: a time index and a mask. The time index  $j = \\left[-\\frac{T}{C}, - \\frac{T - 1}{C},\\dots ,0,\\dots ,\\frac{H - 1}{C}\\right]$  encodes the relative position of each time step, where  $C$  is the maximum context length supported by the model. It provides explicit information about temporal ordering to the model which is beneficial when using patch-based inputs. The mask  $m_d$  is a binary indicator equal to 1 when the value is observed, and 0 otherwise. It serves two purposes: indicating which values are missing in the historical context and specifying which input dimensions correspond to future-known covariates. After construction of the mask, all missing values in  $\\mathbf{u}_d$  are replaced with zeros.\n\nPatching and Embedding. The input  $\\pmb{u}_d$  with the corresponding meta features,  $\\pmb{j}$  and  $\\pmb{m}_d$ , are split into non-overlapping patches of length  $P$  (Nie et al., 2023). The context and future sections of the time series and meta features are split into patches separately. When  $T$  and  $H$  are not multiples of  $P$ , zero padding is applied on the left (context) or right (future). Let  $\\overline{\\pmb{u}}_p, \\overline{\\pmb{j}}_p,$  and  $\\overline{\\pmb{m}}_p$  denote the  $p$ -th patches of the input, time index, and mask, respectively. These are concatenated and mapped into the embedding space using a residual network,  $f_{\\phi}^{\\mathrm{in}}: \\mathbb{R}^{3P} \\to \\mathbb{R}^{D_{\\mathrm{model}}}$ ,\n\n$$\n\\boldsymbol {h} _ {p} = f _ {\\phi} ^ {\\text {i n}} \\left(\\left[ \\overline {{\\boldsymbol {u}}} _ {p}, \\overline {{\\boldsymbol {j}}} _ {p}, \\overline {{\\boldsymbol {m}}} _ {p} \\right]\\right), \\tag {3}\n$$\n\nwhere  $\\phi$  denotes parameters of the residual network and  $D_{\\mathrm{model}}$  is the hidden dimension of the transformer model. Between the patch embeddings of the context and future, we include a special REG token which serves both as a separator token and an attention sink (Xiao et al., 2024).\n\n# 3.2 Architecture\n\nChronos-2 is an encoder-only transformer (Vaswani et al., 2017) model which closely follows the design of the T5 encoder (Raffel et al., 2020). In the following, we discuss the key architectural components of Chronos-2.\n\nTime Attention. The time attention layer is the usual attention layer found in typical sequence models. It applies self-attention along the temporal axis and aggregates information across patches of the same input dimension. We replace relative position embeddings used in the self-attention layers of the original T5 model with rotary position embeddings (RoPE) (Su et al., 2024) which have become the de-facto standard for position embeddings in modern transformer-based models (Touvron et al., 2023).\n\nGroup Attention. We introduce a group attention layer into the transformer stack, which is central to enabling the in-context learning capabilities of Chronos-2. This layer aggregates information across time series that belong to the same group at a given patch index. A group refers to a set of related time series and may refer to different things depending on the forecasting task. For example, a group may consist of:\n\n- a single time series: the minimal grouping where the model makes univariate predictions without referring to other time series in the batch.  \n- a set of time series with shared source or metadata: this grouping enables the model to perform cross learning across items by making joint predictions for related time series (also referred to as few-shot learning) instead of generating univariate forecasts by solely taking the histories of individual time series into account. Sharing information between related time series could be especially helpful when all or some (cold start scenario) time series have short histories and when the characteristics of the downstream dataset differ considerably from the training data distribution.  \n- a set of variates with shared dynamics: this grouping enables multivariate forecasting where the model jointly predicts all variates with shared dynamics.  \n- a set of target(s), past-only covariates and known covariates: the most general case where the model forecasts targets while taking covariates into account.\n\nWithin a batch of size  $B$ , multiple groups of varying sizes are possible, each identified by group IDs  $g$ , a vector of length  $B$ . Internally, the group attention layer maps these IDs to a two-dimensional attention mask, ensuring that aggregation occurs only within groups and not across them. Since time series within a group lack a natural ordering, the group attention layer omits positional embeddings.\n\nQuantile Head. After a sequence of alternating time and group attention layers, the embeddings of future patches of the  $D$  target dimensions are passed through a residual block to produce the direct multi-step quantile forecast  $\\hat{\\mathbf{Z}}\\in \\mathbb{R}^{H\\times D\\times |\\mathcal{Q}|}$ . By producing forecasts for multiple target patches within a single forward pass, the model can efficiently generate predictions over long forecast horizons. Chronos-2 predicts a set of 21 quantiles  $\\mathcal{Q} = \\{0.01,0.05,0.1,\\ldots ,0.9,0.95,0.99\\}$ . This results in a richer representation of the predictive distribution compared to the 9-quantile grid  $\\{0.1,0.2,\\dots,0.9\\}$  commonly used in existing pretrained models. The inclusion of extreme quantiles (0.01 and 0.99) improves coverage of rare events and enhances the model's applicability to tasks such as anomaly detection and risk-aware forecasting.\n\n# 3.3 Training\n\nDuring training, batches are constructed to include heterogeneous forecasting tasks: univariate forecasting, multivariate forecasting (which also covers tasks with past-only covariates), and multivariate forecasting with known covariates. Each task is characterized by the number of target dimensions  $D$ , the number of covariates  $M$ , and the role of each dimension (target, past-only covariate, or known covariate). A unique group ID is assigned to each task, and the combination of group IDs  $g$  with whether the future input  $W$  is observed allows the model to infer the specific forecasting setup.\n\nThe model is trained using the quantile regression objective\n\n$$\n\\sum_ {q \\in \\mathcal {Q}} \\left(q \\cdot \\max  \\left(z - \\hat {z} ^ {q}, 0\\right) + (1 - q) \\cdot \\max  \\left(\\hat {z} ^ {q} - z, 0\\right)\\right), \\tag {4}\n$$\n\nwhere  $\\hat{z}^q$  is the forecast at quantile level  $q$ , and  $z$  is the corresponding target value normalized as in Eq. (1). The loss is averaged over all forecast steps and items in the batch and is computed only on target dimensions, with entries corresponding to known covariates or missing target values excluded from the objective. The number of output patches is randomly sampled for each batch during training.\n\nTraining proceeds in two stages. First, the model is pretrained with a maximum context length of 2048 and a low number of maximum output patches. In the second stage, the context length is extended to 8192, and the maximum number of sampled output patches is increased. Longer contexts enable the model to capture long-term seasonaities in high-frequency time series, while multi-patch outputs allow for long-horizon forecasts without relying on heuristics.\n\n# 3.4 Inference\n\nForecasts are generated by de-normalizing the model predictions  $\\hat{z}_{t,d}^{q}$  and inverting Eq. (1). Formally, the quantile head output  $\\hat{z}_{t,d}^{q}$  is transformed as\n\n$$\n\\hat {y} _ {t, d} ^ {q} = \\mu_ {d} + \\sigma_ {d} \\cdot \\sinh \\left(\\hat {z} _ {t, d} ^ {q}\\right), \\tag {5}\n$$\n\nto obtain the prediction  $\\hat{y}_{t,d}^q$  of the quantile level  $q$  at time step  $t$  along the target dimension  $d$ .\n\nDuring inference, multiple time series in a batch can be grouped to solve different forecasting tasks:\n\n- univariate forecasting: each item in the batch is assigned a unique group ID. This ensures that the model makes independent predictions for each time series in the batch.  \n- multivariate forecasting: each variate which belongs to the same multivariate series is assigned the same group ID with variates from different multivariate series having distinct group IDs. This allows the model to share dynamics information between different variates of a multivariate time series.  \n- forecasting with covariates: all target(s), past-only and known covariates belonging to the same task are assigned the same group ID. The future inputs  $\\mathbf{W}$  corresponding to known covariates contain their known future values. The predictions generated by the model for covariates are ignored.\n\n<table><tr><td>Task Type</td><td colspan=\"2\">Group IDs g</td><td colspan=\"2\">Future Inputs W</td></tr><tr><td rowspan=\"2\">Univariate Forecasting (3 independent series)</td><td rowspan=\"2\" colspan=\"2\">g = (1,2,3)</td><td colspan=\"2\">W = [ * ... * ] ∈ R3×H</td></tr><tr><td colspan=\"2\">* ... * ] * ... * ]</td></tr><tr><td rowspan=\"2\">Multivariate Forecasting (3 targets)</td><td rowspan=\"2\" colspan=\"2\">g = (1,1,1)</td><td colspan=\"2\">W = [ * ... * ] * ... * ] ∈ R3×H</td></tr><tr><td colspan=\"2\">* ... * ] * ... * ]</td></tr><tr><td>Forecasting with Covariates (1 target, 1 past-only covariate, 2 known covariates)</td><td>g = (1,1,1,1)</td><td colspan=\"2\">W = [ * ... * ] * ... * xT+1,3 ... xT+H,3 xT+1,4 ... xT+H,4]</td><td>∈ R4×H</td></tr></table>\n\nTable 2: Diverse forecasting tasks can be solved by specifying group IDs and future inputs appropriately. Here,  $\\mathbf{g}$  and  $\\mathbf{W}$  denote the group IDs and future values provided to the model. Future inputs for targets and past-only covariates are masked as missing values, denoted as  $*$ . The examples use fixed numbers of variates for clarity, but Chronos-2 can handle arbitrary dimensions.\n\nTable 2 summarizes how group IDs and future inputs must be specified to solve different forecasting tasks. In addition to these, Chronos-2 can also be used in the full cross learning mode where each item in the batch is assigned the same group ID regardless of whether the item is a target, a past-only covariate or a known covariate. Since each item belongs to the same group, the model shares information across items in the batch and makes joint predictions for the entire batch.\n",
  "experiments": "# 5 Experiments\n\nIn this section, we present empirical results, beginning with an evaluation of Chronos-2 against state-of-the-art approaches across three comprehensive benchmarks (Section 5.1). We then demonstrate the gains achieved through in-context learning on univariate, multivariate, and covariate-informed forecasting tasks (Section 5.2). Next, we examine Chronos-2's performance on tasks from the energy and retail domains, where covariates are often important for accurate forecasting (Section 5.3). Finally, we report results for ablated variants of Chronos-2 (Section 5.4), including a smaller model, a version trained only on synthetic data, and the model prior to long-context post-training.\n\n# 5.1 Benchmark Results\n\n<table><tr><td>Model</td><td>Avg. Win Rate (%)</td><td>Skill Score (%)</td><td>Median runtime (s)</td><td>Leakage (%)</td><td>#Failures</td></tr><tr><td>Chronos-2</td><td>90.7</td><td>47.3</td><td>3.6</td><td>0</td><td>0</td></tr><tr><td>TiRex</td><td>80.8</td><td>42.6</td><td>1.4</td><td>1</td><td>0</td></tr><tr><td>TimesFM-2.5</td><td>75.9</td><td>42.3</td><td>16.9</td><td>8</td><td>0</td></tr><tr><td>Toto-1.0</td><td>66.6</td><td>40.7</td><td>90.7</td><td>8</td><td>0</td></tr><tr><td>COSMIC</td><td>65.6</td><td>39.0</td><td>34.4</td><td>0</td><td>0</td></tr><tr><td>Moirai-2.0</td><td>61.1</td><td>39.3</td><td>2.5</td><td>28</td><td>0</td></tr><tr><td>Chronos-Bolt</td><td>60.3</td><td>38.9</td><td>1.0</td><td>0</td><td>0</td></tr><tr><td>TabPFN-TS</td><td>59.3</td><td>39.6</td><td>305.5</td><td>0</td><td>2</td></tr><tr><td>Sundial</td><td>41.0</td><td>33.4</td><td>35.6</td><td>1</td><td>0</td></tr><tr><td>Stat. Ensemble</td><td>40.4</td><td>20.2</td><td>690.6</td><td>0</td><td>11</td></tr><tr><td>AutoARIMA</td><td>35.2</td><td>20.6</td><td>186.8</td><td>0</td><td>10</td></tr><tr><td>AutoETS</td><td>29.1</td><td>-26.8</td><td>17.0</td><td>0</td><td>3</td></tr><tr><td>AutoTheta</td><td>21.8</td><td>5.5</td><td>9.3</td><td>0</td><td>0</td></tr><tr><td>SeasonalNaive</td><td>14.5</td><td>0.0</td><td>2.3</td><td>0</td><td>0</td></tr><tr><td>Naive</td><td>7.8</td><td>-45.4</td><td>2.2</td><td>0</td><td>0</td></tr></table>\n\nTable 3: fev-bench results. The average win rate and skill score are computed with respect to the scaled quantile loss (SQL) metric. Higher values are better for both. Chronos-2 outperforms all existing pretrained models by a substantial margin on this benchmark that includes univariate, multivariate, and covariate-informed forecasting tasks. Baseline results and the imputation strategy for handling data leakage in certain tasks are both taken from Shchur et al. (2025). Results for additional forecasting metrics are provided in Tables 7 to 9 (Appendix).\n\nWe evaluated the base Chronos-2 model with 120M parameters on three comprehensive forecasting benchmarks: fev-bench (Shchur et al., 2025), GIFT-Eval (Aksu et al., 2024), and Chronos Benchmark II (Ansari et al., 2024). To contextualize its performance, we compared it against state-of-the-art time series foundation models that achieved the strongest results on these benchmarks. These include TiRex (Auer et al., 2025b), TimesFM-2.5 (Das et al., 2024b), Toto-1.0 (Cohen et al., 2025), Moirai-2.0 (Woo et al., 2024), TabPFN-TS (Hoo et al., 2025), COSMIC (Auer et al., 2025a), Sundial (Liu et al., 2025), and Chronos-Bolt (Ansari et al., 2024), the latest publicly released version of Chronos. As additional baselines, we also included AutoARIMA, AutoETS, AutoTheta, and their ensemble (Petropoulos & Svetunkov, 2020), representing well-established methods from the statistical forecasting literature (Hyndman & Athanasopoulos, 2018). We compare Chronos-2 only with the aforementioned models and exclude task-specific deep learning models from our evaluation, as prior studies (Aksu et al., 2024; Ansari et al., 2024) - which include\n\nGIFT-Eval and Chronos Benchmark II, two of the three benchmarks considered in our work - have shown that pretrained models perform comparably to or better than task-specific models on average.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/88650bf31a50ebfe6269808ea1431ab54e6201de460487162064a081a2d22745.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/f73f416089ccb328f08f36b85afbbd076fac788ef3335106515833cf643b20d3.jpg)  \n(b)  \nFigure 2: The pairwise win rates (a) and skill scores (b) of the top-4 pretrained models on fev-bench with  $95\\%$  confidence intervals (CIs) obtained through bootstrapping. Chronos-2 outperforms the next best models (TiRex and TimesFM) by a statistically significant margin on both metrics. The complete plot and results for other forecasting metrics can be found in Figures 12 to 19 (Appendix).\n\nFollowing Shchur et al. (2025), we report both average win rates  $(W)$  and skill scores  $(S)$  for all models. These metrics are mathematically equivalent to the average rank  $(R)$  and geometric mean relative error  $(G)$  metrics used in prior work (Ansari et al., 2024; Aksu et al., 2024). Specifically,  $R = 1 + (1 - \\frac{W}{100})(N - 1)$  and  $G = 1 - \\frac{S}{100}$ , where  $N$  is the number of evaluated models. However, win rates and skill scores provide more interpretable summaries. The win rate measures the proportion of pairwise comparisons in which a model outperforms other models, while the skill score reflects the average percentage improvement over a baseline — in our case, the Seasonal Naive model. For a detailed discussion, we refer the reader to Shchur et al. (2025).\n\nfev-bench. This benchmark consists of 100 forecasting tasks and offers the most comprehensive coverage of diverse real-world scenarios, including tasks with covariates. None of these datasets or tasks were seen by Chronos-2 during training. Table 3 reports results on fev-bench with respect to the scaled quantile loss (SQL) metric which evaluates the probabilistic forecasting performance. Chronos-2 outperforms existing time series foundation models by a significant margin, both in win rate and skill score. fev-bench also provides tooling to answer questions like: \"Does Model A outperform Model B in a statistically significant way?\" These pairwise comparisons with  $95\\%$  confidence intervals (CIs), shown in Figure 2, further confirm that Chronos-2 surpasses the next best models (TiRex and TimesFM-2.5) by a statistically significant margin. Specifically, the CIs of the pairwise win rates and skill scores of Chronos-2 against any baseline do not include  $50\\%$  and  $0\\%$ , respectively.\n\n<table><tr><td>Model</td><td>Avg. Win Rate (%)</td><td>Skill Score (%)</td></tr><tr><td>Chronos-2</td><td>81.9</td><td>51.4</td></tr><tr><td>TimesFM-2.5</td><td>77.5</td><td>51.0</td></tr><tr><td>TiRex</td><td>76.5</td><td>50.2</td></tr><tr><td>Toto-1.0</td><td>67.4</td><td>48.6</td></tr><tr><td>Moirai-2.0</td><td>64.4</td><td>48.4</td></tr><tr><td>COSMIC</td><td>56.4</td><td>44.5</td></tr><tr><td>Chronos-Bolt</td><td>53.8</td><td>42.6</td></tr><tr><td>TabPFN-TS</td><td>53.5</td><td>43.1</td></tr><tr><td>Sundial</td><td>49.1</td><td>44.1</td></tr><tr><td>AutoARIMA</td><td>21.8</td><td>8.8</td></tr><tr><td>Seasonal Naive</td><td>16.6</td><td>0.0</td></tr><tr><td>AutoTheta</td><td>16.0</td><td>-24.4</td></tr><tr><td>AutoETS</td><td>15.2</td><td>-648.9</td></tr></table>\n\n(a)  \n\n<table><tr><td>Model</td><td>Avg. Win Rate (%)</td><td>Skill Score (%)</td></tr><tr><td>Chronos-2</td><td>83.8</td><td>30.2</td></tr><tr><td>TimesFM-2.5</td><td>77.7</td><td>29.5</td></tr><tr><td>TiRex</td><td>71.9</td><td>27.6</td></tr><tr><td>Moirai-2.0</td><td>64.3</td><td>27.2</td></tr><tr><td>Toto-1.0</td><td>61.3</td><td>25.2</td></tr><tr><td>Chronos-Bolt</td><td>58.4</td><td>19.2</td></tr><tr><td>Sundial</td><td>53.4</td><td>25.0</td></tr><tr><td>COSMIC</td><td>51.9</td><td>20.8</td></tr><tr><td>TabPFN-TS</td><td>45.4</td><td>16.6</td></tr><tr><td>AutoARIMA</td><td>24.4</td><td>-7.4</td></tr><tr><td>AutoETS</td><td>19.5</td><td>-21.2</td></tr><tr><td>Seasonal Naive</td><td>19.4</td><td>0.0</td></tr><tr><td>AutoTheta</td><td>18.5</td><td>-9.0</td></tr></table>\n\nGIFT-Eval. The GIFT-Eval benchmark comprises 97 tasks derived from 55 datasets, with a particular emphasis on high-frequency time series and long-horizon forecasting. The results in Table 4 show that Chronos-2 surpasses the previously leading models (TiRex and TimesFM-2.5) in win rate and skill score under both the weighted quantile loss (WQL) and mean absolute scaled error (MASE) metrics. When constructing the pretraining corpus for Chronos-2, we carefully ensured that it did not overlap with the test portions of any GIFT-Eval task at any sampling frequency. Nonetheless, the corpus does include partial overlap with the training portions of some GIFT-Eval datasets. For strictly zero-shot results, we refer the reader to Section 5.4, where we evaluate a variant of Chronos-2 trained exclusively on synthetic data.\n\n(b)  \nTable 4: GIFT-Eval results. The average win rate and skill score with respect to the (a) weighted quantile loss (WQL) and (b) mean absolute scaled error (MASE) metrics. Higher values are better for both. Chronos-2 outperforms previous best models, TimesFM-2.5 and TiRex. Baseline results have been taken from the GIFT-Eval leaderboard (Aksu et al., 2024).  \n\n<table><tr><td>Model</td><td>Avg. Win Rate (%)</td><td>Skill Score (%)</td></tr><tr><td>Chronos-2</td><td>79.8</td><td>46.6</td></tr><tr><td>TiRex</td><td>70.4</td><td>41.7</td></tr><tr><td>TimesFM-2.5</td><td>70.0</td><td>42.4</td></tr><tr><td>Toto-1.0</td><td>60.9</td><td>41.9</td></tr><tr><td>Moirai-2.0</td><td>56.0</td><td>40.9</td></tr><tr><td>Chronos-Bolt</td><td>49.4</td><td>39.3</td></tr><tr><td>TabPFN-TS</td><td>46.3</td><td>32.6</td></tr><tr><td>COSMIC</td><td>42.8</td><td>36.7</td></tr><tr><td>Sundial</td><td>14.4</td><td>24.1</td></tr><tr><td>Seasonal Naive</td><td>10.1</td><td>0.0</td></tr></table>\n\n(a)  \n\n<table><tr><td>Model</td><td>Avg. Win Rate (%)</td><td>Skill Score (%)</td></tr><tr><td>Chronos-2</td><td>81.5</td><td>26.5</td></tr><tr><td>TimesFM-2.5</td><td>71.6</td><td>23.3</td></tr><tr><td>TiRex</td><td>67.1</td><td>22.2</td></tr><tr><td>Toto-1.0</td><td>58.0</td><td>22.3</td></tr><tr><td>Moirai-2.0</td><td>53.5</td><td>19.8</td></tr><tr><td>Chronos-Bolt</td><td>50.6</td><td>20.4</td></tr><tr><td>COSMIC</td><td>42.0</td><td>18.1</td></tr><tr><td>TabPFN-TS</td><td>40.1</td><td>10.5</td></tr><tr><td>Sundial</td><td>21.8</td><td>9.5</td></tr><tr><td>Seasonal Naive</td><td>13.8</td><td>0.0</td></tr></table>\n\n(b)\n\nTable 5: Chronos Benchmark II results. The average win rate and skill score with respect to the (a) weighted quantile loss (WQL) and (b) mean absolute scaled error (MASE) metrics. Higher values are better for both. Chronos-2 achieves the best results across all metrics.\n\nChronos Benchmark II. Originally proposed in Ansari et al. (2024) to evaluate the first Chronos models, this benchmark comprises 27 tasks, the majority of which involve short histories (fewer than 300 time steps on average). None of these datasets were included in the training corpus of Chronos-2. On this benchmark, Chronos-2 consistently outperforms existing models in terms of the win rate and skill score under both probabilistic (WQL) and point (MASE) forecasting metrics, as shown in Table 5.\n\nTaken together, these results show that Chronos-2 not only outperforms all competing models across the three benchmarks but also substantially improves over Chronos-Bolt, its predecessor, highlighting the impact of the architectural and training improvements in Chronos-2.\n\n# 5.2 Improvements with In-context Learning\n\nThe results in Section 5.1 correspond to Chronos-2 with in-context learning (ICL) enabled, specifically in the full cross learning mode described in Section 3.4. In this section, we disentangle the gains from ICL compared to univariate inference. To this end, we split fev-bench into three subsets: the univariate subset with 32 tasks involving a single target time series without covariates, the multivariate subset with 26 tasks containing multiple targets but no covariates, and the covariates subset with 42 tasks that include at least one past-only or known covariate. We compare Chronos-2 with ICL to its univariate inference mode on these three subsets, as well as on GIFT-Eval and Chronos Benchmark II. In the univariate mode, each time series in the batch is forecast independently, and covariates, if present, are ignored.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/2e773da5c31a72bea222b2502dd24979efd2c8fee97a64d067e579605f1a1018.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/a567efcc46692f6e2ada5cfc642550dd965fc8feb3382acd6e7a359cba10857b.jpg)  \n(b)  \nFigure 3: Chronos-2's probabilistic forecasting results in univariate mode and the corresponding improvements from in-context learning (ICL), shown as stacked bars on (a) the univariate subset of fev-bench, (b) GIFT-Eval, and (c) Chronos Benchmark II. For these univariate benchmarks, ICL enables cross-learning, allowing the model to share information across items within a batch and thereby generate more accurate forecasts than univariate inference alone. Results for point forecasting metrics are available in Figure 9 (Appendix).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/11e51e7c43b325c145e65a312c6a89116188c124f28b4a10290e5d4f22b7318c.jpg)  \n(c)\n\nUnivariate Tasks. ICL provides improvements in skill score on univariate tasks, as shown in Figure 3. The effect is especially strong on Chronos Benchmark II (Figure 3 (b)), which contains many tasks with short contexts. This demonstrates that Chronos-2 can leverage information from related time series to improve predictions when ICL is enabled, particularly when limited time series history is available.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/09c2b7e99914493874bf18283f51d914c5765223a3b1c57bd79922cd662da259.jpg)  \n(a)  \nFigure 4: Chronos-2's probabilistic forecasting results in univariate mode and the corresponding gains from in-context learning (ICL), shown as stacked bars on the multivariate and covariates subsets of fev-bench. On multivariate tasks, ICL provides only modest improvements, though Chronos-2 in univariate mode already surpasses the multivariate Toto-1.0 model. On the covariates subset, however, ICL delivers the largest gains, demonstrating Chronos-2's ability to effectively use covariates. Besides Chronos-2, only TabPFN-TS and COSMIC support covariates, and Chronos-2 outperforms all baselines (including TabPFN-TS and COSMIC) by a wide margin. Results for point forecasting metrics are available in Figures 10a and 10b (Appendix).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/9bb6aed09e207ccf9c4ca7fdb6aaa5cdc261529f3abb522ecccb44f299496285.jpg)  \n(b)\n\nMultivariate Tasks. On the multivariate subset of fev-bench, ICL yields only modest gains over univariate inference (Figure 4a (a)). Interestingly, in univariate mode, Chronos-2 even outperforms Toto-1.0, a model which natively supports multivariate forecasting. This suggests that while these tasks involve multiple variates with potentially shared dynamics, the benefits of explicit multivariate modeling can be limited. One possible intuition comes from Takens's Embedding Theorem (Takens, 2006), which implies that the dynamics of a system can often\n\nbe reconstructed from delayed observations of a single variable. In practice, this means that with sufficiently long histories, a strong univariate model may capture much of the same structure as a multivariate model. Similar empirical findings have been reported elsewhere; for example, Nie et al. (2023) observed that univariate (\"channel-independent\") models often perform on par with multivariate (\"channel-dependent\") models, albeit on a different benchmark.\n\nTasks with Covariates. The largest gains with ICL are observed on tasks with covariates (Figure 4a (b)). Here, the performance margin clearly demonstrates that Chronos-2 with ICL can effectively exploit covariates to improve predictions compared to univariate inference, which ignores them. Chronos-2 outperforms baselines by a large margin on this subset. Unsurprisingly, the second spot is taken by TabPFN-TS, another model which supports (known) covariates. These results underscore both the strength of Chronos-2 and the limitations of existing pretrained models, most of which lack covariate support - a capability of immense practical importance.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/88d538525b1d263acd4b89e0e1c20c87792e76cae543e1653e4121b305365d31.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/c6e25a455aca647ce246f3411b022c6b849dd407044dfa7ee5f27f181c48714a.jpg)  \n(b)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/da235d1c438895cc6f387a080e2848847eda1d749521659f6227b386a4e61018.jpg)  \nFigure 5: Comparison of Chronos-2 against baselines on tasks which include dynamic covariates from the energy and retail domains. Chronos-2 outperforms all baselines by a wide margin, including TabPFN-TS and TiRex, the strongest baselines on the covariates subset of fev-bench (Figure 4b). For retail, we consider the domain-appropriate WQL metric. Results for point forecasting metrics are available in Figures 11a and 11b (Appendix).  \nFigure 6: Forecasts generated by Chronos-2 in univariate mode (top), i.e., without covariates, and with in-context learning (second from top) on the energy price forecasting task. The dashed vertical gray line indicates the forecast start date and the shaded region represents  $80\\%$  prediction interval around the median forecast. With ICL, Chronos-2 leverages Ampirion Load and Solar + Wind covariates to produce a more accurate prediction.\n\n# 5.3 Domain Case Studies\n\nWe conducted further analysis on tasks from the energy and retail domains, where covariates often provide crucial information for accurate forecasting. For both domains, we selected all tasks with dynamic covariates from fev-bench resulting in 16 and 17 tasks for energy and retail, respectively (see Tables 10 and 11 in the Appendix for details). As baselines, we used TabPFN-TS and TiRex, the two strongest models on the covariates subset of fev-bench, as shown in Figure 4b. The results in Figures 5a and 5b demonstrate that Chronos-2 consistently outperforms these baselines by a wide margin. Incorporating covariates provides a substantial boost in performance for Chronos-2, reinforcing their critical role in real-world forecasting tasks. Consistent with Figure 4b, the second-best results are achieved by TabPFN-TS, another model capable of leveraging covariates.\n\nTo illustrate how Chronos-2 with ICL uses covariates, we compared forecasts produced in univariate mode versus with ICL. We selected one task from each domain where ICL delivers the largest gains.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/33968ac694b316464dd8451eb238ace849ae2bc8f14c4b317baeba2a47f2bb16.jpg)  \nFigure 7: Forecasts generated by Chronos-2 in univariate mode (top), i.e., without covariates, and with in-context learning (second from top) on the Rossmann sales forecasting task. The dashed vertical gray line indicates the forecast start date and the shaded region represents  $80\\%$  prediction interval around the median forecast. With ICL, Chronos-2 produces a substantially more accurate forecast by capturing the influence of promotion and holiday covariates on future sales.\n\nFigure 6 shows forecasts on the energy price forecasting task for Germany (EPF-DE), where the goal is to predict the hourly energy price for the next day using historical prices, day-ahead forecasts of the load and renewable (solar and wind) energy generation. In the univariate mode, Chronos-2 makes reasonable but imprecise predictions. However, with ICL, Chronos-2 effectively uses the covariates, producing significantly more accurate predictions.\n\nThe retail task in Figure 7 involves predicting next quarter's weekly store sales of Rossmann, a European drug store chain, using historical sales and covariates: historical customer footfall plus known covariates indicating store operation, promotion periods, and holidays. Chronos-2's univariate forecast is nearly flat with high uncertainty. In contrast, the ICL forecast leverages covariates - particularly promotion and holiday information - to capture the true sales dynamics over the forecast horizon.\n\n# 5.4 Ablation Studies\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/2208425ce75c5af379f254c4720711ade9d2ea5a256b2e80653eaba0b11a6f6b.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/f2822077b6a5eb7ba4d2ddb822d3317ee719c6d609af3932a0551826a1de7f30.jpg)  \n(b)  \nFigure 8: Comparison of the main Chronos-2 model (120M parameters) with (a) a smaller 28M-parameter model, (b) a model trained exclusively on synthetic data, and (c) the main model prior to long-context post-training.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b285eab9-f7ff-4a52-997e-c24dbbbc7ded/2ff95a1cd210899deb6c68871256b56123a2cdec6b4752043c2e70cb91bad783.jpg)  \n(c)\n\nIn this section, we present additional experiments and ablations that disentangle the impact of different design choices. We investigate the performance of Chronos-2 across different parameter counts, evaluate models trained exclusively on synthetic data, and demonstrate the importance of post-training on long-context scenarios.\n\nModel Size. We trained a small model with 28M parameters to understand the impact of model size on forecasting performance. As shown in Figure 8a, the small model delivers strong performance despite its reduced size. On GIFT-Eval, for instance, its skill score lags the base model by as little as  $1\\%$  points, while offering nearly  $2\\times$  faster inference. This makes it particularly suitable for low-resource environments, such as CPU-only settings, or applications where inference speed is prioritized over maximum forecast accuracy.\n\nSynthetic Data Only. Synthetic time series data has played a pivotal role in advancing pretrained forecasting models (Ansari et al., 2024; Das et al., 2024b). TabPFN-TS (Hoo et al., 2025) demonstrated that strong performance is achievable even when training relies exclusively on synthetic data. To examine the limits of this approach, we trained a version of Chronos-2 using only synthetic data. On Chronos Benchmark II and GIFT-Eval, this model (Chronos-2-Synth) performs only slightly below the version with real data in its pretraining corpus (Figure 8b). It also delivers strong results on fev-bench, though with a larger performance gap. These results underscore the importance of synthetic data, suggesting that with further research, real data may not even be required for effective pretraining.\n\nLong context Post-training. As described in Section 3.3, Chronos-2 is initially trained with a context length of 2,048 time steps and then post-trained with an extended context of 8,192 steps. Figure 8c compares the base model (denoted Chronos-2-2K) with the post-trained variant. Extending the context length yields gains, particularly on the GIFT-Eval benchmark, which contains many high-frequency datasets with long seasonal periods.\n",
  "hyperparameter": "Model size: 120M parameters (base model), 28M parameters (small model); Hidden dimension (D_model): not explicitly specified; Context length: 2048 (initial training), 8192 (post-training); Patch length (P): not explicitly specified; Number of quantiles: 21 (Q = {0.01, 0.05, 0.1, ..., 0.9, 0.95, 0.99}); Maximum context length (C): 8192; Training stages: 2-stage training (first stage with context 2048 and low output patches, second stage with context 8192 and increased output patches); Position embeddings: RoPE (Rotary Position Embeddings); Architecture: T5-encoder-based transformer with alternating time attention and group attention layers; Quantile head: residual block producing forecasts for H×D×|Q| dimensions"
}