{
  "id": "SegRNN_2023",
  "paper_title": "SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting",
  "alias": "SegRNN",
  "year": 2023,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "SegRNN addresses the challenge of RNNs in long-term time series forecasting by reducing recurrent iterations through two key strategies: (1) segment-wise iterations in encoding that partition sequences into segments and process them as units rather than individual time points, reducing iterations from L to L/w, and (2) Parallel Multi-step Forecasting (PMF) in decoding that generates all future predictions simultaneously using positional embeddings instead of recursive autoregressive decoding, reducing iterations from H/w to 1. This design enables RNNs to effectively model long-term dependencies while achieving significant improvements in both accuracy and inference speed.",
  "introduction": "# 1 Introduction\n\nTime series forecasting involves using past observed time series data to predict future unknown time series. It finds applications in various fields, such as energy and smart grids, traffic flow control, server energy optimization (Aslam et al. 2021). Recurrent Neural Networks (RNNs) (Lipton, Berkowitz, and Elkan 2015), as a deep learning architecture, have been extensively adopted for conventional time series forecasting due to their effectiveness in capturing sequential dependencies (Lim and Zohren 2021).\n\nIn recent years, there has been a shift in focus towards predicting longer horizons, known as Long-term Time Series Forecasting (LTSF) (Zhou et al. 2021). Figure 1 (a) illustrates the concept of LTSF, where the objective is to provide richer semantic information by predicting a longer future sequence, thus offering more practical guidance. However, extending the forecast horizon poses significant challenges: (i) Forecasting further into the future leads to increased uncertainty, resulting in decreased forecast accuracy. (ii) Longer forecast horizons require models to consider a more extensive historical context for accurate predictions, significantly increasing the complexity of modeling.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/f4813450c4021246392cb925aec4713e69de34a5fb253feef77d03521f4f24cf.jpg)  \na) Long-term Time Series Forecasting (LTSF)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/19e57b5de1963e400f6cee26bd3eacb6b477d52b65dd92d3fe0a377131baa86c.jpg)  \nb) Forecast Error\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/d01a4ece50fed92fbef4e79c64a01a3ce8f2d19604d3622a7610c37e26cd64c0.jpg)  \nc) Inference Time  \nFigure 1: Challenges faced by vanilla RNN and its variants in LTSF. Data is obtained from the ETTh1 dataset.\n\nWhile RNNs have exhibited remarkable performance in conventional time-series tasks, they have gradually lost prominence in the LTSF domain. Figure 1 (b) and (c) illustrate the limitations of RNNs (either vanilla RNN or its variants: Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and Gated Recurrent Unit (GRU) (Cho et al. 2014)) in LTSF: (i) The model's forecast error rapidly increases as the forecast horizon expands, especially when the horizon reaches 64, at which point it becomes comparable to random forecasting. (ii) The models' inference time rapidly increases with the length of horizon. These observations widely support the belief that RNNs are no longer suitable for LTSF tasks that involve modeling long-term dependencies (Zhou et al. 2021, 2022). Consequently, there is currently no prominent RNN-based solution in the LTSF field.\n\nIn contrast, Transformers (Vaswani et al. 2017), an advanced neural network architecture designed to model long-term dependencies in sequences, have achieved remarkable success in natural language processing, computer vision, and other fields. Consequently, there has been a surge in Transformer-based LTSF solutions, breaking several state-of-the-art (SOTA) records (Zhou et al. 2021, 2022; Nie et al.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/45995ac320b5b09aa0f9e6c516a315173783bdc57a31da4b3199dbf7fda80c6f.jpg)  \na) RMF (Traditional)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/ac84f1d7be17a652314d3100030fdd3e3b0683557a6f059b48b3dc55fe38da7f.jpg)  \nb) PMF (Ours)  \nFigure 2: Comparison of Recurrent Multi-step Forecasting (RMF) and Parallel Multi-step Forecasting (PMF). The positional embedding  $pe_i$  in PMF serves as a replacement for the sequential information of the recurrent structure.\n\n2023). The undeniable efficacy of Transformers notwithstanding, their intricate design and substantial computational requirements have constrained their accessibility. Recently, there has been significant debate regarding whether the self-attention mechanism in Transformers is suitable for modeling time-series tasks (Zeng et al. 2023; Li et al. 2023). This leads us to contemplate: Are RNNs, which are conceptually simple and structurally well-suited for modeling time-series data, truly unsuitable for LTSF tasks?\n\nThe answer might be No. It is well-known that RNNs suffer from the vanishing/exploding gradient problem (Bengio, Simard, and Frasconi 1994), which limits the length of sequences they can effectively model. We can hypothesize that the current RNNs' failure in LTSF stems from excessively long look-back and forecast horizons, which result in prohibitively high recurrent iteration counts. To address this, we propose a straightforward yet powerful strategy: Minimizing the count of recurrent iterations in RNNs while striving to retain sequential information. Specifically, we introduce SegRNN, which is designed with two key components:\n\n1. The incorporation of segment technology in RNNs, replacing point-wise iterations with segment-wise iterations, significantly reducing the number of recurrent iterations.  \n2. The introduction of the Parallel Multi-step Forecasting (PMF) strategy, further reducing the number of recurrent iterations. The comparison between PMF and the traditional Recurrent Multi-step Forecasting (RMF) is illustrated in Figure 2.\n\nExperimental results on popular LTSF benchmarks demonstrate that these two key design components significantly improve RNNs' performance in the LTSF domain. Reducing the number of recurrent iterations not only greatly improves the prediction accuracy of RNNs but also significantly enhances their inference speed. In most scenarios, SegRNN outperforms SOTA Transformer-based models, featuring a runtime and memory reduction of over  $78\\%$ . We provide strong evidence that RNNs still possess powerful capabilities in the LTSF domain.\n\nIn summary, our contributions are as follows:\n\n- We propose SegRNN, which utilizes time-series segment technique to replace point-wise iterations with segment-wise iterations in LTSF.  \n- We further introduce the PMF technique to enhance the inference speed and performance of RNNs.  \n- The proposed SegRNN outperforms the current SOTA methods while significantly reducing runtime and memory usage.  \n- The success of SegRNN demonstrates substantial improvements over existing RNN methods, highlighting the strong potential of RNNs in the LTSF domain.",
  "method": "# 4 Model Architecture\n\nThe recurrent iterative nature of RNNs poses challenges for effective convergence when modeling extensive long sequences. SegRNN aims to reduce the number of recurrent iterations to facilitate its convergence. Specifically, SegRNN employs the following strategies:\n\n1. In the encoding phase, it replaces the original time pointwise iterations with sequence segment-wise iterations, effectively reducing the number of iterations from  $L$  to  $L / w$ .  \n2. In the decoding phase, it utilizes the PMF strategy to further reduce the number of iterations from  $H / w$  to 1.\n\nThe substantial reduction in the number of recurrent iterations in RNN not only leads to a remarkable performance improvement but also results in a significant increase in inference speed. The model architecture of SegRNN is illustrated in Figure 4.\n\n# 4.1 Encoding\n\nSegment partition and projection. Given a sequence channel  $X^{(i)} \\in \\mathbb{R}^L$ , it can be partitioned into segments  $X_w^{(i)} \\in \\mathbb{R}^{n \\times w}$ , where  $w$  represents the window length of each segment, and  $n = \\frac{L}{w}$  denotes the number of segments. These segments,  $X_w^{(i)} \\in \\mathbb{R}^{n \\times w}$ , are then transformed to  $X_d^{(i)} \\in \\mathbb{R}^{n \\times d}$  through a learnable linear projection  $W_{prj} \\in \\mathbb{R}^{w \\times d}$  followed by a ReLU activation, where  $d$  represents the dimensionality of the hidden state of the GRU.\n\nRecursive encoding. Subsequently, the transformed  $X_{d}^{(i)}$  is fed into the GRU for recurrent iterations to capture temporal features. Specifically, for  $x_{t} \\in \\mathbb{R}^{d}$  in  $X_{d}^{(i)}$ , the entire process within the GRU cell can be formulated as:\n\n$$\n\\begin{array}{l} z _ {t} = \\sigma \\left(W _ {z} \\cdot [ h _ {t - 1}, x _ {t} ]\\right), \\\\ r _ {t} = \\sigma \\left(W _ {r} \\cdot \\left[ h _ {t - 1}, x _ {t} \\right]\\right), \\\\ \\tilde {h} _ {t} = \\tanh  \\left(W \\cdot \\left[ r _ {t} \\times h _ {t - 1}, x _ {t} \\right]\\right), \\\\ \\end{array}\n$$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/ca875917f1b0b6520bc6207a0ac3918781c2831ee657cbf2bd217d9f1f4b93a2.jpg)  \nFigure 4: The model architecture of SegRNN.\n\n$$\nh _ {t} = \\left(1 - z _ {t}\\right) \\times h _ {t - 1} + z _ {t} \\times \\tilde {h} _ {t}.\n$$\n\nAfter  $n$  recurrent iterations, the hidden feature  $h_n$  obtained from the last step already encapsulates all the temporal features of the original sequence  $X^{(i)}$ . This hidden feature will be passed to the Decoding part for the subsequent steps of inference and prediction.\n\n# 4.2 Decoding\n\nRMF is a straightforward method for accomplishing multi-step prediction. In RMF, single-step predictions are made to obtain  $\\bar{y}_t$ . The predicted  $\\bar{y}_t$  is then used as input for the subsequent prediction of  $\\bar{y}_{t + 1}$ , and this process continues until the complete set of prediction results is obtained. By incorporating the segment technique from the Encoding phase with RMF, the number of iterations required for a prediction horizon of  $H$  is reduced to  $H / w$ .\n\nHowever, despite the reduction in the number of recurrent iterations, recursive decoding has its limitations. These include: (i) the accumulation of errors resulting from recursive predictions, and (ii) the sequential nature of recursion, which hampers parallel computation within training examples and restricts improvements in inference speed (Vaswani et al. 2017). To address these limitations, we propose a novel prediction strategy called Parallel Multi-step Forecasting (PMF), as described below.\n\nPositional embeddings. During the decoding phase, the sequential order between segments is lost due to the break in the recurrent recursion. To address this,  $m$  corresponding positional embeddings, denoted as  $PE^{(i)} \\in \\mathbb{R}^{m \\times d}$ , are generated to identify the positions of the segments. Here,  $m = \\frac{H}{w}$  represents the number of windows obtained by partitioning the prediction horizon into segments. Each positional embedding  $pe^{(i)} \\in \\mathbb{R}^d$  is constructed by concatenating the relative position encoding  $rp \\in \\mathbb{R}^{\\frac{d}{2}}$  and the channel position encoding  $cp \\in \\mathbb{R}^{\\frac{d}{2}}$ .\n\nFigure 5 illustrates the positional embeddings for the target sequence  $Y^{(i)}$ . The relative position encoding indi\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/139be39e2a4e76d62454dae0a30de2ea37f608e3246efcfa07122fdc4fe1d246.jpg)  \nFigure 5: Positional embeddings  $PE^{(i)}$  for the target sequence  $Y^{(i)}$ .\n\ncates the position of each segment that needs to be predicted within the complete sequence  $Y^{(i)}$ . Meanwhile, the channel position encoding represents the channel position  $i \\in 1,2,\\dots,C$  of the current sequence  $Y^{(i)}$  within the multi-channel sequence  $Y \\in \\mathbb{R}^{H \\times C}$  (Shao et al. 2022). The inclusion of the channel position encoding partially compensates for the limitations of the CI strategy in capturing relationships between variables, thereby enhancing the model's performance.\n\nParallel decoding. In the Decoding phase, the same GRU cell used in the Encoding phase is shared. Specifically, the final state  $h_n$  obtained from the Encoding phase is duplicated  $m$  times and combined with the  $m$  positional embeddings from  $PE^{(i)}$ . These pairs are then simultaneously processed in parallel by the GRU cell. This parallel processing generates  $m$  output vectors, each with a length of  $d$ , denoted as  $\\bar{Y}_d^{(i)} \\in \\mathbb{R}^{m \\times d}$ . It is important to note that this approach differs from the previous recursive processing in RMF, as the computation of each vector is independent of the previous time step's result. As a result, intra-sample parallel computation is achieved, leading to improved inference speed. Additionally, prediction errors do not accumulate with the number of iterations, resulting in enhanced prediction accuracy.\n\nPrediction and sequence recovery. The  $\\bar{Y}_d^{(i)}$  undergoes a Dropout layer, randomly dropping out a certain proportion of values for regularization purposes. Subsequently, it is transformed into  $\\bar{Y}_w^{(i)}\\in \\mathbb{R}^{m\\times w}$  using a learnable linear prediction layer  $W_{prd}\\in \\mathbb{R}^{d\\times w}$ . Furthermore,  $\\bar{Y}_w^{(i)}$  is reshaped\n\ninto  $\\bar{Y}^{(i)}\\in \\mathbb{R}^H$  , representing the final prediction result.\n\n# 4.3 Normalization and Evaluation\n\nInstance normalization. Time series data often experience distribution shift issues, and employing simple sample normalization strategies can help alleviate this problem. In this paper, we utilize a simple sample normalization strategy (Zeng et al. 2023) that involves subtracting the last value of the sequence from the input before encoding and subsequently adding back the value after decoding, formulated as:\n\n$$\nx _ {1: L} ^ {(i)} = x _ {1: L} ^ {(i)} - x _ {L} ^ {(i)},\n$$\n\n$$\n\\bar {y} _ {1: L} ^ {(i)} = \\bar {y} _ {1: L} ^ {(i)} + x _ {L} ^ {(i)}.\n$$\n\nLoss function. The mean absolute error (MAE) is employed as the loss function for our model, defined as:\n\n$$\n\\mathcal {L} (Y, \\bar {Y}) = \\frac {1}{H C} \\sum_ {t = 1} ^ {H} \\sum_ {i = 1} ^ {C} | \\bar {y} _ {t} ^ {(i)} - y _ {t} ^ {(i)} |.\n$$\n",
  "experiments": "# 5 Experiments\n\nIn this section, we present the main experimental results on popular LTSF benchmarks. Furthermore, we conduct an ablation study to analyze the impact of the segment-wise iterations and the PMF strategies on the effectiveness of RNN in LTSF. We also investigate the influence of some important parameters on SegRNN. All experiments in this section are implemented in PyTorch and executed on two NVIDIA T4 GPUs, each equipped with 16GB of memory.\n\n# 5.1 Experimental Setup\n\nAn overview of the experimental setup is presented here, and for further details, please refer to the Appendix.\n\nDatasets. The performance evaluation of SegRNN is carried out on 7 popular datasets in the LTSF domain, comprising 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), as well as Traffic, Electricity, and Weather datasets. The statistics of these datasets are presented in Table 1.\n\n<table><tr><td>Datasets</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>Electricity</td><td>Traffic</td><td>Weather</td></tr><tr><td>Channels</td><td>7</td><td>7</td><td>7</td><td>7</td><td>321</td><td>862</td><td>21</td></tr><tr><td>Frequency</td><td>1 hour</td><td>1 hour</td><td>15 mins</td><td>15 mins</td><td>1 hour</td><td>1 hour</td><td>10 mins</td></tr><tr><td>Timesteps</td><td>17,420</td><td>17,420</td><td>69,680</td><td>69,680</td><td>26,304</td><td>17,544</td><td>52,696</td></tr></table>\n\nTable 1: Summary of datasets for evaluation.\n\nBaselines and metrics. As baselines, we select SOTA and representative models in the LTSF domain, including the following categories: (i) Transformers: PatchTST (Nie et al. 2023), FEDformer (Zhou et al. 2022), Informer (Zhou et al. 2021); (ii) MLPs: TiDE (Das et al. 2023), Dlinear (Zeng et al. 2023); (iii) CNNs: MICN (Wang et al. 2023), TimesNet (Wu et al. 2023); (iv) RNNs: DeepAR (Salinas et al. 2020), GRU (Cho et al. 2014). It is worth mentioning that while DeepAR and GRU were not initially designed for LTSF, we include them as baselines due to the limited presence of prominent RNN solutions in the field. The proposed SegRNN aims to fill this gap.\n\nTwo commonly employed evaluation metrics, Mean Squared Error (MSE) and Mean Absolute Error (MAE), are used here to assess the performance of the LTSF models.\n\nModel configuration. The uniform configuration of SegRNN consists of a look-back of 720, a segment length of 48, a single GRU layer, a hidden size of 512, 30 training epochs, a learning rate decay of 0.8 after the initial 3 epochs, and early stopping with a patience of 10. The dropout rate, batch size, and learning rate vary based on the scale of the data.\n\n# 5.2 Main Result\n\nThe multivariate long-term time series forecasting results of SegRNN and other baselines are presented in Table 2. Remarkably, SegRNN achieved top-two positions in 50 out of 56 metrics across all scenarios, including 45 first-place rankings, signifying its significant superiority over other baselines, including the current SOTA transformer-based model, PatchTST. SegRNN demonstrated outstanding performance on the ETT and Weather datasets, nearly surpassing the SOTA performance across all metrics. In larger-scale datasets such as Electricity and Traffic, where the channel numbers exceed 300 and 800, respectively, SegRNN's performance experienced a slight decrease. This is likely due to the relatively smaller capacity of the SegRNN model, as it is built on a single GRU layer. However, even in these cases, SegRNN demonstrated competitive or superior performance compared to the competing models.\n\nRegarding the RNN-based methods GRU and DeepAR, SegRNN achieved MSE improvements of  $75\\%$  and  $80\\%$ , respectively. This provides strong evidence that SegRNN significantly enhances the performance of existing RNN methods in the LTSF domain. These results highlight the success of the SegRNN design and demonstrate that RNN methods still hold strong potential in the current LTSF domain.\n\n# 5.3 Ablation Studies\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/d56718ee26349328d81a7a4149f601b3a4bddc44d97bbb0fe7e573dd53158b20.jpg)  \nFigure 6: The forecast error (bar plot) and the inference time (line plot) of SegRNN with different segment length on ETTm1 dataset. Both look-back and horizon are 192.\n\nSegment-wise iterations vs. point-wise iterations. Figure 6 illustrates the performance differences between segment-wise and point-wise iterations. It is essential to note that the segment length directly determines the number of iterations, and when the segment length  $w = 1$ , segment-wise\n\n<table><tr><td colspan=\"2\">Categories</td><td colspan=\"6\">RNNs</td><td colspan=\"7\">Transformers</td><td colspan=\"5\">MLPs</td><td colspan=\"4\">CNNs</td></tr><tr><td colspan=\"2\">Models</td><td colspan=\"2\">SegRNN(ours)</td><td colspan=\"2\">DeepAR(2020)</td><td colspan=\"2\">GRU(2014)</td><td colspan=\"2\">PatchTST(2023)</td><td colspan=\"2\">FEDformer(2022)</td><td colspan=\"3\">Informer(2021)</td><td colspan=\"2\">TiDE(2023)</td><td colspan=\"2\">Dlinear(2023)</td><td colspan=\"2\">MICN(2023)</td><td colspan=\"2\">TimesNet(2023)</td><td></td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.341</td><td>0.376</td><td>1.128</td><td>0.81</td><td>1.126</td><td>0.831</td><td>0.37</td><td>0.4</td><td>0.376</td><td>0.415</td><td>0.941</td><td>0.769</td><td>0.375</td><td>0.398</td><td>0.375</td><td>0.399</td><td>0.421</td><td>0.431</td><td>0.384</td><td>0.402</td><td></td><td></td></tr><tr><td>192</td><td>0.385</td><td>0.402</td><td>1.077</td><td>0.776</td><td>1.133</td><td>0.79</td><td>0.413</td><td>0.429</td><td>0.423</td><td>0.446</td><td>1.007</td><td>0.786</td><td>0.412</td><td>0.422</td><td>0.405</td><td>0.416</td><td>0.474</td><td>0.487</td><td>0.436</td><td>0.429</td><td></td><td></td></tr><tr><td>336</td><td>0.401</td><td>0.417</td><td>1.043</td><td>0.766</td><td>1.053</td><td>0.774</td><td>0.422</td><td>0.44</td><td>0.444</td><td>0.462</td><td>1.038</td><td>0.784</td><td>0.435</td><td>0.433</td><td>0.439</td><td>0.443</td><td>0.569</td><td>0.551</td><td>0.491</td><td>0.469</td><td></td><td></td></tr><tr><td>720</td><td>0.434</td><td>0.447</td><td>1.075</td><td>0.795</td><td>1.077</td><td>0.795</td><td>0.447</td><td>0.468</td><td>0.469</td><td>0.492</td><td>1.144</td><td>0.857</td><td>0.454</td><td>0.465</td><td>0.472</td><td>0.49</td><td>0.77</td><td>0.672</td><td>0.521</td><td>0.5</td><td></td><td></td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.263</td><td>0.32</td><td>2.602</td><td>1.329</td><td>2.1</td><td>1.094</td><td>0.274</td><td>0.337</td><td>0.332</td><td>0.374</td><td>1.549</td><td>0.952</td><td>0.27</td><td>0.336</td><td>0.289</td><td>0.353</td><td>0.299</td><td>0.364</td><td>0.34</td><td>0.374</td><td></td><td></td></tr><tr><td>192</td><td>0.321</td><td>0.36</td><td>3.198</td><td>1.368</td><td>2.548</td><td>1.249</td><td>0.341</td><td>0.382</td><td>0.407</td><td>0.446</td><td>3.792</td><td>1.542</td><td>0.332</td><td>0.38</td><td>0.383</td><td>0.418</td><td>0.441</td><td>0.454</td><td>0.402</td><td>0.414</td><td></td><td></td></tr><tr><td>336</td><td>0.325</td><td>0.374</td><td>3.139</td><td>1.353</td><td>3.142</td><td>1.343</td><td>0.329</td><td>0.384</td><td>0.4</td><td>0.447</td><td>4.215</td><td>1.642</td><td>0.36</td><td>0.407</td><td>0.448</td><td>0.465</td><td>0.654</td><td>0.567</td><td>0.452</td><td>0.452</td><td></td><td></td></tr><tr><td>720</td><td>0.394</td><td>0.424</td><td>3.134</td><td>1.352</td><td>3.138</td><td>1.343</td><td>0.379</td><td>0.422</td><td>0.412</td><td>0.469</td><td>3.656</td><td>1.619</td><td>0.419</td><td>0.451</td><td>0.605</td><td>0.551</td><td>0.956</td><td>0.716</td><td>0.462</td><td>0.468</td><td></td><td></td></tr><tr><td rowspan=\"4\">ETThm1</td><td>96</td><td>0.282</td><td>0.335</td><td>1.283</td><td>0.857</td><td>0.706</td><td>0.587</td><td>0.293</td><td>0.346</td><td>0.326</td><td>0.39</td><td>0.626</td><td>0.56</td><td>0.306</td><td>0.349</td><td>0.299</td><td>0.343</td><td>0.316</td><td>0.362</td><td>0.338</td><td>0.375</td><td></td><td></td></tr><tr><td>192</td><td>0.319</td><td>0.36</td><td>1.181</td><td>0.839</td><td>0.946</td><td>0.738</td><td>0.333</td><td>0.37</td><td>0.365</td><td>0.415</td><td>0.725</td><td>0.619</td><td>0.335</td><td>0.366</td><td>0.335</td><td>0.365</td><td>0.363</td><td>0.39</td><td>0.374</td><td>0.387</td><td></td><td></td></tr><tr><td>336</td><td>0.349</td><td>0.383</td><td>1.249</td><td>0.846</td><td>1.048</td><td>0.767</td><td>0.369</td><td>0.392</td><td>0.392</td><td>0.425</td><td>1.005</td><td>0.741</td><td>0.364</td><td>0.384</td><td>0.369</td><td>0.386</td><td>0.408</td><td>0.426</td><td>0.41</td><td>0.411</td><td></td><td></td></tr><tr><td>720</td><td>0.41</td><td>0.418</td><td>1.075</td><td>0.77</td><td>1.076</td><td>0.765</td><td>0.416</td><td>0.42</td><td>0.446</td><td>0.458</td><td>1.133</td><td>0.845</td><td>0.413</td><td>0.413</td><td>0.425</td><td>0.421</td><td>0.481</td><td>0.476</td><td>0.478</td><td>0.45</td><td></td><td></td></tr><tr><td rowspan=\"4\">ETThm2</td><td>96</td><td>0.158</td><td>0.241</td><td>3.418</td><td>1.581</td><td>0.487</td><td>0.518</td><td>0.166</td><td>0.256</td><td>0.18</td><td>0.271</td><td>0.355</td><td>0.462</td><td>0.161</td><td>0.251</td><td>0.167</td><td>0.26</td><td>0.179</td><td>0.275</td><td>0.187</td><td>0.267</td><td></td><td></td></tr><tr><td>192</td><td>0.215</td><td>0.283</td><td>3.894</td><td>1.635</td><td>1.725</td><td>0.984</td><td>0.223</td><td>0.296</td><td>0.252</td><td>0.318</td><td>0.595</td><td>0.586</td><td>0.215</td><td>0.289</td><td>0.224</td><td>0.303</td><td>0.307</td><td>0.376</td><td>0.249</td><td>0.309</td><td></td><td></td></tr><tr><td>336</td><td>0.263</td><td>0.317</td><td>3.247</td><td>1.377</td><td>2.091</td><td>1.082</td><td>0.274</td><td>0.329</td><td>0.324</td><td>0.364</td><td>1.27</td><td>0.871</td><td>0.267</td><td>0.326</td><td>0.281</td><td>0.342</td><td>0.325</td><td>0.388</td><td>0.321</td><td>0.351</td><td></td><td></td></tr><tr><td>720</td><td>0.33</td><td>0.366</td><td>2.588</td><td>1.313</td><td>2.301</td><td>1.144</td><td>0.362</td><td>0.385</td><td>0.41</td><td>0.42</td><td>3.001</td><td>1.267</td><td>0.352</td><td>0.383</td><td>0.397</td><td>0.421</td><td>0.502</td><td>0.49</td><td>0.408</td><td>0.403</td><td></td><td></td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.128</td><td>0.219</td><td>0.575</td><td>0.535</td><td>0.404</td><td>0.434</td><td>0.129</td><td>0.222</td><td>0.186</td><td>0.302</td><td>0.304</td><td>0.393</td><td>0.132</td><td>0.229</td><td>0.14</td><td>0.237</td><td>0.164</td><td>0.269</td><td>0.168</td><td>0.272</td><td></td><td></td></tr><tr><td>192</td><td>0.148</td><td>0.239</td><td>1.061</td><td>0.809</td><td>0.844</td><td>0.717</td><td>0.147</td><td>0.24</td><td>0.197</td><td>0.311</td><td>0.327</td><td>0.417</td><td>0.147</td><td>0.243</td><td>0.153</td><td>0.249</td><td>0.177</td><td>0.285</td><td>0.184</td><td>0.289</td><td></td><td></td></tr><tr><td>336</td><td>0.166</td><td>0.258</td><td>1.04</td><td>0.795</td><td>1.015</td><td>0.775</td><td>0.163</td><td>0.259</td><td>0.213</td><td>0.328</td><td>0.333</td><td>0.422</td><td>0.161</td><td>0.261</td><td>0.169</td><td>0.267</td><td>0.193</td><td>0.304</td><td>0.198</td><td>0.3</td><td></td><td></td></tr><tr><td>720</td><td>0.201</td><td>0.29</td><td>1.048</td><td>0.804</td><td>1.041</td><td>0.783</td><td>0.197</td><td>0.29</td><td>0.233</td><td>0.344</td><td>0.351</td><td>0.427</td><td>0.196</td><td>0.294</td><td>0.203</td><td>0.301</td><td>0.212</td><td>0.321</td><td>0.22</td><td>0.32</td><td></td><td></td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.543</td><td>0.235</td><td>1.377</td><td>0.717</td><td>1.349</td><td>0.715</td><td>0.36</td><td>0.249</td><td>0.576</td><td>0.359</td><td>0.733</td><td>0.41</td><td>0.336</td><td>0.253</td><td>0.41</td><td>0.282</td><td>0.519</td><td>0.309</td><td>0.593</td><td>0.321</td><td></td><td></td></tr><tr><td>192</td><td>0.567</td><td>0.246</td><td>1.442</td><td>0.75</td><td>1.42</td><td>0.75</td><td>0.379</td><td>0.256</td><td>0.61</td><td>0.38</td><td>0.777</td><td>0.435</td><td>0.346</td><td>0.257</td><td>0.423</td><td>0.287</td><td>0.537</td><td>0.315</td><td>0.617</td><td>0.336</td><td></td><td></td></tr><tr><td>336</td><td>0.602</td><td>0.256</td><td>1.489</td><td>0.778</td><td>1.47</td><td>0.776</td><td>0.392</td><td>0.264</td><td>0.608</td><td>0.375</td><td>0.776</td><td>0.434</td><td>0.355</td><td>0.26</td><td>0.436</td><td>0.296</td><td>0.534</td><td>0.313</td><td>0.629</td><td>0.336</td><td></td><td></td></tr><tr><td>720</td><td>0.671</td><td>0.281</td><td>1.526</td><td>0.793</td><td>1.524</td><td>0.794</td><td>0.432</td><td>0.286</td><td>0.621</td><td>0.375</td><td>0.827</td><td>0.466</td><td>0.386</td><td>0.273</td><td>0.466</td><td>0.315</td><td>0.577</td><td>0.325</td><td>0.64</td><td>0.35</td><td></td><td></td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.142</td><td>0.181</td><td>0.278</td><td>0.301</td><td>0.183</td><td>0.231</td><td>0.149</td><td>0.198</td><td>0.238</td><td>0.314</td><td>0.354</td><td>0.405</td><td>0.166</td><td>0.222</td><td>0.176</td><td>0.237</td><td>0.161</td><td>0.229</td><td>0.172</td><td>0.22</td><td></td><td></td></tr><tr><td>192</td><td>0.186</td><td>0.227</td><td>0.376</td><td>0.369</td><td>0.299</td><td>0.336</td><td>0.194</td><td>0.241</td><td>0.275</td><td>0.329</td><td>0.419</td><td>0.434</td><td>0.209</td><td>0.263</td><td>0.22</td><td>0.282</td><td>0.22</td><td>0.281</td><td>0.219</td><td>0.261</td><td></td><td></td></tr><tr><td>336</td><td>0.237</td><td>0.269</td><td>0.568</td><td>0.527</td><td>0.376</td><td>0.374</td><td>0.245</td><td>0.282</td><td>0.339</td><td>0.377</td><td>0.583</td><td>0.543</td><td>0.254</td><td>0.301</td><td>0.265</td><td>0.319</td><td>0.278</td><td>0.331</td><td>0.28</td><td>0.306</td><td></td><td></td></tr><tr><td>720</td><td>0.31</td><td>0.32</td><td>0.571</td><td>0.533</td><td>0.459</td><td>0.433</td><td>0.314</td><td>0.334</td><td>0.389</td><td>0.409</td><td>0.916</td><td>0.705</td><td>0.313</td><td>0.34</td><td>0.323</td><td>0.362</td><td>0.311</td><td>0.356</td><td>0.365</td><td>0.359</td><td></td><td></td></tr><tr><td colspan=\"2\">Count</td><td colspan=\"21\">50</td><td></td></tr></table>\n\nTable 2: Multivariate long-term time series forecasting results. The forecast horizon  $H \\in \\{96,192,336,720\\}$  is set for all datasets. The reported SegRNN results are averaged over 5 runs. The best results are highlighted in **bold** and the second best are **underlined**. The Count row counts the total number of times each method obtained the best or second results.\n\niterations degenerates into point-wise iterations. The following observations can be made:\n\n1. As the segment length increases (i.e., the number of iterations decreases), the forecast error consistently decreases. However, when the segment length equals the look-back length, the model degenerates into a multilayer perceptron, leading to an increase in prediction error.  \n2. With the continuous increase of the segment length, the inference time steadily decreases.\n\nThese findings indicate that a relatively large yet appropriate segment length (i.e., minimizing the number of iterations) significantly improves the performance of the RNN method in LTSF.\n\nPMF vs. RMF. Figure 7 illustrates the performance disparities between RMF and PMF. Concerning the forecast error, PMF significantly outperforms RMF for various forecast horizons, exhibiting a more stable distribution. The advantage of PMF becomes increasingly evident as the forecast horizon increases.\n\nAs for inference time, when the forecast horizon  $H < 192$ , PMF is slightly slower than RMF. This is because the intra-sample parallel computation advantage of PMF is not fully manifested with fewer iterations; instead, it incurs additional overhead due to data replication in memory. However, when the forecast horizon  $H > 192$ , the larger number\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/b00f58ac2d013794161768164a4a454938baa13cf21dd70294d7634b0130404a.jpg)  \nFigure 7: The forecast error distribution (violin plot) and the inference time (line plot) of SegRNN with different decoding strategy (i.e. RMF and PMF) on ETTm1 dataset. The look-back is 720.\n\nof iterations allows PMF to leverage its intra-sample parallel computation advantage, leading to improved hardware utilization and accelerated computation.\n\nIn conclusion, these results demonstrate that PMF significantly enhances the performance of RNNs in LTSF tasks compared to RMF.\n\n# 5.4 Model Analysis\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/ea0fe3e6b0cd4fc34958c6892cf5409cae0f6ef5a2cdf39ce99230ffec3cea33.jpg)  \nFigure 8: The forecast error of SegRNN with different lookback length on ETTm1 dataset. The horizon is 192.\n\nImpact of look-back length. A powerful LTSF model typically performs better with a longer look-back context as it contains more trend and periodic information. The ability to leverage a longer look-back context directly reflects the model's capability to capture long-term dependencies. However, longer look-back contexts also increase the modeling complexity, and many Transformer-based models encounter difficulties when dealing with long look-back scenarios (i.e.,  $L > 96$ ) (Zeng et al. 2023).\n\nRegarding SegRNN, as illustrated in Figure 8, the forecast error consistently diminishes as the look-back length increases. Notably, SegRNN also exhibits commendable performance with relatively shorter look-backs, showcasing its robustness across diverse look-back lengths. This finding suggests that SegRNN not only excels in modeling long-term dependencies but also maintains its robustness in handling varying look-back requirements.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/1d4cb32a-614a-4702-8af7-54cd16866b62/23f6b0ee6abc2c68dd60c9a380ca3c01ae2fad1d6a838bd084c46793e339931f.jpg)  \nFigure 9: The forecast error of SegRNN with different RNN variants on ETTm1 dataset. The look-back is 720 and the horizon is 192.\n\nImpact of RNN variants. The proposed SegRNN exhibits versatility across various RNN types. Nevertheless, in practical implementations, it consistently achieves lower forecast errors and demonstrates greater stability when employed with the GRU variant, as depicted in Figure 9. This advantage is attributed to the integration of gating mechanisms in the GRU, which enhances its capacity to model long-term\n\ndependencies while retaining a simpler structure in comparison to LSTM. Consequently, SegRNN defaults to the adoption of GRU. However, should more potent RNN variants emerge in the future, SegRNN holds the potential to further bolster its robustness.\n\n<table><tr><td>Metric</td><td>Datasets</td><td>PatchTST</td><td>SegRNN</td><td>Imp.</td></tr><tr><td rowspan=\"3\">Training Time (s/epoch)</td><td>ETTm1</td><td>94.9</td><td>29.7</td><td>69%</td></tr><tr><td>Weather</td><td>273.4</td><td>50.3</td><td>82%</td></tr><tr><td>Electricity</td><td>1.97k</td><td>313.8</td><td>84%</td></tr><tr><td rowspan=\"3\">MACs (MMac)</td><td>ETTm1</td><td>265.9</td><td>213.4</td><td>20%</td></tr><tr><td>Weather</td><td>797.8</td><td>640.2</td><td>20%</td></tr><tr><td>Electricity</td><td>12.2k</td><td>9.79k</td><td>20%</td></tr><tr><td rowspan=\"3\">Parameters (M)</td><td>ETTm1</td><td>2.62</td><td>1.63</td><td>38%</td></tr><tr><td>Weather</td><td>2.62</td><td>1.63</td><td>38%</td></tr><tr><td>Electricity</td><td>2.62</td><td>1.71</td><td>35%</td></tr><tr><td rowspan=\"3\">Max Memory (MB)</td><td>ETTm1</td><td>289</td><td>77</td><td>73%</td></tr><tr><td>Weather</td><td>780</td><td>124</td><td>84%</td></tr><tr><td>Electricity</td><td>11.3k</td><td>1.23k</td><td>89%</td></tr></table>\n\nTable 3: Comparison of performance metrics between SegRNN and PatchTST on ETTm1 dataset with a single NVIDIA T4 GPU. The look-back is 720 and the horizon is 192.\n\nSegRNN vs. PatchTST. We conducted a comparison between SegRNN and the latest SOTA Transformer-based model, PatchTST, to showcase the runtime performance advantage of SegRNN. Table 3 reveals that compared to PatchTST, SegRNN demonstrates a reduction of over  $78\\%$  in average training time and a decrease of over  $82\\%$  in average maximum GPU memory consumption. This significant improvement in efficiency is particularly beneficial for practical model training and deployment.\n",
  "hyperparameter": "Look-back length: 720; Segment length (w): 48; Number of GRU layers: 1; Hidden size (d): 512; Training epochs: 30; Learning rate decay: 0.8 (applied after initial 3 epochs); Early stopping patience: 10; Dropout rate: varies by dataset scale; Batch size: varies by dataset scale; Learning rate: varies by dataset scale"
}