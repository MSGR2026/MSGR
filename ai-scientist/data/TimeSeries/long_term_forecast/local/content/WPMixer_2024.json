{
  "id": "WPMixer_2024",
  "paper_title": "WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting",
  "alias": "WPMixer",
  "year": 2024,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "WPMixer introduces efficient multi-resolution mixing for long-term time series forecasting through multi-level wavelet decomposition. The core innovation is processing each wavelet coefficient series (approximation and detail coefficients at different frequency levels) through separate resolution branches with patch mixing and embedding mixing modules, avoiding repeated time-frequency domain conversions. This approach extracts features from both time and frequency domains simultaneously while maintaining computational efficiency through single decomposition and reconstruction operations at the model's input and output.",
  "introduction": "# Introduction\n\nTypically, time series data volume accumulates to vast amounts in various applications due to recording observations and events over long time horizons. The study of predicting time series data has been essential because of its extensive use in various domains such as finance, weather forecasting, and energy consumption prediction.\n\nWhile research in time-series forecasting, for a long time, relied on traditional statistical methods such as ARIMA (Ariyo, Adewumi, and Ayo 2014), HMM (Hassan and Nath 2005), and SSM (Durbin and Koopman 2012), with the increasing availability of large datasets and high computational power, deep learning methods gained prevalence due to their superior performance in complex tasks. Specifically, RNN and CNN-based models like DeepAR (Salinas et al. 2020), and SCINet (Liu et al. 2022a), as well as transformer-based time series forecasting models, have become popular over time.\n\nTransformer models for time series forecasting, such as Informer (Zhou et al. 2021), Autoformer (Wu et al. 2021),\n\nFedformer (Zhou et al. 2022b), and Crossformer (Zhang and Yan 2023) have become popular thanks to their improved capability of learning long-term dependencies. However, recently, questions have arisen about the performance of the transformer variants in time series forecasting. The study (Zeng et al. 2023) demonstrated that a simple linear model can outperform or perform similarly with the state-of-the-art transformers on the popular benchmark datasets for time series forecasting.\n\nRecently, MLP-based models have outperformed transformer variants in this domain. TimeMixer (Wang et al. 2024) and TSMixer (Chen et al. 2023) showed excellent prospects in multivariate time series forecasting. TSMixer, an MLP-mixer-based variant, mixes data in the time and channel domain but is computationally expensive for long-term forecasting due to a longer look-back window. TimeMixer, which achieves the state-of-the-art results on most benchmark datasets, decomposes a multi-scaled time series into seasonal and trend series using the moving average method and then employs the mixing among the multi-scaled data. However, due to complex seasonality patterns, decomposing a signal into seasonal and trend data is inadequate, and mixing among the multi-scaled data can cause information loss (Hyndman et al. 2011). Additionally, real-world time series data can have abrupt spikes and dips, which is difficult to explain using multi-scaled moving average-based decomposition techniques. Furthermore, capturing the information only in the time domain is not sufficient due to the complex nature of the time series data. SWformer, a variant of Sepformer (Fan et al. 2022), extracts information in the time and frequency domain utilizing wavelet transform-based decomposition. However, a multi-level wavelet transform is required to achieve its full potential.\n\nTo address these challenges, we propose a novel MLP-mixer-based model, called Wavelet Patch Mixer (WPMixer). What sets our model apart is its ability to capture intricate information in both the time and frequency domains, achieved through the use of multi-level wavelet decomposition. WPMixer decomposes the time series into multiple approximation and detail coefficient series using the multilevel wavelet transform. Distinct resolution branches handle each coefficient series, preventing information loss from mixing among multiple coefficient series. We utilize patch\n\ning to capture local information and reduce the computational cost. We also employ patch mixer followed by embedding mixer to capture global information. Our contributions can be summarized as follows:\n\n- We propose a novel model consisting of three core parts. Multi-level wavelet decomposition enables utilizing time and frequency domain properties due to spikes and dips, which cannot be captured by moving average-based decomposition methods in the time domain. Patching and mixing, on the other hand, capture local and global information, respectively.  \n- We analyze each decomposed series using a distinct resolution branch. This approach ensures that information from each resolution is maintained separately, thereby minimizing potential information loss.  \n- We enhance the performance of the patch mixer by applying an embedding mixer after each patch mixer.  \n- Our model, WPMixer, efficiently achieves state-of-the-art performance in long-term forecasting on several benchmark datasets.\n",
  "method": "# Proposed Method\n\nGiven a multivariate time series  $\\mathbf{X}_L = \\{\\pmb{x}_{t-L+1}, \\dots, \\pmb{x}_{t-1}, \\pmb{x}_t\\}$ , with a look-back window  $L$  at time step  $t$ , we aim to forecast the subsequent  $T$  data points  $\\mathbf{X}_T = \\{\\pmb{x}_{t+1}, \\pmb{x}_{t+2}, \\dots, \\pmb{x}_{t+T}\\}$ , where  $\\pmb{x}_t \\in \\mathbb{R}^{1 \\times C}$  denotes a multivariate data point at time  $t$ ,  $C$  is the number of the variates, and  $T$  is the prediction length.\n\n# Model Architecture\n\nThe architecture of the proposed model is illustrated in Figure 1. Our approach begins with decomposing the normalized time series data into approximation and detail coefficient series through multi-level wavelet decomposition. This multi-level decomposition facilitates feature extraction from the time series data at various resolutions, where each resolution represents a distinct frequency level. As we progress to higher decomposition levels, the frequency range of the approximation coefficients becomes narrower. At the same time, we get multiple detail coefficient series that represent detailed information at various frequency levels. However, higher-level coefficient series may not always yield relevant information for forecasting tasks. Additionally, different wavelets offer varying trade-offs between time and frequency localization, making the selection of an optimal de\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/813bda97-3755-4fd8-9555-2cafe9caf460/4c5b933fc7495b4a5bf1cd4a7c16ec9c84e6d52898d29d50de6d6419acc8fa99.jpg)  \nFigure 1: WPMixer with  $m$  levels of wavelet decomposition.  $X_{A_i}$  and  $X_{D_i}$  are the approximation and detail coefficient series corresponding to the input time series  $X_L$ .  $Y_{A_i}$  and  $Y_{D_i}$  are the predicted approximation and detail coefficient series corresponding to the predicted time series  $X_T$ . To simplify notation,  $X_{W_i}$  denotes either  $X_{A_i}$  or  $X_{D_i}$ . Code is available at https://github.com/Secure-and-Intelligent-Systems-Lab\n\ncomposition level and wavelet type a crucial aspect of the optimization process.\n\nOur model processes each wavelet coefficient series through a distinct resolution branch, which prevents the intermixing of information across different frequency scales. Each resolution branch comprises an instance normalization module, a patch and embedding module, several mixer modules, a head module, and an instance denormalization module. The patch and embedding module transforms the normalized wavelet coefficient series into a series of patches. The patch mixer modules then aggregate the local information contained within these patches into a global information context. In the mixer module, which is a fusion of a patch mixer and an embedding mixer, the embedding mixer captures the global information in a higher dimensional space. The head module subsequently forecasts the wavelet coefficient series, providing the information needed for predicting the time series. A denormalization layer is employed to reintegrate the stationary information into the predicted wavelet coefficient series. Finally, the multi-level wavelet reconstruction module reconstructs the predicted time series by utilizing the predicted approximation and detail wavelet coefficient series. In the following subsections, we describe\n\nthe key modules of our model.\n\nInstance Normalization: One of the main challenges for time series forecasting is to deal with the time-varying mean and variation. To overcome this challenge, Reversible Instance Normalization (RevIN) with learnable affine transform has been proposed in (Kim et al. 2021). We initially employ RevIN normalization and denormalization directly in the time series data before decomposition and after reconstruction, respectively. We also employ RevIN normalization and denormalization in the wavelet coefficient series. The positions of the RevIN normalization and denormalization layers are shown in Fig 1.\n\nDecomposition: We utilize the multi-level discrete wavelet transform to decompose the time series data. This transformation involves an iterative decomposition process utilizing high-pass and low-pass filters to extract wavelet coefficients at multiple levels (Mallat 1989). The coefficients of the filters depend on the type of wavelet. The output of the high-pass filter refers to detailed information, called detail coefficients, whereas the output of the low-pass filter refers to low-frequency information, called approximation coefficients. At each level, the approximation coefficients\n\nfrom the preceding level is split into new approximation and detail coefficients, allowing for a deeper data analysis. We adapt the implementation of the multi-level discrete wavelet transform from (Cotter 2019) to work with PyTorch mixed precision analysis.\n\nThe decomposition module disintegrates the normalized time series  $\\underline{\\mathbf{X}}_L^T\\in \\mathbb{R}^{C\\times L}$  into approximation and detail coefficient series:\n\n$$\n\\left[ \\boldsymbol {X} _ {A _ {m}}, \\boldsymbol {X} _ {D _ {m}}, \\boldsymbol {X} _ {D _ {m - 1}}, \\dots , \\boldsymbol {X} _ {D _ {1}} \\right] = D e c o m p \\left(\\underline {{\\boldsymbol {X}}} _ {L} ^ {T}, \\psi , m\\right). \\tag {1}\n$$\n\nIn this context,  $m$  denotes the decomposition level,  $\\psi$  denotes the wavelet type,  $X_{A_i} \\in \\mathbb{R}^{C \\times L_i}$  and  $X_{D_i} \\in \\mathbb{R}^{C \\times L_i}$  represent the approximation and detail coefficient series at the  $i$ -th level of decomposition, respectively. Here,  $L_i$  indicates the number of wavelet coefficients in the coefficient series at the  $i$ -th decomposition level. To avoid information redundancy, we retain only the approximation coefficient series from the final level  $m$  while discarding those from levels 1 through  $(m - 1)$ , as they are further decomposed into new approximation and detail coefficient series. However, we include the detail coefficient series from all levels in our analysis. In our experiments, we optimized the wavelet type by considering the Daubechies, Symlets, Coiflets, and Biorthogonal wavelet families.\n\nEach series of wavelet coefficient is processed through a distinct resolution branch within the model, encompassing a RevIN normalization module, a patching and embedding module, multiple mixer modules, a head module, and a RevIN denormalization module. The total number of multivariate coefficient series or resolution branches in the model is given by  $(m + 1)$  due to the  $m$  detail and 1 approximation coefficient series.\n\nTo simplify the notation, we will refer both the approximation coefficient series  $\\mathbf{X}_{A_i}$  and the detail coefficient series  $\\mathbf{X}_{D_i}$  with  $\\mathbf{X}_{W_i} \\in \\mathbb{R}^{C \\times L_i}$  in the following steps.\n\nPatching and Embedding module: To capture the local information efficiently, we adopt patching and embedding techniques from (Nie et al. 2023). Each normalized univariate wavelet coefficient series  $\\underline{\\mathbf{X}}_{W_i}^{(j)}\\in \\mathbb{R}^{1\\times L_i}$ $j = 1,\\dots ,C$  is divided into overlapping patches of length  $P$  . The nonoverlapping portion is denoted as stride  $S$  . Before patching,  $\\underline{\\mathbf{X}}_{W_i}^{(j)}$  is padded with  $S$  number of repeated last values of the sequence  $\\underline{\\mathbf{X}}_{W_i}^{(j)}$  . So, each univariate wavelet coefficient series  $\\underline{\\mathbf{X}}_{W_i}^{(j)}$  is converted to  $\\mathbf{X}_{P_i}^{(j)}\\in \\mathbb{R}^{1\\times N_i\\times P}$  , where  $N_{i} = \\frac{(L_{i} - P)}{S} +2$  is the number of patches.\n\nThe multivariate output of the patching block,\n\n$$\n\\boldsymbol {X} _ {P _ {i}} = \\operatorname {P a t c h} \\left(\\underline {{\\boldsymbol {X}}} _ {W _ {i}}\\right) \\in \\mathbb {R} ^ {C \\times N _ {i} \\times P} \\tag {2}\n$$\n\nis passed through a linear embedding layer to encode into  $d$  dimensions. This embedding layer is shareable across all variates of  $X_{P_i}$ , i.e.,\n\n$$\n\\boldsymbol {X} _ {d _ {i}} = \\operatorname {E m b e d d i n g} \\left(\\boldsymbol {X} _ {P _ {i}}\\right) \\in \\mathbb {R} ^ {C \\times N _ {i} \\times d}. \\tag {3}\n$$\n\nMixer module: The Mixer module consists of two primary components, the Patch Mixer and a subsequent Embedding Mixer. The Patch Mixer functions similarly to the token-mixing MLP as outlined in (Tolstikhin et al. 2021).\n\nBefore intermixing information across the patch dimension, 2D-Batch normalization followed by dimension permutation operation is applied on  $X_{d_i} \\in \\mathbb{R}^{C \\times N_i \\times d}$ . Within the patch mixer, two linear layers are employed alongside the GELU activation function. The first layer expands the dimensionality with factor  $t_f$  while the subsequent layer restores it to its original dimension. The operations in the patch mixer can be summarized as,\n\n$$\n\\boldsymbol {X} _ {d _ {i}} ^ {\\prime} = \\mathcal {P} \\left(B N \\left(\\boldsymbol {X} _ {d _ {i}}\\right)\\right) \\in \\mathbb {R} ^ {d \\times C \\times N _ {i}} \\tag {4}\n$$\n\n$$\n\\boldsymbol {X} _ {d _ {i}} ^ {\\prime \\prime} = \\mathcal {L} _ {2} \\left(\\mathcal {G} \\left(\\mathcal {L} _ {1} \\left(\\boldsymbol {X} _ {d _ {i}} ^ {\\prime}\\right)\\right)\\right) \\in \\mathbb {R} ^ {d \\times C \\times N _ {i}} \\tag {5}\n$$\n\nwhere  $BN(.)$  represents the 2D-Batch normalization,  $\\mathcal{P}(.)$  represents dimension permutation,  $\\mathcal{G}(.)$  represents GELU activation,  $\\mathcal{L}_1:\\mathbb{R}^{d\\times C\\times N_i}\\to \\mathbb{R}^{d\\times C\\times N_i.t_f}$  represents layer-1 and  $\\mathcal{L}_2:\\mathbb{R}^{d\\times C\\times N_i.t_f}\\to \\mathbb{R}^{d\\times C\\times N_i}$  represents layer-2 in the patch mixer MLP.\n\nPrior to processing in the Embedding Mixer,  $\\mathbf{X}_{d_i}^{\\prime \\prime}$  is subjected to dimension permutation and 2D-Batch normalization. In the Embedding Mixer,  $\\overline{\\mathbf{X}_{d_i}^{\\prime\\prime}}$  traverses two linear layers incorporating GELU activation similarly to the Patch Mixer. However, the initial layer increases the embedding dimensionality  $d$  with factor  $d_{f}$ , while the subsequent layer restores it to its original dimension. Different than Patch Mixer, a residual connection is also included with the MLP. The operations in the Embedding Mixer can be summarized as,\n\n$$\n\\underline {{\\boldsymbol {X}}} _ {d _ {i}} ^ {\\prime \\prime} = B N \\left(\\mathcal {P} \\left(\\boldsymbol {X} _ {d _ {i}} ^ {\\prime \\prime}\\right)\\right) \\in \\mathbb {R} ^ {C \\times N _ {i} \\times d} \\tag {6}\n$$\n\n$$\n\\boldsymbol {X} _ {d _ {i 2}} = \\underline {{\\boldsymbol {X}}} _ {d _ {i}} ^ {\\prime \\prime} + \\mathcal {L} _ {2} ^ {\\prime} (\\mathcal {G} \\left(\\mathcal {L} _ {1} ^ {\\prime} \\left(\\underline {{\\boldsymbol {X}}} _ {d _ {i}} ^ {\\prime \\prime}\\right)\\right)) \\in \\mathbb {R} ^ {C \\times N _ {i} \\times d}, \\tag {7}\n$$\n\nwhere  $\\mathcal{L}_1^{\\prime}:\\mathbb{R}^{C\\times N_i\\times d}\\to \\mathbb{R}^{C\\times N_i\\times d.d_f}$  represents layer- 1 and  $\\mathcal{L}_2^{\\prime}:\\mathbb{R}^{C\\times N_i\\times d.d_f}\\to \\mathbb{R}^{C\\times N_i\\times d}$  represents layer-2. Two sequential Mixer modules are employed in our model, where the second Mixer module has a residual connection followed by 2D-Batch normalization.\n\nHead module: The Head module comprises a flatten and a linear projection layers. The flatten layer flattens the last two dimensions of the input  $Y_{d_i} \\in \\mathbb{R}^{C \\times N_i \\times d}$ .\n\n$$\n\\mathbf {Y} _ {f _ {i}} = \\operatorname {F l a t t e n} \\left(\\mathbf {Y} _ {d _ {i}}\\right) \\in \\mathbb {R} ^ {C \\times N _ {i}. d}, \\tag {8}\n$$\n\nand the linear layer transforms  $\\mathbf{Y}_{f_i}$  to\n\n$$\n\\mathbf {Y} _ {h _ {i}} = \\operatorname {L i n e a r} \\left(\\mathbf {Y} _ {f _ {i}}\\right) \\in \\mathbb {R} ^ {C \\times T _ {i}}, \\tag {9}\n$$\n\nwhere  $T_{i}$  is the prediction length of the wavelet coefficient series. To determine the value of  $T_{i}$ , an auxiliary time series of equivalent length to the predicted series  $X_{T}$  undergoes the decomposition module while initializing the model.  $T_{i}$  is set as the length of the auxiliary decomposed wavelet coefficient series.\n\nReconstruction: The Reconstruction module can be described as,\n\n$$\n\\boldsymbol {Y} = \\text {R e c o n s t r u c t i o n} _ {\\psi} \\left(\\boldsymbol {Y} _ {A _ {m}}, \\boldsymbol {Y} _ {D _ {m}}, \\boldsymbol {Y} _ {D _ {m - 1}}, \\dots , \\boldsymbol {Y} _ {D _ {1}}\\right); \\tag {10}\n$$\n\nwhere  $\\mathbf{Y}_{A_i} \\in \\mathbb{R}^{C \\times T_i}$  and  $\\mathbf{Y}_{D_i} \\in \\mathbb{R}^{C \\times T_i}$  are the predicted approximation and detail wavelet coefficient series.  $\\mathbf{Y} \\in \\mathbb{R}^{C \\times T}$  is the reconstructed time series, which is transformed by instance denormalization to obtain the final prediction  $\\mathbf{X}_T \\in \\mathbb{R}^{T \\times C}$ .\n\nTraining: SmoothL1Loss is employed to train our model with the default threshold value. Separate dropout values are used for the Embedding and Mixer modules. We used Optuna (Akiba et al. 2019) with the default setting of Tree-structured Parzen Estimator (TPE) for optimizing the hyperparameters. The optimized hyperparameter values are shown in Table 7 in Supplementary.\n\n# Differences with the existing models\n\nTimeMixer leverages moving average-based seasonal and trend decomposition of multi-scaled time series data and integrates data across multiple scales. WPMixer, on the other hand, employs multi-level wavelet transform-based decomposition, processing each coefficient series individually through a resolution branch. TSMixer incorporates time mixing and channel mixing while WPMixer employs patch mixing followed by embedding mixing. Both TimeMixer and TSMixer handle solely time-domain data, whereas WPMixer extracts features from both the time and frequency domains. Fedformer enhances time series using multi-wavelet transform, frequently converting data between the time and frequency domains. SWformer uses single-level wavelet transform for time series decomposition. However, WPMixer utilizes multi-level wavelet transform, which is computationally less expensive than multi-wavelet transform and more effective than single-level wavelet transform (Zhang and Zhang 2019). Additionally, WPMixer performs time series decomposition at the beginning of the model and reconstructs the series from the predicted coefficient series at the end, avoiding multiple conversions between the time and frequency domains.\n",
  "experiments": "# Experiments\n\nWe extensively evaluate the long-term forecasting performance of WPMixer on 7 popular datasets: ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, and Traffic. The specifications of datasets are given in Table 1.\n\n<table><tr><td>Dataset</td><td>Variates</td><td>Dataset Size</td><td>Freq.</td></tr><tr><td>ETTh1, ETTh2</td><td>7</td><td>(8545, 2881, 2881)</td><td>Hourly</td></tr><tr><td>ETTm1, ETTm2</td><td>7</td><td>(34465, 11521, 11521)</td><td>15 min</td></tr><tr><td>Weather</td><td>21</td><td>(36792, 5271, 10540)</td><td>10 min</td></tr><tr><td>Electricity</td><td>321</td><td>(18317, 2633, 5261)</td><td>Hourly</td></tr><tr><td>Traffic</td><td>862</td><td>(12185, 1757, 3509)</td><td>Hourly</td></tr></table>\n\nTable 1: Specifications of the datasets. Dataset size refers to the training, validation, and testing dataset sizes.\n\nBaselines: We compare WPMixer with seven recent time series forecasting methods, namely TimeMixer (Wang et al. 2024), TSMixer (Chen et al. 2023), TimesNet (Wu et al. 2023), FiLM (Zhou et al. 2022a), DLinear (Zeng et al. 2023), PatchTST (Nie et al. 2023), and Crossformer (Zhang and Yan 2023). TimeMixer and TSMixer, which can be considered as the state-of-the-art models based on their performances on the benchmark datasets, derive their architectures from the MLP-Mixer model while PatchTST and Crossformer utilize transformer architectures.\n\nSetup: Following the practice in Informer, Autoformer, PatchTST, TSMixer, and TimeMixer, all datasets were normalized to a zero mean and unit standard deviation. The normalized datasets served as the basis for ground truth in our evaluations. In long-term forecasting, the lengths of predictions were set at 96, 192, 336, and 720, in alignment with prior studies. During the training phase, SmoothL1Loss was employed, whereas Mean Squared Error (MSE) and Mean Absolute Error (MAE) were utilized for evaluation purposes. Experiments with the ETT and Weather datasets were performed on a single NVIDIA GeForce RTX 4090 GPU while the experiments with the Electricity and Traffic datasets were carried out using two NVIDIA A100 GPUs.\n\n# Multivariate Long-Term Forecasting Results\n\nIn long-term multivariate time series forecasting, existing studies employed distinct look-back window lengths to optimize performance. For a comprehensive comparison, we present our results under two experimental setups following TimeMixer (Wang et al. 2024).\n\nIn the first setup, we calibrated the look-back window length alongside other hyperparameters to enhance forecasting accuracy. We determined the optimal look-back window lengths for each dataset, exploring values of 96, 192, 336, 512, 1024, and 1200. The comprehensive results under this setup are presented in Table 2 while the optimized hyperparameter values and run information are given in Table 7 in Supplementary. The performance of other models listed in Table 2 are also their optimized results (Wang et al. 2024). Our analysis revealed that our model's performance is notably superior compared to its counterparts. Specifically, our model decreased MSE on average across the ETTh1, ETTh2, ETTm1, and ETTm2 datasets by  $7.8\\%$ ,  $2.2\\%$ ,  $3.4\\%$ , and  $3.9\\%$ , respectively. Similarly, MAE was reduced by  $3.3\\%$ ,  $6.4\\%$ ,  $0.5\\%$ , and  $2.5\\%$ , respectively, for these datasets. On the Weather and Traffic datasets, our model demonstrated lower MSE and MAE in average prediction relative to the state-of-the-art TimeMixer model. Moreover, on the Electricity dataset, our model achieved the highest performance following the TimeMixer model.\n\nIn the second setup, we followed the unified setting of TimeMixer for all the datasets. The detailed results are presented in Table 9 in Supplementary. We achieved lower MSE and MAE scores on average on the ETT and Electricity datasets compared to the TimeMixer model.\n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">WPMixer (Ours)</td><td colspan=\"2\">TimeMixer* 2024</td><td colspan=\"2\">PatchTST 2023</td><td colspan=\"2\">TSMixer 2023</td><td colspan=\"2\">TimesNet 2023</td><td colspan=\"2\">Crossformer* 2023</td><td colspan=\"2\">FiLM* 2022a</td><td colspan=\"2\">Dlinear* 2023</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"5\">ETTh1</td><td>96</td><td>0.347</td><td>0.383</td><td>0.361</td><td>0.390</td><td>0.370</td><td>0.400</td><td>0.361</td><td>0.392</td><td>0.384</td><td>0.402</td><td>0.418</td><td>0.438</td><td>0.422</td><td>0.432</td><td>0.375</td><td>0.399</td></tr><tr><td>192</td><td>0.381</td><td>0.408</td><td>0.409</td><td>0.414</td><td>0.413</td><td>0.429</td><td>0.404</td><td>0.418</td><td>0.436</td><td>0.429</td><td>0.539</td><td>0.517</td><td>0.462</td><td>0.458</td><td>0.405</td><td>0.416</td></tr><tr><td>336</td><td>0.382</td><td>0.412</td><td>0.430</td><td>0.429</td><td>0.422</td><td>0.440</td><td>0.420</td><td>0.431</td><td>0.491</td><td>0.469</td><td>0.709</td><td>0.638</td><td>0.501</td><td>0.483</td><td>0.439</td><td>0.443</td></tr><tr><td>720</td><td>0.405</td><td>0.432</td><td>0.445</td><td>0.460</td><td>0.447</td><td>0.468</td><td>0.463</td><td>0.472</td><td>0.521</td><td>0.500</td><td>0.733</td><td>0.636</td><td>0.544</td><td>0.526</td><td>0.472</td><td>0.490</td></tr><tr><td>Avg</td><td>0.379</td><td>0.409</td><td>0.411</td><td>0.423</td><td>0.413</td><td>0.434</td><td>0.412</td><td>0.428</td><td>0.458</td><td>0.450</td><td>0.600</td><td>0.557</td><td>0.482</td><td>0.475</td><td>0.423</td><td>0.437</td></tr><tr><td rowspan=\"5\">ETTh2</td><td>96</td><td>0.253</td><td>0.328</td><td>0.271</td><td>0.330</td><td>0.274</td><td>0.337</td><td>0.274</td><td>0.341</td><td>0.340</td><td>0.374</td><td>0.425</td><td>0.463</td><td>0.323</td><td>0.370</td><td>0.289</td><td>0.353</td></tr><tr><td>192</td><td>0.303</td><td>0.364</td><td>0.317</td><td>0.402</td><td>0.341</td><td>0.382</td><td>0.339</td><td>0.385</td><td>0.402</td><td>0.414</td><td>0.473</td><td>0.500</td><td>0.391</td><td>0.415</td><td>0.383</td><td>0.418</td></tr><tr><td>336</td><td>0.305</td><td>0.371</td><td>0.332</td><td>0.396</td><td>0.329</td><td>0.384</td><td>0.361</td><td>0.406</td><td>0.452</td><td>0.452</td><td>0.581</td><td>0.562</td><td>0.415</td><td>0.440</td><td>0.448</td><td>0.465</td></tr><tr><td>720</td><td>0.373</td><td>0.417</td><td>0.342</td><td>0.408</td><td>0.379</td><td>0.422</td><td>0.445</td><td>0.470</td><td>0.462</td><td>0.468</td><td>0.775</td><td>0.665</td><td>0.441</td><td>0.459</td><td>0.605</td><td>0.551</td></tr><tr><td>Avg</td><td>0.309</td><td>0.370</td><td>0.316</td><td>0.384</td><td>0.331</td><td>0.381</td><td>0.355</td><td>0.401</td><td>0.414</td><td>0.427</td><td>0.564</td><td>0.548</td><td>0.393</td><td>0.421</td><td>0.431</td><td>0.447</td></tr><tr><td rowspan=\"5\">ETTm1</td><td>96</td><td>0.275</td><td>0.333</td><td>0.291</td><td>0.340</td><td>0.293</td><td>0.346</td><td>0.285</td><td>0.339</td><td>0.338</td><td>0.375</td><td>0.361</td><td>0.403</td><td>0.302</td><td>0.345</td><td>0.299</td><td>0.343</td></tr><tr><td>192</td><td>0.319</td><td>0.362</td><td>0.327</td><td>0.365</td><td>0.333</td><td>0.370</td><td>0.327</td><td>0.365</td><td>0.374</td><td>0.387</td><td>0.387</td><td>0.422</td><td>0.338</td><td>0.368</td><td>0.335</td><td>0.365</td></tr><tr><td>336</td><td>0.347</td><td>0.384</td><td>0.360</td><td>0.381</td><td>0.369</td><td>0.392</td><td>0.356</td><td>0.382</td><td>0.410</td><td>0.411</td><td>0.605</td><td>0.572</td><td>0.373</td><td>0.388</td><td>0.369</td><td>0.386</td></tr><tr><td>720</td><td>0.403</td><td>0.414</td><td>0.415</td><td>0.417</td><td>0.416</td><td>0.420</td><td>0.419</td><td>0.414</td><td>0.478</td><td>0.450</td><td>0.703</td><td>0.645</td><td>0.420</td><td>0.420</td><td>0.425</td><td>0.421</td></tr><tr><td>Avg</td><td>0.336</td><td>0.373</td><td>0.348</td><td>0.375</td><td>0.353</td><td>0.382</td><td>0.347</td><td>0.375</td><td>0.400</td><td>0.406</td><td>0.514</td><td>0.510</td><td>0.358</td><td>0.380</td><td>0.357</td><td>0.379</td></tr><tr><td rowspan=\"5\">ETTm2</td><td>96</td><td>0.159</td><td>0.246</td><td>0.164</td><td>0.254</td><td>0.166</td><td>0.256</td><td>0.163</td><td>0.252</td><td>0.187</td><td>0.267</td><td>0.275</td><td>0.358</td><td>0.165</td><td>0.256</td><td>0.167</td><td>0.260</td></tr><tr><td>192</td><td>0.214</td><td>0.286</td><td>0.223</td><td>0.295</td><td>0.223</td><td>0.296</td><td>0.216</td><td>0.290</td><td>0.249</td><td>0.309</td><td>0.345</td><td>0.400</td><td>0.222</td><td>0.296</td><td>0.224</td><td>0.303</td></tr><tr><td>336</td><td>0.266</td><td>0.322</td><td>0.279</td><td>0.330</td><td>0.274</td><td>0.329</td><td>0.268</td><td>0.324</td><td>0.321</td><td>0.351</td><td>0.657</td><td>0.528</td><td>0.277</td><td>0.333</td><td>0.281</td><td>0.342</td></tr><tr><td>720</td><td>0.344</td><td>0.374</td><td>0.359</td><td>0.383</td><td>0.362</td><td>0.385</td><td>0.420</td><td>0.422</td><td>0.408</td><td>0.403</td><td>1.208</td><td>0.753</td><td>0.371</td><td>0.389</td><td>0.397</td><td>0.421</td></tr><tr><td>Avg</td><td>0.246</td><td>0.307</td><td>.256</td><td>0.315</td><td>0.256</td><td>0.317</td><td>0.267</td><td>0.322</td><td>0.291</td><td>0.333</td><td>0.621</td><td>0.510</td><td>0.259</td><td>0.319</td><td>0.267</td><td>0.332</td></tr><tr><td rowspan=\"5\">Weather</td><td>96</td><td>0.141</td><td>0.188</td><td>0.147</td><td>0.197</td><td>0.149</td><td>0.198</td><td>0.145</td><td>0.198</td><td>0.172</td><td>0.220</td><td>0.232</td><td>0.302</td><td>0.199</td><td>0.262</td><td>0.176</td><td>0.237</td></tr><tr><td>192</td><td>0.185</td><td>0.229</td><td>0.189</td><td>0.239</td><td>0.194</td><td>0.241</td><td>0.191</td><td>0.242</td><td>0.219</td><td>0.261</td><td>0.371</td><td>0.410</td><td>0.228</td><td>0.288</td><td>0.220</td><td>0.282</td></tr><tr><td>336</td><td>0.236</td><td>0.271</td><td>0.241</td><td>0.280</td><td>0.245</td><td>0.282</td><td>0.242</td><td>0.280</td><td>0.280</td><td>0.306</td><td>0.495</td><td>0.515</td><td>0.267</td><td>0.323</td><td>0.265</td><td>0.319</td></tr><tr><td>720</td><td>0.307</td><td>0.321</td><td>0.310</td><td>0.330</td><td>0.314</td><td>0.334</td><td>0.320</td><td>0.336</td><td>0.365</td><td>0.359</td><td>0.526</td><td>0.542</td><td>0.319</td><td>0.361</td><td>0.323</td><td>0.362</td></tr><tr><td>Avg</td><td>0.217</td><td>0.252</td><td>0.222</td><td>0.262</td><td>0.226</td><td>0.264</td><td>0.225</td><td>0.264</td><td>0.259</td><td>0.287</td><td>0.406</td><td>0.442</td><td>0.253</td><td>0.309</td><td>0.246</td><td>0.300</td></tr><tr><td rowspan=\"5\">Electricity</td><td>96</td><td>0.128</td><td>0.222</td><td>0.129</td><td>0.224</td><td>0.129</td><td>0.222</td><td>0.131</td><td>0.229</td><td>0.168</td><td>0.272</td><td>0.150</td><td>0.251</td><td>0.154</td><td>0.267</td><td>0.140</td><td>0.237</td></tr><tr><td>192</td><td>0.145</td><td>0.237</td><td>0.140</td><td>0.220</td><td>0.147</td><td>0.240</td><td>0.151</td><td>0.246</td><td>0.184</td><td>0.289</td><td>0.161</td><td>0.260</td><td>0.164</td><td>0.258</td><td>0.153</td><td>0.249</td></tr><tr><td>336</td><td>0.161</td><td>0.256</td><td>0.161</td><td>0.255</td><td>0.163</td><td>0.259</td><td>0.161</td><td>0.261</td><td>0.198</td><td>0.300</td><td>0.182</td><td>0.281</td><td>0.188</td><td>0.283</td><td>0.169</td><td>0.267</td></tr><tr><td>720</td><td>0.196</td><td>0.287</td><td>0.194</td><td>0.287</td><td>0.197</td><td>0.290</td><td>0.197</td><td>0.293</td><td>0.220</td><td>0.320</td><td>0.251</td><td>0.339</td><td>0.236</td><td>0.332</td><td>0.203</td><td>0.301</td></tr><tr><td>Avg</td><td>0.158</td><td>0.251</td><td>0.156</td><td>0.246</td><td>0.159</td><td>0.253</td><td>0.160</td><td>0.257</td><td>0.192</td><td>0.295</td><td>0.186</td><td>0.283</td><td>0.186</td><td>0.285</td><td>0.166</td><td>0.264</td></tr><tr><td rowspan=\"5\">Traffic</td><td>96</td><td>0.354</td><td>0.246</td><td>0.360</td><td>0.249</td><td>0.360</td><td>0.249</td><td>0.376</td><td>0.264</td><td>0.593</td><td>0.321</td><td>0.514</td><td>0.267</td><td>0.416</td><td>0.294</td><td>0.410</td><td>0.282</td></tr><tr><td>192</td><td>0.371</td><td>0.253</td><td>0.375</td><td>0.250</td><td>0.379</td><td>0.256</td><td>0.397</td><td>0.277</td><td>0.617</td><td>0.336</td><td>0.549</td><td>0.252</td><td>0.408</td><td>0.288</td><td>0.423</td><td>0.287</td></tr><tr><td>336</td><td>0.387</td><td>0.267</td><td>0.385</td><td>0.270</td><td>0.392</td><td>0.264</td><td>0.413</td><td>0.290</td><td>0.629</td><td>0.336</td><td>0.530</td><td>0.300</td><td>0.425</td><td>0.298</td><td>0.436</td><td>0.296</td></tr><tr><td>720</td><td>0.431</td><td>0.289</td><td>0.430</td><td>0.281</td><td>0.432</td><td>0.286</td><td>0.444</td><td>0.306</td><td>0.640</td><td>0.350</td><td>0.573</td><td>0.313</td><td>0.520</td><td>0.353</td><td>0.466</td><td>0.315</td></tr><tr><td>Avg</td><td>0.386</td><td>0.264</td><td>0.387</td><td>0.262</td><td>0.391</td><td>0.264</td><td>0.408</td><td>0.284</td><td>0.620</td><td>0.336</td><td>0.542</td><td>0.283</td><td>0.442</td><td>0.308</td><td>0.434</td><td>0.295</td></tr><tr><td></td><td>1st Count:</td><td>29</td><td>26</td><td>7</td><td>9</td><td>0</td><td>2</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td></tr></table>\n\n# Computational efficiency and robustness\n\nWe evaluate WPMixer's computational cost in terms of the number of giga floating point operations (GFLOPs), a hardware-independent metric. We compute the GFLOPs for WPMixer and TimeMixer using the unified setting outlined by (Wang et al. 2024) with embedding dimension  $d = 16$  for the ETTh1 dataset. The comparison is presented in Table 3. WPMixer consistently requires less than one tenth GFLOPs across all prediction lengths compared to TimeMixer.\n\nWe also evaluate our model with three different random seeds by computing the mean and standard deviation for MSE and MAE. Results are averaged over the prediction lengths of 96, 192, 336, and 720. As shown in Table 4, our model exhibits a lower standard deviation than TimeMixer in all cases, highlighting the robustness of our approach.\n\nTable 2: Multivariate long-term forecasting results. Four commonly used prediction lengths (96,192,336,720) from the literature are considered for each dataset. The length of the look-back window is a hyperparameter. The results of the models marked with * are taken from (Wang et al. 2024); other results are taken from the corresponding papers.  \n\n<table><tr><td></td><td colspan=\"4\">WPMixer</td><td colspan=\"4\">TimeMixer</td></tr><tr><td></td><td>T</td><td>MSE</td><td>MAE</td><td>GFLOPs</td><td>MSE</td><td>MAE</td><td>GFLOPs</td><td></td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.370</td><td>0.390</td><td>0.210</td><td>0.375</td><td>0.400</td><td>2.774</td><td></td></tr><tr><td>192</td><td>0.424</td><td>0.420</td><td>0.226</td><td>0.429</td><td>0.421</td><td>3.281</td><td></td></tr><tr><td>336</td><td>0.462</td><td>0.433</td><td>0.211</td><td>0.484</td><td>0.458</td><td>4.040</td><td></td></tr><tr><td>720</td><td>0.455</td><td>0.449</td><td>0.481</td><td>0.498</td><td>0.482</td><td>6.066</td><td></td></tr></table>\n\nTable 3: WPMixer is ten folds more efficient for  $d = {16}$  .\n\n<table><tr><td></td><td colspan=\"2\">WPMixer</td><td colspan=\"2\">TimeMixer</td></tr><tr><td></td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>(1)</td><td>0.422 ± 0.001</td><td>0.423 ± 0.001</td><td>0.447 ± 0.002</td><td>0.440 ± 0.005</td></tr><tr><td>(2)</td><td>0.355 ± 0.003</td><td>0.387 ± 0.001</td><td>0.364 ± 0.008</td><td>0.395 ± 0.010</td></tr><tr><td>(3)</td><td>0.376 ± 0.002</td><td>0.388 ± 0.001</td><td>0.381 ± 0.003</td><td>0.395 ± 0.006</td></tr><tr><td>(4)</td><td>0.271 ± 0.001</td><td>0.317 ± 0.001</td><td>0.275 ± 0.001</td><td>0.323 ± 0.003</td></tr><tr><td>(5)</td><td>0.243 ± 0.001</td><td>0.269 ± 0.000</td><td>0.240 ± 0.010</td><td>0.271 ± 0.009</td></tr><tr><td>(6)</td><td>0.177 ± 0.000</td><td>0.267 ± 0.000</td><td>0.182 ± 0.017</td><td>0.272 ± 0.006</td></tr><tr><td>(7)</td><td>0.489 ± 0.005</td><td>0.297 ± 0.001</td><td>0.484 ± 0.015</td><td>0.297 ± 0.013</td></tr></table>\n\nTable 4: Model robustness under the unified setting, including similar look-back window length, batch size, and epochs for all models. (1), (2), (3), (4), (5), (6), and (7) refer to ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, and Traffic datasets, respectively.  \n\n<table><tr><td></td><td colspan=\"6\">Modules</td><td>ETTh1</td><td>ETTh2</td><td>ETTh1</td><td>ETTh2</td></tr><tr><td>Case</td><td>D</td><td>P</td><td>E</td><td>Px</td><td>Ex</td><td>H</td><td>MSE</td><td>MSE</td><td>MSE</td><td>MSE</td></tr><tr><td>I</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>0.379</td><td>0.308</td><td>0.336</td><td>0.245</td></tr><tr><td>II</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>0.388</td><td>0.311</td><td>0.339</td><td>0.247</td></tr><tr><td>III</td><td>✓</td><td>×</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>0.384</td><td>0.316</td><td>0.339</td><td>0.250</td></tr><tr><td>IV</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>0.392</td><td>0.325</td><td>0.345</td><td>0.249</td></tr><tr><td>V</td><td>✓</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>0.378</td><td>0.314</td><td>0.339</td><td>0.247</td></tr><tr><td>VI</td><td>×</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>0.390</td><td>0.320</td><td>0.343</td><td>0.248</td></tr><tr><td>VII</td><td>✓</td><td>✓</td><td>✓</td><td>×</td><td>×</td><td>✓</td><td>0.394</td><td>0.311</td><td>0.353</td><td>0.252</td></tr><tr><td>VIII</td><td>×</td><td>✓</td><td>✓</td><td>×</td><td>×</td><td>✓</td><td>0.399</td><td>0.312</td><td>0.354</td><td>0.252</td></tr><tr><td>IX</td><td>✓</td><td>×</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>0.400</td><td>0.315</td><td>0.356</td><td>0.251</td></tr><tr><td>X</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>0.403</td><td>0.315</td><td>0.355</td><td>0.252</td></tr><tr><td>XI</td><td>✓</td><td>✓</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>0.400</td><td>0.312</td><td>0.355</td><td>0.251</td></tr><tr><td>XII</td><td>×</td><td>✓</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>0.403</td><td>0.314</td><td>0.355</td><td>0.252</td></tr><tr><td>XIII</td><td>✓</td><td>✓</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>0.377</td><td>0.314</td><td>0.339</td><td>0.247</td></tr><tr><td>XIV</td><td>×</td><td>✓</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>0.392</td><td>0.314</td><td>0.342</td><td>0.248</td></tr></table>\n",
  "hyperparameter": "Key hyperparameters include: (1) Decomposition level m (optimized per dataset), (2) Wavelet type ψ (selected from Daubechies, Symlets, Coiflets, and Biorthogonal families), (3) Look-back window length L (optimized from values: 96, 192, 336, 512, 1024, 1200), (4) Prediction length T (96, 192, 336, 720), (5) Patch length P and stride S for patching, (6) Embedding dimension d (e.g., d=16 for ETTh1 in efficiency experiments), (7) Expansion factors tf and df for patch mixer and embedding mixer respectively, (8) Separate dropout values for Embedding and Mixer modules. Training uses SmoothL1Loss with default threshold, and hyperparameters are optimized using Optuna with Tree-structured Parzen Estimator (TPE). Specific optimized values are provided in Table 7 (Supplementary) per dataset."
}