{
  "id": "MultiPatchFormer_2025",
  "paper_title": "A multiscale model for multivariate time series forecasting",
  "alias": "MultiPatchFormer",
  "year": 2025,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "MultiPatchFormer introduces a multiscale time series forecasting framework that combines three key innovations: (1) multi-scale embedding using multiple patch sizes (8, 16, 24, 48) with 1D convolutions to capture temporal dynamics at different granularities, (2) channel-wise multi-head self-attention to model inter-series correlations while reducing overfitting in high-dimensional data, and (3) a multi-step decoder that breaks the final linear prediction layer into multiple sequential linear layers to generate predictions progressively, reducing noise in long-horizon forecasting. The model processes each channel independently through multi-scale patching, then applies temporal and inter-series encoders before the multi-step decoding.",
  "introduction": "Transformer based models for time-series forecasting have shown promising performance and during the past few years different Transformer variants have been proposed in time-series forecasting domain. However, most of the existing methods, mainly represent the time-series from a single scale, making it challenging to capture various time granularities or ignore inter-series correlations between the series which might lead to inaccurate forecasts. In this paper, we address the above mentioned shortcomings and propose a Transformer based model which integrates multi-scale patchwise temporal modeling and channel-wise representation. In the multi-scale temporal part, the input time-series is divided into patches of different resolutions to capture temporal correlations associated with various scales. The channel-wise encoder which comes after the temporal encoder, models the relations among the input series to capture the intricate interactions between them. In our framework, we further design a multi-step linear decoder to generate the final predictions for the purpose of reducing over-fitting and noise effects. Extensive experiments on seven real world datasets indicate that our model (MultiPatchFormer) achieves state-of-the-art results by surpassing other current baseline models in terms of error metrics and shows stronger generalizability.\n\nTime series forecasting plays an essential role in many fields, including finance, agriculture, meteorology and energy consumption domains. Motivated by its widespread application in different fields of Natural Language Processing and Computer Vision $^{1,2}$ , Transformer $^{3}$  is exploited in various time series applications (forecasting, imputation and classification) with some modifications in its architecture $^{4,5}$ . While recent studies have raised doubts about the performance of Transformers, specifically when employing linear models with superior performance $^{6}$ , the inherent capabilities of Transformers remain promising $^{7,8}$ , particularly in representing complex and non-linear datasets. This underscores the need for further modifications to fully harness their capability in the domain of time series forecasting.\n\nReal-world multivariate time series exhibit high correlations between different variates and fluctuations at various temporal scales. For example, electricity consumption shows specific temporal variations spanning seasonal, daily and hourly granularities. Figure 1, illustrates a time series of a stock over one year, in which relations between patches of different scales are critical to capture more information regarding the local and global temporal dependencies from various perspectives. This calls for multi-scale modeling of time series<sup>9</sup> and representation of inter series correlations<sup>10</sup>.\n\nMost of the existing time series forecasting models lack an efficient mechanism for multi-scale representation and they totally rely on a single scale or time resolution. Although some of the baseline models consider multiscale representation in their modeling process $^{8,11}$ , however, these models employ separate sets of parameters for capturing temporal dependencies at each scale, which significantly increases time complexity and the risk of overfitting. Furthermore, most of the aforementioned methods, ignore the cross-channel relationships between time series channels which has been proved to be critical in time series analysis task $^{10}$ . Another limitation of the current research is the single step decoding of the encoded representation, particularly in dealing with long horizon prediction, which raises the risk of overfitting and makes the model more susceptible to be affected by noise.\n\nTo leverage the capabilities of Transformers for addressing the above mentioned issues, we aim to enhance the capture of both multi-scale and cross-channel dependencies, thereby aggregating this information for time series representation. We further divide input series into local patches to process the input time series and since the variable independence has been proved to be more efficient<sup>7</sup>, we feed the multivariate series to the Transformer blocks by keeping the variate (channel) dimension intact. We further divide the series into patches of different size and map patch length to the model dimension by using 1-dimensional convolution in order to leverage the local information inside a patch (intra-patch relations). Instead of using several Transformer blocks, we first embed the time series using different scales and aggregate them into the same model dimension (feature space). Then, we process the mapped series using a Transformer block, followed by an channel-wise\n\n<sup>1</sup>Computer Science, Université du Québec à Montréal, Montreal, Canada. <sup>2</sup>Vahid Naghashi, Mounir Boukadoum and Abdoulaye Banire Diallo contributed equally to this work. <sup>3</sup>email: diallo.abdoulaye@uqam.ca\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/69a2f700-d49a-470f-886f-de5f589c3103/f1aee8f16ebda43262e35b839d1f061dd2c62767459e61973e28b88dd89d776b.jpg)  \nFig. 1. Multi-scale dependencies in a real-world time series instance. The temporal patterns within patches of lengths Patch Size 1 and Patch Size 2 show similar trends and seasonality. Capturing these relations across different scales is crucial for analyzing time series data effectively.\n\nTransformer component to capture cross-series dependencies. To further improve the Transformer for time series forecasting task, we introduce a channel dimensionality reduction method inside the multi-head self attention module when applied on the channel dimension, which helps to reduce over-fitting noise, specifically when dealing with the time series composed of many variates (channels). We additionally, devise a multiple step and pseudo auto-regressive decoding of prediction window by generating the predictions segment by segment using the encoded series and previously generated segment, in order to reduce the noise impact, particularly in long-term forecasting scenarios. The effectiveness of the proposed model is verified through different real-world benchmarks.\n\nHere is the key summarization of our contributions:\n\n1. We propose a time series forecasting model which effectively utilizes the multi-scale information and captures the inter-series dependencies among different channels.  \n2. Our model demonstrates superior performance on various time series datasets and shows robust performance in time series forecasting task with long input length.  \n3. We design a multi-scale time series embedding method which captures the local temporal information from various scales and divides time series to patches of different lengths, preparing it to represent long term temporal correlations from multiple temporal aspects.  \n4. We propose a pseudo auto-regressive (multi-step) decoding scheme to forecast the time series in multiple sequential steps rather than decoding the whole prediction length using a single linear layer in order to reduce the noise effect.\n",
  "method": "# Methods\n\nIn this paper, we deal with the problem of time series forecasting which can be stated as following: Given a set of historical time series of  $C$  variates over  $L$  timestamps  $(X_{1}, X_{2}, \\dots, X_{L})$ , with  $X_{i}$  indicating the observation at timestamp  $i$  of dimension  $C$ , we aim to predict  $F$  steps of the future time steps:  $(X_{L+1}, \\dots, X_{L+F})$ . To effectively capture multi-scale information and inter-series correlations, we propose MultiPatchFormer, which utilizes a novel predictor composed of a sequence of linear layers to simulate auto-regressive decoding in patch level. The overall architecture is illustrated in Fig. 2. The whole framework consists of instance normalization<sup>31</sup>, multiscale embedding, temporal encoder, inter-series encoder module and predictor. Instance normalization<sup>31</sup> is a technique employed by many models to avoid distribution shifts between training and test sets. The multiscale embedding first divides the input series into patches of various sizes and then embeds all of them to the same model space. In other words, given 4 temporal scales, time series is reshaped to  $((B, C), L, 1)$ , then is convolved with filters of different sizes and strides, corresponding to the patch sizes and strides (overlapping region between to consecutive patches).  $B, C$  and  $L$  indicate batch size, channels and series length, respectively. We deliberately decided to exploit one-dimensional convolution to split series for the purpose of capturing local temporal dynamics inside a patch (intra-patch relations). After splitting the input time series by using patches of four various lengths, they are merged together into the model dimension  $(d_{model})$ . To be more specific, the output channel of the individual convolutions are set to  $\\frac{d_{model}}{4}$ , mapping each patched series into its own smaller sub-space and then merging them to the main model dimension. This strategy employs separate 1-dimensional convolutions with input channel of 1 and output channel of  $\\frac{d_{model}}{4}$  to project multi-scale information into the embedded space and each channel of time series is embedded independently in this manner, as we reshape the series into  $((B, C), L, 1)$  before applying the convolutions. In order to reduce the over-fitting effect in channel-wise multi-head self-attention, we devise a simple but yet effective solution for reducing noise and time complexity of time series with large number of channels. The regular linear predictor which is usually used as the final layer of time-series forecasting models, is susceptible to over-fitting, specifically when the forecast horizon is relatively long. Rather than using a simple linear layer to map from model dimension to the prediction length, we break the final linear layer to multiple linear layers to decode the predictions in multiple steps by generating part of\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/69a2f700-d49a-470f-886f-de5f589c3103/a4803955ec399f1bbc155875e670a4273eef23216cc00886ff561b0e6342d01b.jpg)  \nFig. 2. General overview of the proposed framework (MultiPatchFormer). Left: the main steps include normalization, multi-scale embedding using different patch sizes, temporal encoder, reshaping the output of the temporal encoder using a 1-dimensional convolution layer and sending the result to the channel-wise encoder. The output of the channel-wise encoder is passed through the multi-step decoder to generate the final predictions followed by the de-normalization step. Right: Multi-scale embedding is illustrated using 4 different patch sizes (scales), where  $D$  refers to the model dimension ( $d_{model}$ ),  $Ch$  indicates time series channels (variates), and  $PN1$  refers to the number of patches fixed across four scales by choosing appropriate strides.\n\nthe prediction horizon at each step. In the following section, each component of our model is described in more detail.",
  "experiments": "# Results\n\n# Datasets and baselines\n\nDatasets We conduct different experiments using 7 real world datasets including electricity Transformer Temperature $^{24}$ , weather forecasting $^{24}$ , traffic $^{24}$  and electricity consumption $^{24}$ . These datasets consist of the well-known ETT (ETTh1, ETTh2, ETTm1, ETTm2), Weather, Electricity and Traffic. More details about the datasets are reported in Table 1. Baseline and metrics 14 well-known time series forecasting models are selected as baselines, including Pathformer $^{8}$ , PatchTST $^{7}$ , DLinear $^{6}$ , NLinear $^{6}$ , Crossformer $^{32}$ , Scaleformer $^{11}$ , TIDE $^{17}$ , FEDformer $^{4}$ , Pyraformer $^{33}$ , Autoformer $^{24}$ , Time-LLM $^{26}$ , GPT4TS $^{27}$ , MTGNN $^{18}$  and SDGL $^{21}$ . To ensure fairness in our experiments, the input length is fixed for all models ( $L = 96$ ) and prediction length consists of ( $F = 96$ , 192, 336, 720). Since the authors of Time-LLM used input length of 512 in their paper $^{26}$ , we set the context window to 512 in LLM comparison experiments and trained our model on different datasets with input length of 512 and prediction length in  $\\{96, 192, 336, 512\\}$ . We compare our model with graph models based on input window of length 96. Two well-known error metrics in time series forecasting are selected: Mean Absolute Error (MAE)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/69a2f700-d49a-470f-886f-de5f589c3103/934f521c386d29e93f1d39f549bdf57a3684d038c6a44e64c3cfa70334efe567.jpg)  \nFig. 3. Multi-step decoder is proposed to reduce noise effect in long prediction length.  $C, D$  and  $L$  refer to channels, model dimension and prediction length, respectively. At each step the feature map from the channel-wise encoder is concatenated with previously generated prediction segments and passed through a linear layer to generate the next prediction segment (part). The final prediction is the concatenation result of the individual predicted segments.\n\n<table><tr><td>Dataset</td><td>Dim</td><td>Pred length</td><td>Dataset size</td><td>Mean corr</td><td>Max corr</td><td>Trend</td><td>Freq</td><td>Field</td></tr><tr><td>Electricity</td><td>321</td><td>96, 192, 336, 720</td><td>(18,317, 2633, 5261)</td><td>0.981</td><td>0.999</td><td>0.315</td><td>Hourly</td><td>Electricity</td></tr><tr><td>ETTh1, ETTh2</td><td>7</td><td>96, 192, 336, 720</td><td>(8545, 2881, 2881)</td><td>0.551 (0.551)</td><td>0.897 (0.946)</td><td>0.797 (0.761)</td><td>Hourly</td><td>Electricity</td></tr><tr><td>ETTm1, ETTm2</td><td>7</td><td>96, 192, 336, 720</td><td>(34,465, 11,521, 11,521)</td><td>0.551 (0.551)</td><td>0.897 (0.945)</td><td>0.896 (0.822)</td><td>15 min</td><td>Electricity</td></tr><tr><td>Weather</td><td>21</td><td>96, 192, 336, 720</td><td>(36,792, 5271, 10,540)</td><td>0.704</td><td>0.999</td><td>0.039</td><td>10 min</td><td>Weather</td></tr><tr><td>Traffic</td><td>862</td><td>96, 192, 336, 720</td><td>(12,185, 1757, 3509)</td><td>0.751</td><td>0.985</td><td>0.031</td><td>Hourly</td><td>Transportation</td></tr></table>\n\nTable 1. Detailed dataset descriptions. Dim indicates the variate number of each dataset. Dataset Size denotes the total number of time points in (Train, Validation, Test) splits, respectively. Mean and maximum cross-correlations (Dynamic Time Warping based) between the channels of each dataset are also reported in Mean Corr and Max Corr columns. Trend column indicates strength of trend for each dataset.\n\nand Mean Square Error (MSE) to compare the models. Implementation details We utilize Adam optimizer with learning rate in  $\\{10^{-3}, 10^{-4}\\}$  and L1 loss function. The models are trained for 10 epochs with early stopping (patience of 3) based on the validation loss. We implemented our model using Pytorch framework $^{34}$  and the experiments are executed on an NVIDIA A100 40GB GPU. MultiPatchFormer utilizes 4 different scales to patchify and embed input time series (common scales in the time series forecasting domain, e.g., 8, 16, 24, 48 with proper strides, 8, 8, 7, 6 to create the same number of patches).\n\n# Main results\n\nThe main results are summarized in Tables 2, 3 and 4. The results related to the baseline models are taken from Ref. $^{8}$  and we follow the same settings to train and evaluate our model. All experiments are repeated 5 times and the average results are reported. According to Table 2, our MultiPatchFormer, shows better performance across four different prediction horizons. Regarding error measures across four prediction lengths, our model achieves the best or second best performance among the state-of-the-art models in  $82\\%$  of cases in MSE metric and  $86\\%$  of the conducted experiments in terms of MAE measure. Our MultiPatchFormer outperforms the baseline models on benchmarks with a high number of variates and complex structure. For instance, in Traffic dataset (862 covariates), MultiPatchFormer persistently outperforms the second best baseline by more than  $5\\%$  on average MSE and  $7.7\\%$  on average MAE across four prediction windows, while consuming less training time and parameters. By exploiting 321 variates in ECL (Electricity) dataset, we achieved average error reduction of  $3.7\\%$  compared to Pathformer $^{8}$  and  $10.7\\%$  improvement over the PatchTST model. This highlights that MultiPatchFormer is capable of utilizing extensive covariate dependencies for high accuracy prediction. Remarkably, our model makes predictions with lower error in complex problems, including Electricity (ECL) and Traffic, which underscores the efficiency of the channel-wise attention and multi-scale embedding to capture the inter-series dependencies across various scales. It worth noting that while NLinear $^{6}$  achieves promising performance in some datasets, MultiPatchFormer significantly outperforms the MLP-based models (NLinear and DLinear) by over  $26.8\\%$  error reduction on Traffic and more than  $10\\%$  improvement on Electricity dataset. This indicates that Transformer models are still powerful in forecasting long-term time series tasks. In terms of cross-variate modeling, Crossformer $^{32}$ , utilizes a variant of attention to capture those type of correlations, but our model gains impressive improvement of over  $28\\%$  and  $18\\%$  compared to the Crossformer model on\n\n<table><tr><td>Models</td><td colspan=\"2\">Ours</td><td colspan=\"2\">Pathformer</td><td colspan=\"2\">PatchTST</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">NLinear</td><td colspan=\"2\">Crossformer</td><td colspan=\"2\">Scaleformer</td><td colspan=\"2\">TIDE</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td colspan=\"21\">ETTm1</td></tr><tr><td>96</td><td>0.315</td><td>0.342</td><td>0.316</td><td>0.346</td><td>0.324</td><td>0.361</td><td>0.392</td><td>0.405</td><td>0.386</td><td>0.392</td><td>0.429</td><td>0.440</td><td>0.396</td><td>0.440</td><td>0.356</td><td>0.381</td><td>0.379</td><td>0.419</td><td>0.505</td><td>0.475</td></tr><tr><td>192</td><td>0.367</td><td>0.369</td><td>0.366</td><td>0.370</td><td>0.362</td><td>0.383</td><td>0.441</td><td>0.436</td><td>0.440</td><td>0.430</td><td>0.494</td><td>0.482</td><td>0.434</td><td>0.460</td><td>0.391</td><td>0.399</td><td>0.426</td><td>0.441</td><td>0.553</td><td>0.496</td></tr><tr><td>336</td><td>0.399</td><td>0.394</td><td>0.386</td><td>0.394</td><td>0.390</td><td>0.402</td><td>0.501</td><td>0.478</td><td>0.480</td><td>0.443</td><td>0.706</td><td>0.625</td><td>0.462</td><td>0.476</td><td>0.424</td><td>0.423</td><td>0.445</td><td>0.459</td><td>0.621</td><td>0.537</td></tr><tr><td>720</td><td>0.464</td><td>0.432</td><td>0.460</td><td>0.432</td><td>0.461</td><td>0.438</td><td>0.538</td><td>0.526</td><td>0.486</td><td>0.472</td><td>0.750</td><td>0.689</td><td>0.494</td><td>0.500</td><td>0.480</td><td>0.465</td><td>0.543</td><td>0.490</td><td>0.671</td><td>0.561</td></tr><tr><td>Avg</td><td>0.386</td><td>0.384</td><td>0.383</td><td>0.386</td><td>0.384</td><td>0.396</td><td>0.468</td><td>0.461</td><td>0.448</td><td>0.434</td><td>0.595</td><td>0.559</td><td>0.447</td><td>0.469</td><td>0.413</td><td>0.417</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td></tr><tr><td colspan=\"21\">ETTm2</td></tr><tr><td>96</td><td>0.171</td><td>0.250</td><td>0.170</td><td>0.248</td><td>0.177</td><td>0.260</td><td>0.186</td><td>0.279</td><td>0.177</td><td>0.257</td><td>0.197</td><td>0.321</td><td>0.182</td><td>0.275</td><td>0.182</td><td>0.264</td><td>0.203</td><td>0.287</td><td>0.255</td><td>0.339</td></tr><tr><td>192</td><td>0.236</td><td>0.294</td><td>0.238</td><td>0.295</td><td>0.248</td><td>0.306</td><td>0.266</td><td>0.339</td><td>0.241</td><td>0.297</td><td>0.326</td><td>0.375</td><td>0.251</td><td>0.318</td><td>0.256</td><td>0.323</td><td>0.269</td><td>0.328</td><td>0.281</td><td>0.340</td></tr><tr><td>336</td><td>0.303</td><td>0.337</td><td>0.293</td><td>0.331</td><td>0.304</td><td>0.342</td><td>0.332</td><td>0.376</td><td>0.302</td><td>0.337</td><td>0.372</td><td>0.421</td><td>0.340</td><td>0.375</td><td>0.313</td><td>0.354</td><td>0.325</td><td>0.366</td><td>0.339</td><td>0.372</td></tr><tr><td>720</td><td>0.397</td><td>0.393</td><td>0.390</td><td>0.389</td><td>0.403</td><td>0.397</td><td>0.462</td><td>0.455</td><td>0.405</td><td>0.396</td><td>0.410</td><td>0.448</td><td>0.435</td><td>0.433</td><td>0.419</td><td>0.410</td><td>0.421</td><td>0.415</td><td>0.433</td><td>0.432</td></tr><tr><td>Avg</td><td>0.277</td><td>0.319</td><td>0.273</td><td>0.316</td><td>0.283</td><td>0.326</td><td>0.312</td><td>0.362</td><td>0.281</td><td>0.322</td><td>0.326</td><td>0.391</td><td>0.302</td><td>0.350</td><td>0.293</td><td>0.338</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td></tr><tr><td colspan=\"21\">ETTh1</td></tr><tr><td>96</td><td>0.378</td><td>0.389</td><td>0.382</td><td>0.400</td><td>0.394</td><td>0.408</td><td>0.392</td><td>0.405</td><td>0.386</td><td>0.392</td><td>0.429</td><td>0.440</td><td>0.396</td><td>0.440</td><td>0.427</td><td>0.450</td><td>0.376</td><td>0.419</td><td>0.449</td><td>0.459</td></tr><tr><td>192</td><td>0.430</td><td>0.420</td><td>0.440</td><td>0.427</td><td>0.446</td><td>0.438</td><td>0.441</td><td>0.436</td><td>0.440</td><td>0.430</td><td>0.494</td><td>0.482</td><td>0.434</td><td>0.460</td><td>0.472</td><td>0.486</td><td>0.420</td><td>0.448</td><td>0.500</td><td>0.482</td></tr><tr><td>336</td><td>0.473</td><td>0.442</td><td>0.454</td><td>0.432</td><td>0.485</td><td>0.455</td><td>0.501</td><td>0.478</td><td>0.480</td><td>0.443</td><td>0.706</td><td>0.689</td><td>0.462</td><td>0.476</td><td>0.527</td><td>0.527</td><td>0.459</td><td>0.465</td><td>0.521</td><td>0.496</td></tr><tr><td>720</td><td>0.475</td><td>0.466</td><td>0.479</td><td>0.461</td><td>0.495</td><td>0.474</td><td>0.538</td><td>0.526</td><td>0.486</td><td>0.472</td><td>0.750</td><td>0.689</td><td>0.494</td><td>0.500</td><td>0.644</td><td>0.605</td><td>0.506</td><td>0.507</td><td>0.514</td><td>0.512</td></tr><tr><td>Avg</td><td>0.439</td><td>0.429</td><td>0.439</td><td>0.430</td><td>0.455</td><td>0.444</td><td>0.468</td><td>0.461</td><td>0.448</td><td>0.434</td><td>0.595</td><td>0.575</td><td>0.447</td><td>0.469</td><td>0.518</td><td>0.517</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td></tr><tr><td colspan=\"21\">ETTh2</td></tr><tr><td>96</td><td>0.287</td><td>0.333</td><td>0.279</td><td>0.331</td><td>0.294</td><td>0.343</td><td>0.331</td><td>0.381</td><td>0.290</td><td>0.339</td><td>0.632</td><td>0.547</td><td>0.364</td><td>0.407</td><td>0.304</td><td>0.359</td><td>0.346</td><td>0.388</td><td>0.358</td><td>0.397</td></tr><tr><td>192</td><td>0.366</td><td>0.383</td><td>0.349</td><td>0.380</td><td>0.378</td><td>0.394</td><td>0.432</td><td>0.435</td><td>0.379</td><td>0.395</td><td>0.876</td><td>0.663</td><td>0.466</td><td>0.458</td><td>0.394</td><td>0.422</td><td>0.429</td><td>0.439</td><td>0.456</td><td>0.452</td></tr><tr><td>336</td><td>0.413</td><td>0.420</td><td>0.348</td><td>0.382</td><td>0.382</td><td>0.410</td><td>0.441</td><td>0.451</td><td>0.421</td><td>0.431</td><td>0.924</td><td>0.702</td><td>0.479</td><td>0.476</td><td>0.385</td><td>0.421</td><td>0.496</td><td>0.487</td><td>0.482</td><td>0.486</td></tr><tr><td>720</td><td>0.417</td><td>0.435</td><td>0.398</td><td>0.424</td><td>0.412</td><td>0.433</td><td>0.564</td><td>0.578</td><td>0.436</td><td>0.453</td><td>1.390</td><td>0.863</td><td>0.487</td><td>0.492</td><td>0.463</td><td>0.475</td><td>0.463</td><td>0.474</td><td>0.515</td><td>0.511</td></tr><tr><td>Avg</td><td>0.372</td><td>0.394</td><td>0.344</td><td>0.379</td><td>0.367</td><td>0.395</td><td>0.442</td><td>0.461</td><td>0.382</td><td>0.405</td><td>0.956</td><td>0.694</td><td>0.449</td><td>0.458</td><td>0.387</td><td>0.419</td><td>0.434</td><td>0.447</td><td>0.453</td><td>0.462</td></tr><tr><td colspan=\"21\">Weather</td></tr><tr><td>96</td><td>0.157</td><td>0.197</td><td>0.156</td><td>0.192</td><td>0.177</td><td>0.218</td><td>0.195</td><td>0.253</td><td>0.168</td><td>0.208</td><td>0.181</td><td>0.231</td><td>0.288</td><td>0.365</td><td>0.202</td><td>0.261</td><td>0.238</td><td>0.314</td><td>0.249</td><td>0.329</td></tr><tr><td>192</td><td>0.207</td><td>0.242</td><td>0.206</td><td>0.240</td><td>0.224</td><td>0.258</td><td>0.239</td><td>0.299</td><td>0.217</td><td>0.255</td><td>0.219</td><td>0.275</td><td>0.368</td><td>0.425</td><td>0.242</td><td>0.298</td><td>0.275</td><td>0.329</td><td>0.325</td><td>0.370</td></tr><tr><td>336</td><td>0.267</td><td>0.286</td><td>0.254</td><td>0.282</td><td>0.277</td><td>0.297</td><td>0.282</td><td>0.333</td><td>0.267</td><td>0.292</td><td>0.274</td><td>0.332</td><td>0.447</td><td>0.469</td><td>0.287</td><td>0.335</td><td>0.339</td><td>0.377</td><td>0.351</td><td>0.391</td></tr><tr><td>720</td><td>0.345</td><td>0.339</td><td>0.340</td><td>0.336</td><td>0.350</td><td>0.345</td><td>0.352</td><td>0.390</td><td>0.351</td><td>0.346</td><td>0.356</td><td>0.387</td><td>0.640</td><td>0.574</td><td>0.351</td><td>0.386</td><td>0.389</td><td>0.409</td><td>0.415</td><td>0.426</td></tr><tr><td>Avg</td><td>0.244</td><td>0.266</td><td>0.239</td><td>0.263</td><td>0.257</td><td>0.280</td><td>0.267</td><td>0.319</td><td>0.251</td><td>0.275</td><td>0.258</td><td>0.306</td><td>0.436</td><td>0.458</td><td>0.271</td><td>0.320</td><td>0.310</td><td>0.357</td><td>0.335</td><td>0.379</td></tr><tr><td colspan=\"21\">Electricity</td></tr><tr><td>96</td><td>0.146</td><td>0.233</td><td>0.145</td><td>0.236</td><td>0.180</td><td>0.264</td><td>0.194</td><td>0.276</td><td>0.185</td><td>0.266</td><td>0.254</td><td>0.347</td><td>0.182</td><td>0.297</td><td>0.194</td><td>0.277</td><td>0.186</td><td>0.302</td><td>0.196</td><td>0.313</td></tr><tr><td>192</td><td>0.163</td><td>0.247</td><td>0.167</td><td>0.256</td><td>0.188</td><td>0.275</td><td>0.193</td><td>0.279</td><td>0.189</td><td>0.276</td><td>0.261</td><td>0.353</td><td>0.188</td><td>0.300</td><td>0.193</td><td>0.280</td><td>0.197</td><td>0.311</td><td>0.211</td><td>0.324</td></tr><tr><td>336</td><td>0.178</td><td>0.263</td><td>0.186</td><td>0.275</td><td>0.206</td><td>0.291</td><td>0.206</td><td>0.294</td><td>0.204</td><td>0.289</td><td>0.273</td><td>0.364</td><td>0.210</td><td>0.324</td><td>0.206</td><td>0.296</td><td>0.213</td><td>0.328</td><td>0.214</td><td>0.327</td></tr><tr><td>720</td><td>0.213</td><td>0.293</td><td>0.231</td><td>0.309</td><td>0.247</td><td>0.328</td><td>0.241</td><td>0.328</td><td>0.245</td><td>0.319</td><td>0.303</td><td>0.388</td><td>0.232</td><td>0.339</td><td>0.242</td><td>0.328</td><td>0.233</td><td>0.344</td><td>0.236</td><td>0.342</td></tr><tr><td>Avg</td><td>0.175</td><td>0.259</td><td>0.182</td><td>0.269</td><td>0.205</td><td>0.290</td><td>0.209</td><td>0.294</td><td>0.206</td><td>0.288</td><td>0.273</td><td>0.363</td><td>0.203</td><td>0.315</td><td>0.209</td><td>0.295</td><td>0.207</td><td>0.321</td><td>0.214</td><td>0.327</td></tr><tr><td colspan=\"21\">Traffic</td></tr><tr><td>96</td><td>0.438</td><td>0.260</td><td>0.479</td><td>0.283</td><td>0.492</td><td>0.324</td><td>0.648</td><td>0.396</td><td>0.645</td><td>0.388</td><td>0.558</td><td>0.320</td><td>2.678</td><td>1.071</td><td>0.568</td><td>0.352</td><td>0.576</td><td>0.359</td><td>0.597</td><td>0.371</td></tr><tr><td>192</td><td>0.456</td><td>0.268</td><td>0.484</td><td>0.292</td><td>0.487</td><td>0.303</td><td>0.613</td><td>0.386</td><td>0.599</td><td>0.365</td><td>0.572</td><td>0.331</td><td>0.564</td><td>0.351</td><td>0.612</td><td>0.371</td><td>0.610</td><td>0.380</td><td>0.607</td><td>0.382</td></tr><tr><td>336</td><td>0.475</td><td>0.276</td><td>0.503</td><td>0.299</td><td>0.505</td><td>0.317</td><td>0.614</td><td>0.383</td><td>0.606</td><td>0.367</td><td>0.587</td><td>0.342</td><td>0.570</td><td>0.349</td><td>0.605</td><td>0.374</td><td>0.608</td><td>0.375</td><td>0.623</td><td>0.387</td></tr><tr><td>512</td><td>0.514</td><td>0.295</td><td>0.537</td><td>0.322</td><td>0.542</td><td>0.337</td><td>0.655</td><td>0.405</td><td>0.645</td><td>0.388</td><td>0.652</td><td>0.359</td><td>0.576</td><td>0.349</td><td>0.647</td><td>0.410</td><td>0.621</td><td>0.375</td><td>0.639</td><td>0.395</td></tr><tr><td>Avg</td><td>0.470</td><td>0.275</td><td>0.501</td><td>0.299</td><td>0.507</td><td>0.320</td><td>0.632</td><td>0.393</td><td>0.624</td><td>0.377</td><td>0.592</td><td>0.338</td><td>1.097</td><td>0.530</td><td>0.608</td><td>0.377</td><td>0.604</td><td>0.372</td><td>0.617</td><td>0.384</td></tr></table>\n\nTable 2. Multivariate time series forecasting results. The input length (lookback window) is fixed to 96 and prediction length is in \\{96, 192, 336, 720\\}. Bold and italics indicate the significant and second best values.\n\nthe Electricity and Traffic datasets, respectively. According to Table 3, our model consistently outperforms or achieves similar performance to LLM-based methods while using significantly fewer parameters. Notably, compared to Time-LLM, which utilizes 7 billion parameters, our model maintains competitive performance across different datasets. For example, when time series forecasting of the ETTh1 dataset at a prediction window of 720, MultiPatchFormer obtains an MSE of 0.434, lower than Time-LLM's 0.442 while consuming far fewer computational resources. This underscores the effectiveness of our approach relative to larger models. Graph learning models have been proved to be promising in forecasting traffic flow by modeling the temporal correlations and the spatial dependencies between the variables through the graph learning strategy[18,21]. We compare our MultiPatchFormer with the spatio-temporal graph models on various benchmarks, specially on Traffic dataset. As illustrated by Table 4, our model outperforms SDGL and MTGNN with a large margin, particularly on Traffic forecasting task, with average MSE improvement of  $23\\%$  and  $28\\%$ , respectively.\n\n<table><tr><td>Methods</td><td colspan=\"2\">Ours</td><td colspan=\"2\">Time-LLM</td><td colspan=\"2\">GPT4TS</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td colspan=\"7\">Traffic</td></tr><tr><td>96</td><td>0.370</td><td>0.236</td><td>0.362</td><td>0.248</td><td>0.388</td><td>0.282</td></tr><tr><td>192</td><td>0.383</td><td>0.242</td><td>0.374</td><td>0.247</td><td>0.407</td><td>0.290</td></tr><tr><td>336</td><td>0.398</td><td>0.250</td><td>0.385</td><td>0.271</td><td>0.412</td><td>0.294</td></tr><tr><td>720</td><td>0.433</td><td>0.270</td><td>0.430</td><td>0.288</td><td>0.450</td><td>0.312</td></tr><tr><td>Avg</td><td>0.396</td><td>0.250</td><td>0.388</td><td>0.264</td><td>0.414</td><td>0.294</td></tr><tr><td colspan=\"7\">Electricity</td></tr><tr><td>96</td><td>0.131</td><td>0.222</td><td>0.131</td><td>0.224</td><td>0.139</td><td>0.238</td></tr><tr><td>192</td><td>0.149</td><td>0.239</td><td>0.152</td><td>0.241</td><td>0.153</td><td>0.251</td></tr><tr><td>336</td><td>0.162</td><td>0.253</td><td>0.160</td><td>0.248</td><td>0.169</td><td>0.266</td></tr><tr><td>720</td><td>0.191</td><td>0.277</td><td>0.192</td><td>0.298</td><td>0.206</td><td>0.297</td></tr><tr><td>Avg</td><td>0.158</td><td>0.248</td><td>0.158</td><td>0.252</td><td>0.167</td><td>0.263</td></tr><tr><td colspan=\"7\">ETTh1</td></tr><tr><td>96</td><td>0.366</td><td>0.392</td><td>0.362</td><td>0.392</td><td>0.376</td><td>0.397</td></tr><tr><td>192</td><td>0.400</td><td>0.418</td><td>0.398</td><td>0.418</td><td>0.416</td><td>0.418</td></tr><tr><td>336</td><td>0.423</td><td>0.437</td><td>0.430</td><td>0.427</td><td>0.442</td><td>0.433</td></tr><tr><td>720</td><td>0.434</td><td>0.459</td><td>0.442</td><td>0.457</td><td>0.477</td><td>0.456</td></tr><tr><td>Avg</td><td>0.406</td><td>0.427</td><td>0.408</td><td>0.423</td><td>0.465</td><td>0.455</td></tr><tr><td colspan=\"7\">Weather</td></tr><tr><td>96</td><td>0.144</td><td>0.185</td><td>0.147</td><td>0.201</td><td>0.162</td><td>0.212</td></tr><tr><td>192</td><td>0.190</td><td>0.231</td><td>0.189</td><td>0.234</td><td>0.204</td><td>0.248</td></tr><tr><td>336</td><td>0.242</td><td>0.270</td><td>0.262</td><td>0.279</td><td>0.254</td><td>0.286</td></tr><tr><td>720</td><td>0.313</td><td>0.323</td><td>0.304</td><td>0.316</td><td>0.326</td><td>0.337</td></tr><tr><td>Avg</td><td>0.222</td><td>0.252</td><td>0.225</td><td>0.257</td><td>0.237</td><td>0.270</td></tr></table>\n\nTable 3. Comparison result of our model with LLM models. The input length (lookback window) is fixed to 512 and prediction length is in {96, 192, 336, 720}. Significant values are in bold.\n\nAs shown in Table 2, MultiPatchFormer consistently achieves low error rates for datasets with strong seasonality and highly correlated datasets with many variates (Electricity, Traffic, Weather). This can be attributed to the multi-scale embedding and temporal Transformer blocks, which effectively captures temporal correlations at various granularities and channel-wise attention, which represents the complex dependencies between time series channels through learning of the attention scores between channel pairs, extracting meaningful features. However, our model exhibits relatively lower performance on datasets with dominating short-term dependencies and limited training data, such as the ETTh2 dataset. This could be associated with the multi-scale embedding and multi-head attention, which, while being effective for capturing long-range patterns, may not show significant effectiveness for data with simpler temporal patterns and limited training samples.\n\nIn time series forecasting, size of the input series determines the historical information that a model receives to forecast. Different input lengths are configured to verify effectiveness of MultiPatchFormer to deal with various input lengths. As the input length increases, the error measures of MultiPatchFormer continue to decrease, demonstrating its robustness in handling long and noisy sequences evidenced by Table 5, which indicates an MSE reduction of more than  $15\\%$ ,  $11\\%$  and  $10\\%$  achieved on the ETTm1, Weather and Electricity datasets when prolonging the input length from 96 to 720. In this experiment, the best performing predictive models are selected and the results for input length of 48, 192 and 336 are visualized in Fig. 4. According to this figure, MultiPatchFormer consistently outperforms other baselines in most cases by using longer input lengths. This serves as evidence of efficiency of MultiPatchFormer in utilizing information of long input sequences thanks to multi-scale analysis and capturing cross-variable dependencies. MultiPatchFormer is also compared with other best performing baselines using a shorter input length (e.g. 48) on Electricity dataset. As Fig. 4 illustrates, our model forecasts different prediction lengths by using a short input window  $(\\mathrm{H} = 48)$  with the lowest error, indicating its capability in capturing temporal correlations from shorter input sequences.\n",
  "hyperparameter": "Model dimension (d_model): divided into 4 sub-spaces of d_model/4 for each scale; Patch sizes: {8, 16, 24, 48}; Patch strides: {8, 8, 7, 6} to create equal number of patches across scales; Input length (L): 96 (main experiments), 512 (LLM comparison); Prediction horizons (F): {96, 192, 336, 720}; Learning rate: {1e-3, 1e-4}; Optimizer: Adam; Loss function: L1 loss; Training epochs: 10; Early stopping patience: 3; Number of scales: 4; Batch processing: channels reshaped to ((B,C), L, 1) for independent embedding"
}