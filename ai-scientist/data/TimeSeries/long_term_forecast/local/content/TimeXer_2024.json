{
  "id": "TimeXer_2024",
  "paper_title": "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables",
  "alias": "TimeXer",
  "year": 2024,
  "domain": "TimeSeries",
  "task": "exogenous_forecast",
  "idea": "TimeXer introduces a novel Transformer architecture for time series forecasting with exogenous variables by employing asymmetric embedding strategies: patch-level temporal tokens plus a learnable global token for endogenous variables, and variate-level tokens for exogenous variables. The model uses self-attention to capture temporal dependencies within endogenous series and cross-attention between the endogenous global token and exogenous variate tokens to integrate external information, enabling flexible handling of irregular exogenous data (different frequencies, missing values, misaligned timestamps) while maintaining computational efficiency.",
  "introduction": "# 1 Introduction\n\nTime series forecasting is of pressing demand in real-world scenarios and have been widely used in various application domains, such as meteorology [38, 42], electricity [34], and transportation [27]. Thereof, forecasting with exogenous variables is a prevalent and indispensable forecasting paradigm since the variations within time series data are often influenced by external factors, such as economic indicators, demographic changes, and societal events. For example, electricity prices are highly dependent on the supply and demand of the market, and it is intrinsically impossible to predict future prices solely based on historical data. Incorporating external factors in terms of exogenous variables, as illustrated in Figure 1 (Left), allows for a more comprehensive understanding of the correlations and causalities among various variables, leading to better performance and interpretability.\n\nFrom the perspective of time series modeling, exogenous variables are introduced to the forecaster for informative purposes and do not need to be predicted. The distinction between endogenous and exogenous variables poses unique challenges compared to existing multivariate forecasting methods. First, there are always multiple external factors that are illuminating to the prediction of the target series, which requires models to reconcile the discrepancy and dependency among endogenous\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/ba8d9c71b9d16fd81dcdc5cf9aabc5a1474beb7d229e39702dd8a92414cbf6dc.jpg)  \nProblem Formulation\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/cc17de5aabc6a0b9f8bcc5dba2c5c504b40f130be178e30d4ff134a45ce36fc0.jpg)  \nPractical Situations of Exogenous Variables\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/12b46498043ac02a0afa409476f1da5663a994dba3c4c0e55962ec1949e9049e.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/7b10373dda64d7e184eed856791bf1743dd479976fbfee55306ac0ba7879b015.jpg)  \nFigure 1: Left: The forecasting with exogenous variables paradigm includes inputs from multiple external variables as auxiliary information without the need for forecasting. Right: Model performance comparison on existing electricity price forecasting with exogenous variables benchmarks.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/6c936dcf56b99647fbb76d47294c6d019825115a3ac63435c65ffc1c300c9c86.jpg)\n\nand exogenous variables. Regarding exogenous variables equally with endogenous ones will not only cause significant time and memory complexity but also involve unnecessary interactions from endogenous series to external information. Second, external factors may have a causal effect on endogenous series, so models are expected to reason about the systematic time lags among different variables. Moreover, as a practical forecasting paradigm applied extensively in real scenarios, it is essential for models to tackle irregular and heterogeneous exogenous series, including value missing, temporal misalignment, frequency mismatch, and length discrepancy as showcased in Figure 1(Left).\n\nDespite the success of deep learning models in capturing intricate temporal dependencies in time series data, the incorporation of exogenous variables remains underexplored. A common practice to import them is adding or concatenating exogenous features to the endogenous ones. However, given the crucial role of exogenous variables in forecasting, it is imperative to incorporate them precisely and properly. Recent Transformers [32] have exhibited remarkable performance in time series forecasting due to their capability of capturing both temporal dependencies and multivariate correlations. Based on the working dimensions of the attention mechanism, existing Transformer-based works can be roughly divided into patch-oriented models and variate-oriented models. Patching is a basic module to preserve the semantic information underlying temporal variations. Therefore, the attention mechanism is applied over patches to unearth the intricate temporal patterns. Based on the channel independence assumption, PatchTST and follow-ups [28] are capable of capturing temporal dependencies but weak at capturing multivariate correlations. In contrast, variate-oriented models represented by iTransformer [23] successfully reason about interrelationships between variables by considering each variate of time series as a single token and applying attention over multiple variate tokens. Unfortunately, they lack the ability to capture internal temporal variations since the whole series is embedded into a coarse variate token by a temporal linear projection.\n\nTo enable accurate forecasting with exogenous variables in real-world scenarios, it is indispensable to capture both the intra-endogenous temporal dependencies and inter-series correlations between endogenous and exogenous variables. Based on the above observations, we believe that modeling the temporal-wise and variate-wise dependencies within time series data requires hierarchical representations at different levels. In this paper, we unleash the potential of the canonical Transformer without modifying any component, and propose a Time Series Transformer with eXogenous variables (TimeXer). Technologically, we leverage representations and perform attention mechanisms at both patch and variate levels. First, the endogenous patch-level tokens are applied to capture temporal dependencies. Second, to tackle the arbitrarily irregular exogenous variables, TimeXer adopts their variate-level representations to seamlessly ingest the impact of external factors on endogenous ones. Third, inspired by Vision Transformers [10], we introduce learnable global tokens for each endogenous series to reflect the macroscopic information of the series, which interact simultaneously with patch-level endogenous tokens and variate-level exogenous tokens. Throughout this information pathway, the external information can be propagated effectively and selectively to corresponding endogenous patches. In summary, our contributions can be listed as follows.\n\n- Motivated by the universality and importance of exogenous variables in time series forecasting, we empower the canonical Transformer to simultaneously modeling exogenous and endogenous variables without any architectural modifications.  \n- We propose a simple and general TimeXer model, which employs patch-level and variate-level representations respectively for endogenous and exogenous variables, with an en\n\ndogenous global token as a bridge in-between. With this design, TimeXer can capture intra-endogenous temporal dependencies and exogenous-to-endogenous correlations jointly.\n\n- Extensive experiments on twelve datasets show that TimeXer can better utilize exogenous information to facilitate endogenous forecasting, in both univariate and multivariate settings.\n",
  "method": "# 3 TimeXer\n\nIn forecasting with exogenous variables, the endogenous series is the target to be predicted, while the exogenous series are covariates that provide valuable information to boost endogenous predictability.\n\nProblem Settings In forecasting with exogenous variables, we are given an endogenous time series  $\\mathbf{x}_{1:T} = \\{x_1,x_2,\\dots,x_T\\} \\in \\mathbb{R}^{T\\times 1}$  and multiple exogenous series  $\\mathbf{z}_{1:T_{\\mathrm{ex}}} = \\{\\mathbf{z}_{1:T_{\\mathrm{ex}}}^{(1)},\\mathbf{z}_{1:T_{\\mathrm{ex}}}^{(2)},\\dots,\\mathbf{z}_{1:T_{\\mathrm{ex}}}^{(C)}\\} \\in \\mathbb{R}^{T_{\\mathrm{ex}}\\times C}$ . Here  $x_{i}$  denotes the value at the  $i$ -th time point,  $\\mathbf{z}_{1:T_{\\mathrm{ex}}}^{(i)}$  represents the  $i$ -th exogenous variable, and  $C$  is the number of exogenous variables. In addition,  $T$  and  $T_{\\mathrm{ex}}$  are the look-back window lengths of the endogenous and exogenous variables respectively. Noteworthy, any series that provides useful information for endogenous forecasting can be used as an exogenous variable, regardless of their look-back lengths, so we relax to the most flexible settings with  $T_{\\mathrm{ex}} \\neq T$ . The goal of forecasting model  $\\mathcal{F}_{\\theta}$  parameterized by  $\\theta$  is to predict the future  $S$  time steps  $\\widehat{\\mathbf{x}} = \\{x_{T + 1},x_{T + 2},\\dots,x_{T + S}\\}$  based on both historical observations  $\\mathbf{x}_{1:T}$  and corresponding exogenous series  $\\mathbf{z}_{1:T_{\\mathrm{ex}}}$ :\n\n$$\n\\widehat {\\mathbf {x}} _ {T + 1: T + S} = \\mathcal {F} _ {\\theta} \\left(\\mathbf {x} _ {1: T}, \\mathbf {z} _ {1: T _ {\\mathrm {e x}}}\\right). \\tag {1}\n$$\n\nStructure Overview As shown in Figure 2, the proposed TimeXer model repurposes the canonical Transformer without modifying any component, while endogenous and exogenous variables are manipulated by different embedding strategies. TimeXer adopts self-attention and cross-attention to capture temporal-wise and variate-wise dependencies respectively.\n\nEndogenous Embedding Most of the existing Transformer-based forecasting models embed each time point or a segment of time series as a temporal token and apply self-attention to learn temporal dependencies. To finely capture temporal variations within the endogenous variable, TimeXer adopts patch-wise representations. Concretely, the endogenous series is split into non-overlapping patches, and each patch is projected to a temporal token. Given the distinct roles of endogenous and exogenous variables in the prediction, TimeXer embeds them at different granularity. Therefore, directly combining endogenous tokens and exogenous tokens at different granularity will result in information misalignment. To address this, we introduce a learnable global token for each endogenous variable that serves as the macroscopic representation to interact with exogenous variables. This design helps bridge the causal information from the exogenous series to the endogenous temporal patches. The overall endogenous embedding is formally stated as:\n\n$$\n\\left\\{\\mathbf {s} _ {1}, \\mathbf {s} _ {2}, \\dots , \\mathbf {s} _ {N} \\right\\} = \\text {P a t c h i f y} (\\mathbf {x}),\n$$\n\n$$\n\\mathbf {P} _ {\\mathrm {e n}} = \\operatorname {P a t c h E m b e d} \\left(\\mathbf {s} _ {1}, \\mathbf {s} _ {2}, \\dots , \\mathbf {s} _ {N}\\right), \\tag {2}\n$$\n\n$$\n\\mathbf {G} _ {\\mathrm {e n}} = \\text {L e a r n a b l e} (\\mathbf {x}).\n$$\n\nDenote by  $P$  the patch length, by  $N = \\left\\lfloor \\frac{T}{P}\\right\\rfloor$  the number of patches split from the endogenous series, and by  $\\mathbf{s}_i$  the  $i$ -th patch. PatchEmbed  $(\\cdot)$  maps each length- $P$  patch, added by its position embedding, into a  $D$ -dimensional vector via a trainable linear projector. In all,  $N$  patch-level temporal token embeddings  $\\mathbf{P}_{\\mathrm{en}}$  and 1 series-level global token embedding  $\\mathbf{G}_{\\mathrm{en}}$  are fed into the Transformer encoder.\n\nExogenous Embedding The primary use of exogenous variables is to facilitate accurate forecasting of endogenous variables. We will show in Appendix B.3 that the interactions of different variables can be captured more naturally by variate-level representations, which are adaptive to arbitrary irregularities such as missing values, misaligned timestamps, different frequencies, or discrepant look-back lengths. In contrast, patch-level representations are overly fine-grained for exogenous variables, introducing not only significant computational complexity but also unnecessary noise information. These insights lead to a design that each exogenous series is embedded in a series-wise variate token, which is formalized as:\n\n$$\n\\mathbf {V} _ {\\mathrm {e x}, i} = \\text {V a r i a t e E m b e d} \\left(\\mathbf {z} ^ {(i)}\\right), i \\in \\{1, \\dots , C \\}. \\tag {3}\n$$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/3854750502eb8c93723a224ec68ba59652983522c0e988540e2eda2e393f5ed5.jpg)  \nFigure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.\n\nHere VariateEmbed:  $\\mathbb{R}^{T_{\\mathrm{ex}}} \\to \\mathbb{R}^D$  is a trainable linear projector,  $T_{\\mathrm{ex}}$  is the look-back length of exogenous series, and  $\\mathbf{V}_{\\mathrm{ex}} = \\{\\mathbf{V}_{\\mathrm{ex},i}\\}_{i=1}^{C}$  is the set of representations for multiple exogenous series.\n\nEndogenous Self-Attention For accurate time series forecasting, it is vital to discover intrinsic temporal dependencies within the endogenous variable, as well as the interactions with the variate-level representations from exogenous variables. In addition to self-attention over endogenous temporal tokens (Patch-to-Patch), the learnable global token builds a bridge between endogenous and exogenous variables. Concretely, the global token plays an asymmetric role in cross-attention: (1) Patch-to-Global: the global token attends to temporal tokens for aggregating patch-level information across the entire series; (2) Global-to-Patch: each temporal token attends to the global token for receiving the variate-level correlations. This provides a comprehensive view of the temporal dependencies within the endogenous variable, as well as better interactions with the arbitrarily irregular exogenous variables. The attention mechanism can be formalized as follows:\n\n$$\n\\text {P a t c h - t o - P a t c h :} \\quad \\widehat {\\mathbf {P}} _ {\\mathrm {e n}} ^ {l, 1} = \\text {L a y e r N o r m} \\left(\\mathbf {P} _ {\\mathrm {e n}} ^ {l} + \\text {S e l f - A t t e n t i o n} \\left(\\mathbf {P} _ {\\mathrm {e n}} ^ {l}\\right)\\right),\n$$\n\n$$\n\\text {G l o b a l - t o - P a t c h :} \\quad \\widehat {\\mathbf {P}} _ {\\mathrm {e n}} ^ {l, 2} = \\text {L a y e r N o r m} \\left(\\mathbf {P} _ {\\mathrm {e n}} ^ {l} + \\text {C r o s s - A t t e n t i o n} \\left(\\mathbf {P} _ {\\mathrm {e n}} ^ {l}, \\mathbf {G} _ {\\mathrm {e n}} ^ {l}\\right)\\right), \\tag {4}\n$$\n\n$$\n\\text {P a t c h - t o - G l o b a l :} \\quad \\widehat {\\mathbf {G}} _ {\\mathrm {e n}} ^ {l} = \\text {L a y e r N o r m} \\left(\\mathbf {G} _ {\\mathrm {e n}} ^ {l} + \\text {C r o s s - A t t e n t i o n} \\left(\\mathbf {G} _ {\\mathrm {e n}} ^ {l}, \\mathbf {P} _ {\\mathrm {e n}} ^ {l}\\right)\\right).\n$$\n\nThe overall process can be simplified into an endogenous self-attention computation:\n\n$$\n\\widehat {\\mathbf {P}} _ {\\mathrm {e n}} ^ {l}, \\widehat {\\mathbf {G}} _ {\\mathrm {e n}} ^ {l} = \\text {L a y e r N o r m} \\left(\\left[ \\mathbf {P} _ {\\mathrm {e n}} ^ {l}, \\mathbf {G} _ {\\mathrm {e n}} ^ {l} \\right] + \\text {S e l f - A t t e n t i o n} \\left(\\left[ \\mathbf {P} _ {\\mathrm {e n}} ^ {l}, \\mathbf {G} _ {\\mathrm {e n}} ^ {l} \\right]\\right)\\right). \\tag {5}\n$$\n\nwhere  $l \\in \\{0, \\dots, L - 1\\}$  denotes the  $l$ -th TimeXer block, and  $\\mathbf{P}_{\\mathrm{en}}^{0} = \\mathbf{P}_{\\mathrm{en}}, \\mathbf{G}_{\\mathrm{en}}^{0} = \\mathbf{G}_{\\mathrm{en}}$ . Here,  $[\\cdot, \\cdot]$  denotes the concatenation of the patch-wise tokens and global token of the endogenous variable along the sequence dimension. By adopting a self-attention layer over the concatenated tokens  $\\left[\\mathbf{P}_{\\mathrm{en}}^{l}, \\mathbf{G}_{\\mathrm{en}}^{l}\\right]$  of the endogenous series, TimeXer can capture temporal dependencies between patches and the relationships between each patch to the entire series simultaneously.\n\nExogenous-to-Endogenous Cross-Attention Cross-attention has been widely used in multi-modal learning [17] to capture the adaptive token-wise dependencies between different modalities. In TimeXer, the cross-attention layer takes the endogenous variable as query and the exogenous variable as key and value to build the connections between the two types of variables. Since the exogenous variables are embedded into variate-level tokens, we use the learned global token of the endogenous variable to aggregate information from exogenous variables. The above process can be formalized as\n\n$$\n\\text {V a r i a t e - t o - G l o b a l :} \\quad \\widehat {\\mathbf {G}} _ {\\mathrm {e n}} ^ {l} = \\text {L a y e r N o r m} \\left(\\widehat {\\mathbf {G}} _ {\\mathrm {e n}} ^ {l} + \\text {C r o s s - A t t e n t i o n} \\left(\\widehat {\\mathbf {G}} _ {\\mathrm {e n}} ^ {l}, \\mathbf {V} _ {\\mathrm {e x}}\\right)\\right). \\tag {6}\n$$\n\nFinally, all temporal tokens and the learnable global token will be transformed by the feedforward layer, which is formally stated as:\n\n$$\n\\mathbf {P} _ {\\mathrm {e n}} ^ {l + 1} = \\text {F e e d - F o r w a r d} \\left(\\widehat {\\mathbf {P}} _ {\\mathrm {e n}} ^ {l}\\right), \\mathbf {G} _ {\\mathrm {e n}} ^ {l + 1} = \\text {F e e d - F o r w a r d} \\left(\\widehat {\\mathbf {G}} _ {\\mathrm {e n}} ^ {l}\\right), \\tag {7}\n$$\n\nwhere  $l \\in \\{1, \\ldots, L\\}$ . We write each Transformer block as  $\\mathbf{P}_{\\mathrm{en}}^{l+1}$ ,  $\\mathbf{G}_{\\mathrm{en}}^{l+1} = \\mathrm{TrmBlock}(\\mathbf{P}_{\\mathrm{en}}^l, \\mathbf{G}_{\\mathrm{en}}^l)$ .\n\nForecasting Loss In time series forecasting with exogenous variables, the exogenous variables do not need to be predicted. So we generate the forecast  $\\hat{\\mathbf{x}}$  by applying a linear projection on the endogenous output embeddings  $[\\mathbf{P}_{\\mathrm{en}}^{L},\\mathbf{G}_{\\mathrm{en}}^{L}]$ , a common practice in the encoder-only forecasters. We employ the squared loss (L2) to measure the discrepancy between the prediction and the ground truth:\n\n$$\n\\operatorname {L o s s} = \\sum_ {i = 1} ^ {S} \\left\\| \\mathbf {x} _ {i} - \\widehat {\\mathbf {x}} _ {i} \\right\\| _ {2} ^ {2}, \\quad \\text {w h e r e} \\widehat {\\mathbf {x}} = \\operatorname {P r o j e c t i o n} \\left(\\left[ \\mathbf {P} _ {\\mathrm {e n}} ^ {L}, \\mathbf {G} _ {\\mathrm {e n}} ^ {L} \\right]\\right). \\tag {8}\n$$\n\nParallel Multivariate Forecasting Multivariate forecasting can be viewed as predicting each variable in the multivariate data, with the other variables treated as exogenous ones. So for each variable, the other variables are leveraged by TimeXer to facilitate more accurate and causal prediction. Our key discovery is that forecasting with exogenous variables can be a unified forecasting paradigm that generalizes straightforwardly to multivariate forecasting. By employing the channel independence mechanism, for each variable of the multivariate, it is treated as the endogenous one. Then TimeXer is applied in a parallel manner for all variables with shared self-attention and cross-attention layers.\n",
  "experiments": "# 4 Experiments\n\nTo verify the effectiveness and generality of TimeXer, we extensively experiment under two different time series paradigms, i.e. short-term forecasting with exogenous variables and long-term multivariate forecasting, on a diverse range of real-world time series datasets from different domains. We also conduct experiments on long-term forecasting with exogenous variables on the multivariate benchmark, which are presented in Appendix I.3.\n\nDatasets For short-term forecasting tasks, we include short-term electricity price forecasting datasets (EPF) [15], which is a real-world forecasting with exogenous various benchmarks derived from five major power market data spanning six years each. Each dataset contains electricity price as an endogenous variable and two influential exogenous variables in practice. Meanwhile, we adopt seven well-established public long-term multivariate forecasting benchmarks [33] to evaluate the performance of TimeXer in multivariate forecasting.\n\nBaselines We include nine state-of-the-art deep forecasting models, including Transformer-based models: iTransformer [23], PatchTST [28], Crossformer [43], Autoformer [37], CNN-based models: TimesNet [36], SCINet [21], and linear-based models: RLinear [19], DLinear [41], TiDE [5]. Notably, TiDE is a recently developed advanced forecaster specifically designed for exogenous variables.\n\nImplementation Details For short-term electricity price prediction, we follow the standard protocol of NBEATSx [29], where the input series length and prediction length are respectively set as 168 and 24. In addition, we set the patch length as 24 without overlapping. For long-term forecasting datasets, we uniformly use the patch length 16 and fix the length of the look-back series at 96, while the prediction length varies across four lengths {96, 192, 336, 720}.\n\n# 4.1 Main Results\n\nComprehensive forecasting results for short-term and long-term forecasting are listed in Table 2 and Table 3. A lower MSE or MAE indicates better forecasting performance.\n\nThe short-term electricity price forecasting task is derived from real-world scenarios, and presents a unique challenge for the forecasting model for the endogenous variable has been shown to be highly correlated with two exogenous variables in the dataset. Since the interactions between different variables are crucial for this task, linear forecasters, including RLinear [19] and DLinear [41], fail to triumph over Transformer-based forecasters. Similar to TimeXer, Crossformer divides all input series into different segments and captures multivariate correlations over all segments; However,\n\nit fails to outperform other baselines which indicates that modeling all variables at a granular level introduces unnecessary noise into the forecasting. Also designed for capturing cross-variate dependency, iTransformer neglects the temporal-wise attention module, indicating that there are still limitations in capturing temporal dependencies solely through linear projection. By contrast, our proposed TimeXer effectively integrates information from exogenous variables while capturing temporal dependencies of endogenous series. As shown in Table 2, TimeXer achieves consistent state-of-the-art performance on all five datasets, outperforming various baseline models.\n\nTable 2: Full results of the short-term forecasting task on EPF dataset. We follow the standard protocol in short-term electricity price forecasting, where the input length and predict length are set to 168 and 24 respectively for all baselines. Avg means the average results from all five datasets.  \n\n<table><tr><td>Model</td><td>TimeXer</td><td>iTransformer</td><td>RLinear</td><td>PatchTST</td><td>Crossformer</td><td>TiDE</td><td>TimesNet</td><td>DLinear</td><td>SCINet</td><td>Autoformer</td></tr><tr><td>Metric</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>NP</td><td>0.236 0.268</td><td>0.265</td><td>0.300</td><td>0.335</td><td>0.284</td><td>0.240</td><td>0.285</td><td>0.335</td><td>0.340</td><td>0.309</td></tr><tr><td>PJM</td><td>0.093 0.192</td><td>0.097</td><td>0.197</td><td>0.124</td><td>0.229</td><td>0.106</td><td>0.209</td><td>0.101</td><td>0.199</td><td>0.124</td></tr><tr><td>BE</td><td>0.379 0.243</td><td>0.394</td><td>0.270</td><td>0.520</td><td>0.337</td><td>0.400</td><td>0.262</td><td>0.420</td><td>0.290</td><td>0.523</td></tr><tr><td>FR</td><td>0.385 0.208</td><td>0.439</td><td>0.233</td><td>0.507</td><td>0.290</td><td>0.411</td><td>0.220</td><td>0.434</td><td>0.208</td><td>0.510</td></tr><tr><td>DE</td><td>0.440 0.415</td><td>0.479</td><td>0.443</td><td>0.574</td><td>0.498</td><td>0.461</td><td>0.432</td><td>0.574</td><td>0.430</td><td>0.568</td></tr><tr><td>AVG</td><td>0.307 0.265</td><td>0.335</td><td>0.289</td><td>0.412</td><td>0.339</td><td>0.330</td><td>0.282</td><td>0.354</td><td>0.284</td><td>0.412</td></tr></table>\n\nWe also evaluate TimeXer on well-established public benchmarks for conventional multivariate long-term forecasting. As mentioned above, TimeXer has the ability to perform multivariate forecasting by employing the channel independence mechanism. We present the results averaged from all four prediction lengths in Table 3. It can be observed that TimeXer achieves consistent state-of-the-art performance on most of the datasets, highlighting its effectiveness and generality. In addition, since TimeXer is initially designed for exogenous variables, we also conduct vanilla forecasting with exogenous variables on these datasets by taking the last dimension of the multivariate data as endogenous series and others as exogenous variables. Detailed results are listed in Appendix I.3.\n\nTable 3: Multivariate forecasting results. We compare extensive competitive models under different prediction lengths following the setting of iTransformer [23]. The look-back length  $L$  is set to 96 for all baselines. Results are averaged from all prediction lengths  $S = \\{ {96},{192},{336},{720}\\}$  .  \n\n<table><tr><td>Model</td><td>TimeXer</td><td>iTransformer</td><td>RLinear</td><td>PatchTST</td><td>Crossformer</td><td>TiDE</td><td>TimesNet</td><td>DLinear</td><td>SCINet</td><td>Autoformer</td></tr><tr><td>Metric</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>ECL</td><td>0.171 0.270</td><td>0.178 0.270</td><td>0.219 0.298</td><td>0.205 0.290</td><td>0.244 0.334</td><td>0.251 0.244</td><td>0.192 0.295</td><td>0.212 0.300</td><td>0.268 0.365</td><td>0.227 0.338</td></tr><tr><td>Weather</td><td>0.241 0.271</td><td>0.258 0.278</td><td>0.272 0.291</td><td>0.259 0.281</td><td>0.259 0.315</td><td>0.271 0.320</td><td>0.259 0.287</td><td>0.265 0.317</td><td>0.292 0.363</td><td>0.338 0.382</td></tr><tr><td>ETTh1</td><td>0.437 0.437</td><td>0.454 0.447</td><td>0.446 0.434</td><td>0.469 0.454</td><td>0.529 0.522</td><td>0.541 0.507</td><td>0.458 0.450</td><td>0.456 0.452</td><td>0.747 0.647</td><td>0.496 0.487</td></tr><tr><td>ETTh2</td><td>0.367 0.396</td><td>0.383 0.407</td><td>0.374 0.398</td><td>0.387 0.407</td><td>0.942 0.684</td><td>0.611 0.550</td><td>0.414 0.427</td><td>0.559 0.515</td><td>0.954 0.723</td><td>0.450 0.459</td></tr><tr><td>ETTm1</td><td>0.382 0.397</td><td>0.407 0.410</td><td>0.414 0.407</td><td>0.387 0.400</td><td>0.512 0.496</td><td>0.419 0.419</td><td>0.400 0.406</td><td>0.403 0.407</td><td>0.485 0.481</td><td>0.588 0.517</td></tr><tr><td>ETTm2</td><td>0.274 0.322</td><td>0.288 0.332</td><td>0.286 0.327</td><td>0.281 0.326</td><td>0.757 0.610</td><td>0.358 0.404</td><td>0.291 0.333</td><td>0.350 0.401</td><td>0.571 0.537</td><td>0.327 0.371</td></tr><tr><td>Traffic</td><td>0.466 0.287</td><td>0.428 0.282</td><td>0.626 0.378</td><td>0.481 0.304</td><td>0.550 0.304</td><td>0.760 0.473</td><td>0.620 0.336</td><td>0.625 0.383</td><td>0.804 0.509</td><td>0.628 0.379</td></tr></table>\n\n# 4.2 Ablation Study\n\nIn TimeXer, three types of tokens are used to capture temporal-wise and variate-wise dependencies, including multiple patch-level temporal tokens, learnable global tokens of the endogenous variables, and multiple variate-level exogenous tokens. Besides, to incorporate the information from exogenous variables, TimeXer adopts a cross-attention layer to model the mutual relationship between different variables. To elaborate on the validity of TimeXer, we conducted detailed ablations covering both the embedding module and the inclusion of exogenous factors. Specifically, for the embedding design, we replace or remove existing components of the embedded vector from exogenous and endogenous variables respectively. Moreover, we keep the existing embedding design and replace the cross-attention by adding the variate token of exogenous variables to the variate token of endogenous variables or concatenating all the variate tokens and temporal tokens. As listed in Table 4, TimeXer exhibits superior performance compared to various architectural designs across all datasets.\n\nTable 4: Ablation Results. Ex. and En. are abbreviations for Exogenous variable and Endogenous variable.  $P, G$  and  $V$  denote patch token,learnable global token,and variate token respectively.  \n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Design</td><td rowspan=\"2\">En.</td><td rowspan=\"2\">Ex.</td><td colspan=\"2\">NP</td><td colspan=\"2\">PJM</td><td colspan=\"2\">BE</td><td colspan=\"2\">FR</td><td colspan=\"2\">DE</td><td colspan=\"2\">AVG</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"3\">Cross</td><td>Ours</td><td>P+G</td><td>V</td><td>0.236</td><td>0.268</td><td>0.093</td><td>0.192</td><td>0.379</td><td>0.243</td><td>0.385</td><td>0.208</td><td>0.440</td><td>0.415</td><td>0.307</td><td>0.265</td></tr><tr><td>Replace</td><td>P+G</td><td>P</td><td>0.237</td><td>0.269</td><td>0.101</td><td>0.196</td><td>0.376</td><td>0.246</td><td>0.390</td><td>0.206</td><td>0.457</td><td>0.422</td><td>0.312</td><td>0.268</td></tr><tr><td>Remove</td><td>P</td><td>V</td><td>0.239</td><td>0.273</td><td>0.106</td><td>0.200</td><td>0.381</td><td>0.260</td><td>0.393</td><td>0.208</td><td>0.468</td><td>0.425</td><td>0.316</td><td>0.273</td></tr><tr><td colspan=\"2\">Add</td><td>P+G</td><td>V</td><td>0.247</td><td>0.272</td><td>0.125</td><td>0.206</td><td>0.387</td><td>0.247</td><td>0.404</td><td>0.209</td><td>0.483</td><td>0.430</td><td>0.329</td><td>0.273</td></tr><tr><td colspan=\"2\">Concatenate</td><td>P+G</td><td>V</td><td>0.237</td><td>0.266</td><td>0.098</td><td>0.196</td><td>0.383</td><td>0.255</td><td>0.390</td><td>0.209</td><td>0.450</td><td>0.423</td><td>0.312</td><td>0.270</td></tr></table>\n\n# 4.3 TimeXer Generality\n\n# 4.3.1 Practical Situations\n\nIncreasing Look-back Length Theoretically, the forecasting performance of the model could potentially benefit from increasing the look-back length of time series, as a longer historical context encompasses more comprehensive information. However, the attention will be distracted when the look-back length becomes excessively long. In TimeXer, we use the variate-level representation of exogenous variables which allows for the misalignment between endogenous and exogenous variables. This is particularly valuable in real-world scenarios where the time series data may be collected from a newly introduced sensor that has limited historical data. Therefore, we conducted three different experimental settings to assess the generality of TimeXer by increasing the length of either the endogenous or exogenous series, which include \"Fix Endogenous and increase Exogenous\", \"Increase Endogenous and Fix exogenous\", and \"Increase Endogenous and Exogenous\". Results shown in Figure 3 reveal that TimeXer can be adapted to situations where the look-back of endogenous and exogenous are mismatched. Moreover, extending the look-back length indeed yields improvements in forecasting performance. Compared to enlarging the historical exogenous series, increasing the look-back length of the endogenous series brings greater benefits to the model, and the performance is further improved with both increases.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/29a98258e180c24d7c0b1e91f7918410ddbabc9051b79e6a381349aa150ced63.jpg)  \nFigure 3: Performance with the enlarged look-back length varying from \\{96, 192, 336, 512, 720\\}. Different styles of lines represent different prediction lengths. In most cases, the forecasting performance benefits from enlarged look-back lengths of both endogenous and exogenous series.\n\nMissing Values To further verify the generalizability of TimeXer in complex real-world scenarios, we conduct experiments in scenarios where the historical information of time series is missing. Specifically, for both exogenous and endogenous series, we adopt two strategies to evaluate TimeXer's adaptability to series with missing historical information: (1) Zeros: filling the whole series with the scalar value 0. (2) Random: substituting the whole series with random values from a uniform distribution on the interval [0, 1). As shown in Table 5, the forecasting results deteriorate when exogenous variables are replaced with meaningless noise, indicating that the model's performance benefits from the inclusion of informative exogenous variables. Interestingly, neither using zero-filled exogenous series nor employing exogenous series with random numbers results in a significant\n\nTable 5: Model performance under missing values. Zeros and Random represent the cases that the corresponding series is set as zeros or random values respectively.  \n\n<table><tr><td rowspan=\"2\">Variate</td><td rowspan=\"2\">Strategies</td><td colspan=\"2\">NP</td><td colspan=\"2\">PJM</td><td colspan=\"2\">BE</td><td colspan=\"2\">FR</td><td colspan=\"2\">DE</td><td colspan=\"2\">AVG</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"2\">Endogenous</td><td>Zeros</td><td>2.954</td><td>1.396</td><td>0.188</td><td>0.288</td><td>0.930</td><td>0.664</td><td>0.781</td><td>0.534</td><td>0.774</td><td>0.559</td><td>1.125</td><td>0.688</td></tr><tr><td>Random</td><td>3.140</td><td>1.450</td><td>0.233</td><td>0.325</td><td>0.926</td><td>0.667</td><td>0.761</td><td>0.527</td><td>0.692</td><td>0.533</td><td>1.150</td><td>0.701</td></tr><tr><td rowspan=\"2\">Exogenous</td><td>Zeros</td><td>0.257</td><td>0.278</td><td>0.108</td><td>0.210</td><td>0.400</td><td>0.254</td><td>0.416</td><td>0.214</td><td>0.471</td><td>0.430</td><td>0.330</td><td>0.277</td></tr><tr><td>Random</td><td>0.258</td><td>0.280</td><td>0.110</td><td>0.212</td><td>0.399</td><td>0.253</td><td>0.424</td><td>0.221</td><td>0.475</td><td>0.432</td><td>0.333</td><td>0.280</td></tr><tr><td colspan=\"2\">TimeXer</td><td>0.236</td><td>0.268</td><td>0.093</td><td>0.192</td><td>0.379</td><td>0.243</td><td>0.385</td><td>0.208</td><td>0.440</td><td>0.415</td><td>0.307</td><td>0.265</td></tr></table>\n\ndecline in model performance. This robustness can be attributed to TimeXer's design, which uses two attention layers to model endogenous temporal dependencies and the multivariate correlations between endogenous and exogenous variables respectively. This architecture allows endogenous temporal representations to dominate the predictions, ensuring consistent performance even in the presence of uninformative exogenous data. Consequently, it can be observed that when the endogenous series is replaced with meaningless zeros or random values, rendering the time series unpredictable, there is a significant decline in model performance. This underscores that TimeXer's performance is closely tied to the quality of endogenous series, deteriorating markedly when the historical information is severely limited.\n\n# 4.3.2 Scalability\n\nSince recent Transformer-based forecasters have demonstrated promising scalability, leading to the success of Large Time Series Models, we explore the scalability of TimeXer on large-scale time series data. Specifically, we build a large-scale weather dataset for forecasting with exogenous variables. The endogenous series is the hourly temperature of 3,850 stations worldwide, spanning from January 1, 2019, to December 31, 2020, which can be downloaded from the National Centers for Environmental Information (NCEI) [1] and has been well-processed by [38]. Further, we utilize meteorological indicators of corresponding adjacent areas from ERA5 [11] as exogenous variables, which is with a sampling interval of 3 hours. The adjacent area is defined as the  $3 \\times 3$  grid centered on the endogenous weather station, with four meteorological variables per grid cell, totaling 36 exogenous variables. We set the historical horizon of endogenous and exogenous to be 7 days to predict the endogenous variable for the next 3 days. Noteworthy, this is a complex forecasting scenario as we aforementioned where the frequencies of endogenous and exogenous are different. We choose existing state-of-the-art multivariate forecasters as baselines and use identical hidden dimensions and batch sizes for a fair comparison. Since baseline forecasters cannot handle mismatched series, we interpolate the exogenous series into hourly data using the nearest values. Figure 4 demonstrates that TimeXer surpasses other baselines, verifying its capability to handle large-scale forecasting tasks.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/70f3d604af3fe360ac7bacf50390f631d56067028b1a5bac732490e5c3b1b661.jpg)  \nFigure 4: Forecasting performance on large-scale time series datasets. Left: Illustration of the forecasting scenario. The endogenous is the temperature collected from weather stations, and the exogenous variables are meteorological indicators from the surrounding  $3 \\times 3$  grids including the weather station. Each area contains four types of information, namely, temperature, pressure, u- and v- components of wind. Right: TimeXer outperforms other advanced forecasters.\n\n# 4.4 Model Analysis\n\n**Variate-wise Correlations** TimeXer adopts cross-attention between the global endogenous token and variable-level exogenous tokens to capture the multivariate correlation, enhancing the interpretability of the learned attention map. To validate the rationale behind attention on variate tokens, we visualize the learned attention map alongside the time series of the highest and lowest attention scores. As illustrated in Figure 5 (Left), the case study on the Weather dataset reveals a notable distinction in the attention maps of endogenous variables with different exogenous variables. This demonstrates that TimeXer has the ability to distinguish between exogenous variables, allocating greater attention to those that are most informative for prediction, thereby resulting in a more focused and interpretable attention map. Additionally, it is observed that exogenous series exhibiting similar shapes to the endogenous series tend to receive more attention. This phenomenon may arise because time series with analogous shapes often share temporal features, leading to higher similarity scores. Consequently, the exogenous series most prominently highlighted by the attention mechanism may intuitively resemble the endogenous variable. Furthermore, physical interpretations for the visualized are provided. For the endogenous variable CO2-Concentration, there is indeed a strong correlation between it and Air Density, while the Maximum Wind Velocity has a relatively minor impact, which validates the effectiveness of TimeXer.\n\nModel Efficiency To evaluate the efficiency of TimeXer, we evaluate the training time and memory footprint of TimeXer on forecasting with exogenous variables compared with six baseline models with the identical hidden dimension and batch size for a fair comparison. We present the results on the ECL dataset with 320 exogenous variables in Figure 5 (Right). It is notable that when faced with numerous variables TimeXer exhibits its advantage by outperforming iTransformer in terms of memory footprint. Notably, iTransformer embeds each variate series into one token and applies a self-attention mechanism among all variate tokens, whether endogenous or exogenous. Although this design can keep refining the learned variate token in multiple layers, it does cause more complexity. As for TimeXer, exogenous variables will be embedded to variate tokens at the beginning, which will be shared in all layers and interact with the endogenous global token by cross-attention. Thus, TimeXer omits the interaction among learned exogenous variate tokens, resulting in favorable efficiency. We provide a comprehensive theoretical analysis of the model efficiency in Appendix E.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/4ff00fd2-4f27-48ca-9996-08aa36970b67/6a3d878523f8f7e81e9e263a555b21f65f9777e9a1418b46e3fc52447ebb1dde.jpg)  \nFigure 5: Model analysis of TimeXer. Left: Visualization of learned attention map and the endogenous time series and exogenous time series with highest and lowest attention scores. Right: Model efficiency comparison under the forecasting with exogenous variables paradigm on the ECL dataset.\n",
  "hyperparameter": "Patch length P=24 for short-term electricity price forecasting (EPF datasets) and P=16 for long-term forecasting; Look-back window length T=168 for short-term forecasting and T=96 for long-term forecasting; Prediction lengths S=24 for short-term and Sâˆˆ{96, 192, 336, 720} for long-term forecasting; Hidden dimension D (embedding dimension) is used but specific value not mentioned; Number of Transformer blocks L is used but specific value not mentioned; Loss function: L2 (squared loss/MSE)"
}