{
  "id": "ETSformer_2022",
  "paper_title": "ETSformer: Exponential Smoothing Transformers for Time-series Forecasting",
  "alias": "ETSformer",
  "year": 2022,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "ETSformer redesigns the Transformer architecture for time-series forecasting by incorporating exponential smoothing principles. The core innovations are: (1) Two novel attention mechanisms - Exponential Smoothing Attention (ESA) that assigns higher weights to recent observations based on relative time lag rather than content, and Frequency Attention (FA) that uses Fourier transforms to extract dominant seasonal patterns; (2) A decomposition-based architecture that progressively extracts interpretable level, growth (trend), and seasonal components through stacked encoder-decoder layers, with final forecasts composed as the sum of these components; (3) Efficient O(L log L) complexity implementation using FFT-based algorithms instead of O(L²) attention.",
  "introduction": "# 1 Introduction\n\nTransformer models have achieved great success in the fields of NLP [8, 33] and CV [4, 9] in recent times. The success is widely attributed to its self-attention mechanism which is able to explicitly model both short and long range dependencies adaptively via the pairwise query-key interaction. Owing to their powerful capability to model sequential data, Transformer-based architectures [20, 37, 38, 40, 41] have been actively explored for the time-series forecasting, especially for the more challenging Long Sequence Time-series Forecasting (LSTF) task. While showing promising results, it is still quite challenging to extract salient temporal patterns and thus make accurate long-term forecasts for large-scale data. This is because time-series data is usually noisy and non-stationary. Without incorporating appropriate knowledge about time-series structures [1, 13, 31], it is prone to learning the spurious dependencies and lacks interpretability.\n\nMoreover, the use of content-based, dot-product attention in Transformers is not effective in detecting essential temporal dependencies for two reasons. (1) Firstly, time-series data is usually assumed to be generated by a conditional distribution over past observations, with the dependence between observations weakening over time [17, 23]. Therefore, neighboring data points have similar values, and recent tokens should be given a higher weight when measuring their similarity [13, 14]. This indicates that attention measured by a relative time lag is more effective than that measured by the\n\nsimilarity of the content when modeling time-series. (2) Secondly, many real world time-series display strong seasonality - patterns in time-series which repeat with a fixed period. Automatically extracting seasonal patterns has been proved to be critical for the success of forecasting [5, 6, 36]. However, the vanilla attention mechanism is unlikely able to learn these required periodic dependencies without any in-built prior structure.\n\nTo address these limitations, we propose ETSformer, an effective and efficient Transformer architecture for time-series forecasting, inspired by exponential smoothing methods [13] and illustrated in Figure 1. First of all, ETSformer incorporates inductive biases of time-series structures by performing a layer-wise level, growth, and seasonal decomposition. By leveraging the high capacities of deep architectures and an effective residual learning scheme, ETSformer is able to extract a series of latent growth and seasonal patterns and model their complex dependencies. Secondly, ETSformer introduces a novel Exponential Smoothing Attention (ESA) and Frequency Attention (FA) to replace vanilla attention. In particular, ESA constructs attention scores based on the relative time lag to the query, and achieves  $\\mathcal{O}(L\\log L)$  complexity for the length-  $L$  lookback window and demonstrates powerful capability in modeling the growth component. FA leverages the Fourier transformation to extract the dominating seasonal patterns by selecting the Fourier bases with the  $K$  largest amplitudes in frequency domain, and also achieves\n\n$\\mathcal{O}(L \\log L)$  complexity. Finally, the predicted forecast is a composition of level, trend, and seasonal components, which makes it human interpretable. We conduct extensive empirical analysis and show that ETSformer achieves state-of-the-art performance by outperforming competing approaches over 6 real world datasets on both the multivariate and univariate settings, and also visualize the time-series components to verify its interpretability.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/38f92a0357c220c95520cee1d92be4603fb3b830d4de9710605cadc291ee0a14.jpg)  \nFigure 1: Illustration demonstrating how ETSformer generates forecasts via a decomposition (of intermediate representations) into seasonal and trend components. The seasonal component extracts salient periodic patterns and extrapolates them. The trend component which is a combination of the level and growth terms, first estimates the current level of the time-series, and subsequently adds a damped growth term to generate trend forecasts.\n",
  "method": "# 4 ETSformer\n\nIn this section, we redesign the classical Transformer architecture into an exponential smoothing inspired encoder-decoder architecture specialized for tackling the time-series forecasting problem. Our architecture design methodology relies on three key principles: (1) the architecture leverages the stacking of multiple layers to progressively extract a series of level, growth, and seasonal representations from the intermediate latent residual; (2) following the spirit of exponential smoothing, we\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/67462906a2522e983c2c5ba4083eed3eaaa6f17fee30353df9dcfc6b52d49f4e.jpg)  \nFigure 2: ETSformer model architecture.\n\nextract the salient seasonal patterns while modeling level and growth components by assigning higher weight to recent observations; (3) the final forecast is a composition of level, growth, and seasonal components making it human interpretable. We now expound how our ETSformer architecture encompasses these principles.\n\n# 4.1 Overall Architecture\n\nFigure 2 illustrates the overall encoder-decoder architecture of ETSformer. At each layer, the encoder is designed to iteratively extract growth and seasonal latent components from the lookback window. The level is then extracted in a similar fashion to classical level smoothing in Equation (1). These extracted components are then fed to the decoder to further generate the final  $H$ -step ahead forecast via a composition of level, growth, and seasonal forecasts, which is defined:\n\n$$\n\\hat {\\boldsymbol {X}} _ {t: t + H} = \\boldsymbol {E} _ {t: t + H} + \\operatorname {L i n e a r} \\left(\\sum_ {n = 1} ^ {N} \\left(\\boldsymbol {B} _ {t: t + H} ^ {(n)} + \\boldsymbol {S} _ {t: t + H} ^ {(n)}\\right)\\right), \\tag {3}\n$$\n\nwhere  $\\pmb{E}_{t:t + H} \\in \\mathbb{R}^{H \\times m}$ , and  $\\pmb{B}_{t:t + H}^{(n)}, \\pmb{S}_{t:t + H}^{(n)} \\in \\mathbb{R}^{H \\times d}$  represent the level forecasts, and the growth and seasonal latent representations of each time step in the forecast horizon, respectively. The superscript represents the stack index, for a total of  $N$  encoder stacks. Note that  $\\mathrm{Linear}(\\cdot): \\mathbb{R}^d \\to \\mathbb{R}^m$  operates element-wise along each time step, projecting the extracted growth and seasonal representations from latent to observation space.\n\n# 4.1.1 Input Embedding\n\nRaw signals from the lookback window are mapped to latent space via the input embedding module, defined by  $Z_{t - L:t}^{(0)} = E_{t - L:t}^{(0)} = \\mathrm{Conv}(X_{t - L:t})$ , where Conv is a temporal convolutional filter with kernel size 3, input channel  $m$  and output channel  $d$ . In contrast to prior work [20, 37, 38, 41], the inputs of ETSformer do not rely on any other manually designed dynamic time-dependent covariates (e.g. month-of-year, day-of-week) for both the lookback window and forecast horizon. This is because the proposed Frequency Attention module (details in Section 4.2.2) is able to automatically uncover these seasonal patterns, which renders it more applicable for challenging scenarios without these discriminative covariates and reduces the need for feature engineering.\n\n# 4.1.2 Encoder\n\nThe encoder focuses on extracting a series of latent growth and seasonality representations in a cascaded manner from the lookback window. To achieve this goal, traditional methods rely on the assumption of additive or multiplicative seasonality which has limited capability to express complex patterns beyond these assumptions. Inspired by [10, 24], we leverage residual learning to build an\n\nexpressive, deep architecture to characterize the complex intrinsic patterns. Each layer can be interpreted as sequentially analyzing the input signals. The extracted growth and seasonal signals are then removed from the residual and undergo a nonlinear transformation before moving to the next layer. Each encoder layer takes as input the residual from the previous encoder layer  $Z_{t-L:t}^{(n-1)}$  and emits  $Z_{t-L:t}^{(n)}, B_{t-L:t}^{(n)}, S_{t-L:t}^{(n)}$ , the residual, latent growth, and seasonal representations for the lookback window via the Multi-Head Exponential Smoothing Attention (MH-ESA) and Frequency Attention (FA) modules (detailed description in Section 4.2). The following equations formalize the overall pipeline in each encoder layer, and for ease of exposition, we use the notation := for a variable update.\n\nSeasonal:  $S_{t - L:t}^{(n)} = \\mathrm{FA}_{t - L:t}(Z_{t - L:t}^{(n - 1)})$\n\nGrowth:  $\\pmb{B}_{t - L:t}^{(n)} = \\mathrm{MH - ESA}(\\pmb{Z}_{t - L:t}^{(n - 1)})$\n\n$$\n\\mathbf {Z} _ {t - L: t} ^ {(n - 1)} := \\mathbf {Z} _ {t - L: t} ^ {(n - 1)} - \\mathbf {S} _ {t - L: t} ^ {(n)}\n$$\n\n$$\n\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)} := \\operatorname {L N} (\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)} - \\boldsymbol {B} _ {t - L: t} ^ {(n)})\n$$\n\n$$\n\\boldsymbol {Z} _ {t - L: t} ^ {(n)} = \\operatorname {L N} \\left(\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)} + \\operatorname {F F} \\left(\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)}\\right)\\right)\n$$\n\nLN is layer normalization [2],  $\\mathrm{FF}(x) = \\mathrm{Linear}(\\sigma(\\mathrm{Linear}(x)))$  is a position-wise feedforward network [33] and  $\\sigma(\\cdot)$  is the sigmoid function.\n\nLevel Module Given the latent growth and seasonal representations from each layer, we extract the level at each time step  $t$  in the lookback window in a similar way as the level smoothing equation in Equation (1). Formally, the adjusted level is a weighted average of the current (de-seasonalized) level and the level-growth forecast from the previous time step  $t - 1$ . It can be formulated as:\n\n$$\n\\pmb {E} _ {t} ^ {(n)} = \\pmb {\\alpha} * \\Big (\\pmb {E} _ {t} ^ {(n - 1)} - \\mathrm {L i n e a r} (\\pmb {S} _ {t} ^ {(n)}) \\Big) + (1 - \\pmb {\\alpha}) * \\Big (\\pmb {E} _ {t - 1} ^ {(n)} + \\mathrm {L i n e a r} (\\pmb {B} _ {t - 1} ^ {(n)}) \\Big),\n$$\n\nwhere  $\\alpha \\in \\mathbb{R}^m$  is a learnable smoothing parameter,  $^*$  is an element-wise multiplication term, and Linear  $(\\cdot):\\mathbb{R}^{d}\\to \\mathbb{R}^{m}$  maps representations to observation space. Finally, the extracted level in the last layer  $E_{t - L:t}^{(N)}$  can be regarded as the corresponding level for the lookback window. We show in Appendix A.3 that this recurrent exponential smoothing equation can also be efficiently evaluated using the efficient  $\\mathcal{A}_{\\mathrm{ES}}$  algorithm (Algorithm 1) with an auxiliary term.\n\n# 4.1.3 Decoder\n\nThe decoder is tasked with generating the  $H$ -step ahead forecasts. As shown in Equation (3), the final forecast is a composition of level forecasts  $E_{t:t + H}$ , growth representations  $B_{t:t + H}^{(n)}$  and seasonal representations  $S_{t:t + H}^{(n)}$  in the forecast horizon. It comprises  $N$  Growth + Seasonal (G+S) Stacks, and a Level Stack. The G+S Stack consists of the Growth Damping (GD) and FA blocks, which leverage  $B_{t}^{(n)}$ ,  $S_{t - L:t}^{(n)}$  to predict  $B_{t:t + H}^{(n)}$ ,  $S_{t:t + H}^{(n)}$ , respectively.\n\nGrowth:  $B_{t:t + H}^{(n)} = \\mathrm{TD}(B_t^{(n)})$\n\nSeasonal:  $S_{t:t + H}^{(n)} = \\mathrm{FA}_{t:t + H}(S_{t - L:t}^{(n)})$\n\nTo obtain the level in the forecast horizon, the Level Stack repeats the level in the last time step  $t$  along the forecast horizon. It can be defined as  $\\pmb{E}_{t:t + H} = \\mathrm{Repeat}_H(\\pmb{E}_t^{(N)}) = [\\pmb{E}_t^{(N)},\\dots,\\pmb{E}_t^{(N)}]$ , with  $\\mathrm{Repeat}_H(\\cdot):\\mathbb{R}^{1\\times m}\\to \\mathbb{R}^{H\\times m}$ .\n\nGrowth Damping To obtain the growth representation in the forecast horizon, we follow the idea of trend damping in Equation (2) to make robust multi-step forecast. Thus, the trend representations can be formulated as:\n\n$$\n\\mathrm {T D} (\\boldsymbol {B} _ {t} ^ {(n)}) _ {j} = \\sum_ {i = 1} ^ {j} \\gamma^ {i} \\boldsymbol {B} _ {t} ^ {(n)},\n$$\n\n$$\n\\mathrm {T D} \\left(\\boldsymbol {B} _ {t - L: t} ^ {(n)}\\right) = \\left[ \\mathrm {T D} \\left(\\boldsymbol {B} _ {t} ^ {(n)}\\right) _ {t}, \\dots , \\mathrm {T D} \\left(\\boldsymbol {B} _ {t} ^ {(n)}\\right) _ {t + H - 1} \\right],\n$$\n\nwhere  $0 < \\gamma < 1$  is the damping parameter which is learnable, and in practice, we apply a multi-head version of trend damping by making use of  $n_h$  damping parameters. Similar to the implementation for level forecast in the Level Stack, we only use the last trend representation in the lookback window  $B_t^{(n)}$  to forecast the trend representation in the forecast horizon.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/847bff114fa71ad90fd9de18dc93bc0be42a0366bde2b09b670be9552cb334f6.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/53eaac76ea6bd5c5a1518a6dc497815d7c5dae207b9fa57602d283eeb2d10793.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/cb940886e29a80035e4016025e2c9d5cc3e213b6c91dc45c9b480d412e7cf0e8.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/8cbf72266d3fc321e9a0468acfba64ed55359bb25ed0bcd7fb45e645335d987f.jpg)  \n(a) Full Attention (2017)  \n(d) Autocorrelation (2021)  \nAttention  \nFigure 3: Comparison between different attention mechanisms. (a) Full, (b) Sparse, and (c) Log-sparse Attentions are adaptive mechanisms, where the green circles represent the attention weights adaptively calculated by a point-wise dot-product query, and depends on various factors including the time-series value, additional covariates (e.g. positional encodings, time features, etc.). (d) Autocorrelation attention considers sliding dot-product queries to construct attention weights for each rolled input series. We introduce (e) Exponential Smoothing Attention (ESA) and (f) Frequency Attention (FA). ESA directly computes attention weights based on the relative time lag, without considering the input content, while FA attends to patterns which dominate with large magnitudes in the frequency domain.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/867829c526835067e16b9e953ee3673161b167b692a3f1783a29b6a37a14f453.jpg)  \n(b) Sparse Attention (2020, 2021)  \n(e) Exponential Smoothing Attention (Ours)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/c4aa526053cf987a1bfb6baed62e091d0f87a9cd6b078c710397270e64a96647.jpg)  \n(c) Log-sparse Attention (2019)  \n(f) Frequency Attention (Ours)\n\n# 4.2 Exponential Smoothing Attention and Frequency Attention Mechanism\n\nConsidering the ineffectiveness of existing attention mechanisms in handling time-series data, we develop the Exponential Smoothing Attention (ESA) and Frequency Attention (FA) mechanisms to extract latent growth and seasonal representations. ESA is a non-adaptive, learnable attention scheme with an inductive bias to attend more strongly to recent observations by following an exponential decay, while FA is a non-learnable attention scheme, that leverages Fourier transformation to select dominating seasonal patterns. A comparison between existing work and our proposed ESA and FA is illustrated in Figure 3.\n\n# 4.2.1 Exponential Smoothing Attention\n\nVanilla self-attention can be regarded as a weighted combination of an input sequence, where the weights are normalized alignment scores measuring the similarity between input contents [32]. Inspired by the exponential smoothing in Equation (1), we aim to assign a higher weight to recent observations. It can be regarded as a novel form of attention whose weights are computed by the relative time lag, rather than input content. Thus, the ESA mechanism can be defined as  $\\mathcal{A}_{\\mathrm{ES}}:\\mathbb{R}^{L\\times d}\\to \\mathbb{R}^{L\\times d}$ , where  $\\mathcal{A}_{\\mathrm{ES}}(\\boldsymbol {V})_t\\in \\mathbb{R}^d$  denotes the  $t$ -th row of the output matrix, representing the token corresponding to the  $t$ -th time step. Its exponential smoothing formula can be further written as:\n\n$$\n\\mathcal {A} _ {\\mathrm {E S}} (\\boldsymbol {V}) _ {t} = \\alpha \\boldsymbol {V} _ {t} + (1 - \\alpha) \\mathcal {A} _ {\\mathrm {E S}} (\\boldsymbol {V}) _ {t - 1} = \\sum_ {j = 0} ^ {t - 1} \\alpha (1 - \\alpha) ^ {j} \\boldsymbol {V} _ {t - j} + (1 - \\alpha) ^ {t} \\boldsymbol {v} _ {0},\n$$\n\nwhere  $0 < \\alpha < 1$  and  $\\pmb{v}_0$  are learnable parameters known as the smoothing parameter and initial state respectively.\n\nEfficient  $\\mathcal{A}_{\\mathrm{ES}}$  algorithm The straightforward implementation of the ESA mechanism by constructing the attention matrix,  $A_{\\mathrm{ES}}$  and performing a matrix multiplication with the input sequence (detailed algorithm in Appendix A.4) results in an  $\\mathcal{O}(L^2)$  computational complexity.\n\n$$\n\\mathcal {A} _ {\\mathrm {E S}} (\\boldsymbol {V}) = \\left[ \\begin{array}{c} \\mathcal {A} _ {\\mathrm {E S}} (\\boldsymbol {V}) _ {1} \\\\ \\vdots \\\\ \\mathcal {A} _ {\\mathrm {E S}} (\\boldsymbol {V}) _ {L} \\end{array} \\right] = \\boldsymbol {A} _ {\\mathrm {E S}} \\cdot \\left[ \\begin{array}{c} \\boldsymbol {v} _ {0} ^ {T} \\\\ \\boldsymbol {V} \\end{array} \\right],\n$$\n\nYet, we are able to achieve an efficient algorithm by exploiting the unique structure of the exponential smoothing attention matrix,  $A_{\\mathrm{ES}}$ , which is illustrated in Appendix A.1. Each row of the attention\n\nmatrix can be regarded as iteratively right shifting with padding (ignoring the first column). Thus, a matrix-vector multiplication can be computed with a cross-correlation operation, which in turn has an efficient fast Fourier transform implementation [21]. The full algorithm is described in Algorithm 1, Appendix A.2, achieving an  $\\mathcal{O}(L\\log L)$  complexity.\n\nMulti-Head Exponential Smoothing Attention (MH-ESA) We use  $\\mathcal{A}_{\\mathrm{ES}}$  as a basic building block, and develop the Multi-Head Exponential Smoothing Attention to extract latent growth representations. Formally, we obtain the growth representations by taking the successive difference of the residuals.\n\n$$\n\\tilde {\\boldsymbol {Z}} _ {t - L: t} ^ {(n)} = \\operatorname {L i n e a r} (\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)}),\n$$\n\n$$\n\\boldsymbol {B} _ {t - L: t} ^ {(n)} = \\mathrm {M H -} \\mathcal {A} _ {\\mathrm {E S}} (\\tilde {\\boldsymbol {Z}} _ {t - L: t} ^ {(n)} - [ \\tilde {\\boldsymbol {Z}} _ {t - L: t - 1} ^ {(n)}, \\boldsymbol {v} _ {0} ^ {(n)} ]),\n$$\n\n$$\n\\boldsymbol {B} _ {t - L: t} ^ {(n)} := \\operatorname {L i n e a r} (\\boldsymbol {B} _ {t - L: t} ^ {(n)}),\n$$\n\nwhere  $\\mathrm{MH - A_{ES}}$  is a multi-head version of  $\\mathcal{A}_{\\mathrm{ES}}$  and  $\\pmb{v}_0^{(n)}$  is the initial state from the ESA mechanism.\n\n# 4.2.2 Frequency Attention\n\nThe goal of identifying and extracting seasonal patterns from the lookback window is twofold. Firstly, it can be used to perform de-seasonalization on the input signals such that downstream components are able to focus on modeling the level and growth information. Secondly, we are able to extrapolate the seasonal patterns to build representations for the forecast horizon. The main challenge is to automatically identify seasonal patterns. Fortunately, the use of power spectral density estimation for periodicity detection has been well studied [34]. Inspired by these methods, we leverage the discrete Fourier transform (DFT, details in Appendix B) to develop the FA mechanism to extract dominant seasonal patterns.\n\nSpecifically, FA first decomposes input signals into their Fourier bases via a DFT along the temporal dimension,  $\\mathcal{F}(\\mathbf{Z}_{t - L:t}^{(n - 1)})\\in \\mathbb{C}^{F\\times d}$  where  $F = \\lfloor L / 2\\rfloor +1$  , and selects bases with the  $K$  largest amplitudes. An inverse DFT is then applied to obtain the seasonality pattern in time domain. Formally, this is given by the following equations:\n\n$$\n\\boldsymbol {\\Phi} _ {k, i} = \\phi \\Big (\\mathcal {F} (\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)}) _ {k, i} \\Big), \\quad \\boldsymbol {A} _ {k, i} = \\left| \\mathcal {F} (\\boldsymbol {Z} _ {t - L: t} ^ {(n - 1)}) _ {k, i} \\right|,\n$$\n\n$$\n\\kappa_ {i} ^ {(1)}, \\ldots , \\kappa_ {i} ^ {(K)} = \\underset {k \\in \\{2, \\ldots , F \\}} {\\arg \\operatorname {T o p - K}} \\left\\{\\boldsymbol {A} _ {k, i} \\right\\},\n$$\n\n$$\n\\boldsymbol {S} _ {j, i} ^ {(n)} = \\sum_ {k = 1} ^ {K} \\boldsymbol {A} _ {\\kappa_ {i} ^ {(k)}, i} \\left[ \\cos \\left(2 \\pi f _ {\\kappa_ {i} ^ {(k)}} j + \\boldsymbol {\\Phi} _ {\\kappa_ {i} ^ {(k)}, i}\\right) + \\cos \\left(2 \\pi \\bar {f} _ {\\kappa_ {i} ^ {(k)}} j + \\bar {\\boldsymbol {\\Phi}} _ {\\kappa_ {i} ^ {(k)}, i}\\right) \\right], \\tag {4}\n$$\n\nwhere  $\\Phi_{k,i}, A_{k,i}$  are the phase/amplitude of the  $k$ -th frequency for the  $i$ -th dimension, arg Top-K returns the arguments of the top  $K$  amplitudes,  $K$  is a hyperparameter,  $f_{k}$  is the Fourier frequency of the corresponding index, and  $\\bar{f}_k,\\bar{\\Phi}_{k,i}$  are the Fourier frequency/amplitude of the corresponding conjugates.\n\nFinally, the latent seasonal representation of the  $i$ -th dimension for the lookback window is formulated as  $\\pmb{S}_{t-L:t,i}^{(n)} = [\\pmb{S}_{t-L,i}^{(n)}, \\dots, \\pmb{S}_{t-1,i}^{(n)}]$ . For the forecast horizon, the FA module extrapolates beyond the lookback window via,  $\\pmb{S}_{t:t+H,i}^{(n)} = [\\pmb{S}_{t,i}^{(n)}, \\dots, \\pmb{S}_{t+H-1,i}^{(n)}]$ . Since  $K$  is a hyperparameter typically chosen for small values, the complexity for the FA mechanism is similarly  $\\mathcal{O}(L \\log L)$ .\n",
  "experiments": "# 5 Experiments\n\nThis section presents extensive empirical evaluations on the LSTF task over 6 real world datasets, ETT, ECL, Exchange, Traffic, Weather, and ILI, coming from a variety of application areas (details in Appendix D) for both multivariate and univariate settings. This is followed by an ablation study of the various contributing components, and interpretability experiments of our proposed model. An additional analysis on computational efficiency can be found in Appendix H for space. For the main benchmark, datasets are split into train, validation, and test sets chronologically, following a 60/20/20 split for the ETT datasets and 70/10/20 split for other datasets. Inputs are zero-mean normalized and we use MSE and MAE as evaluation metrics. Further details on implementation and hyperparameters can be found in Appendix C.\n\nTable 1: Multivariate forecasting results over various forecast horizons. Best results are bolded, and second best results are underlined.  \n\n<table><tr><td colspan=\"2\">Methods</td><td colspan=\"2\">ETSformer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">LogTrans</td><td colspan=\"2\">Reformer</td><td colspan=\"2\">LSTnet</td><td colspan=\"2\">LSTM</td></tr><tr><td colspan=\"2\">Metrics</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.189</td><td>0.280</td><td>0.255</td><td>0.339</td><td>0.365</td><td>0.453</td><td>0.768</td><td>0.642</td><td>0.658</td><td>0.619</td><td>3.142</td><td>1.365</td><td>2.041</td><td>1.073</td></tr><tr><td>192</td><td>0.253</td><td>0.319</td><td>0.281</td><td>0.340</td><td>0.533</td><td>0.563</td><td>0.989</td><td>0.757</td><td>1.078</td><td>0.827</td><td>3.154</td><td>1.369</td><td>2.249</td><td>1.112</td></tr><tr><td>336</td><td>0.314</td><td>0.357</td><td>0.339</td><td>0.372</td><td>1.363</td><td>0.887</td><td>1.334</td><td>0.872</td><td>1.549</td><td>0.972</td><td>3.160</td><td>1.369</td><td>2.568</td><td>1.238</td></tr><tr><td>720</td><td>0.414</td><td>0.413</td><td>0.422</td><td>0.419</td><td>3.379</td><td>1.388</td><td>3.048</td><td>1.328</td><td>2.631</td><td>1.242</td><td>3.171</td><td>1.368</td><td>2.720</td><td>1.287</td></tr><tr><td rowspan=\"4\">ECL</td><td>96</td><td>0.187</td><td>0.304</td><td>0.201</td><td>0.317</td><td>0.274</td><td>0.368</td><td>0.258</td><td>0.357</td><td>0.312</td><td>0.402</td><td>0.680</td><td>0.645</td><td>0.375</td><td>0.437</td></tr><tr><td>192</td><td>0.199</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.296</td><td>0.386</td><td>0.266</td><td>0.368</td><td>0.348</td><td>0.433</td><td>0.725</td><td>0.676</td><td>0.442</td><td>0.473</td></tr><tr><td>336</td><td>0.212</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.300</td><td>0.394</td><td>0.280</td><td>0.380</td><td>0.350</td><td>0.433</td><td>0.828</td><td>0.727</td><td>0.439</td><td>0.473</td></tr><tr><td>720</td><td>0.233</td><td>0.345</td><td>0.254</td><td>0.361</td><td>0.373</td><td>0.439</td><td>0.283</td><td>0.376</td><td>0.340</td><td>0.420</td><td>0.957</td><td>0.811</td><td>0.980</td><td>0.814</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.085</td><td>0.204</td><td>0.197</td><td>0.323</td><td>0.847</td><td>0.752</td><td>0.968</td><td>0.812</td><td>1.065</td><td>0.829</td><td>1.551</td><td>1.058</td><td>1.453</td><td>1.049</td></tr><tr><td>192</td><td>0.182</td><td>0.303</td><td>0.300</td><td>0.369</td><td>1.204</td><td>0.895</td><td>1.040</td><td>0.851</td><td>1.188</td><td>0.906</td><td>1.477</td><td>1.028</td><td>1.846</td><td>1.179</td></tr><tr><td>336</td><td>0.348</td><td>0.428</td><td>0.509</td><td>0.524</td><td>1.672</td><td>1.036</td><td>1.659</td><td>1.081</td><td>1.357</td><td>0.976</td><td>1.507</td><td>1.031</td><td>2.136</td><td>1.231</td></tr><tr><td>720</td><td>1.025</td><td>0.774</td><td>1.447</td><td>0.941</td><td>2.478</td><td>1.310</td><td>1.941</td><td>1.127</td><td>1.510</td><td>1.016</td><td>2.285</td><td>1.243</td><td>2.984</td><td>1.427</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.607</td><td>0.392</td><td>0.613</td><td>0.388</td><td>0.719</td><td>0.391</td><td>0.684</td><td>0.384</td><td>0.732</td><td>0.423</td><td>1.107</td><td>0.685</td><td>0.843</td><td>0.453</td></tr><tr><td>192</td><td>0.621</td><td>0.399</td><td>0.616</td><td>0.382</td><td>0.696</td><td>0.379</td><td>0.685</td><td>0.390</td><td>0.733</td><td>0.420</td><td>1.157</td><td>0.706</td><td>0.847</td><td>0.453</td></tr><tr><td>336</td><td>0.622</td><td>0.396</td><td>0.622</td><td>0.337</td><td>0.777</td><td>0.420</td><td>0.733</td><td>0.408</td><td>0.742</td><td>0.420</td><td>1.216</td><td>0.730</td><td>0.853</td><td>0.455</td></tr><tr><td>720</td><td>0.632</td><td>0.396</td><td>0.660</td><td>0.408</td><td>0.864</td><td>0.472</td><td>0.717</td><td>0.396</td><td>0.755</td><td>0.423</td><td>1.481</td><td>0.805</td><td>1.500</td><td>0.805</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.197</td><td>0.281</td><td>0.266</td><td>0.336</td><td>0.300</td><td>0.384</td><td>0.458</td><td>0.490</td><td>0.689</td><td>0.596</td><td>0.594</td><td>0.587</td><td>0.369</td><td>0.406</td></tr><tr><td>192</td><td>0.237</td><td>0.312</td><td>0.307</td><td>0.367</td><td>0.598</td><td>0.544</td><td>0.658</td><td>0.589</td><td>0.752</td><td>0.638</td><td>0.560</td><td>0.565</td><td>0.416</td><td>0.435</td></tr><tr><td>336</td><td>0.298</td><td>0.353</td><td>0.359</td><td>0.359</td><td>0.578</td><td>0.523</td><td>0.797</td><td>0.652</td><td>0.639</td><td>0.596</td><td>0.597</td><td>0.587</td><td>0.455</td><td>0.454</td></tr><tr><td>720</td><td>0.352</td><td>0.388</td><td>0.419</td><td>0.419</td><td>1.059</td><td>0.741</td><td>0.869</td><td>0.675</td><td>1.130</td><td>0.792</td><td>0.618</td><td>0.599</td><td>0.535</td><td>0.520</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>2.527</td><td>1.020</td><td>3.483</td><td>1.287</td><td>5.764</td><td>1.677</td><td>4.480</td><td>1.444</td><td>4.400</td><td>1.382</td><td>6.026</td><td>1.770</td><td>5.914</td><td>1.734</td></tr><tr><td>36</td><td>2.615</td><td>1.007</td><td>3.103</td><td>1.148</td><td>4.755</td><td>1.467</td><td>4.799</td><td>1.467</td><td>4.783</td><td>1.448</td><td>5.340</td><td>1.668</td><td>6.631</td><td>1.845</td></tr><tr><td>48</td><td>2.359</td><td>0.972</td><td>2.669</td><td>1.085</td><td>4.763</td><td>1.469</td><td>4.800</td><td>1.468</td><td>4.832</td><td>1.465</td><td>6.080</td><td>1.787</td><td>6.736</td><td>1.857</td></tr><tr><td>60</td><td>2.487</td><td>1.016</td><td>2.770</td><td>1.125</td><td>5.264</td><td>1.564</td><td>5.278</td><td>1.560</td><td>4.882</td><td>1.483</td><td>5.548</td><td>1.720</td><td>6.870</td><td>1.879</td></tr></table>\n\n# 5.1 Results\n\nFor the multivariate benchmark, baselines include recently proposed time-series/efficient Transformers - Autoformer, Informer, LogTrans, and Reformer [16], and RNN variants - LSTM [18], and LSTM [11]. Univariate baselines further include N-BEATS [24], DeepAR [26], ARIMA, Prophet [30], and AutoETS [3]. We obtain baseline results from the following papers: [37, 41], and further run AutoETS from the Merlion library [3]. Table 1 summarizes the results of ETSformer against top performing baselines on a selection of datasets, for the multivariate setting, and Table 4 in Appendix F for space. Results for ETSformer are averaged over three runs (standard deviation in Appendix G).\n\nOverall, ETSformer achieves state-of-the-art performance, achieving the best performance (across all datasets/settings, based on MSE) on 35 out of 40 settings for the multivariate case, and 17 out of 23 for the univariate case. Notably, on Exchange, a dataset with no obvious periodic patterns, ETSformer demonstrates an average (over forecast horizons) improvement of  $39.8\\%$  over the best performing baseline, evidencing its strong trend forecasting capabilities. We highlight that for cases where ETSformer does not achieve the best performance, it is still highly competitive, and is always within the top 2 performing methods, based on MSE, for 40 out of 40 settings in the multivariate benchmark, and 21 out of 23 settings of the univariate case.\n\n# 5.2 Ablation Study\n\nTable 2: Ablation study on the various components of ETSformer, on the horizon  $= {24}$  setting.  \n\n<table><tr><td colspan=\"2\">Datasets</td><td>ETTh2</td><td>ETTm2</td><td>ECL</td><td>Traffic</td></tr><tr><td rowspan=\"2\">ETSformer</td><td>MSE</td><td>0.262</td><td>0.110</td><td>0.163</td><td>0.571</td></tr><tr><td>MAE</td><td>0.337</td><td>0.222</td><td>0.287</td><td>0.373</td></tr><tr><td rowspan=\"2\">w/o Level</td><td>MSE</td><td>0.434</td><td>0.464</td><td>0.275</td><td>0.649</td></tr><tr><td>MAE</td><td>0.466</td><td>0.518</td><td>0.373</td><td>0.393</td></tr><tr><td rowspan=\"2\">w/o Season</td><td>MSE</td><td>0.521</td><td>0.131</td><td>0.696</td><td>1.334</td></tr><tr><td>MAE</td><td>0.450</td><td>0.236</td><td>0.677</td><td>0.779</td></tr><tr><td rowspan=\"2\">w/o Growth</td><td>MSE</td><td>0.290</td><td>0.115</td><td>0.167</td><td>0.583</td></tr><tr><td>MAE</td><td>0.359</td><td>0.226</td><td>0.288</td><td>0.383</td></tr><tr><td rowspan=\"2\">MH-ESA → MHA</td><td>MSE</td><td>0.656</td><td>0.343</td><td>0.205</td><td>0.586</td></tr><tr><td>MAE</td><td>0.639</td><td>0.451</td><td>0.323</td><td>0.380</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/cbad191de7d9735ad0e96a851b373f963993cf916a0c027483aad0758df6cac5.jpg)  \nFigure 4: Left: Visualization of decomposed forecasts from ETSformer and Autoformer on a synthetic dataset. (i) Ground truth and non-decomposed forecasts of ETSformer and Autoformer on synthetic data. (ii) Trend component. (iii) Seasonal component. The data sample on which Autoformer obtained lowest MSE was selected for visualization. Right: Visualization of decomposed forecasts from ETSformer on real world datasets, ETTh1, ECL, and Weather. Note that season is zero-centered, and trend successfully tracks the level of the time-series. Due to the long sequence forecasting setting and with a damping, the growth component is not visually obvious, but notice for the Weather dataset, the trend pattern is has a strong downward slope initially (near time step 0), and is quickly damped.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c6d7eb72-d017-4271-8148-d62abb9824e8/94a5995f98c13123da571f8427fe175232ce49b4de5a314dbe0dc7a74d001e56.jpg)\n\nWe study the contribution of each major component which the final forecast is composed of level, growth, and seasonality. Table 2 first presents the performance of the full model, and subsequently, the performance of the resulting model by removing each component. We observe that the composition of level, growth, and season provides the most accurate forecasts across a variety of application areas, and removing any one component results in a deterioration. In particular, estimation of the level of the time-series is critical. We also analyse the case where MH-ESA is replaced with a vanilla multi-head attention, and observe that our trend attention formulation indeed is more effective.\n\n# 5.3 Interpretability\n\nETSformer generates forecasts based on a composition of interpretable time-series components. This means we can visualize each component individually, and understand how seasonality and trend affects the forecasts. We showcase this ability in Figure 4 on both synthetic and real world data. Experiments with synthetic data are crucial in this case, since we are not able to obtain the ground truth decomposition from real world data. ETSformer is first trained on the synthetic dataset (details in Appendix E) with clear (nonlinear) trend and seasonality patterns which we can control. Given a lookback window (without noise), we visualize the forecast, as well as decomposed trend and seasonal forecasts. ETSformer successfully forecasts interpretable level, trend (level + growth), and seasonal components, as observed in the trend and seasonality components closely tracking the ground truth patterns. Despite obtaining a low MSE, the competing decomposition based approach, Autoformer, struggles to disambiguate between trend and seasonality.\n",
  "hyperparameter": "Number of encoder/decoder stacks N (not specified exact value); Number of attention heads n_h for multi-head mechanisms; Latent dimension d (input channel m, output channel d for Conv); Lookback window length L; Forecast horizon H; Top-K frequencies K for Frequency Attention (described as 'typically chosen for small values'); Smoothing parameter α (0 < α < 1, learnable); Damping parameter γ (0 < γ < 1, learnable, multi-head version with n_h parameters); Convolutional kernel size 3 for input embedding; Train/validation/test split: 60/20/20 for ETT datasets, 70/10/20 for other datasets; Optimizer and learning rate not explicitly mentioned in provided sections; Feedforward network uses sigmoid activation σ(·)"
}