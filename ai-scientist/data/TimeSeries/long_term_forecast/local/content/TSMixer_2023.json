{
  "id": "TSMixer_2023",
  "paper_title": "TSMixer: An All-MLP Architecture for Time Series Forecasting",
  "alias": "TSMixer",
  "year": 2023,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "The paper challenges the effectiveness of Transformer-based models for long-term time series forecasting (LTSF) by introducing LTSF-Linear, an embarrassingly simple baseline using only linear layers. The core contribution is demonstrating that a simple linear model with direct multi-step (DMS) forecasting strategy significantly outperforms complex Transformer-based methods (20-50% improvement) on existing benchmarks, revealing that Transformers fail to effectively capture temporal relations despite their self-attention mechanisms. The paper proposes three variants: vanilla Linear, DLinear (with seasonal-trend decomposition), and NLinear (with normalization), showing that the success of previous Transformer methods was primarily due to the DMS strategy rather than the complex attention mechanisms.",
  "introduction": "# 1. Introduction\n\nTime series are ubiquitous in today's data-driven world. Given historical data, time series forecasting (TSF) is a long-standing task that has a wide range of applications, including but not limited to traffic flow estimation, en\n\nergy management, and financial investment. Over the past several decades, TSF solutions have undergone a progression from traditional statistical methods (e.g., ARIMA [1]) and machine learning techniques (e.g., GBRT [11]) to deep learning-based solutions, e.g., Recurrent Neural Networks [15] and Temporal Convolutional Networks [3, 17].\n\nTransformer [26] is arguably the most successful sequence modeling architecture, demonstrating unparalleled performances in various applications, such as natural language processing (NLP) [7], speech recognition [8], and computer vision [19, 29]. Recently, there has also been a surge of Transformer-based solutions for time series analysis, as surveyed in [27]. Most notable models, which focus on the less explored and challenging long-term time series forecasting (LTSF) problem, include LogTrans [16] (NeurIPS 2019), Informer [30] (AAAI 2021 Best paper), Autoformer [28] (NeurIPS 2021), Pyraformer [18] (ICLR 2022 Oral), Triformer [5] (IJCAI 2022) and the recent FEDformer [31] (ICML 2022).\n\nThe main working power of Transformers is from its multi-head self-attention mechanism, which has a remarkable capability of extracting semantic correlations among elements in a long sequence (e.g., words in texts or 2D patches in images). However, self-attention is permutation-invariant and \"anti-order\" to some extent. While using various types of positional encoding techniques can preserve some ordering information, it is still inevitable to have temporal information loss after applying self-attention on top of them. This is usually not a serious concern for semantic-rich applications such as NLP, e.g., the semantic meaning of a sentence is largely preserved even if we reorder some words in it. However, when analyzing time series data, there is usually a lack of semantics in the numerical data itself, and we are mainly interested in modeling the temporal changes among a continuous set of points. That is, the order itself plays the most crucial role. Consequently, we pose the following intriguing question: Are Transformers really effective for long-term time series forecasting?\n\nMoreover, while existing Transformer-based LTSF so\n\nlutions have demonstrated considerable prediction accuracy improvements over traditional methods, in their experiments, all the compared (non-Transformer) baselines perform autoregressive or iterated multi-step (IMS) forecasting [1,2,22,24], which are known to suffer from significant error accumulation effects for the LTSF problem. Therefore, in this work, we challenge Transformer-based LTSF solutions with direct multi-step (DMS) forecasting strategies to validate their real performance.\n\nNot all time series are predictable, let alone long-term forecasting (e.g., for chaotic systems). We hypothesize that long-term forecasting is only feasible for those time series with a relatively clear trend and periodicity. As linear models can already extract such information, we introduce a set of embarrassingly simple models named LTSF-Linear as a new baseline for comparison. LTSF-Linear regresses historical time series with a one-layer linear model to forecast future time series directly. We conduct extensive experiments on nine widely-used benchmark datasets that cover various real-life applications: traffic, energy, economics, weather, and disease predictions. Surprisingly, our results show that LTSF-Linear outperforms existing complex Transformer-based models in all cases, and often by a large margin (20%  $\\sim 50\\%$ ). Moreover, we find that, in contrast to the claims in existing Transformers, most of them fail to extract temporal relations from long sequences, i.e., the forecasting errors are not reduced (sometimes even increased) with the increase of look-back window sizes. Finally, we conduct various ablation studies on existing Transformer-based TSF solutions to study the impact of various design elements in them.\n\nTo sum up, the contributions of this work include:\n\n- To the best of our knowledge, this is the first work to challenge the effectiveness of the booming Transformers for the long-term time series forecasting task.  \n- To validate our claims, we introduce a set of embarrassingly simple one-layer linear models, named LTSF-Linear, and compare them with existing Transformer-based LTSF solutions on nine benchmarks. LTSF-Linear can be a new baseline for the LTSF problem.  \n- We conduct comprehensive empirical studies on various aspects of existing Transformer-based solutions, including the capability of modeling long inputs, the sensitivity to time series order, the impact of positional encoding and sub-series embedding, and efficiency comparisons. Our findings would benefit future research in this area.\n\nWith the above, we conclude that the temporal modeling capabilities of Transformers for time series are exaggerated, at least for the existing LTSF benchmarks. At the same time, while LTSF-Linear achieves a better prediction\n\naccuracy compared to existing works, it merely serves as a simple baseline for future research on the challenging long-term TSF problem. With our findings, we also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks in the future.\n",
  "method": "# 3. Transformer-Based LTSF Solutions\n\nTransformer-based models [26] have achieved unparalleled performances in many long-standing AI tasks in natural language processing and computer vision fields, thanks to the effectiveness of the multi-head self-attention mechanism. This has also triggered lots of research interest in Transformer-based time series modeling techniques [20, 27]. In particular, a large amount of research works are dedicated to the LTSF task (e.g., [16, 18, 28, 30, 31]). Considering the ability to capture long-range dependencies with Transformer models, most of them focus on the less-explored long-term forecasting problem  $(T \\gg 1)^{1}$ .\n\nWhen applying the vanilla Transformer model to the LTSF problem, it has some limitations, including the quadratic time/memory complexity with the original self-attention scheme and error accumulation caused by the autoregressive decoder design. Informer [30] addresses these issues and proposes a novel Transformer architecture with reduced complexity and a DMS forecasting strategy. Later, more Transformer variants introduce various time series features into their models for performance or efficiency improvements [18,28,31]. We summarize the design elements of existing Transformer-based LTSF solutions as follows (see Figure 1).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/a062633369641ea5e8e1354e3fb323a6525759a812f16dcb94687902ebeb29fe.jpg)  \nFigure 1. The pipeline of existing Transformer-based TSF solutions. In (a) and (b), the solid boxes are essential operations, and the dotted boxes are applied optionally. (c) and (d) are distinct for different methods [16, 18, 28, 30, 31].\n\nTime series decomposition: For data preprocessing, normalization with zero-mean is common in TSF. Besides, Autoformer [28] first applies seasonal-trend decomposition behind each neural block, which is a standard method in time series analysis to make raw data more predictable [6, 13]. Specifically, they use a moving average kernel on the input sequence to extract the trend-cyclical component of the time series. The difference between the original sequence and the trend component is regarded as the seasonal component. On top of the decomposition scheme of Autoformer, FEDformer [31] further proposes the mixture of experts' strategies to mix the trend components extracted by moving average kernels with various kernel sizes.\n\nInput embedding strategies: The self-attention layer in the Transformer architecture cannot preserve the positional information of the time series. However, local positional information, i.e. the ordering of time series, is important. Besides, global temporal information, such as hierarchical timestamps (week, month, year) and agnostic timestamps (holidays and events), is also informative [30]. To enhance the temporal context of time-series inputs, a practical design in the SOTA Transformer-based methods is injecting several embeddings, like a fixed positional encoding, a channel projection embedding, and learnable temporal embeddings into the input sequence. Moreover, temporal embeddings with a temporal convolution layer [16] or learnable timestamps [28] are introduced.\n\nSelf-attention schemes: Transformers rely on the self-attention mechanism to extract the semantic dependencies between paired elements. Motivated by reducing the  $O(L^2)$  time and memory complexity of the vanilla Transformer, recent works propose two strategies for efficiency. On the one hand, LogTrans and Pyraformer explicitly introduce a sparsity bias into the self-attention scheme. Specifically, LogTrans uses a Logsparse mask to reduce the computational complexity to  $O(LlogL)$  while Pyraformer adopts pyramidal attention that captures hierarchically multi-scale temporal dependencies with an  $O(L)$  time and memory complexity. On the other hand, Informer and FEDformer use the low-rank property in the self-attention matrix. Informer proposes a ProbSparse self\n\nattention mechanism and a self-attention distilling operation to decrease the complexity to  $O(L \\log L)$ , and FEDformer designs a Fourier enhanced block and a wavelet enhanced block with random selection to obtain  $O(L)$  complexity. Lastly, Autoformer designs a series-wise auto-correlation mechanism to replace the original self-attention layer.\n\nDecoders: The vanilla Transformer decoder outputs sequences in an autoregressive manner, resulting in a slow inference speed and error accumulation effects, especially for long-term predictions. Informer designs a generative-style decoder for DMS forecasting. Other Transformer variants employ similar DMS strategies. For instance, Pyraformer uses a fully-connected layer concatenating Spatio-temporal axes as the decoder. Autoformer sums up two refined decomposed features from trend-cyclical components and the stacked auto-correlation mechanism for seasonal components to get the final prediction. FEDformer also uses a decomposition scheme with the proposed frequency attention block to decode the final results.\n\nThe premise of Transformer models is the semantic correlations between paired elements, while the self-attention mechanism itself is permutation-invariant, and its capability of modeling temporal relations largely depends on positional encodings associated with input tokens. Considering the raw numerical data in time series (e.g., stock prices or electricity values), there are hardly any point-wise semantic correlations between them. In time series modeling, we are mainly interested in the temporal relations among a continuous set of points, and the order of these elements instead of the paired relationship plays the most crucial role. While employing positional encoding and using tokens to embed sub-series facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. Due to the above observations, we are interested in revisiting the effectiveness of Transformer-based LTSF solutions.\n\n# 4. An Embarrassingly Simple Baseline\n\nIn the experiments of existing Transformer-based LTSF solutions  $(T \\gg 1)$ , all the compared (non-Transformer)\n\nbasielines are IMS forecasting techniques, which are known to suffer from significant error accumulation effects. We hypothesize that the performance improvements in these works are largely due to the DMS strategy used in them.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/5772586ddc7d7bb3fcced184680edf83c6a67a8d1366bc7c2b990aa727f47767.jpg)  \nFigure 2. Illustration of the basic linear model.\n\nTo validate this hypothesis, we present the simplest DMS model via a temporal linear layer, named LTSF-Linear, as a baseline for comparison. The basic formulation of LTSF-Linear directly regresses historical time series for future prediction via a weighted sum operation (as illustrated in Figure 2). The mathematical expression is  $\\hat{X}_i = WX_i$ , where  $W \\in \\mathbb{R}^{T \\times L}$  is a linear layer along the temporal axis.  $\\hat{X}_i$  and  $X_i$  are the prediction and input for each  $i_{th}$  variate. Note that LTSF-Linear shares weights across different variates and does not model any spatial correlations.\n\nLTSF-Linear is a set of linear models. Vanilla Linear is a one-layer linear model. To handle time series across different domains (e.g., finance, traffic, and energy domains), we further introduce two variants with two preprocessing methods, named DLinear and NLinear.\n\n- Specifically, DLinear is a combination of a Decomposition scheme used in Autoformer and FEDformer with linear layers. It first decomposes a raw data input into a trend component by a moving average kernel and a remainder (seasonal) component. Then, two one-layer linear layers are applied to each component, and we sum up the two features to get the final prediction. By explicitly handling trend, DLinear enhances the performance of a vanilla linear when there is a clear trend in the data.  \n- Meanwhile, to boost the performance of LTSF-Linear when there is a distribution shift in the dataset, NLinear first subtracts the input by the last value of the sequence. Then, the input goes through a linear layer, and the subtracted part is added back before making the final prediction. The subtraction and addition in NLinear are a simple normalization for the input sequence.\n",
  "experiments": "# 5. Experiments\n\n# 5.1. Experimental Settings\n\nDataset. We conduct extensive experiments on nine widely-used real-world datasets, including ETT (Electricity Transformer Temperature) [30] (ETTh1, ETTh2, ETTm1, ETTm2), Traffic, Electricity, Weather, ILI, Exchange-Rate [15]. All of them are multivariate time series. We leave data descriptions in the Appendix.\n\nEvaluation metric. Following previous works [28, 30, 31], we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) as the core metrics to compare performance.\n\nCompared methods. We include five recent Transformer-based methods: FEDformer [31], Autoformer [28], Informer [30], Pyraformer [18], and LogTrans [16]. Besides, we include a naive DMS method: Closest Repeat (Repeat), which repeats the last value in the look-back window, as another simple baseline. Since there are two variants of FEDformer, we compare the one with better accuracy (FEDformer-f via Fourier transform).\n\n# 5.2. Comparison with Transformers\n\nQuantitative results. In Table 2, we extensively evaluate all mentioned Transformers on nine benchmarks, following the experimental setting of previous work [28, 30, 31]. Surprisingly, the performance of LTSF-Linear surpasses the SOTA FEDformer in most cases by  $20\\% \\sim 50\\%$  improvements on the multivariate forecasting, where LTSF-Linear even does not model correlations among variates. For different time series benchmarks, NLinear and DLinear show the superiority to handle the distribution shift and trend-seasonality features. We also provide results for univariate forecasting of ETT datasets in the Appendix, where LTSF-Linear still consistently outperforms Transformer-based LTSF solutions by a large margin.\n\nFEDformer achieves competitive forecasting accuracy on ETTh1. This because FEDformer employs classical time series analysis techniques such as frequency processing, which brings in time series inductive bias and benefits the ability of temporal feature extraction. In summary, these results reveal that existing complex Transformer-based LTSF solutions are not seemingly effective on the existing nine benchmarks while LTSF-Linear can be a powerful baseline.\n\nAnother interesting observation is that even though the naive Repeat method shows worse results when predicting long-term seasonal data (e.g., Electricity and Traffic), it surprisingly outperforms all Transformer-based methods on Exchange-Rate (around  $45\\%$ ). This is mainly caused by the wrong prediction of trends in Transformer-based solutions, which may overfit toward sudden change noises in the training data, resulting in significant accuracy degradation (see Figure 3(b)). Instead, Repeat does not have the bias.\n\nQualitative results. As shown in Figure 3, we plot\n\n<table><tr><td>Datasets</td><td>ETTh1&amp;ETTh2</td><td>ETTm1 &amp;ETTm2</td><td>Traffic</td><td>Electricity</td><td>Exchange-Rate</td><td>Weather</td><td>ILI</td></tr><tr><td>Variates</td><td>7</td><td>7</td><td>862</td><td>321</td><td>8</td><td>21</td><td>7</td></tr><tr><td>Timesteps</td><td>17,420</td><td>69,680</td><td>17,544</td><td>26,304</td><td>7,588</td><td>52,696</td><td>966</td></tr><tr><td>Granularity</td><td>1hour</td><td>5min</td><td>1hour</td><td>1hour</td><td>1day</td><td>10min</td><td>1week</td></tr></table>\n\nTable 1. The statistics of the nine popular datasets for the LTSF problem.  \n\n<table><tr><td colspan=\"2\">Methods</td><td>IMP.</td><td colspan=\"2\">Linear*</td><td colspan=\"2\">NLinear*</td><td colspan=\"2\">DLinear*</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">Pyraformer*</td><td colspan=\"2\">LogTrans</td><td colspan=\"2\">Repeat*</td><td></td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td></td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>27.40%</td><td>0.140</td><td>0.237</td><td>0.141</td><td>0.237</td><td>0.140</td><td>0.237</td><td>0.193</td><td>0.308</td><td>0.201</td><td>0.317</td><td>0.274</td><td>0.368</td><td>0.386</td><td>0.449</td><td>0.258</td><td>0.357</td><td>1.588</td><td>0.946</td><td></td></tr><tr><td>192</td><td>23.88%</td><td>0.153</td><td>0.250</td><td>0.154</td><td>0.248</td><td>0.153</td><td>0.249</td><td>0.201</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.296</td><td>0.386</td><td>0.386</td><td>0.443</td><td>0.266</td><td>0.368</td><td>1.595</td><td>0.950</td><td></td></tr><tr><td>336</td><td>21.02%</td><td>0.169</td><td>0.268</td><td>0.171</td><td>0.265</td><td>0.169</td><td>0.267</td><td>0.214</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.300</td><td>0.394</td><td>0.378</td><td>0.443</td><td>0.280</td><td>0.380</td><td>1.617</td><td>0.961</td><td></td></tr><tr><td>720</td><td>17.47%</td><td>0.203</td><td>0.301</td><td>0.210</td><td>0.297</td><td>0.203</td><td>0.301</td><td>0.246</td><td>0.355</td><td>0.254</td><td>0.361</td><td>0.373</td><td>0.439</td><td>0.376</td><td>0.445</td><td>0.283</td><td>0.376</td><td>1.647</td><td>0.975</td><td></td></tr><tr><td rowspan=\"4\">Exchange Electricity</td><td>96</td><td>45.27%</td><td>0.082</td><td>0.207</td><td>0.089</td><td>0.208</td><td>0.081</td><td>0.203</td><td>0.148</td><td>0.278</td><td>0.197</td><td>0.323</td><td>0.847</td><td>0.752</td><td>0.376</td><td>1.105</td><td>0.968</td><td>0.812</td><td>0.081</td><td>0.196</td><td></td></tr><tr><td>192</td><td>42.06%</td><td>0.167</td><td>0.304</td><td>0.180</td><td>0.300</td><td>0.157</td><td>0.293</td><td>0.271</td><td>0.380</td><td>0.300</td><td>0.369</td><td>1.204</td><td>0.895</td><td>1.748</td><td>1.151</td><td>1.040</td><td>0.851</td><td>0.167</td><td>0.289</td><td></td></tr><tr><td>336</td><td>33.69%</td><td>0.328</td><td>0.432</td><td>0.331</td><td>0.415</td><td>0.305</td><td>0.414</td><td>0.460</td><td>0.500</td><td>0.509</td><td>0.524</td><td>1.672</td><td>1.036</td><td>1.874</td><td>1.172</td><td>1.659</td><td>1.081</td><td>0.305</td><td>0.396</td><td></td></tr><tr><td>720</td><td>46.19%</td><td>0.964</td><td>0.750</td><td>1.033</td><td>0.780</td><td>0.643</td><td>0.601</td><td>1.195</td><td>0.841</td><td>1.447</td><td>0.941</td><td>2.478</td><td>1.310</td><td>1.943</td><td>1.206</td><td>1.941</td><td>1.127</td><td>0.823</td><td>0.681</td><td></td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>30.15%</td><td>0.410</td><td>0.282</td><td>0.410</td><td>0.279</td><td>0.410</td><td>0.282</td><td>0.587</td><td>0.366</td><td>0.613</td><td>0.388</td><td>0.719</td><td>0.391</td><td>2.085</td><td>0.468</td><td>0.684</td><td>0.384</td><td>2.723</td><td>1.079</td><td></td></tr><tr><td>192</td><td>29.96%</td><td>0.423</td><td>0.287</td><td>0.423</td><td>0.284</td><td>0.423</td><td>0.287</td><td>0.604</td><td>0.373</td><td>0.616</td><td>0.382</td><td>0.696</td><td>0.379</td><td>0.867</td><td>0.467</td><td>0.685</td><td>0.390</td><td>2.756</td><td>1.087</td><td></td></tr><tr><td>336</td><td>29.95%</td><td>0.436</td><td>0.295</td><td>0.435</td><td>0.290</td><td>0.436</td><td>0.296</td><td>0.621</td><td>0.383</td><td>0.622</td><td>0.337</td><td>0.777</td><td>0.420</td><td>0.869</td><td>0.469</td><td>0.734</td><td>0.408</td><td>2.791</td><td>1.095</td><td></td></tr><tr><td>720</td><td>25.87%</td><td>0.466</td><td>0.315</td><td>0.464</td><td>0.307</td><td>0.466</td><td>0.315</td><td>0.626</td><td>0.382</td><td>0.660</td><td>0.408</td><td>0.864</td><td>0.472</td><td>0.881</td><td>0.473</td><td>0.717</td><td>0.396</td><td>2.811</td><td>1.097</td><td></td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>18.89%</td><td>0.176</td><td>0.236</td><td>0.182</td><td>0.232</td><td>0.176</td><td>0.237</td><td>0.217</td><td>0.296</td><td>0.266</td><td>0.336</td><td>0.300</td><td>0.384</td><td>0.896</td><td>0.556</td><td>0.458</td><td>0.490</td><td>0.259</td><td>0.254</td><td></td></tr><tr><td>192</td><td>21.01%</td><td>0.218</td><td>0.276</td><td>0.225</td><td>0.269</td><td>0.220</td><td>0.282</td><td>0.276</td><td>0.336</td><td>0.307</td><td>0.367</td><td>0.598</td><td>0.544</td><td>0.622</td><td>0.624</td><td>0.658</td><td>0.589</td><td>0.309</td><td>0.292</td><td></td></tr><tr><td>336</td><td>22.71%</td><td>0.262</td><td>0.312</td><td>0.271</td><td>0.301</td><td>0.265</td><td>0.319</td><td>0.339</td><td>0.380</td><td>0.359</td><td>0.395</td><td>0.578</td><td>0.523</td><td>0.739</td><td>0.753</td><td>0.797</td><td>0.652</td><td>0.377</td><td>0.338</td><td></td></tr><tr><td>720</td><td>19.85%</td><td>0.326</td><td>0.365</td><td>0.338</td><td>0.348</td><td>0.323</td><td>0.362</td><td>0.403</td><td>0.428</td><td>0.419</td><td>0.428</td><td>1.059</td><td>0.741</td><td>1.004</td><td>0.934</td><td>0.869</td><td>0.675</td><td>0.465</td><td>0.394</td><td></td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>47.86%</td><td>1.947</td><td>0.985</td><td>1.683</td><td>0.858</td><td>2.215</td><td>1.081</td><td>3.228</td><td>1.260</td><td>3.483</td><td>1.287</td><td>5.764</td><td>1.677</td><td>1.420</td><td>2.012</td><td>4.480</td><td>1.444</td><td>6.587</td><td>1.701</td><td></td></tr><tr><td>36</td><td>36.43%</td><td>2.182</td><td>1.036</td><td>1.703</td><td>0.859</td><td>1.963</td><td>0.963</td><td>2.679</td><td>1.080</td><td>3.103</td><td>1.148</td><td>4.755</td><td>1.467</td><td>7.394</td><td>2.031</td><td>4.799</td><td>1.467</td><td>7.130</td><td>1.884</td><td></td></tr><tr><td>48</td><td>34.43%</td><td>2.256</td><td>1.060</td><td>1.719</td><td>0.884</td><td>2.130</td><td>1.024</td><td>2.622</td><td>1.078</td><td>2.669</td><td>1.085</td><td>4.763</td><td>1.469</td><td>7.551</td><td>2.057</td><td>4.800</td><td>1.468</td><td>6.575</td><td>1.798</td><td></td></tr><tr><td>60</td><td>34.33%</td><td>2.390</td><td>1.104</td><td>1.819</td><td>0.917</td><td>2.368</td><td>1.096</td><td>2.857</td><td>1.157</td><td>2.770</td><td>1.125</td><td>5.264</td><td>1.564</td><td>7.662</td><td>2.100</td><td>5.278</td><td>1.560</td><td>5.893</td><td>1.677</td><td></td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.80%</td><td>0.375</td><td>0.397</td><td>0.374</td><td>0.394</td><td>0.375</td><td>0.399</td><td>0.376</td><td>0.419</td><td>0.449</td><td>0.459</td><td>0.865</td><td>0.713</td><td>0.664</td><td>0.612</td><td>0.878</td><td>0.740</td><td>1.295</td><td>0.713</td><td></td></tr><tr><td>192</td><td>3.57%</td><td>0.418</td><td>0.429</td><td>0.408</td><td>0.415</td><td>0.405</td><td>0.416</td><td>0.420</td><td>0.448</td><td>0.500</td><td>0.482</td><td>1.008</td><td>0.792</td><td>0.790</td><td>0.681</td><td>1.037</td><td>0.824</td><td>1.325</td><td>0.733</td><td></td></tr><tr><td>336</td><td>6.54%</td><td>0.479</td><td>0.476</td><td>0.429</td><td>0.427</td><td>0.439</td><td>0.443</td><td>0.459</td><td>0.465</td><td>0.521</td><td>0.496</td><td>1.107</td><td>0.809</td><td>0.891</td><td>0.738</td><td>1.238</td><td>0.932</td><td>1.323</td><td>0.744</td><td></td></tr><tr><td>720</td><td>13.04%</td><td>0.624</td><td>0.592</td><td>0.440</td><td>0.453</td><td>0.472</td><td>0.490</td><td>0.506</td><td>0.507</td><td>0.514</td><td>0.512</td><td>1.181</td><td>0.865</td><td>0.963</td><td>0.782</td><td>1.135</td><td>0.852</td><td>1.339</td><td>0.756</td><td></td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>19.94%</td><td>0.288</td><td>0.352</td><td>0.277</td><td>0.338</td><td>0.289</td><td>0.353</td><td>0.346</td><td>0.388</td><td>0.358</td><td>0.397</td><td>3.755</td><td>1.525</td><td>0.645</td><td>0.569</td><td>2.116</td><td>1.197</td><td>0.432</td><td>0.422</td><td></td></tr><tr><td>192</td><td>19.81%</td><td>0.377</td><td>0.413</td><td>0.344</td><td>0.381</td><td>0.383</td><td>0.418</td><td>0.429</td><td>0.439</td><td>0.456</td><td>0.452</td><td>5.602</td><td>1.931</td><td>0.788</td><td>0.683</td><td>4.315</td><td>1.635</td><td>0.534</td><td>0.473</td><td></td></tr><tr><td>336</td><td>25.93%</td><td>0.452</td><td>0.461</td><td>0.357</td><td>0.400</td><td>0.448</td><td>0.465</td><td>0.496</td><td>0.487</td><td>0.482</td><td>0.486</td><td>4.721</td><td>1.835</td><td>0.907</td><td>0.747</td><td>1.124</td><td>1.604</td><td>0.591</td><td>0.508</td><td></td></tr><tr><td>720</td><td>14.25%</td><td>0.698</td><td>0.595</td><td>0.394</td><td>0.436</td><td>0.605</td><td>0.551</td><td>0.463</td><td>0.474</td><td>0.515</td><td>0.511</td><td>3.647</td><td>1.625</td><td>0.963</td><td>0.783</td><td>3.188</td><td>1.540</td><td>0.588</td><td>0.517</td><td></td></tr><tr><td rowspan=\"4\">ETTm1</td><td>96</td><td>21.10%</td><td>0.308</td><td>0.352</td><td>0.306</td><td>0.348</td><td>0.299</td><td>0.343</td><td>0.379</td><td>0.419</td><td>0.505</td><td>0.475</td><td>0.672</td><td>0.571</td><td>0.543</td><td>0.510</td><td>0.600</td><td>0.546</td><td>1.214</td><td>0.665</td><td></td></tr><tr><td>192</td><td>21.36%</td><td>0.340</td><td>0.369</td><td>0.349</td><td>0.375</td><td>0.335</td><td>0.365</td><td>0.426</td><td>0.441</td><td>0.553</td><td>0.496</td><td>0.795</td><td>0.669</td><td>0.557</td><td>0.537</td><td>0.837</td><td>0.700</td><td>1.261</td><td>0.690</td><td></td></tr><tr><td>336</td><td>17.07%</td><td>0.376</td><td>0.393</td><td>0.375</td><td>0.388</td><td>0.369</td><td>0.386</td><td>0.445</td><td>0.459</td><td>0.621</td><td>0.537</td><td>1.212</td><td>0.871</td><td>0.754</td><td>0.655</td><td>1.124</td><td>0.832</td><td>1.283</td><td>0.707</td><td></td></tr><tr><td>720</td><td>21.73%</td><td>0.440</td><td>0.435</td><td>0.433</td><td>0.422</td><td>0.425</td><td>0.421</td><td>0.543</td><td>0.490</td><td>0.671</td><td>0.561</td><td>1.166</td><td>0.823</td><td>0.908</td><td>0.724</td><td>1.153</td><td>0.820</td><td>1.319</td><td>0.729</td><td></td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>17.73%</td><td>0.168</td><td>0.262</td><td>0.167</td><td>0.255</td><td>0.167</td><td>0.260</td><td>0.203</td><td>0.287</td><td>0.255</td><td>0.339</td><td>0.365</td><td>0.453</td><td>0.435</td><td>0.435</td><td>0.507</td><td>0.768</td><td>0.642</td><td>0.266</td><td>0.328</td></tr><tr><td>192</td><td>17.84%</td><td>0.232</td><td>0.308</td><td>0.221</td><td>0.293</td><td>0.224</td><td>0.303</td><td>0.269</td><td>0.328</td><td>0.281</td><td>0.340</td><td>0.533</td><td>0.563</td><td>0.730</td><td>0.673</td><td>0.989</td><td>0.757</td><td>0.340</td><td>0.371</td><td></td></tr><tr><td>336</td><td>15.69%</td><td>0.320</td><td>0.373</td><td>0.274</td><td>0.327</td><td>0.281</td><td>0.342</td><td>0.325</td><td>0.366</td><td>0.339</td><td>0.372</td><td>1.363</td><td>0.887</td><td>1.201</td><td>0.845</td><td>1.334</td><td>0.872</td><td>0.412</td><td>0.410</td><td></td></tr><tr><td>720</td><td>12.58%</td><td>0.413</td><td>0.435</td><td>0.368</td><td>0.384</td><td>0.397</td><td>0.421</td><td>0.421</td><td>0.415</td><td>0.433</td><td>0.432</td><td>3.379</td><td>1.338</td><td>3.625</td><td>1.451</td><td>3.048</td><td>1.328</td><td>0.521</td><td>0.465</td><td></td></tr></table>\n\nMethods* are implemented by us; Other results are from FEDformer [31]  \nTable 2. Multivariate long-term forecasting errors in terms of MSE and MAE, the lower the better. Among them, ILI dataset is with forecasting horizon  $T \\in \\{ {24},{36},{48},{60}\\}$  . For the others,  $T \\in  \\{ {96},{192},{336},{720}\\}$  . Repeat repeats the last value in the look-back window. The best results are highlighted in bold and the best results of Transformers are highlighted with a underline. Accordingly, IMP is the best result of linear models compared to the results of Transformer-based solutions.\n\nthe prediction results on three selected time series datasets with Transformer-based solutions and LTSF-Linear: Electricity (Sequence 1951, Variate 36), Exchange-Rate (Sequence 676, Variate 3), and ETTh2 (Sequence 1241, Variate 2), where these datasets have different temporal patterns. When the input length is 96 steps, and the output horizon is 336 steps, Transformers [28, 30, 31] fail to capture the scale and bias of the future data on Electricity and ETTh2. Moreover, they can hardly predict a proper trend on aperiodic data such as Exchange-Rate. These phenomena further indicate the inadequacy of existing Transformer-based solutions for the LTSF task.\n\n# 5.3. More Analyses on LTSF-Transformers\n\nCan existing LTSF-Transformers extract temporal relations well from longer input sequences? The size of the look-back window greatly impacts forecasting accuracy as\n\nit determines how much we can learn from historical data. Generally speaking, a powerful TSF model with a strong temporal relation extraction capability should be able to achieve better results with larger look-back window sizes.\n\nTo study the impact of input look-back window sizes, we conduct experiments with  $L \\in \\{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720\\}$  for long-term forecasting (T=720). Figure 4 demonstrates the MSE results on two datasets. Similar to the observations from previous studies [27, 30], existing Transformer-based models' performance deteriorates or stays stable when the look-back window size increases. In contrast, the performances of all LTSF-Linear are significantly boosted with the increase of look-back window size. Thus, existing solutions tend to overfit temporal noises instead of extracting temporal information if given a longer sequence, and the input size 96 is exactly suitable for most Transformers.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/6324fd9f116e36d39bfdc06a2688ec83545bbc1a0d97d031cb297d6999d5918b.jpg)  \n(a) Electricity\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/5c8640db174bda5eadba3157b44fe953d1e3b6aef69c6dc253ed3d14a473ab8c.jpg)  \n(b) Exchange-Rate  \nFigure 3. Illustration of the long-term forecasting output (Y-axis) of five models with an input length  $L = 96$  and output length  $T = 192$  (X-axis) on Electricity, Exchange-Rate, and ETTh2, respectively.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/9730ec10c53d1ae25547326678f4b69a123ebf73443bb4865074605b23af7374.jpg)  \n(c) ETTh2\n\nAdditionally, we provide more quantitative results in the Appendix, and our conclusion holds in almost all cases.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/b3d2019745538c1e2b3b4d970194501bdf50376fbd7ed422b82125d1dca0e7c3.jpg)  \n(a) 720 steps-Traffic  \nFigure 4. The MSE results (Y-axis) of models with different lookback window sizes (X-axis) of long-term forecasting  $(\\mathrm{T} = 720)$  on the Traffic and Electricity datasets.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/c2e62f75-5bd0-4b86-9d94-94dee13aa852/4dc7edba58e6ef8df8adf2b9e15668d193258ad2904e9d7793f18d36e665a64b.jpg)  \n(b) 720 steps-Electricity\n\nWhat can be learned for long-term forecasting? While the temporal dynamics in the look-back window significantly impact the forecasting accuracy of short-term time series forecasting, we hypothesize that long-term forecasting depends on whether models can capture the trend and periodicity well only. That is, the farther the forecasting horizon, the less impact the look-back window itself has.\n\n<table><tr><td>Methods</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td></tr><tr><td>Input</td><td>Close</td><td>Far</td><td>Close</td><td>Far</td></tr><tr><td>Electricity</td><td>0.251</td><td>0.265</td><td>0.255</td><td>0.287</td></tr><tr><td>Traffic</td><td>0.631</td><td>0.645</td><td>0.677</td><td>0.675</td></tr></table>\n\nTo validate the above hypothesis, in Table 3, we compare the forecasting accuracy for the same future 720 time steps with data from two different look-back windows: (i). the original input  $\\mathrm{L} = 96$  setting (called Close) and (ii). the far input  $\\mathrm{L} = 96$  setting (called Far) that is before the original\n\n96 time steps. From the experimental results, the performance of the SOTA Transformers drops slightly, indicating these models only capture similar temporal information from the adjacent time series sequence. Since capturing the intrinsic characteristics of the dataset generally does not require a large number of parameters, i.e. one parameter can represent the periodicity. Using too many parameters will even cause overfitting, which partially explains why LTSF-Linear performs better than Transformer-based methods.\n\nAre the self-attention scheme effective for LTSF? We verify whether these complex designs in the existing Transformer (e.g., Informer) are essential. In Table 4, we gradually transform Informer to Linear. First, we replace each self-attention layer by a linear layer, called Att.-Linear, since a self-attention layer can be regarded as a fully-connected layer where weights are dynamically changed. Furthermore, we discard other auxiliary designs (e.g., FFN) in Informer to leave embedding layers and linear layers, named Embed + Linear. Finally, we simplify the model to one linear layer. Surprisingly, the performance of Informer grows with the gradual simplification, indicating the unnecessary of the self-attention scheme and other complex modules at least for existing LTSF benchmarks.\n\nTable 3. Comparison of different input sequences under the MSE metric to explore what LTSF-Transformers depend on. If the input is Close, we use the  $96_{th}, \\ldots, 191_{th}$  time steps as the input sequence. If the input is Far, we use the  $0_{th}, \\ldots, 95_{th}$  time steps. Both of them forecast the  $192_{th}, \\ldots, (192 + 720)_{th}$  time steps.  \n\n<table><tr><td colspan=\"2\">Methods</td><td>Informer</td><td>Att.-Linear</td><td>Embed + Linear</td><td>Linear</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.847</td><td>1.003</td><td>0.173</td><td>0.084</td></tr><tr><td>192</td><td>1.204</td><td>0.979</td><td>0.443</td><td>0.155</td></tr><tr><td>336</td><td>1.672</td><td>1.498</td><td>1.288</td><td>0.301</td></tr><tr><td>720</td><td>2.478</td><td>2.102</td><td>2.026</td><td>0.763</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.865</td><td>0.613</td><td>0.454</td><td>0.400</td></tr><tr><td>192</td><td>1.008</td><td>0.759</td><td>0.686</td><td>0.438</td></tr><tr><td>336</td><td>1.107</td><td>0.921</td><td>0.821</td><td>0.479</td></tr><tr><td>720</td><td>1.181</td><td>0.902</td><td>1.051</td><td>0.515</td></tr></table>\n\nTable 4. The MSE comparisons of gradually transforming Informer to a Linear from the left to right columns. Att.-Linear is a structure that replaces each attention layer with a linear layer. Embed + Linear is to drop other designs and only keeps embedding layers and a linear layer. The look-back window size is 96.\n\nCan existing LTSF-Transformers preserve temporal order well? Self-attention is inherently permutation-\n\n<table><tr><td colspan=\"2\">Methods</td><td colspan=\"3\">Linear</td><td colspan=\"3\">FEDformer</td><td colspan=\"3\">Autoformer</td><td colspan=\"3\">Informer</td></tr><tr><td colspan=\"2\">Predict Length</td><td>Ori.</td><td>Shuf.</td><td>Half-Ex.</td><td>Ori.</td><td>Shuf.</td><td>Half-Ex.</td><td>Ori.</td><td>Shuf.</td><td>Half-Ex.</td><td>Ori.</td><td>Shuf.</td><td>Half-Ex.</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.080</td><td>0.133</td><td>0.169</td><td>0.161</td><td>0.160</td><td>0.162</td><td>0.152</td><td>0.158</td><td>0.160</td><td>0.952</td><td>1.004</td><td>0.959</td></tr><tr><td>192</td><td>0.162</td><td>0.208</td><td>0.243</td><td>0.274</td><td>0.275</td><td>0.275</td><td>0.278</td><td>0.271</td><td>0.277</td><td>1.012</td><td>1.023</td><td>1.014</td></tr><tr><td>336</td><td>0.286</td><td>0.320</td><td>0.345</td><td>0.439</td><td>0.439</td><td>0.439</td><td>0.435</td><td>0.430</td><td>0.435</td><td>1.177</td><td>1.181</td><td>1.177</td></tr><tr><td>720</td><td>0.806</td><td>0.819</td><td>0.836</td><td>1.122</td><td>1.122</td><td>1.122</td><td>1.113</td><td>1.113</td><td>1.113</td><td>1.198</td><td>1.210</td><td>1.196</td></tr><tr><td></td><td>Average Drop</td><td>N/A</td><td>27.26%</td><td>46.81%</td><td>N/A</td><td>-0.09%</td><td>0.20%</td><td>N/A</td><td>0.09%</td><td>1.12%</td><td>N/A</td><td>-0.12%</td><td>-0.18%</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.395</td><td>0.824</td><td>0.431</td><td>0.376</td><td>0.753</td><td>0.405</td><td>0.455</td><td>0.838</td><td>0.458</td><td>0.974</td><td>0.971</td><td>0.971</td></tr><tr><td>192</td><td>0.447</td><td>0.824</td><td>0.471</td><td>0.419</td><td>0.730</td><td>0.436</td><td>0.486</td><td>0.774</td><td>0.491</td><td>1.233</td><td>1.232</td><td>1.231</td></tr><tr><td>336</td><td>0.490</td><td>0.825</td><td>0.505</td><td>0.447</td><td>0.736</td><td>0.453</td><td>0.496</td><td>0.752</td><td>0.497</td><td>1.693</td><td>1.693</td><td>1.691</td></tr><tr><td>720</td><td>0.520</td><td>0.846</td><td>0.528</td><td>0.468</td><td>0.720</td><td>0.470</td><td>0.525</td><td>0.696</td><td>0.524</td><td>2.720</td><td>2.716</td><td>2.715</td></tr><tr><td></td><td>Average Drop</td><td>N/A</td><td>81.06%</td><td>4.78%</td><td>N/A</td><td>73.28%</td><td>3.44%</td><td>N/A</td><td>56.91%</td><td>0.46%</td><td>N/A</td><td>1.98%</td><td>0.18%</td></tr></table>\n\ninvariant, i.e., regardless of the order. However, in timeseries forecasting, the sequence order often plays a crucial role. We argue that even with positional and temporal embeddings, existing Transformer-based methods still suffer from temporal information loss. In Table 5, we shuffle the raw input before the embedding strategies. Two shuffling strategies are presented: Shuf. randomly shuffles the whole input sequences and Half-Ex. exchanges the first half of the input sequence with the second half. Interestingly, compared with the original setting (Ori.) on the Exchange Rate, the performance of all Transformer-based methods does not fluctuate even when the input sequence is randomly shuffled. By contrary, the performance of LTSF-Linear is damaged significantly. These indicate that LTSF-Transformers with different positional and temporal embeddings preserve quite limited temporal relations and are prone to overfit on noisy financial data, while the LTSF-Linear can model the order naturally and avoid overfitting with fewer parameters.\n\nFor the ETTh1 dataset, FEDformer and Autoformer introduce time series inductive bias into their models, making them can extract certain temporal information when the dataset has more clear temporal patterns (e.g., periodicity) than the Exchange Rate. Therefore, the average drops of the two Transformers are  $73.28\\%$  and  $56.91\\%$  under the Shuf. setting, where it loses the whole order information. Moreover, Informer still suffers less from both Shuf. and Half-Ex. settings due to its no such temporal inductive bias. Overall, the average drops of LTSF-Linear are larger than Transformer-based methods for all cases, indicating the existing Transformers do not preserve temporal order well.\n\nHow effective are different embedding strategies? We study the benefits of position and timestamp embeddings used in Transformer-based methods. In Table 6, the forecasting errors of Informer largely increase without positional embeddings (wo/Pos.). Without timestamp embeddings (wo/Temp.) will gradually damage the performance of Informer as the forecasting lengths increase. Since Informer uses a single time step for each token, it is necessary to introduce temporal information in tokens.\n\nTable 5. The MSE comparisons of models when shuffling the raw input sequence. Shuf. randomly shuffles the input sequence. Half-EX. randomly exchanges the first half of the input sequences with the second half. Average Drop is the average performance drop under all forecasting lengths after shuffling. All results are the average test MSE of five runs.  \n\n<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Embedding</td><td colspan=\"4\">Traffic</td></tr><tr><td>96</td><td>192</td><td>336</td><td>720</td></tr><tr><td rowspan=\"4\">FEDformer</td><td>All</td><td>0.597</td><td>0.606</td><td>0.627</td><td>0.649</td></tr><tr><td>wo/Pos.</td><td>0.587</td><td>0.604</td><td>0.621</td><td>0.626</td></tr><tr><td>wo/Temp.</td><td>0.613</td><td>0.623</td><td>0.650</td><td>0.677</td></tr><tr><td>wo/Pos.-Temp.</td><td>0.613</td><td>0.622</td><td>0.648</td><td>0.663</td></tr><tr><td rowspan=\"4\">Autoformer</td><td>All</td><td>0.629</td><td>0.647</td><td>0.676</td><td>0.638</td></tr><tr><td>wo/Pos.</td><td>0.613</td><td>0.616</td><td>0.622</td><td>0.660</td></tr><tr><td>wo/Temp.</td><td>0.681</td><td>0.665</td><td>0.908</td><td>0.769</td></tr><tr><td>wo/Pos.-Temp.</td><td>0.672</td><td>0.811</td><td>1.133</td><td>1.300</td></tr><tr><td rowspan=\"4\">Informer</td><td>All</td><td>0.719</td><td>0.696</td><td>0.777</td><td>0.864</td></tr><tr><td>wo/Pos.</td><td>1.035</td><td>1.186</td><td>1.307</td><td>1.472</td></tr><tr><td>wo/Temp.</td><td>0.754</td><td>0.780</td><td>0.903</td><td>1.259</td></tr><tr><td>wo/Pos.-Temp.</td><td>1.038</td><td>1.351</td><td>1.491</td><td>1.512</td></tr></table>\n\nTable 6. The MSE comparisons of different embedding strategies on Transformer-based methods with look-back window size 96 and forecasting lengths {96, 192, 336, 720}.\n\nRather than using a single time step in each token, FEDformer and Autoformer input a sequence of timestamps to embed the temporal information. Hence, they can achieve comparable or even better performance without fixed positional embeddings. However, without timestamp embeddings, the performance of Autoformer declines rapidly because of the loss of global temporal information. Instead, thanks to the frequency-enhanced module proposed in FEDformer to introduce temporal inductive bias, it suffers less from removing any position/timestamp embeddings.\n\nIs training data size a limiting factor for existing LTSF-Transformers? Some may argue that the poor performance of Transformer-based solutions is due to the small sizes of the benchmark datasets. Unlike computer vision or natural language processing tasks, TSF is performed on collected time series, and it is difficult to scale up the training data size. In fact, the size of the training data would indeed have a significant impact on the model performance. Accordingly, we conduct experiments on Traffic, comparing the performance of the model trained on a full dataset (17,544*0.7 hours), named Ori., with that trained on a shortened dataset (8,760 hours, i.e., 1 year), called Short. Unexpectedly, Table 7 presents that the prediction errors\n\nwith reduced training data are lower in most cases. This might because the whole-year data maintains more clear temporal features than a longer but incomplete data size. While we cannot conclude that we should use less data for training, it demonstrates that the training data scale is not the limiting reason for the performances of Autoformer and FEDformer.\n\n<table><tr><td>Methods</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td></tr><tr><td>Dataset</td><td>Ori.</td><td>Short</td><td>Ori.</td><td>Short</td></tr><tr><td>96</td><td>0.587</td><td>0.568</td><td>0.613</td><td>0.594</td></tr><tr><td>192</td><td>0.604</td><td>0.584</td><td>0.616</td><td>0.621</td></tr><tr><td>336</td><td>0.621</td><td>0.601</td><td>0.622</td><td>0.621</td></tr><tr><td>720</td><td>0.626</td><td>0.608</td><td>0.660</td><td>0.650</td></tr></table>\n\nIs efficiency really a top-level priority? Existing LTSF-Transformers claim that the  $O(L^2)$  complexity of the vanilla Transformer is unaffordable for the LTSF problem. Although they prove to be able to improve the theoretical time and memory complexity from  $O(L^2)$  to  $O(L)$ , it is unclear whether 1) the actual inference time and memory cost on devices are improved, and 2) the memory issue is unacceptable and urgent for today's GPU (e.g., an NVIDIA Titan XP here). In Table 8, we compare the average practical efficiencies with 5 runs. Interestingly, compared with the vanilla Transformer (with the same DMS decoder), most Transformer variants incur similar or even worse inference time and parameters in practice. These follow-ups introduce more additional design elements to make practical costs high. Moreover, the memory cost of the vanilla Transformer is practically acceptable, even for output length  $L = 720$ , which weakens the importance of developing a memory-efficient Transformers, at least for existing benchmarks.\n\nTable 7. The MSE comparison of two training data sizes.  \n\n<table><tr><td>Method</td><td>MACs</td><td>Parameter</td><td>Time</td><td>Memory</td></tr><tr><td>DLinear</td><td>0.04G</td><td>139.7K</td><td>0.4ms</td><td>687MiB</td></tr><tr><td>Transformer√ó</td><td>4.03G</td><td>13.61M</td><td>26.8ms</td><td>6091MiB</td></tr><tr><td>Informer</td><td>3.93G</td><td>14.39M</td><td>49.3ms</td><td>3869MiB</td></tr><tr><td>Autoformer</td><td>4.41G</td><td>14.91M</td><td>164.1ms</td><td>7607MiB</td></tr><tr><td>Pyraformer</td><td>0.80G</td><td>241.4M*</td><td>3.4ms</td><td>7017MiB</td></tr><tr><td>FEDformer</td><td>4.41G</td><td>20.68M</td><td>40.5ms</td><td>4143MiB</td></tr></table>\n\n-  $\\times$  is modified into the same one-step decoder, which is implemented in the source code from Autoformer.  \n- * 236.7M parameters of Pyraformer come from its linear decoder.\n\nTable 8. Comparison of practical efficiency of LTSF-Transformers under  $\\mathrm{L} = {96}$  and  $\\mathrm{T} = {720}$  on the Electricity. MACs are the number of multiply-accumulate operations. We use Dlinear for comparison since it has the double cost in LTSF-Linear. The inference time averages 5 runs.\n",
  "hyperparameter": "Look-back window size L: 96 (standard), with experiments varying from {24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}; Forecasting horizon T: {96, 192, 336, 720} for most datasets, {24, 36, 48, 60} for ILI dataset; Moving average kernel for DLinear decomposition: size 25; Training data split: 70% training for Traffic dataset (17,544 * 0.7 hours); Batch size and learning rate: not explicitly specified in the provided sections; Model parameters: LTSF-Linear uses only 139.7K parameters (for DLinear variant) compared to 13-20M for Transformer variants"
}