{
  "id": "PatchTST（Classification）_2022",
  "paper_title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
  "alias": "PatchTST（Classification）",
  "year": 2022,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "PatchTST introduces two key innovations for long-term time series forecasting: (1) Patching - dividing time series into subseries-level patches to serve as input tokens, which reduces computational complexity quadratically and enables longer lookback windows; (2) Channel-independence - processing each univariate series independently through a shared Transformer backbone, which improves performance and allows flexibility in handling varying numbers of channels. The model also supports self-supervised pre-training via masked patch reconstruction, enabling effective transfer learning.",
  "introduction": "# 1 INTRODUCTION\n\nForecasting is one of the most important tasks in time series analysis. With the rapid growth of deep learning models, the number of research works has increased significantly on this topic (Bryan & Stefan, 2021; Torres et al., 2021; Lara-Benitez et al., 2021). Deep models have shown excellent performance not only on forecasting tasks, but also on representation learning where abstract representation can be extracted and transferred to various downstream tasks such as classification and anomaly detection to attain state-of-the-art performance.\n\nAmong deep learning models, Transformer has achieved great success on various application fields such as natural language processing (NLP) (Kalyan et al., 2021), computer vision (CV) (Khan et al., 2021), speech (Karita et al., 2019), and more recently time series (Wen et al., 2022), benefiting from its attention mechanism which can automatically learn the connections between elements in a sequence, thus becomes ideal for sequential modeling tasks. Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), and FEDformer (Zhou et al., 2022) are among the best variants of the Transformer model successfully applying to time series data. Unfortunately, regardless of the complicated design of Transformer-based models, it is shown in the recent paper (Zeng et al., 2022) that a very simple linear model can outperform all of the previous models on a variety of common benchmarks and it challenges the usefulness of Transformer for time series forecasting. In this paper, we attempt to answer this question by proposing a channel-independence patch time series Transformer (PatchTST) model that contains two key designs:\n\n- Patching. Time series forecasting aims to understand the correlation between data in each different time steps. However, a single time step does not have semantic meaning like a word in a sentence, thus extracting local semantic information is essential in analyzing their connections. Most of the previous works only use point-wise input tokens, or just a handcrafted information\n\n<table><tr><td>Models</td><td>L</td><td>N</td><td>patch</td><td>method</td><td>MSE</td></tr><tr><td rowspan=\"5\">Channel-independent PatchTST</td><td>96</td><td>96</td><td></td><td></td><td>0.518</td></tr><tr><td>380</td><td>96</td><td></td><td>down-sampled</td><td>0.447</td></tr><tr><td>336</td><td>336</td><td></td><td></td><td>0.397</td></tr><tr><td>336</td><td>42</td><td>✓</td><td></td><td>0.367</td></tr><tr><td>336</td><td>42</td><td>✓</td><td>self-supervised</td><td>0.349</td></tr><tr><td rowspan=\"2\">Channel-mixing FEDFormer DLinear</td><td>336</td><td>336</td><td></td><td></td><td>0.597</td></tr><tr><td>336</td><td>336</td><td></td><td></td><td>0.410</td></tr></table>\n\n<table><tr><td colspan=\"4\">Running time (s) with L = 336</td></tr><tr><td>Dataset</td><td>w. patch</td><td>w.o. patch</td><td>Gain</td></tr><tr><td>Traffic</td><td>464</td><td>10040</td><td>x 22</td></tr><tr><td>Electricity</td><td>300</td><td>5730</td><td>x 19</td></tr><tr><td>Weather</td><td>156</td><td>680</td><td>x 4</td></tr></table>\n\nTable 1: A case study of multivariate time series forecasting on Traffic dataset. The prediction horizon is 96. Results with different look-back window  $L$  and number of input tokens  $N$  are reported. The best result is in bold and the second best is underlined. Down-sampled means sampling every 4 step and adding the last value. All the results are from supervised training except the best result which uses self-supervised learning.\n\nfrom series. In contrast, we enhance the locality and capture comprehensive semantic information that is not available in point-level by aggregating time steps into subseries-level patches.\n\n- Channel-independence. A multivariate time series is a multi-channel signal, and each Transformer input token can be represented by data from either a single channel or multiple channels. Depending on the design of input tokens, different variants of the Transformer architecture have been proposed. Channel-mixing refers to the latter case where the input token takes the vector of all time series features and projects it to the embedding space to mix information. On the other hand, channel-independence means that each input token only contains information from a single channel. This was proven to work well with CNN (Zheng et al., 2014) and linear models (Zeng et al., 2022), but hasn't been applied to Transformer-based models yet.\n\nWe offer a snapshot of our key results in Table 1 by doing a case study on Traffic dataset, which consists of 862 time series. Our model has several advantages:\n\n1. Reduction on time and space complexity: The original Transformer has  $O(N^2)$  complexity on both time and space, where  $N$  is the number of input tokens. Without pre-processing,  $N$  will have the same value as input sequence length  $L$ , which becomes a primary bottleneck of computation time and memory in practice. By applying patching, we can reduce  $N$  by a factor of the stride:  $N \\approx L / S$ , thus reducing the complexity quadratically. Table 1 illustrates the usefulness of patching. By setting patch length  $P = 16$  and stride  $S = 8$  with  $L = 336$ , the training time is significantly reduced as much as 22 time on large datasets.  \n2. Capability of learning from longer look-back window: Table 1 shows that by increasing lookback window  $L$  from 96 to 336, MSE can be reduced from 0.518 to 0.397. However, simply extending  $L$  comes at the cost of larger memory and computational usage. Since time series often carries heavily temporal redundant information, some previous work tried to ignore parts of data points by using downsampling or a carefully designed sparse connection of attention (Li et al., 2019) and the model still yields sufficient information to forecast well. We study the case when  $L = 380$  and the time series is sampled every 4 steps with the last point added to sequence, resulting in the number of input tokens being  $N = 96$ . The model achieves better MSE score (0.447) than using the data sequence containing the most recent 96 time steps (0.518), indicating that longer look-back window conveys more important information even with the same number of input tokens. This leads us to think of a question: is there a way to avoid throwing values while maintaining a long look-back window? Patching is a good answer to it. It can group local time steps that may contain similar values while at the same time enables the model to reduce the input token length for computational benefit. As evident in Table 1, MSE score is further reduced from 0.397 to 0.367 with patching when  $L = 336$ .  \n3. Capability of representation learning: With the emergence of powerful self-supervised learning techniques, sophisticated models with multiple non-linear layers of abstraction are required to capture abstract representation of the data. Simple models like linear ones (Zeng et al., 2022) may not be preferred for that task due to its limited expressibility. With our PatchTST model, we not only confirm that Transformer is actually effective for time series forecasting, but also demonstrate the representation capability that can further enhance the forecasting performance. Our PatchTST has achieved the best MSE (0.349) in Table 1.\n\nWe introduce our approach in more detail and conduct extensive experiments in the following sections to conclusively prove our claims. We not only demonstrate the model effectiveness with supervised forecasting results and ablation studies, but also achieves SOTA self-supervised representation learning and transfer learning performance.\n",
  "method": "# 3 PROPOSED METHOD\n\n# 3.1 MODEL STRUCTURE\n\nWe consider the following problem: given a collection of multivariate time series samples with lookback window  $L:(\\pmb{x}_1,\\dots,\\pmb{x}_L)$  where each  $\\pmb{x}_t$  at time step  $t$  is a vector of dimension  $M$ , we would like to forecast  $T$  future values  $(\\pmb{x}_{L + 1},\\dots,\\pmb{x}_{L + T})$ . Our PatchTST is illustrated in Figure 1 where the model makes use of the vanilla Transformer encoder as its core architecture.\n\nForward Process. We denote a  $i$ -th univariate series of length  $L$  starting at time index 1 as  $\\pmb{x}_{1:L}^{(i)} = (x_1^{(i)},\\dots,x_L^{(i)})$  where  $i = 1,\\dots,M$ . The input  $(\\pmb{x}_1,\\dots,\\pmb{x}_L)$  is split to  $M$  univariate series  $\\pmb{x}^{(i)}\\in \\mathbb{R}^{1\\times L}$ , where each of them is fed independently into the Transformer backbone according to\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/389d9dfa-1118-42e7-9e7c-6a5189864169/a1cc77082e5a4be9e0b95927052b1c04fe513412a7a43185c7bab0afccf8e82f.jpg)  \n(a) PatchTST Model Overview\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/389d9dfa-1118-42e7-9e7c-6a5189864169/ecd8f01c592014872af1a9f755a2bae56d400c8f36cb9445e66411bb5de87444.jpg)  \n(b) Transformer Backbone (Supervised)  \n(c) Transformer Backbone (Self-supervised)  \nFigure 1: PatchTST architecture. (a) Multivariate time series data is divided into different channels. They share the same Transformer backbone, but the forward processes are independent. (b) Each channel univariate series is passed through instance normalization operator and segmented into patches. These patches are used as Transformer input tokens. (c) Masked self-supervised representation learning with PatchTST where patches are randomly selected and set to zero. The model will reconstruct the masked patches.\n\nour channel-independence setting. Then the Transformer backbone will provide prediction results  $\\hat{\\pmb{x}}^{(i)} = (\\hat{x}_{L + 1}^{(i)},\\dots,\\hat{x}_{L + T}^{(i)})\\in \\mathbb{R}^{1\\times T}$  accordingly.\n\n**Patching.** Each input univariate time series  $\\boldsymbol{x}^{(i)}$  is first divided into patches which can be either overlapped or non-overlapped. Denote the patch length as  $P$  and the stride - the non-overlapping region between two consecutive patches as  $S$ , then the patching process will generate the a sequence of patches  $\\boldsymbol{x}_p^{(i)} \\in \\mathbb{R}^{P \\times N}$  where  $N$  is the number of patches,  $N = \\left\\lfloor \\frac{(L - P)}{S} \\right\\rfloor + 2$ . Here, we pad  $S$  repeated numbers of the last value  $x_L^{(i)} \\in \\mathbb{R}$  to the end of the original sequence before patching.\n\nWith the use of patches, the number of input tokens can reduce from  $L$  to approximately  $L / S$ . This implies the memory usage and computational complexity of the attention map are quadratically decreased by a factor of  $S$ . Thus constrained on the training time and GPU memory, patch design can allow the model to see the longer historical sequence, which can significantly improve the forecasting performance, as demonstrated in Table 1.\n\nTransformer Encoder. We use a vanilla Transformer encoder that maps the observed signals to the latent representations. The patches are mapped to the Transformer latent space of dimension  $D$  via a trainable linear projection  $\\mathbf{W}_p\\in \\mathbb{R}^{D\\times P}$ , and a learnable additive position encoding  $\\mathbf{W}_{\\mathrm{pos}}\\in \\mathbb{R}^{D\\times N}$  is applied to monitor the temporal order of patches:  $\\boldsymbol{x}_d^{(i)} = \\mathbf{W}_p\\boldsymbol{x}_p^{(i)} + \\mathbf{W}_{\\mathrm{pos}}$ , where  $\\boldsymbol{x}_d^{(i)}\\in \\mathbb{R}^{D\\times N}$  denote the input that will be fed into Transformer encoder in Figure 1. Then each head  $h = 1,\\dots,H$  in multi-head attention will transform them into query matrices  $Q_h^{(i)} = (\\boldsymbol{x}_d^{(i)})^T\\mathbf{W}_h^Q$ , key matrices  $K_h^{(i)} = (\\boldsymbol{x}_d^{(i)})^T\\mathbf{W}_h^K$  and value matrices  $V_h^{(i)} = (\\boldsymbol{x}_d^{(i)})^T\\mathbf{W}_h^V$ , where  $\\mathbf{W}_h^Q$ ,  $\\mathbf{W}_h^K\\in \\mathbb{R}^{D\\times d_k}$  and\n\n$\\mathbf{W}_h^V \\in \\mathbb{R}^{D \\times D}$ . After that a scaled production is used for getting attention output  $\\mathbf{O}_h^{(i)} \\in \\mathbb{R}^{D \\times N}$ :\n\n$$\n(\\mathbf {O} _ {h} ^ {(i)}) ^ {T} = \\operatorname {A t t e n t i o n} (Q _ {h} ^ {(i)}, K _ {h} ^ {(i)}, V _ {h} ^ {(i)}) = \\operatorname {S o f t m a x} (\\frac {Q _ {h} ^ {(i)} K _ {h} ^ {(i) ^ {T}}}{\\sqrt {d _ {k}}}) V _ {h} ^ {(i)}\n$$\n\nThe multi-head attention block also includes BatchNorm $^1$  layers and a feed forward network with residual connections as shown in Figure 1. Afterwards it generates the representation denoted as  $\\pmb{z}^{(i)} \\in \\mathbb{R}^{D \\times N}$ . Finally a flatten layer with linear head is used to obtain the prediction result  $\\hat{\\pmb{x}}^{(i)} = (\\hat{x}_{L+1}^{(i)}, \\dots, \\hat{x}_{L+T}^{(i)}) \\in \\mathbb{R}^{1 \\times T}$ .\n\nLoss Function. We choose to use the MSE loss to measure the discrepancy between the prediction and the ground truth. The loss in each channel is gathered and averaged over  $M$  time series to get the overall objective loss:  $\\mathcal{L} = \\mathbb{E}_{\\boldsymbol{x}}\\frac{1}{M}\\sum_{i = 1}^{M}\\| \\hat{\\boldsymbol{x}}_{L + 1:L + T}^{(i)} - \\boldsymbol{x}_{L + 1:L + T}^{(i)}\\| _2^2.$\n\nInstance Normalization. This technique has recently been proposed to help mitigating the distribution shift effect between the training and testing data (Ulyanov et al., 2016; Kim et al., 2022). It simply normalizes each time series instance  $\\boldsymbol{x}^{(i)}$  with zero mean and unit standard deviation. In essence, we normalize each  $\\boldsymbol{x}^{(i)}$  before patching and the mean and deviation are added back to the output prediction.\n\n# 3.2 REPRESENTATION LEARNING\n\nSelf-supervised representation learning has become a popular approach to extract high level abstract representation from unlabelled data. In this section, we apply PatchTST to obtain useful representation of the multivariate time series. We will show that the learnt representation can be effectively transferred to forecasting tasks. Among popular methods to learn representation via self-supervise pre-training, masked autoencoder has been applied successfully to NLP (Devlin et al., 2018) and CV (He et al., 2021) domains. This technique is conceptually simple: a portion of input sequence is intentionally removed at random and the model is trained to recover the missing contents.\n\nMasked encoder has been recently employed in time series and delivered notable performance on classification and regression tasks (Zerveas et al., 2021). The authors proposed to apply the multivariate time series to Transformer, where each input token is a vector  $\\pmb{x}_i$  consisting of time series values at time step  $i$ -th. Masking is placed randomly within each time series and across different series. However, there are two potential issues with this setting: First, masking is applied at the level of single time steps. The masked values at the current time step can be easily inferred by interpolating with the immediate proceeding or succeeding time values without high level understanding of the entire sequence, which deviates from our goal of learning important abstract representation of the whole signal. Zerveas et al. (2021) proposed complex randomization strategies to resolve the problem in which groups of time series with different sizes are randomly masked.\n\nSecond, the design of the output layer for forecasting task can be troublesome. Given the representation vectors  $\\mathbf{z}_t \\in \\mathbb{R}^D$  corresponding to all  $L$  time steps, mapping these vectors to the output containing  $M$  variables each with prediction horizon  $T$  via a linear map requires a parameter matrix  $\\mathbf{W}$  of dimension  $(L \\cdot D) \\times (M \\cdot T)$ . This matrix can be particularly oversized if either one or all of these four values are large. This may cause overfitting when the number of downstream training samples is scarce.\n\nOur proposed PatchTST can naturally overcome the aforementioned issues. As shown in Figure 1, we use the same Transformer encoder as the supervised settings. The prediction head is removed and a  $D \\times P$  linear layer is attached. As opposed to supervised model where patches can be overlapped, we divide each input sequence into regular non-overlapping patches. It is for convenience to ensure observed patches do not contain information of the masked ones. We then select a subset of the patch indices uniformly at random and mask the patches according to these selected indices with zero values. The model is trained with MSE loss to reconstruct the masked patches.\n\nWe emphasize that each time series will have its own latent representation that are cross-learned via a shared weight mechanism. This design can allow the pre-training data to contain different number of time series than the downstream data, which may not be feasible by other approaches.",
  "experiments": "# 4 EXPERIMENTS\n\n# 4.1 LONG-TERM TIME SERIES FORECASTING\n\nDatasets. We evaluate the performance of our proposed PatchTST on 8 popular datasets, including Weather, Traffic, Electricity, ILI and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). These datasets have been extensively utilized for benchmarking and publicly available on (Wu et al., 2021). The statistics of those datasets are summarized in Table 2. We would like to highlight several large datasets: Weather, Traffic, and Electricity. They have many more number of time series, thus the results would be more stable and less susceptible to overfitting than other smaller datasets.\n\n<table><tr><td>Datasets</td><td>Weather</td><td>Traffic</td><td>Electricity</td><td>ILI</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td></tr><tr><td>Features</td><td>21</td><td>862</td><td>321</td><td>7</td><td>7</td><td>7</td><td>7</td><td>7</td></tr><tr><td>Timesteps</td><td>52696</td><td>17544</td><td>26304</td><td>966</td><td>17420</td><td>17420</td><td>69680</td><td>69680</td></tr></table>\n\nTable 2: Statistics of popular datasets for benchmark.\n\nBaselines and Experimental Settings. We choose the SOTA Transformer-based models, including FEDformer (Zhou et al., 2022), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021), Pyraformer (Liu et al., 2022), LogTrans (Li et al., 2019), and a recent non-Transformer-based model DLinear (Zeng et al., 2022) as our baselines. All of the models are following the same experimental setup with prediction length  $T \\in \\{24,36,48,60\\}$  for ILI dataset and  $T \\in \\{96,192,336,720\\}$  for other datasets as in the original papers. We collect baseline results from Zeng et al. (2022) with the default look-back window  $L = 96$  for Transformer-based models, and  $L = 336$  for DLinear. But in order to avoid under-estimating the baselines, we also run FEDformer, Autoformer and Informer for six different look-back window  $L \\in \\{24,48,96,192,336,720\\}$ , and always choose the best results to create strong baselines. More details about the baselines could be found in Appendix A.1.2. We calculate the MSE and MAE of multivariate time series forecasting as metrics.\n\nModel Variants. We propose two versions of PatchTST in Table 3. PatchTST/64 implies the number of input patches is 64, which uses the look-back window  $L = 512$ . PatchTST/42 means the number of input patches is 42, which has the default look-back window  $L = 336$ . Both of them use patch length  $P = 16$  and stride  $S = 8$ . Thus, we could use PatchTST/42 as a fair comparison to DLinear and other Transformer-based models, and PatchTST/64 to explore even better results on larger datasets. More experimental details are provided in Appendix A.1.\n\nResults. Table 3 shows the multivariate long-term forecasting results. Overall, our model outperform all baseline methods. Quantitatively, compared with the best results that Transformer-based models can offer, PatchTST/64 achieves an overall  $21.0\\%$  reduction on MSE and  $16.7\\%$  reduction on MAE, while PatchTST/42 attains an overall  $20.2\\%$  reduction on MSE and  $16.4\\%$  reduction on MAE. Compared with the DLinear model, PatchTST can still outperform it in general, especially on large datasets (Weather, Traffic, Electricity) and ILI dataset. We also experiment with univariate datasets where the results are provided in Appendix A.3.\n\n# 4.2 REPRESENTATION LEARNING\n\nIn this section, we conduct experiments with masked self-supervised learning where we set the patches to be non-overlapped. Otherwise stated, across all representation learning experiments the input sequence length is chosen to be 512 and patch size is set to 12, which results in 42 patches. We consider high masking ratio where  $40\\%$  of the patches are masked with zero values. We first apply self-supervised pre-training on the datasets mentioned in Section 4.1 for 100 epochs. Once the pre-trained model on each dataset is available, we perform supervised training to evaluate the learned representation with two options: (a) linear probing and (b) end-to-end fine-tuning. With (a), we only train the model head for 20 epochs while freezing the rest of the network; With (b), we apply linear probing for 10 epochs to update the model head and then end-to-end fine-tuning the entire network for 20 epochs. It was proven that a two-step strategy with linear probing followed by fine-tuning can outperform only doing fine-tuning directly (Kumar et al., 2022). We select a few representative results on below, and a full benchmark can be found in Appendix A.5.\n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">PatchTST/64</td><td colspan=\"2\">PatchTST/42</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">Pyraformer</td><td colspan=\"2\">LogTrans</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.149</td><td>0.198</td><td>0.152</td><td>0.199</td><td>0.176</td><td>0.237</td><td>0.238</td><td>0.314</td><td>0.249</td><td>0.329</td><td>0.354</td><td>0.405</td><td>0.896</td><td>0.556</td><td>0.458</td><td>0.490</td></tr><tr><td>192</td><td>0.194</td><td>0.241</td><td>0.197</td><td>0.243</td><td>0.220</td><td>0.282</td><td>0.275</td><td>0.329</td><td>0.325</td><td>0.370</td><td>0.419</td><td>0.434</td><td>0.622</td><td>0.624</td><td>0.658</td><td>0.589</td></tr><tr><td>336</td><td>0.245</td><td>0.282</td><td>0.249</td><td>0.283</td><td>0.265</td><td>0.319</td><td>0.339</td><td>0.377</td><td>0.351</td><td>0.391</td><td>0.583</td><td>0.543</td><td>0.739</td><td>0.753</td><td>0.797</td><td>0.652</td></tr><tr><td>720</td><td>0.314</td><td>0.334</td><td>0.320</td><td>0.335</td><td>0.323</td><td>0.362</td><td>0.389</td><td>0.409</td><td>0.415</td><td>0.426</td><td>0.916</td><td>0.705</td><td>1.004</td><td>0.934</td><td>0.869</td><td>0.675</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.360</td><td>0.249</td><td>0.367</td><td>0.251</td><td>0.410</td><td>0.282</td><td>0.576</td><td>0.359</td><td>0.597</td><td>0.371</td><td>0.733</td><td>0.410</td><td>2.085</td><td>0.468</td><td>0.684</td><td>0.384</td></tr><tr><td>192</td><td>0.379</td><td>0.256</td><td>0.385</td><td>0.259</td><td>0.423</td><td>0.287</td><td>0.610</td><td>0.380</td><td>0.607</td><td>0.382</td><td>0.777</td><td>0.435</td><td>0.867</td><td>0.467</td><td>0.685</td><td>0.390</td></tr><tr><td>336</td><td>0.392</td><td>0.264</td><td>0.398</td><td>0.265</td><td>0.436</td><td>0.296</td><td>0.608</td><td>0.375</td><td>0.623</td><td>0.387</td><td>0.776</td><td>0.434</td><td>0.869</td><td>0.469</td><td>0.734</td><td>0.408</td></tr><tr><td>720</td><td>0.432</td><td>0.286</td><td>0.434</td><td>0.287</td><td>0.466</td><td>0.315</td><td>0.621</td><td>0.375</td><td>0.639</td><td>0.395</td><td>0.827</td><td>0.466</td><td>0.881</td><td>0.473</td><td>0.717</td><td>0.396</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.129</td><td>0.222</td><td>0.130</td><td>0.222</td><td>0.140</td><td>0.237</td><td>0.186</td><td>0.302</td><td>0.196</td><td>0.313</td><td>0.304</td><td>0.393</td><td>0.386</td><td>0.449</td><td>0.258</td><td>0.357</td></tr><tr><td>192</td><td>0.147</td><td>0.240</td><td>0.148</td><td>0.240</td><td>0.153</td><td>0.249</td><td>0.197</td><td>0.311</td><td>0.211</td><td>0.324</td><td>0.327</td><td>0.417</td><td>0.386</td><td>0.443</td><td>0.266</td><td>0.368</td></tr><tr><td>336</td><td>0.163</td><td>0.259</td><td>0.167</td><td>0.261</td><td>0.169</td><td>0.267</td><td>0.213</td><td>0.328</td><td>0.214</td><td>0.327</td><td>0.333</td><td>0.422</td><td>0.378</td><td>0.443</td><td>0.280</td><td>0.380</td></tr><tr><td>720</td><td>0.197</td><td>0.290</td><td>0.202</td><td>0.291</td><td>0.203</td><td>0.301</td><td>0.233</td><td>0.344</td><td>0.236</td><td>0.342</td><td>0.351</td><td>0.427</td><td>0.376</td><td>0.445</td><td>0.283</td><td>0.376</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>1.319</td><td>0.754</td><td>1.522</td><td>0.814</td><td>2.215</td><td>1.081</td><td>2.624</td><td>1.095</td><td>2.906</td><td>1.182</td><td>4.657</td><td>1.449</td><td>1.420</td><td>2.012</td><td>4.480</td><td>1.444</td></tr><tr><td>36</td><td>1.579</td><td>0.870</td><td>1.430</td><td>0.834</td><td>1.963</td><td>0.963</td><td>2.516</td><td>1.021</td><td>2.585</td><td>1.038</td><td>4.650</td><td>1.463</td><td>7.394</td><td>2.031</td><td>4.799</td><td>1.467</td></tr><tr><td>48</td><td>1.553</td><td>0.815</td><td>1.673</td><td>0.854</td><td>2.130</td><td>1.024</td><td>2.505</td><td>1.041</td><td>3.024</td><td>1.145</td><td>5.004</td><td>1.542</td><td>7.551</td><td>2.057</td><td>4.800</td><td>1.468</td></tr><tr><td>60</td><td>1.470</td><td>0.788</td><td>1.529</td><td>0.862</td><td>2.368</td><td>1.096</td><td>2.742</td><td>1.122</td><td>2.761</td><td>1.114</td><td>5.071</td><td>1.543</td><td>7.662</td><td>2.100</td><td>5.278</td><td>1.560</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.370</td><td>0.400</td><td>0.375</td><td>0.399</td><td>0.375</td><td>0.399</td><td>0.376</td><td>0.415</td><td>0.435</td><td>0.446</td><td>0.941</td><td>0.769</td><td>0.664</td><td>0.612</td><td>0.878</td><td>0.740</td></tr><tr><td>192</td><td>0.413</td><td>0.429</td><td>0.414</td><td>0.421</td><td>0.405</td><td>0.416</td><td>0.423</td><td>0.446</td><td>0.456</td><td>0.457</td><td>1.007</td><td>0.786</td><td>0.790</td><td>0.681</td><td>1.037</td><td>0.824</td></tr><tr><td>336</td><td>0.422</td><td>0.440</td><td>0.431</td><td>0.436</td><td>0.439</td><td>0.443</td><td>0.444</td><td>0.462</td><td>0.486</td><td>0.487</td><td>1.038</td><td>0.784</td><td>0.891</td><td>0.738</td><td>1.238</td><td>0.932</td></tr><tr><td>720</td><td>0.447</td><td>0.468</td><td>0.449</td><td>0.466</td><td>0.472</td><td>0.490</td><td>0.469</td><td>0.492</td><td>0.515</td><td>0.517</td><td>1.144</td><td>0.857</td><td>0.963</td><td>0.782</td><td>1.135</td><td>0.852</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.274</td><td>0.337</td><td>0.274</td><td>0.336</td><td>0.289</td><td>0.353</td><td>0.332</td><td>0.374</td><td>0.332</td><td>0.368</td><td>1.549</td><td>0.952</td><td>0.645</td><td>0.597</td><td>2.116</td><td>1.197</td></tr><tr><td>192</td><td>0.341</td><td>0.382</td><td>0.339</td><td>0.379</td><td>0.383</td><td>0.418</td><td>0.407</td><td>0.446</td><td>0.426</td><td>0.434</td><td>3.792</td><td>1.542</td><td>0.788</td><td>0.683</td><td>4.315</td><td>1.635</td></tr><tr><td>336</td><td>0.329</td><td>0.384</td><td>0.331</td><td>0.380</td><td>0.448</td><td>0.465</td><td>0.400</td><td>0.447</td><td>0.477</td><td>0.479</td><td>4.215</td><td>1.642</td><td>0.907</td><td>0.747</td><td>1.124</td><td>1.604</td></tr><tr><td>720</td><td>0.379</td><td>0.422</td><td>0.379</td><td>0.422</td><td>0.605</td><td>0.551</td><td>0.412</td><td>0.469</td><td>0.453</td><td>0.490</td><td>3.656</td><td>1.619</td><td>0.963</td><td>0.783</td><td>3.188</td><td>1.540</td></tr><tr><td rowspan=\"4\">ETTm1</td><td>96</td><td>0.293</td><td>0.346</td><td>0.290</td><td>0.342</td><td>0.299</td><td>0.343</td><td>0.326</td><td>0.390</td><td>0.510</td><td>0.492</td><td>0.626</td><td>0.560</td><td>0.543</td><td>0.510</td><td>0.600</td><td>0.546</td></tr><tr><td>192</td><td>0.333</td><td>0.370</td><td>0.332</td><td>0.369</td><td>0.335</td><td>0.365</td><td>0.365</td><td>0.415</td><td>0.514</td><td>0.495</td><td>0.725</td><td>0.619</td><td>0.557</td><td>0.537</td><td>0.837</td><td>0.700</td></tr><tr><td>336</td><td>0.369</td><td>0.392</td><td>0.366</td><td>0.392</td><td>0.369</td><td>0.386</td><td>0.392</td><td>0.425</td><td>0.510</td><td>0.492</td><td>1.005</td><td>0.741</td><td>0.754</td><td>0.655</td><td>1.124</td><td>0.832</td></tr><tr><td>720</td><td>0.416</td><td>0.420</td><td>0.420</td><td>0.424</td><td>0.425</td><td>0.421</td><td>0.446</td><td>0.458</td><td>0.527</td><td>0.493</td><td>1.133</td><td>0.845</td><td>0.908</td><td>0.724</td><td>1.153</td><td>0.820</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.166</td><td>0.256</td><td>0.165</td><td>0.255</td><td>0.167</td><td>0.260</td><td>0.180</td><td>0.271</td><td>0.205</td><td>0.293</td><td>0.355</td><td>0.462</td><td>0.435</td><td>0.507</td><td>0.768</td><td>0.642</td></tr><tr><td>192</td><td>0.223</td><td>0.296</td><td>0.220</td><td>0.292</td><td>0.224</td><td>0.303</td><td>0.252</td><td>0.318</td><td>0.278</td><td>0.336</td><td>0.595</td><td>0.586</td><td>0.730</td><td>0.673</td><td>0.989</td><td>0.757</td></tr><tr><td>336</td><td>0.274</td><td>0.329</td><td>0.278</td><td>0.329</td><td>0.281</td><td>0.342</td><td>0.324</td><td>0.364</td><td>0.343</td><td>0.379</td><td>1.270</td><td>0.871</td><td>1.201</td><td>0.845</td><td>1.334</td><td>0.872</td></tr><tr><td>720</td><td>0.362</td><td>0.385</td><td>0.367</td><td>0.385</td><td>0.397</td><td>0.421</td><td>0.410</td><td>0.420</td><td>0.414</td><td>0.419</td><td>3.001</td><td>1.267</td><td>3.625</td><td>1.451</td><td>3.048</td><td>1.328</td></tr></table>\n\nTable 3: Multivariate long-term forecasting results with supervised PatchTST. We use prediction lengths  $T \\in \\{24,36,48,60\\}$  for ILI dataset and  $T \\in \\{96,192,336,720\\}$  for the others. The best results are in bold and the second best are underlined.  \n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Models</td><td colspan=\"6\">PatchTST</td><td rowspan=\"2\" colspan=\"2\">DLinear</td><td rowspan=\"2\" colspan=\"2\">FEDformer</td><td rowspan=\"2\" colspan=\"2\">Autoformer</td><td rowspan=\"2\" colspan=\"2\">Informer</td></tr><tr><td colspan=\"2\">Fine-tuning</td><td colspan=\"2\">Lin. Prob.</td><td colspan=\"2\">Sup.</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.144</td><td>0.193</td><td>0.158</td><td>0.209</td><td>0.152</td><td>0.199</td><td>0.176</td><td>0.237</td><td>0.238</td><td>0.314</td><td>0.249</td><td>0.329</td><td>0.354</td><td>0.405</td></tr><tr><td>192</td><td>0.190</td><td>0.236</td><td>0.203</td><td>0.249</td><td>0.197</td><td>0.243</td><td>0.220</td><td>0.282</td><td>0.275</td><td>0.329</td><td>0.325</td><td>0.370</td><td>0.419</td><td>0.434</td></tr><tr><td>336</td><td>0.244</td><td>0.280</td><td>0.251</td><td>0.285</td><td>0.249</td><td>0.283</td><td>0.265</td><td>0.319</td><td>0.339</td><td>0.377</td><td>0.351</td><td>0.391</td><td>0.583</td><td>0.543</td></tr><tr><td>720</td><td>0.320</td><td>0.335</td><td>0.321</td><td>0.336</td><td>0.320</td><td>0.335</td><td>0.323</td><td>0.362</td><td>0.389</td><td>0.409</td><td>0.415</td><td>0.426</td><td>0.916</td><td>0.705</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.352</td><td>0.244</td><td>0.399</td><td>0.294</td><td>0.367</td><td>0.251</td><td>0.410</td><td>0.282</td><td>0.576</td><td>0.359</td><td>0.597</td><td>0.371</td><td>0.733</td><td>0.410</td></tr><tr><td>192</td><td>0.371</td><td>0.253</td><td>0.412</td><td>0.298</td><td>0.385</td><td>0.259</td><td>0.423</td><td>0.287</td><td>0.610</td><td>0.380</td><td>0.607</td><td>0.382</td><td>0.777</td><td>0.435</td></tr><tr><td>336</td><td>0.381</td><td>0.257</td><td>0.425</td><td>0.306</td><td>0.398</td><td>0.265</td><td>0.436</td><td>0.296</td><td>0.608</td><td>0.375</td><td>0.623</td><td>0.387</td><td>0.776</td><td>0.434</td></tr><tr><td>720</td><td>0.425</td><td>0.282</td><td>0.460</td><td>0.323</td><td>0.434</td><td>0.287</td><td>0.466</td><td>0.315</td><td>0.621</td><td>0.375</td><td>0.639</td><td>0.395</td><td>0.827</td><td>0.466</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.126</td><td>0.221</td><td>0.138</td><td>0.237</td><td>0.130</td><td>0.222</td><td>0.140</td><td>0.237</td><td>0.186</td><td>0.302</td><td>0.196</td><td>0.313</td><td>0.304</td><td>0.393</td></tr><tr><td>192</td><td>0.145</td><td>0.238</td><td>0.156</td><td>0.252</td><td>0.148</td><td>0.240</td><td>0.153</td><td>0.249</td><td>0.197</td><td>0.311</td><td>0.211</td><td>0.324</td><td>0.327</td><td>0.417</td></tr><tr><td>336</td><td>0.164</td><td>0.256</td><td>0.170</td><td>0.265</td><td>0.167</td><td>0.261</td><td>0.169</td><td>0.267</td><td>0.213</td><td>0.328</td><td>0.214</td><td>0.327</td><td>0.333</td><td>0.422</td></tr><tr><td>720</td><td>0.193</td><td>0.291</td><td>0.208</td><td>0.297</td><td>0.202</td><td>0.291</td><td>0.203</td><td>0.301</td><td>0.233</td><td>0.344</td><td>0.236</td><td>0.342</td><td>0.351</td><td>0.427</td></tr></table>\n\nTable 4: Multivariate long-term forecasting results with self-supervised PatchTST. We use prediction lengths  $T \\in \\{ {96},{192},{336},{720}\\}$  . The best results are in bold and the second best are underlined.\n\nComparison with Supervised Methods. Table 4 compares the performance of PatchTST (with fine-tuning, linear probing, and supervising from scratch) with other supervised method. As shown in the table, on large datasets our pre-training procedure contributes a clear improvement compared to supervised training from scratch. By just fine-tuning the model head (linear probing), the forecasting performance is already comparable with training the entire network from scratch and better than DLinear. The best results are observed with end-to-end fine-tuning. Self-supervised PatchTST significantly outperforms other Transformer-based models on all the datasets.\n\nTransfer Learning. We test the capability of transferring the pre-trained model to downstream tasks. In particular, we pre-train the model on Electricity dataset and fine-tune on other datasets. We observe from Table 5 that overall the fine-tuning MSE is lightly worse than pre-training and fine-tuning on the same dataset, which is reasonable. The fine-tuning performance is also worse than supervised training in some cases. However, the forecasting performance is still better than other\n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Models</td><td colspan=\"6\">PatchTST</td><td rowspan=\"2\" colspan=\"2\">DLinear</td><td rowspan=\"2\" colspan=\"2\">FEDformer</td><td rowspan=\"2\" colspan=\"2\">Autoformer</td><td rowspan=\"2\" colspan=\"2\">Informer</td></tr><tr><td colspan=\"2\">Fine-tuning</td><td colspan=\"2\">Lin. Prob.</td><td colspan=\"2\">Sup.</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.145</td><td>0.195</td><td>0.163</td><td>0.216</td><td>0.152</td><td>0.199</td><td>0.176</td><td>0.237</td><td>0.238</td><td>0.314</td><td>0.249</td><td>0.329</td><td>0.354</td><td>0.405</td></tr><tr><td>192</td><td>0.193</td><td>0.243</td><td>0.205</td><td>0.252</td><td>0.197</td><td>0.243</td><td>0.220</td><td>0.282</td><td>0.275</td><td>0.329</td><td>0.325</td><td>0.370</td><td>0.419</td><td>0.434</td></tr><tr><td>336</td><td>0.244</td><td>0.280</td><td>0.253</td><td>0.289</td><td>0.249</td><td>0.283</td><td>0.265</td><td>0.319</td><td>0.339</td><td>0.377</td><td>0.351</td><td>0.391</td><td>0.583</td><td>0.543</td></tr><tr><td>720</td><td>0.321</td><td>0.337</td><td>0.320</td><td>0.336</td><td>0.320</td><td>0.335</td><td>0.323</td><td>0.362</td><td>0.389</td><td>0.409</td><td>0.415</td><td>0.426</td><td>0.916</td><td>0.705</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.388</td><td>0.273</td><td>0.400</td><td>0.288</td><td>0.367</td><td>0.251</td><td>0.410</td><td>0.282</td><td>0.576</td><td>0.359</td><td>0.597</td><td>0.371</td><td>0.733</td><td>0.410</td></tr><tr><td>192</td><td>0.400</td><td>0.277</td><td>0.412</td><td>0.293</td><td>0.385</td><td>0.259</td><td>0.423</td><td>0.287</td><td>0.610</td><td>0.380</td><td>0.607</td><td>0.382</td><td>0.777</td><td>0.435</td></tr><tr><td>336</td><td>0.408</td><td>0.280</td><td>0.425</td><td>0.307</td><td>0.398</td><td>0.265</td><td>0.436</td><td>0.296</td><td>0.608</td><td>0.375</td><td>0.623</td><td>0.387</td><td>0.776</td><td>0.434</td></tr><tr><td>720</td><td>0.447</td><td>0.310</td><td>0.457</td><td>0.317</td><td>0.434</td><td>0.287</td><td>0.466</td><td>0.315</td><td>0.621</td><td>0.375</td><td>0.639</td><td>0.395</td><td>0.827</td><td>0.466</td></tr></table>\n\nTable 5: Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is transferred to other datasets. The best results are in bold and the second best are underlined.  \n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Models</td><td rowspan=\"2\">IMP.</td><td colspan=\"4\">PatchTST</td><td rowspan=\"2\" colspan=\"2\">BTSF</td><td rowspan=\"2\" colspan=\"2\">TS2Vec</td><td rowspan=\"2\" colspan=\"2\">TNC</td><td rowspan=\"2\" colspan=\"2\">TS-TCC</td></tr><tr><td colspan=\"2\">Transferred</td><td colspan=\"2\">Self-supervised</td></tr><tr><td colspan=\"2\">Metrics</td><td>MSE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"5\">ETh1</td><td>24</td><td>42.3%</td><td>0.312</td><td>0.362</td><td>0.322</td><td>0.369</td><td>0.541</td><td>0.519</td><td>0.599</td><td>0.534</td><td>0.632</td><td>0.596</td><td>0.653</td><td>0.610</td></tr><tr><td>48</td><td>44.7%</td><td>0.339</td><td>0.378</td><td>0.354</td><td>0.385</td><td>0.613</td><td>0.524</td><td>0.629</td><td>0.555</td><td>0.705</td><td>0.688</td><td>0.720</td><td>0.693</td></tr><tr><td>168</td><td>34.5%</td><td>0.424</td><td>0.437</td><td>0.419</td><td>0.424</td><td>0.640</td><td>0.532</td><td>0.755</td><td>0.636</td><td>1.097</td><td>0.993</td><td>1.129</td><td>1.044</td></tr><tr><td>336</td><td>48.5%</td><td>0.472</td><td>0.472</td><td>0.445</td><td>0.446</td><td>0.864</td><td>0.689</td><td>0.907</td><td>0.717</td><td>1.454</td><td>0.919</td><td>1.492</td><td>1.076</td></tr><tr><td>720</td><td>48.8%</td><td>0.508</td><td>0.507</td><td>0.487</td><td>0.478</td><td>0.993</td><td>0.712</td><td>1.048</td><td>0.790</td><td>1.604</td><td>1.118</td><td>1.603</td><td>1.206</td></tr></table>\n\nTable 6: Representation learning methods comparison. Column name transferred implies pretraining PatchTST on Traffic dataset and transferring the representation to ETTh1, while self-supervised implies both pre-training and linear probing on ETTh1. The best and second best results are in bold and underlined. IMP denotes the improvement on best results of PatchTST compared to that of baselines, which is in the range of  $34.5\\%$  to  $48.8\\%$  on various prediction lengths.\n\nmodels. Note that as opposed to supervised PatchTST where the entire model is trained for each prediction horizon, here we only retrain the linear head or the entire model for much fewer epochs, which results in significant computational time reduction.\n\nComparison with Other Self-supervised Methods. We compare our self-supervised model with BTSF (Yang & Hong, 2022), TS2Vec (Yue et al., 2022), TNC (Tonekaboni et al., 2021), and TS-TCC (Eldele et al., 2021) which are among the state-of-the-art contrastive learning representation methods for time series  ${}^{2}$  . We test the forecasting performance on ETTh1 dataset, where we only apply linear probing after the learned representation is obtained (only fine-tune the last linear layer) to make the comparison fair. Results from Table 6 strongly indicate the superior performance of PatchTST, both from pre-training on its own ETTh1 data (self-supervised columns) or pre-training on Traffic (transferred columns).\n\n# 4.3 ABLATION STUDY\n\nPatching and Channel-independence. We study the effects of patching and channel-independence in Table 7. We include FEDformer as the SOTA benchmark for Transformer-based model. By comparing results with and without the design of patching / channel-independence accordingly, one can observe that both of them are important factors in improving the forecasting performance. The motivation of patching is natural; furthermore this technique improves the running time and memory consumption as shown in Table 1 due to shorter Transformer sequence input. Channel-independence, on the other hand, may not be as intuitive as patching is in terms of the technical advantages. Therefore, we provide an in-depth analysis on the key factors that make channel-independence more preferable in Appendix A.7. More ablation study results are available in Appendix A.4.\n\nVarying Look-back Window. In principle, a longer look-back window increases the receptive field, which will potentially improve the forecasting performance. However, as argued in (Zeng et al., 2022), this phenomenon hasn't been observed in most of the Transformer-based models. We also demonstrate in Figure 2 that in most cases, these Transformer-based baselines have not benefited from longer look-back window  $L$ , which indicates their ineffectiveness in capturing temporal information. In contrast, our PatchTST consistently reduces the MSE scores as the receptive field increases, which confirms our model's capability to learn from longer look-back window.\n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Models</td><td colspan=\"8\">PatchTST</td><td rowspan=\"2\" colspan=\"2\">FEDformer</td></tr><tr><td colspan=\"2\">P+CI</td><td colspan=\"2\">CI</td><td colspan=\"2\">P</td><td colspan=\"2\">Original</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.152</td><td>0.199</td><td>0.164</td><td>0.213</td><td>0.168</td><td>0.223</td><td>0.177</td><td>0.236</td><td>0.238</td><td>0.314</td></tr><tr><td>192</td><td>0.197</td><td>0.243</td><td>0.205</td><td>0.250</td><td>0.213</td><td>0.262</td><td>0.221</td><td>0.270</td><td>0.275</td><td>0.329</td></tr><tr><td>336</td><td>0.249</td><td>0.283</td><td>0.255</td><td>0.289</td><td>0.266</td><td>0.300</td><td>0.271</td><td>0.306</td><td>0.339</td><td>0.377</td></tr><tr><td>720</td><td>0.320</td><td>0.335</td><td>0.327</td><td>0.343</td><td>0.351</td><td>0.359</td><td>0.340</td><td>0.353</td><td>0.389</td><td>0.409</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.367</td><td>0.251</td><td>0.397</td><td>0.271</td><td>0.595</td><td>0.376</td><td>-</td><td>-</td><td>0.576</td><td>0.359</td></tr><tr><td>192</td><td>0.385</td><td>0.259</td><td>0.411</td><td>0.276</td><td>0.612</td><td>0.387</td><td>-</td><td>-</td><td>0.610</td><td>0.380</td></tr><tr><td>336</td><td>0.398</td><td>0.265</td><td>0.423</td><td>0.282</td><td>0.651</td><td>0.391</td><td>-</td><td>-</td><td>0.608</td><td>0.375</td></tr><tr><td>720</td><td>0.434</td><td>0.287</td><td>0.457</td><td>0.309</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.621</td><td>0.375</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.130</td><td>0.222</td><td>0.136</td><td>0.231</td><td>0.196</td><td>0.307</td><td>0.205</td><td>0.318</td><td>0.186</td><td>0.302</td></tr><tr><td>192</td><td>0.148</td><td>0.240</td><td>0.164</td><td>0.263</td><td>0.215</td><td>0.323</td><td>-</td><td>-</td><td>0.197</td><td>0.311</td></tr><tr><td>336</td><td>0.167</td><td>0.261</td><td>0.168</td><td>0.262</td><td>0.228</td><td>0.338</td><td>-</td><td>-</td><td>0.213</td><td>0.328</td></tr><tr><td>720</td><td>0.202</td><td>0.291</td><td>0.219</td><td>0.312</td><td>0.244</td><td>0.345</td><td>-</td><td>-</td><td>0.233</td><td>0.344</td></tr></table>\n\nTable 7: Ablation study of patching and channel-independence in PatchTST. 4 cases are included: (a) both patching and channel-independence are included in model  $(\\mathrm{P} + \\mathrm{CI})$ ; (b) only channel-independence (CI); (c) only patching (P); (d) neither of them is included (Original TST model). PatchTST means supervised PatchTST/42. '-' in table means the model runs out of GPU memory (NVIDIA A40 48GB) even with batch size 1. The best results are in bold.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/389d9dfa-1118-42e7-9e7c-6a5189864169/5b32d84a30f94872dcd33fb254f4ce7f90950107390a6f9f0257e0cf69b6cf73.jpg)  \nFigure 2: Forecasting performance (MSE) with varying look-back windows on 3 large datasets: Electricity, Traffic, and Weather. The look-back windows are selected to be  $L = 24, 48, 96, 192, 336, 720$ , and the prediction horizons are  $T = 96, 720$ . We use supervised PatchTST/42 and other open-source Transformer-based baselines for this experiment.",
  "hyperparameter": "Patch length P=16, stride S=8 for supervised learning; patch length P=12 for self-supervised learning. Lookback window L=512 for PatchTST/64 (64 patches) and L=336 for PatchTST/42 (42 patches). Transformer hidden dimension D (latent space dimension), number of attention heads H, key/query dimension d_k. For self-supervised pre-training: 40% masking ratio, 100 pre-training epochs, 10 epochs linear probing + 20 epochs fine-tuning. MSE loss function used for both supervised and self-supervised training."
}