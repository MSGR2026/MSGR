{
  "id": "Koopa_2023",
  "paper_title": "Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors",
  "alias": "Koopa",
  "year": 2023,
  "domain": "TimeSeries",
  "task": "long_term_forecast",
  "idea": "",
  "introduction": "# 1 Introduction\n\nTime series forecasting has become an essential part of real-world applications, such as weather forecasting, energy consumption, and financial assessment. With numerous available observations, deep learning approaches exhibit superior performance and bring the boom of deep forecasting models. TCNs [4, 43, 47] utilize convolutional kernels and RNNs [12, 22, 37] leverage the recurrent structure to capture underlying temporal patterns. Afterward, attention mechanism [42] becomes the mainstream of sequence modeling and Transformers [31, 48, 53] show great predictive power with the capability of learning point-wise temporal dependencies. And the recent revival of MLPs [32, 51, 52] presents a simple but effective approach to exhibit temporal dependencies by dense weighting.\n\nIn spite of elaboratively designed models, it is a fundamental problem for deep models to generalize on varying distribution [1, 25, 33], which is widely reflected in real-world time series because of inherent non-stationarity. Non-stationary time series is characterized by time-variant statistics and temporal dependencies in different periods [2, 14], inducing a huge distribution gap between training and inference and even among each lookback window. While previous methods [16, 28] tailor existing architectural design to attenuate the adverse effect of non-stationarity, few works research on the theoretical basis that can be applied to deal with time-variant temporal patterns naturally.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/3357f035731e2148c9c8213f5412095a6319688e4cfa186a54a3794c8c79d887.jpg)  \nFigure 1: The measurement function  $g$  maps between non-stationary time series and the nonlinear dynamical system so that the timeline will correspond to a system trajectory. Therefore, time series variations in different periods are reflected as sub-regions of nonlinear dynamics, which can be portrayed and advanced forward in time by linear Koopman operators  $\\{\\mathcal{K}_1,\\mathcal{K}_2,\\mathcal{K}_3\\}$  respectively.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/9b1317351fd9dbfc9a180c70dc54fe8546ec2cb6f207464f030334b18a9a8c1d.jpg)\n\nFrom another perspective, real-world time series acts like time-variant dynamics [6]. As one of the principal approaches to analyze complex dynamics, Koopman theory [20] provides an enlightenment to transform nonlinear system into measurement function space, which can be described by a linear Koopman operator. Several pilot works accomplish the integration with deep learning approaches by employing autoencoder networks [40] and operator-learning [26, 49]. More importantly, it is supported by Koopman theory that for time-variant dynamics, there exists a coordinate transformation of the system, where localized Koopman operators are valid to describe the whole measurement function space into several subspaces with linearization [23, 36]. Therefore, Koopman-based methods are appropriate to learn non-stationary time series dynamics (Figure 1). Besides, the linearity of measurement function space enables us to utilize spectral analysis to interpret nonlinear systems.\n\nIn this paper, we disentangle non-stationary series into time-invariant and time-variant dynamics and propose Kooppa as a novel Koopman forecaster, which is composed of modular Koopman Predictors (KP) to hierarchically describe and advance forward series dynamics. Concretely, we utilize Fourier analysis for dynamics disentangling. And for time-invariant dynamics, the model learns Koopman embedding and linear operators to reveal the implicit transition underlying long-term series. As for the remaining time-variant components that exhibit strong locality, Koopa performs context-aware operator calculation and adaptation within different lookback windows. Besides, Koopman Predictor goes beyond the canonical design of Koopman Autoencoder without the binding reconstruction loss, and we incorporate modular blocks into deep residual architecture [32] to realize end-to-end time series forecasting. Our contributions are summarized as follows:\n\n- From the perspective of modern dynamics Koopman theory, we propose Koopa composed of modular Fourier Filter and Koopman Predictor, which can hierarchically disentangle and exploit time-invariant and time-variant dynamics for time series forecasting.  \n- Based on the linearity of Koopman operators, the proposed model is able to utilize incoming series and adapt to varying dynamics for scaling up forecast horizon.  \n- Compared with state-of-the-art methods, our model achieves competitive performance while saving  $77.3\\%$  training time and  $76.0\\%$  memory averaged from six real-world benchmarks.",
  "method": "# 4 Koopa\n\nWe propose Koopa composed of stackable Koopa Blocks (Figure 2). Each block is obliged to learn the input dynamics and advance it forward for prediction. Instead of struggling to seek one unified operator that governs the whole measurement function space, each Koopa Block is encouraged to learn operators hierarchically by taking the residual of previous block fitted dynamics as its input.\n\n**Koopa Block** As aforementioned, it is essential to disentangle different dynamics and adopt proper operators for non-stationary series forecasting. The proposed block shown in Figure 2 contains Fourier Filter that utilizes frequency domain statistics to disentangle time-variant and time-invariant components and implements two types of Koopman Predictor (KP) to obtain Koopman embedding respectively. In Time-invariant KP, we set the operator as a model parameter to be globally learned from lookback-forecast windows. In Time-variant KP, analytical operator solutions are calculated locally within the lookback window, with series segments arranged as snapshots. In detail, we formulate the  $b$ -th block input  $X^{(b)}$  as  $[x_1,x_2,\\dots ,x_T]^\\top \\in \\mathbb{R}^{T\\times C}$ , where  $T$  and  $C$  denote the lookback window length and the variate number. The target is to output a forecast window of length  $H$ . Our proposed Fourier Filter conducts disentanglement at the beginning of each block:\n\n$$\nX _ {\\text {v a r}} ^ {(b)}, X _ {\\text {i n v}} ^ {(b)} = \\text {F o u r i e r F i l t e r} (X ^ {(b)}). \\tag {3}\n$$\n\nRespective KPs will predict with time-invariant input  $X_{\\mathrm{inv}}^{(b)}$  and time-variant input  $X_{\\mathrm{var}}^{(b)}$ , and Time-variant KP simultaneously outputs the fitted input  $\\hat{X}_{\\mathrm{var}}^{(b)}$ :\n\n$$\nY _ {\\text {i n v}} ^ {(b)} = \\operatorname {T i m e I n v K P} \\left(X _ {\\text {i n v}} ^ {(b)}\\right), \\tag {4}\n$$\n\n$$\n\\hat {X} _ {\\text {v a r}} ^ {(b)}, Y _ {\\text {v a r}} ^ {(b)} = \\operatorname {T i m e V a r K P} \\left(X _ {\\text {v a r}} ^ {(b)}\\right).\n$$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/7e21dbbc10313cb1a9a5c7ecb61b1c44c81765bbb54af114579ce3a17fb90670.jpg)  \nFigure 3: Left: Time-invariant KP learns Koopman embedding and operator with time-invariant components globally from all windows. Right: Time-variant KP conducts localized operator calculation within lookback window and advances dynamics forward with the obtained operator for predictions.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/d2950a21f9dfd1663191cc064b7916592f2020139f592ab5e8431937c58e3b13.jpg)\n\nUnlike KAEs [45, 44] that introduce a loss term for rigorous reconstruction of the lookback-window series, we feed the residual  $X^{(b + 1)}$  as the input of next block for learning a corrective operator. And the model forecast  $Y$  is the sum of predicted components  $Y_{\\mathrm{var}}^{(b)}, Y_{\\mathrm{inv}}^{(b)}$  gathered from all Koopa Blocks:\n\n$$\nX ^ {(b + 1)} = X _ {\\text {v a r}} ^ {(b)} - \\hat {X} _ {\\text {v a r}} ^ {(b)}, Y = \\sum \\left(Y _ {\\text {v a r}} ^ {(b)} + Y _ {\\text {i n v}} ^ {(b)}\\right). \\tag {5}\n$$\n\nFourier Filter To disentangle the series components, we leverage Fourier analysis to find the globally shared and localized frequency spectrums reflected on different periods. Concretely, we precompute the Fast Fourier Transform (FFT) of each lookback window of the training set, calculate the averaged amplitude of each spectrum  $S = \\{0,1,\\dots ,[T / 2]\\}$ , and sort them by corresponding amplitude. We take the top percent of  $\\alpha$  as the subset  $\\mathcal{G}_{\\alpha}\\subset S$ , which contains dominant spectrums shared among all lookback windows and exhibits time-invariant dynamics underlying the dataset. And the remaining spectrums are the specific ingredient for varying windows in different periods. Therefore, we divide the spectrums  $S$  into  $\\mathcal{G}_{\\alpha}$  and its complementary set  $\\bar{\\mathcal{G}}_{\\alpha}$ . During training and inference, FourierFilter(·) conducts the disentanglement of input  $X$  (block superscript omitted) as\n\n$$\nX _ {\\text {i n v}} = \\mathcal {F} ^ {- 1} \\left(\\operatorname {F i l t e r} \\left(\\mathcal {G} _ {\\alpha}, \\mathcal {F} (X)\\right)\\right),\n$$\n\n$$\n\\begin{array}{l} X _ {\\text {i n v}} = \\mathcal {F} ^ {- 1} \\left(\\operatorname {F i l t e r} \\left(\\bar {\\mathcal {G}} _ {\\alpha}, \\mathcal {F} (X)\\right)\\right) = X - X _ {\\text {i n v}}, \\end{array} \\tag {6}\n$$\n\nwhere  $\\mathcal{F}$  means FFT,  $\\mathcal{F}^{-1}$  is its inverse and  $\\mathrm{Filter}(\\cdot)$  only passes corresponding frequency spectrums with the given set. We validate the disentangling effect of our proposed Fourier Filter in Section 5.2 by calculating the variation degree of temporal dependencies in the disentangled series.\n\nTime-invariant KP Time-invariant KP is designed to portray the globally shared dynamics, which discovers the direct transition from lookback window to forecast window as  $\\mathbf{F}:X_{\\mathrm{inv}}\\mapsto Y_{\\mathrm{inv}}$ . Concretely, we introduce a pair of Encoder:  $\\mathbb{R}^{T\\times C}\\mapsto \\mathbb{R}^D$  and Decoder:  $\\mathbb{R}^D\\mapsto \\mathbb{R}^{H\\times C}$  to learn the common Koopman embedding for the time-invariant components of running window pairs, where  $D$  denotes the embedding dimension. Working on the data-driven measurement function, we introduce the operator  $K_{\\mathrm{inv}}\\in \\mathbb{R}^{D\\times D}$  as a learnable parameter in each Time-invariant KP, which regards the embedding of lookback and forecast window  $Z_{\\mathrm{back}},Z_{\\mathrm{fore}}\\in \\mathbb{R}^{D}$  as running snapshot pairs. The procedure is shown in Figure 3 and TimeInvKP(·) is formulated as follows:\n\n$$\nZ _ {\\text {b a c k}} = \\operatorname {E n c o d e r} \\left(X _ {\\text {i n v}}\\right), Z _ {\\text {f o r e}} = K _ {\\text {i n v}} Z _ {\\text {b a c k}}, Y _ {\\text {i n v}} = \\operatorname {D e c o d e r} \\left(Z _ {\\text {f o r e}}\\right). \\tag {7}\n$$\n\nTime-variant KP As time-variant dynamics changes continuously, we utilize localized snapshots in a window, which constitute a temporal neighborhood more likely to be linearized. To obtain semantic snapshots and reduce iterations, the input  $X_{\\mathrm{var}}$  is divided into  $\\frac{T}{S}$  segments  $\\mathbf{x}_j$  of length  $S$ :\n\n$$\n\\mathbf {x} _ {j} = \\left[ x _ {(j - 1) S + 1}, \\dots , x _ {j S} \\right] ^ {\\top} \\in \\mathbb {R} ^ {S \\times C}, j = 1, 2, \\dots , T / S. \\tag {8}\n$$\n\nWe assume  $S$  is divisible by  $T$  and  $H$ ; otherwise, we pad the input or truncate the output to make it compatible. Time-variant KP aims to portray localized dynamics, which is manifested analytically as the segment-wise transition  $\\mathbf{F}:\\mathbf{x}_t\\mapsto \\mathbf{x}_{t + 1}$  with observed snapshots. We utilize another pair of Encoder:  $\\mathbb{R}^{S\\times C}\\mapsto \\mathbb{R}^D$  to transform each segment into Koopman embedding  $z_{j}$  and Decoder:  $\\mathbb{R}^D\\mapsto \\mathbb{R}^{S\\times C}$  to transform the fitted or predicted embedding  $\\hat{z}_j$  back to time segments  $\\hat{\\mathbf{x}}_j$ :\n\n$$\nz _ {j} = \\operatorname {E n c o d e r} \\left(\\mathbf {x} _ {j}\\right), \\hat {\\mathbf {x}} _ {j} = \\operatorname {D e c o d e r} \\left(\\hat {z} _ {j}\\right). \\tag {9}\n$$\n\nGiven snapshots collection  $Z = [z_{1},\\dots ,z_{\\frac{T}{S}}]\\in \\mathbb{R}^{D\\times \\frac{T}{S}}$ , we leverage eDMD [45] to find the best fitted matrix that advances forward the system. We apply one-step operator approximation as follows:\n\n$$\nZ _ {\\text {b a c k}} = \\left[ z _ {1}, z _ {2}, \\dots , z _ {\\frac {T}{S} - 1} \\right], Z _ {\\text {f o r e}} = \\left[ z _ {2}, z _ {3}, \\dots , z _ {\\frac {T}{S}} \\right], K _ {\\text {v a r}} = Z _ {\\text {f o r e}} Z _ {\\text {b a c k}} ^ {\\dagger}, \\tag {10}\n$$\n\nwhere  $Z_{\\mathrm{back}}^{\\dagger} \\in \\mathbb{R}^{\\left(\\frac{T}{S} - 1\\right) \\times D}$  is the Moore-Penrose inverse of lookback window embedding collection. The calculated  $K_{\\mathrm{var}} \\in \\mathbb{R}^{D \\times D}$  varies with windows and helps to analyze local temporal variations as a linear system. With the calculated operator, the fitted embedding is formulated as follows:\n\n$$\n[ \\hat {z} _ {1}, \\hat {z} _ {2}, \\dots , \\hat {z} _ {\\frac {T}{S}} ] = [ z _ {1}, K _ {\\text {v a r}} z _ {1}, \\dots , K _ {\\text {v a r}} z _ {\\frac {T}{S} - 1} ] = [ z _ {1}, K _ {\\text {v a r}} Z _ {\\text {b a c k}} ]. \\tag {11}\n$$\n\nTo obtain a prediction of length  $H$ , we iterate operator forwarding to get  $\\frac{H}{S}$  predicted embedding:\n\n$$\n\\hat {z} _ {\\frac {T}{S} + t} = \\left(K _ {\\text {v a r}}\\right) ^ {t} z _ {\\frac {T}{S}}, t = 1, 2, \\dots , H / S. \\tag {12}\n$$\n\nFinally, we arrange the segments transformed by  $\\operatorname{Decoder}(\\cdot)$  as the module outputs  $\\hat{X}_{\\mathrm{var}}, Y_{\\mathrm{var}}$ . The whole procedure is shown in Figure 3 and TimeVarKP( $\\cdot$ ) can be formulated as Equation 8-13.\n\n$$\n\\hat {X} _ {\\text {v a r}} = \\left[ \\hat {\\mathbf {x}} _ {1}, \\dots , \\hat {\\mathbf {x}} _ {\\frac {T}{S}} \\right] ^ {\\top}, Y _ {\\text {v a r}} = \\left[ \\hat {\\mathbf {x}} _ {\\frac {T}{S} + 1}, \\dots , \\hat {\\mathbf {x}} _ {\\frac {T}{S} + \\frac {H}{S}} \\right] ^ {\\top}. \\tag {13}\n$$\n\nForecasting Objective In Koopa, Encoder, Decoder and  $K_{\\mathrm{inv}}$  are learnable parameters, while  $K_{\\mathrm{var}}$  is calculated on-the-fly. To maintain the Koopman embedding consistency in different blocks, we share Encoder, Decoder in Time-variant and Time-invariant KPs, which are formulated as  $\\phi_{\\mathrm{var}}$  and  $\\phi_{\\mathrm{inv}}$  respectively, and use the MSE loss with the ground truth  $Y_{\\mathrm{gt}}$  for parameter optimization:\n\n$$\n\\operatorname {a r g m i n} _ {K _ {\\text {i n v}}, \\phi_ {\\text {v a r}}, \\phi_ {\\text {i n v}}} \\mathcal {L} _ {\\mathrm {M S E}} (Y, Y _ {\\mathrm {g t}}). \\tag {14}\n$$\n\nOptimizing by a single forecasting objective based on the assumption that if reconstruction failed, the prediction must also fail. Thus eliminating forecast discrepancy helps for fitting observed dynamics.\n",
  "experiments": "# 5 Experiments\n\nDatasets We conduct extensive experiments to evaluate the performance and efficiency of Koopa. For multivariate forecasting, we include six real-world benchmarks used in Autoformer [48]: ECL (UCI), ETT [53], Exchange [22], ILI (CDC), Traffic (PeMS), and Weather (Wetterstation). For univariate forecasting, we evaluate the performance on the well-acknowledged M4 dataset [39], which contains four subsets of periodically collected univariate marketing data. And we follow the data processing and split ratio used in TimesNet [47].\n\nNotably, instead of setting a fixed lookback window length, for every forecast window length  $H$ , we set the length of lookback window  $T = 2H$  as the same with N-BEATS [32], because historical observations are always available in real-world scenarios and it can be beneficial for deep models to leverage more observed data with the increasing forecast horizon.\n\nBaselines We extensively compare Koopa with the state-of-the-art deep forecasting models, including Transformer-based model: Autoformer [48], PatchTST [31]; TCN-based model: TimesNet [47], MICN [43]; MLP-based model: DLinear [51]; Fourier forecaster: FiLM [54], and Koopman forecaster: KNF [44]. We also introduce additional specialized models N-HiTS [7] and N-BEATS [32] for univariate forecasting as competitive baselines. All the baselines we reproduced are implemented based on the original paper or official code. We repeat each experiment three times with different random seeds and report the test MSE/MAE. And we provide detailed code implementation and hyperparameters sensitivity in Appendix C.\n\n# 5.1 Time Series Forecasting\n\nForecasting results We list the results in Table 1-2 with the best in bold and the second underlined. Koopa shows competitive forecasting performance in both multivariate and univariate forecasting. Concretely, Koopa achieves state-of-the-art performance in more than  $70\\%$  multivariate settings and consistently outperforms other deep models in the univariate settings.\n\nNotably, Koopa surpasses the state-of-the-art Koopman-based forecaster KNF by a large margin in real-world time series, which can be attributed to our hierarchical dynamics learning and disentangling mechanism. Also, as the representative of efficient linear models, the performance of DLinear is still subpar in ILI, Traffic and Weather, indicating that nonlinear dynamics underlying the time series poses challenges for model capacity and point-wise weighting may not be appropriate to portray time-variant dynamics. Besides, compared with painstakingly trained PatchTST with channel-independence mechanism, our model can achieve a close and even better performance with naturally addressed non-stationary properties of real-world time series.\n\nTable 1: Multivariate forecasting results with different forecast lengths  $H \\in \\{24,36,48,60\\}$  for ILI and  $H \\in \\{48,96,144,192\\}$  for others. We set the lookback length  $T = 2H$ . Additional results (ETTm1, ETTm2, ETTh1) are provided in Appendix D.1.  \n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">Koopa</td><td colspan=\"2\">PatchTST</td><td colspan=\"2\">TimesNet</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">MICN</td><td colspan=\"2\">KNF</td><td colspan=\"2\">FiLM</td><td colspan=\"2\">Autoformer</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">ECL</td><td>48</td><td>0.130</td><td>0.234</td><td>0.147</td><td>0.246</td><td>0.149</td><td>0.254</td><td>0.158</td><td>0.241</td><td>0.156</td><td>0.271</td><td>0.175</td><td>0.265</td><td>0.197</td><td>0.270</td><td>0.164</td><td>0.272</td></tr><tr><td>96</td><td>0.136</td><td>0.236</td><td>0.143</td><td>0.241</td><td>0.170</td><td>0.275</td><td>0.153</td><td>0.245</td><td>0.165</td><td>0.277</td><td>0.198</td><td>0.284</td><td>0.238</td><td>0.341</td><td>0.182</td><td>0.289</td></tr><tr><td>144</td><td>0.149</td><td>0.247</td><td>0.145</td><td>0.241</td><td>0.183</td><td>0.287</td><td>0.152</td><td>0.245</td><td>0.163</td><td>0.274</td><td>0.204</td><td>0.297</td><td>0.234</td><td>0.338</td><td>0.210</td><td>0.315</td></tr><tr><td>192</td><td>0.156</td><td>0.254</td><td>0.147</td><td>0.240</td><td>0.189</td><td>0.291</td><td>0.153</td><td>0.246</td><td>0.171</td><td>0.284</td><td>0.245</td><td>0.321</td><td>0.240</td><td>0.339</td><td>0.221</td><td>0.324</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>48</td><td>0.226</td><td>0.300</td><td>0.223</td><td>0.297</td><td>0.241</td><td>0.319</td><td>0.226</td><td>0.305</td><td>0.260</td><td>0.336</td><td>0.385</td><td>0.376</td><td>0.261</td><td>0.324</td><td>0.355</td><td>0.380</td></tr><tr><td>96</td><td>0.297</td><td>0.349</td><td>0.300</td><td>0.353</td><td>0.325</td><td>0.376</td><td>0.294</td><td>0.351</td><td>0.343</td><td>0.393</td><td>0.433</td><td>0.446</td><td>0.322</td><td>0.372</td><td>0.427</td><td>0.432</td></tr><tr><td>144</td><td>0.333</td><td>0.381</td><td>0.346</td><td>0.390</td><td>0.374</td><td>0.408</td><td>0.354</td><td>0.397</td><td>0.374</td><td>0.411</td><td>0.441</td><td>0.456</td><td>0.352</td><td>0.397</td><td>0.457</td><td>0.461</td></tr><tr><td>192</td><td>0.356</td><td>0.393</td><td>0.383</td><td>0.406</td><td>0.394</td><td>0.434</td><td>0.385</td><td>0.418</td><td>0.455</td><td>0.464</td><td>0.528</td><td>0.503</td><td>0.361</td><td>0.410</td><td>0.503</td><td>0.491</td></tr><tr><td rowspan=\"4\">Exchange</td><td>48</td><td>0.042</td><td>0.143</td><td>0.044</td><td>0.144</td><td>0.059</td><td>0.172</td><td>0.043</td><td>0.145</td><td>0.117</td><td>0.248</td><td>0.128</td><td>0.271</td><td>0.071</td><td>0.192</td><td>0.125</td><td>0.252</td></tr><tr><td>96</td><td>0.083</td><td>0.207</td><td>0.085</td><td>0.204</td><td>0.120</td><td>0.255</td><td>0.084</td><td>0.220</td><td>0.108</td><td>0.251</td><td>0.294</td><td>0.394</td><td>0.112</td><td>0.245</td><td>0.280</td><td>0.386</td></tr><tr><td>144</td><td>0.130</td><td>0.261</td><td>0.132</td><td>0.260</td><td>0.206</td><td>0.334</td><td>0.132</td><td>0.253</td><td>0.152</td><td>0.301</td><td>0.597</td><td>0.578</td><td>0.174</td><td>0.306</td><td>0.520</td><td>0.523</td></tr><tr><td>192</td><td>0.184</td><td>0.309</td><td>0.174</td><td>0.300</td><td>0.377</td><td>0.463</td><td>0.178</td><td>0.299</td><td>0.187</td><td>0.331</td><td>0.654</td><td>0.595</td><td>0.241</td><td>0.364</td><td>0.653</td><td>0.592</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>1.621</td><td>0.800</td><td>2.063</td><td>0.881</td><td>2.464</td><td>1.039</td><td>2.624</td><td>1.118</td><td>4.380</td><td>1.558</td><td>3.722</td><td>1.432</td><td>3.590</td><td>1.424</td><td>2.831</td><td>1.085</td></tr><tr><td>36</td><td>1.803</td><td>0.855</td><td>2.178</td><td>0.943</td><td>2.388</td><td>1.007</td><td>2.693</td><td>1.156</td><td>3.314</td><td>1.313</td><td>3.941</td><td>1.448</td><td>4.200</td><td>1.383</td><td>2.801</td><td>1.088</td></tr><tr><td>48</td><td>1.768</td><td>0.903</td><td>1.916</td><td>0.896</td><td>2.370</td><td>1.040</td><td>2.852</td><td>1.229</td><td>2.457</td><td>1.085</td><td>3.287</td><td>1.377</td><td>3.317</td><td>1.417</td><td>2.322</td><td>1.006</td></tr><tr><td>60</td><td>1.743</td><td>0.891</td><td>1.981</td><td>0.917</td><td>2.193</td><td>1.003</td><td>2.554</td><td>1.144</td><td>2.379</td><td>1.040</td><td>2.974</td><td>1.301</td><td>4.077</td><td>1.444</td><td>2.470</td><td>1.061</td></tr><tr><td rowspan=\"4\">Traffic</td><td>48</td><td>0.415</td><td>0.274</td><td>0.426</td><td>0.286</td><td>0.567</td><td>0.306</td><td>0.488</td><td>0.352</td><td>0.496</td><td>0.301</td><td>0.621</td><td>0.382</td><td>0.498</td><td>0.312</td><td>0.640</td><td>0.361</td></tr><tr><td>96</td><td>0.401</td><td>0.275</td><td>0.413</td><td>0.283</td><td>0.611</td><td>0.337</td><td>0.485</td><td>0.336</td><td>0.511</td><td>0.312</td><td>0.645</td><td>0.376</td><td>0.451</td><td>0.297</td><td>0.668</td><td>0.367</td></tr><tr><td>144</td><td>0.397</td><td>0.276</td><td>0.405</td><td>0.278</td><td>0.603</td><td>0.322</td><td>0.452</td><td>0.317</td><td>0.498</td><td>0.309</td><td>0.683</td><td>0.402</td><td>0.430</td><td>0.288</td><td>0.681</td><td>0.379</td></tr><tr><td>192</td><td>0.403</td><td>0.284</td><td>0.404</td><td>0.277</td><td>0.604</td><td>0.321</td><td>0.438</td><td>0.309</td><td>0.494</td><td>0.312</td><td>0.699</td><td>0.405</td><td>0.425</td><td>0.288</td><td>0.692</td><td>0.385</td></tr><tr><td rowspan=\"4\">Weather</td><td>48</td><td>0.126</td><td>0.168</td><td>0.140</td><td>0.179</td><td>0.138</td><td>0.191</td><td>0.156</td><td>0.198</td><td>0.157</td><td>0.217</td><td>0.201</td><td>0.288</td><td>0.160</td><td>0.206</td><td>0.185</td><td>0.240</td></tr><tr><td>96</td><td>0.154</td><td>0.205</td><td>0.160</td><td>0.206</td><td>0.180</td><td>0.231</td><td>0.186</td><td>0.229</td><td>0.187</td><td>0.250</td><td>0.295</td><td>0.308</td><td>0.189</td><td>0.233</td><td>0.230</td><td>0.279</td></tr><tr><td>144</td><td>0.172</td><td>0.225</td><td>0.174</td><td>0.221</td><td>0.190</td><td>0.244</td><td>0.199</td><td>0.244</td><td>0.197</td><td>0.257</td><td>0.394</td><td>0.401</td><td>0.200</td><td>0.245</td><td>0.268</td><td>0.308</td></tr><tr><td>192</td><td>0.193</td><td>0.241</td><td>0.195</td><td>0.243</td><td>0.212</td><td>0.265</td><td>0.217</td><td>0.261</td><td>0.214</td><td>0.270</td><td>0.462</td><td>0.437</td><td>0.219</td><td>0.263</td><td>0.325</td><td>0.347</td></tr><tr><td colspan=\"2\">\\( 1^{\\text{st}} \\) Count</td><td colspan=\"2\">34</td><td colspan=\"2\">11</td><td colspan=\"2\">0</td><td colspan=\"2\">3</td><td colspan=\"2\">0</td><td colspan=\"2\">0</td><td colspan=\"2\">0</td><td colspan=\"2\">0</td></tr></table>\n\nTable 2: Univariate forecasting results for the M4 dataset. We report the weighted average forecasting error from all four subsets and full results are provided in Appendix D.1.  \n\n<table><tr><td></td><td colspan=\"2\">Models</td><td>Koopa</td><td>N-HiTS</td><td>N-BEATS</td><td>PatchTST</td><td>TimesNet</td><td>DLinear</td><td>MICN</td><td>KNF</td><td>FiLM</td><td>Autoformer</td></tr><tr><td rowspan=\"3\">Weighted Average</td><td>sMAPE</td><td>11.863</td><td>11.960</td><td>11.910</td><td>13.022</td><td>11.930</td><td>12.418</td><td>13.023</td><td>12.126</td><td>12.489</td><td>14.057</td><td></td></tr><tr><td>MASE</td><td>1.595</td><td>1.606</td><td>1.613</td><td>1.814</td><td>1.597</td><td>1.656</td><td>1.836</td><td>1.641</td><td>1.690</td><td>1.954</td><td></td></tr><tr><td>OWA</td><td>0.858</td><td>0.861</td><td>0.862</td><td>0.954</td><td>0.867</td><td>0.891</td><td>0.960</td><td>0.874</td><td>0.902</td><td>1.029</td><td></td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/9d98b854030c3c8152dbc8aa1a7edf4b9f9d3495fcf0cf5d10b2a01c86d44374.jpg)  \nFigure 4: Model efficiency comparison. The performance comes from Table 1 with forecast window length  $H = 144$ . Training time and memory footprint are recorded with the same batch size and official code configuration. Full results of all six datasets are provided in Appendix D.3.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/84d70b9dd185f134674c87e71dfef2201906e07e3e25047ed49e985983e9e414.jpg)\n\nModel efficiency We comprehensively evaluate the model efficiency from three aspects: forecasting performance, training speed, and memory footprint. In Figure 4, we compare the efficiency under two representative datasets with different variate numbers (7 in ETTh2 and 862 in Traffic).\n\nCompared with the state-of-the-art forecasting model PatchTST, Koopa saves  $62.3\\%$  and  $96.5\\%$  training time respectively in the ETTh2 and Traffic datasets with only  $26.8\\%$  and  $2.9\\%$  memory footprint. Concretely, the averaged training time and memory ratio of Koopa compared to PatchTST are  $22.7\\%$  and  $24.0\\%$  in all six datasets (see Appendix D.3 for the detail). Besides, as an efficient MLP-based forecaster, Koopa is also capable of learning nonlinear dynamics from time-variant and time-invariant components, and thus achieves a better performance.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/35aa04646e75477c4ed3b7e5b32b8e252cfe88d86ec5b8f4672bc6913904de8a.jpg)  \nFigure 5: Left: Comparison of Degree of Variation (the standard deviation of linear weighting fitted on different periods), we plot respective values of disengaged components on all six datasets. Right: A case of localized Koopman operators calculated on the Exchange dataset at the interval of one year.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/7235d28b196f5ff5a8414abed33f6bda5480162d63148ae6d705458a63b7ad24.jpg)\n\n# 5.2 Model Analysis\n\nDynamics disentanglement To validate the disentangling effect of our proposed Fourier Filter, we divide the whole time series into 20 subsets of different periods and conduct respective linear regression on the components disentangled by Fourier Filter. The standard deviation of the linear weighting reflects the variation of point-to-point temporal dependencies, which works as the manifestation of time-variant property. We plot the value as Degree of Variation (Figure 5 Left). It can be observed that larger deviations occur in the time-variant component, which indicates the proposed module successfully disentangles two types of dynamics from the perspective of frequency domain.\n\nCase study We present a case study on real-world time series (exchange rate) on the right of Figure 5. We sample the lookback window at the interval of one year and visualize the Koopman operators calculated in Time-variant KP. It can be clearly observed that localized operators can exhibit changing temporal patterns in different periods, indicating the necessity of utilizing varying operators to describe time-variant dynamics. And interpretable insights are also presented as series uptrends correspond to heatmaps with large value and downtrends are reflected with small value.\n\nAblation study We conduct ablations on Koopa. As shown in Table 3, Time-variant and Time-invariant KPs perform as complementary modules to explore the dynamics underlying time series, and discarding any one of them will lead to the inferior performance. Besides, we evaluate alternative decomposition filters to disentangle time series dynamics. We find the proposed Fourier Filter conducts effective disentanglement, where the amplitude statistics of frequency spectrums from different periods are utilized to exhibit time-agnostic information. Therefore, Koopa tackling the right dynamics with complementary modules can achieve the best performance.\n\nTable 3: Model ablation. Only  $K_{inv}$  uses one-block Time-invariant KP; Only  $K_{var}$  stacks Time-variant KPs only; Truncated Filter replaces Fourier Filter with High-Low Pass Filter; Branch Switch changes the order of KPs on disentangled components. The averaged results are listed here.  \n\n<table><tr><td>Dataset</td><td colspan=\"2\">ECL</td><td colspan=\"2\">ETTh2</td><td colspan=\"2\">Exchange</td><td colspan=\"2\">ILI</td><td colspan=\"2\">Traffic</td><td colspan=\"2\">Weather</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>Only Kinv</td><td>0.148</td><td>0.250</td><td>0.312</td><td>0.358</td><td>0.120</td><td>0.241</td><td>2.146</td><td>0.963</td><td>0.740</td><td>0.446</td><td>0.170</td><td>0.213</td></tr><tr><td>Only Kvar</td><td>1.547</td><td>0.782</td><td>0.371</td><td>0.405</td><td>0.205</td><td>0.316</td><td>2.370</td><td>1.006</td><td>0.947</td><td>0.544</td><td>0.180</td><td>0.232</td></tr><tr><td>Truncated Filter</td><td>0.155</td><td>0.255</td><td>0.311</td><td>0.362</td><td>0.129</td><td>0.246</td><td>1.988</td><td>0.907</td><td>0.536</td><td>0.334</td><td>0.172</td><td>0.220</td></tr><tr><td>Branch Switch</td><td>0.696</td><td>0.393</td><td>0.344</td><td>0.385</td><td>0.231</td><td>0.325</td><td>2.130</td><td>0.964</td><td>0.451</td><td>0.304</td><td>0.173</td><td>0.221</td></tr><tr><td>Koopa</td><td>0.146</td><td>0.246</td><td>0.303</td><td>0.356</td><td>0.111</td><td>0.230</td><td>1.734</td><td>0.862</td><td>0.419</td><td>0.293</td><td>0.162</td><td>0.211</td></tr></table>\n\nAvoiding rigorous reconstruction Unlike previous Koopman Autoencoders, the proposed Koopman Predictor does not reconstruct the whole dynamics at once, but aims to portray the partial dynamics evolution. Thus we remove the reconstruction branch, which is only utilized during training in previous KAEs. In our deep residual structure, the predictive objective function works as a good optimization indicator. We validate the design in Table 4, where the performance of sorely forecasting objective optimized model is better than with an additional reconstruction loss. Because the end-to-end forecasting objective helps to reduce the optimization gap between training and inference, making it a valuable contribution of applying Koopman operators on end-to-end time series forecasting.\n\nTable 4: Performance comparison of the dynamics learning blocks implemented by our proposed Koopman Predictor (Koopa) and the canonical Koopman Autoencoder [29](KAE).  \n\n<table><tr><td>Dataset</td><td colspan=\"2\">ETTh2</td><td colspan=\"2\">Exchange</td><td colspan=\"2\">ECL</td><td colspan=\"2\">Traffic</td><td colspan=\"2\">Weather</td><td colspan=\"2\">ILI</td></tr><tr><td>Model</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>Koopa</td><td>0.303</td><td>0.356</td><td>0.110</td><td>0.230</td><td>0.143</td><td>0.243</td><td>0.404</td><td>0.277</td><td>0.161</td><td>0.210</td><td>1.734</td><td>0.862</td></tr><tr><td>KAE</td><td>0.312</td><td>0.361</td><td>0.129</td><td>0.248</td><td>0.169</td><td>0.269</td><td>0.463</td><td>0.329</td><td>0.170</td><td>0.217</td><td>2.189</td><td>0.974</td></tr><tr><td>Promotion</td><td colspan=\"2\">2.88%</td><td colspan=\"2\">14.73%</td><td colspan=\"2\">15.38%</td><td colspan=\"2\">12.74%</td><td colspan=\"2\">5.29%</td><td colspan=\"2\">20.79%</td></tr></table>\n\nLearning stable operators We turn to analyze our architectural design from the spectral perspective. The eigenvalues of the operator determine the amplitude of dynamics evolution. As most of nonstationary time series experience the distribution shift and can be regarded as an unstable evolution, the learned Koopman operator with the modulus far from the unit circle will cause non-divergent and even explosive trending in the long term, leading to training failures.\n\nTo tackle this problem generally faced by Koopman-based forecasters, we propose to utilize the disentanglement and deep residual structure. We measure the stability of the operator as the average distance of eigenvalues from the unit circle. As shown in Figure 6, the operator can become more stable by the above two techniques. The disentanglement helps to describe complex dynamics based on the decomposition and appropriate inductive bias can be applied. The architecture where each block is employed to fill the residual of the previously fitted dynamics reduces the difficulty of directly reconstructing complicated dynamics. Each block portrays the basic process driven by a stable operator within its power, which can be aggregated for a complicated non-stationary process.\n\n# 5.3 Scaling Up Forecast Horizon\n\nMost deep forecasting models work as a settled function once trained (e.g. input-  $T$ -output- $H$ ). For scenarios where the prediction horizon is mismatched or long-term, it poses two challenges for the trained model: (1) reuse parameters learned from observed series; (2) utilize incoming ground truth for model adaptation. The practical scenarios, which we name as scaling up forecast horizon, may\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/5c2bb89ce13355f59c2925e0237c26db7c2648dd800f80c4f93afb161f2345c2.jpg)  \nFigure 6: Visualization of the operator stability on the highly non-stationary Exchange dataset. We plot the first block time-invariant operator eigenvalues of the following design: (a) Single-block model with only time-invariant operator. (b) Single-block model with time-invariant and time-variant operators. (c) Two-block model with time-invariant and time-variant operators.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/ce8157933d9ab86e242f0bb51dd30e831c79b75c8b80befb168b879be1cdebb6.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/cab81fbc-531c-4ed2-9ce5-9e92ac626c88/2abf8a1b42db77c00ef50acc7a8d230e0d09a52ebe9174338d0995a6691b2838.jpg)\n\nlead to failure on most deep models but can be naturally tackled by Koopa. In detail, we first train Koopa with forecast length  $H_{\\mathrm{tr}}$  and attempt to apply it on a larger forecast length  $H_{\\mathrm{te}}$ .\n\nMethod Koopa scales up forecast horizon as follows: Since Time-invariant KP has learned the globally shared dynamics and Time-variant KP can calculate localized operator  $K_{\\mathrm{var}}$  within the lookback window, we freeze the parameters of trained Koopa but only use the incoming ground truth to adapt  $K_{\\mathrm{var}}$ . The naive implementation uses incremental Koopman embedding with dimension  $D$  and conducts Equation 10 to obtain an updated operator, which has a complexity of  $\\mathcal{O}(H_{\\mathrm{te}}D^3)$ . We further propose an iterative algorithm with improved  $\\mathcal{O}((H_{\\mathrm{te}} + D)D^2)$  complexity. The detailed method implementations and complexity analysis can be found in Appendix A.\n\nResults As shown in Table 5, the proposed operator adaption mechanism further boosts the performance on the scaling up scenario, which can be attributed to more accurately fitted time-variant dynamics with incoming ground truth snapshots. Besides, the promotion becomes more significant when applied to non-stationary datasets (manifested as large ADF Test Statistic [10]).\n\nTable 5: Scaling up forecast horizon:  $(H_{\\mathrm{tr}}, H_{\\mathrm{te}}) = (24, 48)$  for ILI and  $(H_{\\mathrm{tr}}, H_{\\mathrm{te}}) = (48, 144)$  for others. Koopa conducts vanilla rolling forecast and Koopa OA further introduces operator adaptation.  \n\n<table><tr><td>Dataset ADF Test Statistic</td><td colspan=\"2\">Exchange (-1.889)</td><td colspan=\"2\">ETTh2 (-4.135)</td><td colspan=\"2\">ILI (-5.406)</td><td colspan=\"2\">ECL (-8.483)</td><td colspan=\"2\">Traffic (-15.046)</td><td colspan=\"2\">Weather (-26.661)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>Koopa</td><td>0.214</td><td>0.348</td><td>0.437</td><td>0.429</td><td>2.836</td><td>1.065</td><td>0.199</td><td>0.298</td><td>0.709</td><td>0.437</td><td>0.237</td><td>0.276</td></tr><tr><td>Koopa OA</td><td>0.172</td><td>0.319</td><td>0.372</td><td>0.404</td><td>2.427</td><td>0.907</td><td>0.182</td><td>0.271</td><td>0.699</td><td>0.426</td><td>0.225</td><td>0.264</td></tr><tr><td>Promotion (MSE)</td><td colspan=\"2\">19.6%</td><td colspan=\"2\">14.9%</td><td colspan=\"2\">14.1%</td><td colspan=\"2\">8.5%</td><td colspan=\"2\">1.4%</td><td colspan=\"2\">5.1%</td></tr></table>",
  "hyperparameter": ""
}