{
  "id": "TiDE_2023",
  "paper_title": "Long-term Forecasting with TiDE: Time-series Dense Encoder",
  "alias": "TiDE",
  "year": 2023,
  "domain": "TimeSeries",
  "task": "imputation",
  "idea": "TiDE (Time-series Dense Encoder) is an MLP-based architecture for long-term forecasting that addresses the limitations of both Transformer models and simple linear models. The key innovations are: (1) A feature projection step that reduces dynamic covariate dimensions, followed by a dense encoder that combines past values, projected covariates, and static attributes into a compact representation. (2) A temporal decoder that creates a 'highway' connection from future covariates directly to predictions at each time-step, enabling the model to capture strong direct effects of covariates. (3) A global linear residual connection ensuring linear models remain a subclass. This design achieves superior accuracy while being significantly more efficient than Transformer-based methods, with O(n_e*h^2 + h*L) complexity versus O(K*n_a*L^2/P^2) for PatchTST.",
  "introduction": "# 1 Introduction\n\nLong-term forecasting, which is to predict several steps into the future given a long context or look-back, is one of the most fundamental problems in time series analysis, with broad applications in energy, finance, and transportation. Deep learning models (Wu et al., 2021; Nie et al., 2022) have emerged as a popular approach for forecasting rich, multivariate, time series data, often outperforming classical statistical approaches such as ARIMA or GARCH (Box et al., 2015). In several forecasting competitions such as the M5 competition (Makridakis et al., 2020) and IARAI Traffic4cast contest (Kreil et al., 2020), deep neural networks performed quite well.\n\nVarious neural network architectures have been explored for forecasting, ranging from recurrent neural networks to convolutional networks to graph neural networks. For sequence modeling tasks in domains such as language, speech and vision, Transformers (Vaswani et al., 2017) have emerged as the most successful model,\n\neven outperforming recurrent neural networks (LSTMs)(Hochreiter and Schmidhuber, 1997). Subsequently, there has been a surge of Transformer-based forecasting papers (Wu et al., 2021; Zhou et al., 2021; 2022) in the time-series community that have claimed state-of-the-art (SoTA) forecasting performance for long-horizon tasks. However, recent work (Zeng et al., 2023) has shown that these Transformers-based architectures may not be as powerful as one might expect for time series forecasting, and can be easily outperformed by a simple linear model on forecasting benchmarks. Such a linear model however has deficiencies since it is ill-suited for modeling non-linear dependencies among the time-series sequence and the time-independent covariates. Indeed, a very recent paper (Nie et al., 2022) proposed a new Transformer-based architecture that obtains SoTA performance for deep neural networks on the standard multivariate forecasting benchmarks.\n\nIn this paper, we present a simple and effective deep learning architecture for long-term forecasting that obtains superior performance when compared to existing SoTA neural network based models on the time series forecasting benchmarks. Our Multi-Layer Perceptron (MLP)-based model is embarrassingly simple without any self-attention, recurrent or convolutional mechanism. Therefore, it enjoys a linear computational scaling in terms of the context and horizon lengths unlike many Transformer-based solutions.\n\nThe main contributions of this work are as follows:\n\n- We propose the Time-series Dense Encoder (TiDE) model architecture for long-term time series forecasting. TiDE encodes the past of a time-series along with covariates using dense MLPs and then decodes time-series along with future covariates, again using dense MLPs.  \n- We analyze a simplified linear analogue of our model and prove that this linear model can achieve near optimal error rate in linear dynamical systems (LDS) (Kalman, 1963) when the design matrix of the LDS has maximum singular value bounded away from 1. We empirically verify this on a simulated dataset where the linear model outperforms LSTMs and Transformers.  \n- On popular real-world long-term forecasting benchmarks, our model achieves better or similar performance compared to prior neural network based baselines ( $>10\\%$  lower Mean Squared Error on the largest dataset). At the same time, TiDE is 5x faster in terms of inference and more than 10x faster in training when compared to the best Transformer based model.",
  "method": "# 4 Model\n\nRecently, it has been observed that simple linear models (Zeng et al., 2023) can outperform Transformers based models in several long-term forecasting benchmarks. On the other hand, linear models will fall short when there are inherent non-linearities in the dependence of the future on the past. Furthermore, linear models would not be able to model the dependence of the prediction on the covariates as evidenced by the fact that (Zeng et al., 2023) do not use time-covariates as they hurt performance.\n\nIn this section, we introduce a simple and efficient MLP based architecture for long-term time-series forecasting. In our model we add non-linearities in the form of MLPs in a manner that can handle past data and covariates. The model is dubbed TiDE (Time-series Dense Encoder) as it encodes the past of a time-series along with covariates using dense MLP's and then decodes the encoded time-series along with future covariates.\n\nAn overview of our architecture has been presented in Figure 1. Our model is applied in a channel independent manner (the term was used in (Nie et al., 2022)) i.e the input to the model is the past and covariates of one time-series at a time  $\\left(\\mathbf{y}_{1:L}^{(i)},\\pmb{x}_{1:L}^{(i)},\\pmb{a}^{(i)}\\right)$  and it maps it to the prediction of that time-series  $\\hat{\\mathbf{y}}_{L + 1:L + H}^{(i)}$ . Note that the weights of the model are trained globally using the whole dataset. A key component in our model is the MLP residual block towards the right of the figure.\n\nResidual Block. We use the residual block as the basic layer in our architecture. It is an MLP with one hidden layer with ReLU activation. It also has a skip connection that is fully linear. We use dropout on the linear layer that maps the hidden layer to the output and also use layer norm at the output.\n\nWe separate the model into encoding and decoding sections. The encoding section has a novel feature projection step followed by a dense MLP encoder. The decoder section consists of a dense decoder followed by a novel temporal decoder. Note that the dense encoder (green block with  $n_e$  layers) and decoder blocks (yellow block with  $n_d$  layers) in Figure 1 can be merged into a single block. For the sake of exposition we keep them separate as we tune the hidden layer size in the two blocks separately. Also the last layer of the decoder block is unique in the sense that its output dimension needs to be  $H \\times p$  before the reshape operation.\n\n# 4.1 Encoding\n\nThe task of the encoding step is to map the past and the covariates of a time-series to a dense representation of the features. The encoding in our model has two key steps.\n\nFeature Projection. We use a residual block to map  $\\pmb{x}_t^{(i)}$  at each time-step (both in the look-back and the horizon) into a lower dimensional projection of size  $\\tilde{r} \\ll r$  (temporalWidth). This operation can be described as,\n\n$$\n\\tilde {\\boldsymbol {x}} _ {t} ^ {(i)} = \\operatorname {R e s i d u a l B l o c k} \\left(\\boldsymbol {x} _ {t} ^ {(i)}\\right). \\tag {3}\n$$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/017638d5-07ba-49e1-918d-804194503790/078d23c60b4452e8a9836daf9e53643937d3ae37ce86e78913a7ec86342a3d01.jpg)  \nFigure 1: Overview of TiDE architecture. The dynamic covariates per time-point are mapped to a lower dimensional space using a feature projection step. Then the encoder combines the look-back along with the projected covariates with the static attributes to form an encoding. The decoder maps this encoding to a vector per time-step in the horizon. Then a temporal decoder combines this vector (per time-step) with the projected features of that time-step in the horizon to form the final predictions. We also add a global linear residual connection from the look-back to the horizon.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/017638d5-07ba-49e1-918d-804194503790/b53fa72a283c5f23e6383801fba4bae2169e85372b906b8d726d86669e7e7c23.jpg)\n\nThis is essentially a dimensionality reduction step since flattening the dynamic covariates for the whole look-back and horizon would lead to an input vector of size  $(L + H)r$  which can be prohibitively large. On the other hand, flattening the reduced features would only lead to a dimension of  $(L + H)\\tilde{r}$ .\n\nDense Encoder. As the input to the dense encoder, we stack and flatten all the past and future projected covariates, concatenate them with the static attributes and the past of the time-series. Then we map them to an embedding using an encoder which contains multiple residual blocks. This can be written as,\n\n$$\n\\boldsymbol {e} ^ {(i)} = \\operatorname {E n c o d e r} \\left(\\mathbf {y} _ {1: L} ^ {(i)}; \\tilde {\\boldsymbol {x}} _ {1: L + H} ^ {(i)}; \\boldsymbol {a} ^ {(i)}\\right) \\tag {4}\n$$\n\nThe encoder internal layer sizes are all set to hiddenSize and the total number of layers in the encoder is set to  $n_e$  (numEncoderLayers).\n\n# 4.2 Decoding\n\nThe decoding in our model maps the encoded hidden representations into future predictions of time series. It also comprises of two operations, dense decoder and temporal decoder.\n\nDense Decoder. The first decoding unit is a stacking of several residual blocks like the encoder with the same hidden layer sizes. It takes as an input the encoding  $\\pmb{e}^{(i)}$  and maps it to a vector  $\\pmb{g}^{(i)}$  of size  $H \\times p$  where  $p$  is the decoderOutputDim. This vector is then reshaped to a matrix  $D^{(i)} \\in \\mathbb{R}^{d \\times H}$ . The  $t$ -th column i.e  $d_{t}^{(i)}$  can be thought of as the decoded vector for the  $t$ -th time-period in the horizon for all  $t \\in [H]$ . This whole operation can be described as,\n\n$$\n\\boldsymbol {g} ^ {(i)} = \\operatorname {D e c o d e r} \\left(\\boldsymbol {e} ^ {(i)}\\right) \\quad \\in \\mathbb {R} ^ {p. H}\n$$\n\n$$\n\\boldsymbol {D} ^ {(i)} = \\operatorname {R e s h a p e} \\left(\\boldsymbol {g} ^ {(i)}\\right) \\quad \\in \\mathbb {R} ^ {p \\times H}.\n$$\n\nThe number of layers in the dense decoder is  $n_d$  (numDecoderLayers).\n\nTemporal Decoder. Finally, we use the temporal decoder to generate the final predictions. The temporal decoder is just a residual block with output size 1 that maps the decoded vector  $\\pmb{d}_t^{(i)}$  at  $t$ -th horizon time-step concatenated with the projected covariates  $\\tilde{\\pmb{x}}_{L + t}^{(i)}$  i.e\n\n$$\n\\hat {\\mathbf {y}} _ {L + t} ^ {(i)} = \\mathrm {T e m p o r a l D e c o d e r} \\Big (\\boldsymbol {d} _ {t} ^ {(i)}; \\tilde {\\boldsymbol {x}} _ {L + t} ^ {(i)} \\Big) \\quad \\forall t \\in [ H ].\n$$\n\nThis operation adds a \"highway\" from the future covariates at time-step  $L + t$  to the prediction at time-step  $L + t$ . This can be useful if some covariates have a strong direct effect on a particular time-step's actual value. For instance, in retail demand forecasting a holiday like Mother's day might strongly affect the sales of certain gift items. Such signals can be lost or take longer for the model to learn in absence of such a highway. We denote the hyperparameter controlling the hidden size of the temporal decoder as temporalDecoderHidden.\n\nFinally, we add a global residual connection that linearly maps the look-back  $\\mathbf{y}_{1:L}^{(i)}$  to a vector the size of the horizon which is added to the prediction  $\\hat{\\mathbf{y}}_{L+1:L+H}^{(i)}$ . This ensures that a purely linear model like the one in (Zeng et al., 2023) is always a subclass of our model.\n\nTraining and Evaluation. The model is trained using mini-batch gradient descent where each batch consists of a batchSize number of time-series and the corresponding look-back and horizon time-points. We use MSE as the training loss. Each epoch consists of all look-back and horizon pairs that can be constructed from the training period i.e. two mini-batches can have overlapping time-points. This was standard practice in all prior work on long-term forecasting (Zeng et al., 2023; Liu et al., 2021; Wu et al., 2021; Li et al., 2019a).\n\nThe model is evaluated on a test set on every (look-back, horizon) pair that can be constructed from the test set. This is usually known as rolling validation/evaluation. A similar evaluation on a validation set can be used optionally used to tune parameters for model selection.\n",
  "experiments": "# 5 Experimental Results\n\nIn this section we present our main experimental results on popular long-term forecasting benchmarks. We also perform an ablation study that shows the usefulness of the temporal decoder.\n\n# 5.1 Long-Term Time-Series Forecasting\n\nDatasets. We use seven commonly used long-term forecasting benchmark datasets: Weather, Traffic, Electricity and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). We refer the reader to (Wu et al., 2021) for a detailed discussion on the datasets. In Table 1 we provide some statistics about the datasets. Note\n\nthat Traffic and Electricity are the largest datasets with  $>800$  and  $>300$  time-series each having tens of thousands of time-points. Since we are only interested in long-term forecasting results in this section, we omit the shorter horizon ILI dataset.\n\n<table><tr><td>Dataset</td><td>#Time-Series</td><td>#Time-Points</td><td>Frequency</td></tr><tr><td>Electricity</td><td>321</td><td>26304</td><td>1 Hour</td></tr><tr><td>Traffic</td><td>862</td><td>17544</td><td>1 Hour</td></tr><tr><td>Weather</td><td>21</td><td>52696</td><td>10 Minutes</td></tr><tr><td>ETTh1</td><td>7</td><td>17420</td><td>1 Hour</td></tr><tr><td>ETTh2</td><td>7</td><td>17420</td><td>1 Hour</td></tr><tr><td>ETTm1</td><td>7</td><td>69680</td><td>15 Minutes</td></tr><tr><td>ETTm2</td><td>7</td><td>69680</td><td>15 Minutes</td></tr></table>\n\nTable 1: Summary of datasets.\n\nBaselines and Setup. We choose SOTA Transformers based models for time-series including Fedformer (Zhou et al., 2022), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021), Pyraformer (Liu et al., 2021) and LongTrans (Li et al., 2019a). Recently, DLinear (Zeng et al., 2023) showed that simple linear models can outperform the above methods and therefore DLinear serves as an important baseline. We include N-HiTS (Challu et al., 2023) which is an improvement over the famous NBeats (Oreshkin et al.) model. Finally we compare with PatchTST (Nie et al., 2022) where they showed that vanilla Transformers applied to time-series patches can be very effective. The results for all Transformer based baselines are reported from (Nie et al., 2022).\n\nFor each method, the look-back window was tuned in  $\\{24, 48, 96, 192, 336, 720\\}$ . We report the DLinear numbers directly from the original paper (Zeng et al., 2023). For our method we always use context length of 720 for all horizon lengths in  $\\{96, 192, 336, 720\\}$ . All models were trained using MSE as the training loss. In all the datasets, the train:validation:test ratio is 7:1:2 as dictated by prior work. Note that all the experiments are performed on standard normalized datasets (using the mean and the standard deviations in the training period) in order to be consistent with prior work (Wu et al., 2021).\n\nOur Model. We use the architecture described in Figure 1. We tune our hyper-parameters using the validation set rolling validation error. We provide details about our hyper-parameters in Appendix B.3. As global dynamic covariates, we use simple time-derived features like minute of the hour, hour of the day, day of the week etc which are normalized similar to (Alexandrov et al., 2020). Note that these features were turned off in DLinear since it was observed to hurt the performance of the linear model, however our model can easily handle such features. Our model is trained in Tensorflow (Abadi, 2016) and we optimize using the default settings of the Adam optimizer (Kingma and Ba, 2014). We provide our implementation in the supplementary with scripts to reproduce the results in Table 2.\n\nResults. We present Mean Squared Error (MSE) and Mean Absolute Error (MSE) for all datasets and methods in Table 2. For our model we report the mean metric out of 5 independent runs for each setting. The bold-faced numbers are from the best model or within statistical significance of the best model in terms of two standard error intervals. Note that different predictive statistics are optimal for different target metrics (Awasthi et al., 2021; Gneiting, 2011) and therefore we should look at a target metric that is closely aligned with the training loss in this case. Since all models were trained using MSE let us focus on that column for comparisons.\n\nWe can see that TiDE, PatchTST, N-HiTS and DLinear are much better than the other baselines in all datasets. This can be attributed to the fact that sub-quadratic approximations to the full self-attention mechanism is perhaps not best suited for long term forecasting. The same was observed in the PatchTST (Nie\n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">TiDE</td><td colspan=\"2\">PatchTST/64</td><td colspan=\"2\">N-HiTS</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">Pyraformer</td><td colspan=\"2\">LogTrans</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.166</td><td>0.222</td><td>0.149</td><td>0.198</td><td>0.158</td><td>0.195</td><td>0.176</td><td>0.237</td><td>0.238</td><td>0.314</td><td>0.249</td><td>0.329</td><td>0.354</td><td>0.405</td><td>0.896</td><td>0.556</td><td>0.458</td><td>0.490</td></tr><tr><td>192</td><td>0.209</td><td>0.263</td><td>0.194</td><td>0.241</td><td>0.211</td><td>0.247</td><td>0.220</td><td>0.282</td><td>0.275</td><td>0.329</td><td>0.325</td><td>0.370</td><td>0.419</td><td>0.434</td><td>0.622</td><td>0.624</td><td>0.658</td><td>0.589</td></tr><tr><td>336</td><td>0.254</td><td>0.301</td><td>0.245</td><td>0.282</td><td>0.274</td><td>0.300</td><td>0.265</td><td>0.319</td><td>0.339</td><td>0.377</td><td>0.351</td><td>0.391</td><td>0.583</td><td>0.543</td><td>0.739</td><td>0.753</td><td>0.797</td><td>0.652</td></tr><tr><td>720</td><td>0.313</td><td>0.340</td><td>0.314</td><td>0.334</td><td>0.401</td><td>0.413</td><td>0.323</td><td>0.362</td><td>0.389</td><td>0.409</td><td>0.415</td><td>0.426</td><td>0.916</td><td>0.705</td><td>1.004</td><td>0.934</td><td>0.869</td><td>0.675</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.336</td><td>0.253</td><td>0.360</td><td>0.249</td><td>0.402</td><td>0.282</td><td>0.410</td><td>0.282</td><td>0.576</td><td>0.359</td><td>0.597</td><td>0.371</td><td>0.733</td><td>0.410</td><td>2.085</td><td>0.468</td><td>0.684</td><td>0.384</td></tr><tr><td>192</td><td>0.346</td><td>0.257</td><td>0.379</td><td>0.256</td><td>0.420</td><td>0.297</td><td>0.423</td><td>0.287</td><td>0.610</td><td>0.380</td><td>0.607</td><td>0.382</td><td>0.777</td><td>0.435</td><td>0.867</td><td>0.467</td><td>0.685</td><td>0.390</td></tr><tr><td>336</td><td>0.355</td><td>0.260</td><td>0.392</td><td>0.264</td><td>0.448</td><td>0.313</td><td>0.436</td><td>0.296</td><td>0.608</td><td>0.375</td><td>0.623</td><td>0.387</td><td>0.776</td><td>0.434</td><td>0.869</td><td>0.469</td><td>0.734</td><td>0.408</td></tr><tr><td>720</td><td>0.386</td><td>0.273</td><td>0.432</td><td>0.286</td><td>0.539</td><td>0.353</td><td>0.466</td><td>0.315</td><td>0.621</td><td>0.375</td><td>0.639</td><td>0.395</td><td>0.827</td><td>0.466</td><td>0.881</td><td>0.473</td><td>0.717</td><td>0.396</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.132</td><td>0.229</td><td>0.129</td><td>0.222</td><td>0.147</td><td>0.249</td><td>0.140</td><td>0.237</td><td>0.186</td><td>0.302</td><td>0.196</td><td>0.313</td><td>0.304</td><td>0.393</td><td>0.386</td><td>0.449</td><td>0.258</td><td>0.357</td></tr><tr><td>192</td><td>0.147</td><td>0.243</td><td>0.147</td><td>0.240</td><td>0.167</td><td>0.269</td><td>0.153</td><td>0.249</td><td>0.197</td><td>0.311</td><td>0.211</td><td>0.324</td><td>0.327</td><td>0.417</td><td>0.386</td><td>0.443</td><td>0.266</td><td>0.368</td></tr><tr><td>336</td><td>0.161</td><td>0.261</td><td>0.163</td><td>0.259</td><td>0.186</td><td>0.290</td><td>0.169</td><td>0.267</td><td>0.213</td><td>0.328</td><td>0.214</td><td>0.327</td><td>0.333</td><td>0.422</td><td>0.378</td><td>0.443</td><td>0.280</td><td>0.380</td></tr><tr><td>720</td><td>0.196</td><td>0.294</td><td>0.197</td><td>0.290</td><td>0.243</td><td>0.340</td><td>0.203</td><td>0.301</td><td>0.233</td><td>0.344</td><td>0.236</td><td>0.342</td><td>0.351</td><td>0.427</td><td>0.376</td><td>0.445</td><td>0.283</td><td>0.376</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.375</td><td>0.398</td><td>0.379</td><td>0.401</td><td>0.378</td><td>0.393</td><td>0.375</td><td>0.399</td><td>0.376</td><td>0.415</td><td>0.435</td><td>0.446</td><td>0.941</td><td>0.769</td><td>0.664</td><td>0.612</td><td>0.878</td><td>0.740</td></tr><tr><td>192</td><td>0.412</td><td>0.422</td><td>0.413</td><td>0.429</td><td>0.427</td><td>0.436</td><td>0.412</td><td>0.420</td><td>0.423</td><td>0.446</td><td>0.456</td><td>0.457</td><td>1.007</td><td>0.786</td><td>0.790</td><td>0.681</td><td>1.037</td><td>0.824</td></tr><tr><td>336</td><td>0.435</td><td>0.433</td><td>0.435</td><td>0.436</td><td>0.458</td><td>0.484</td><td>0.439</td><td>0.443</td><td>0.444</td><td>0.462</td><td>0.486</td><td>0.487</td><td>1.038</td><td>0.784</td><td>0.891</td><td>0.738</td><td>1.238</td><td>0.932</td></tr><tr><td>720</td><td>0.454</td><td>0.465</td><td>0.446</td><td>0.464</td><td>0.472</td><td>0.561</td><td>0.501</td><td>0.490</td><td>0.469</td><td>0.492</td><td>0.515</td><td>0.517</td><td>1.144</td><td>0.857</td><td>0.963</td><td>0.782</td><td>1.135</td><td>0.852</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.270</td><td>0.336</td><td>0.274</td><td>0.337</td><td>0.274</td><td>0.345</td><td>0.289</td><td>0.353</td><td>0.332</td><td>0.374</td><td>0.332</td><td>0.368</td><td>1.549</td><td>0.952</td><td>0.645</td><td>0.597</td><td>2.116</td><td>1.197</td></tr><tr><td>192</td><td>0.332</td><td>0.380</td><td>0.338</td><td>0.376</td><td>0.353</td><td>0.401</td><td>0.383</td><td>0.418</td><td>0.407</td><td>0.446</td><td>0.426</td><td>0.434</td><td>3.792</td><td>1.542</td><td>0.788</td><td>0.683</td><td>4.315</td><td>1.635</td></tr><tr><td>336</td><td>0.360</td><td>0.407</td><td>0.363</td><td>0.397</td><td>0.382</td><td>0.425</td><td>0.448</td><td>0.465</td><td>0.400</td><td>0.447</td><td>0.477</td><td>0.479</td><td>4.215</td><td>1.642</td><td>0.907</td><td>0.747</td><td>1.124</td><td>1.604</td></tr><tr><td>720</td><td>0.419</td><td>0.451</td><td>0.393</td><td>0.430</td><td>0.625</td><td>0.557</td><td>0.605</td><td>0.551</td><td>0.412</td><td>0.469</td><td>0.453</td><td>0.490</td><td>3.656</td><td>1.619</td><td>0.963</td><td>0.783</td><td>3.188</td><td>1.540</td></tr><tr><td rowspan=\"4\">ETTm1</td><td>96</td><td>0.306</td><td>0.349</td><td>0.293</td><td>0.346</td><td>0.302</td><td>0.350</td><td>0.299</td><td>0.343</td><td>0.326</td><td>0.390</td><td>0.510</td><td>0.492</td><td>0.626</td><td>0.560</td><td>0.543</td><td>0.510</td><td>0.600</td><td>0.546</td></tr><tr><td>192</td><td>0.335</td><td>0.366</td><td>0.333</td><td>0.370</td><td>0.347</td><td>0.383</td><td>0.335</td><td>0.365</td><td>0.365</td><td>0.415</td><td>0.514</td><td>0.495</td><td>0.725</td><td>0.619</td><td>0.557</td><td>0.537</td><td>0.837</td><td>0.700</td></tr><tr><td>336</td><td>0.364</td><td>0.384</td><td>0.369</td><td>0.392</td><td>0.369</td><td>0.402</td><td>0.369</td><td>0.386</td><td>0.392</td><td>0.425</td><td>0.510</td><td>0.492</td><td>1.005</td><td>0.741</td><td>0.754</td><td>0.655</td><td>1.124</td><td>0.832</td></tr><tr><td>720</td><td>0.413</td><td>0.413</td><td>0.416</td><td>0.420</td><td>0.431</td><td>0.441</td><td>0.425</td><td>0.421</td><td>0.446</td><td>0.458</td><td>0.527</td><td>0.493</td><td>1.133</td><td>0.845</td><td>0.908</td><td>0.724</td><td>1.153</td><td>0.820</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.161</td><td>0.251</td><td>0.166</td><td>0.256</td><td>0.176</td><td>0.255</td><td>0.167</td><td>0.260</td><td>0.180</td><td>0.271</td><td>0.205</td><td>0.293</td><td>0.355</td><td>0.462</td><td>0.435</td><td>0.507</td><td>0.768</td><td>0.642</td></tr><tr><td>192</td><td>0.215</td><td>0.289</td><td>0.223</td><td>0.296</td><td>0.245</td><td>0.305</td><td>0.224</td><td>0.303</td><td>0.252</td><td>0.318</td><td>0.278</td><td>0.336</td><td>0.595</td><td>0.586</td><td>0.730</td><td>0.673</td><td>0.989</td><td>0.757</td></tr><tr><td>336</td><td>0.267</td><td>0.326</td><td>0.274</td><td>0.329</td><td>0.295</td><td>0.346</td><td>0.281</td><td>0.342</td><td>0.324</td><td>0.364</td><td>0.343</td><td>0.379</td><td>1.270</td><td>0.871</td><td>1.201</td><td>0.845</td><td>1.334</td><td>0.872</td></tr><tr><td>720</td><td>0.352</td><td>0.383</td><td>0.362</td><td>0.385</td><td>0.401</td><td>0.413</td><td>0.397</td><td>0.421</td><td>0.410</td><td>0.420</td><td>0.414</td><td>0.419</td><td>3.001</td><td>1.267</td><td>3.625</td><td>1.451</td><td>3.048</td><td>1.328</td></tr></table>\n\nTable 2: Multivariate long-term forecasting results with our model.  $T \\in \\{96,192,336,720\\}$  for all datasets. The best results including the ones that cannot be statistically distinguished from the best mean numbers are in bold. We calculate standard error intervals for our method over 5 runs. The rest of the numbers are taken from the results from (Nie et al., 2022) $^1$ . All metrics are reported on standard normalized datasets. We provide the standard errors for our method in Table 5 in Appendix B.\n\net al., 2022) where it was shown that full self attention over patches was much more effective even when applied in a channel dependent manner. For a more in depth discussion about the pitfalls of sub-quadratic attention approximation in the context of forecasting, we refer the reader to Section 3 of (Zeng et al., 2023). In Appendix A, we prove that a linear analogue of our model can be optimal for predicting linear dynamical systems when compared against sequence models, thus shedding some light on why our model and even simpler models like DLinear can be so competitive for long context and/or horizon forecasting.\n\nFurther, we outperform DLinear significantly in all settings except for horizon 192 in ETTh1 where the performances are equal. This shows the value of the additional non-linearity in our model. In some datasets like Weather and ETTh1, N-HiTS performs similar to TiDE and PatchTST for horizon 96, but fails to uphold the performance for longer horizons. In all datasets except Weather, we either outperform PatchTST or perform within its statistical significance for most horizons. In the Weather dataset, PatchTST performs the best for horizons 96-336 while our model is the most performant for horizon 720. In the biggest dataset (Traffic), we significantly outperform PatchTST in all settings. For instance, for horizon 720 our prediction is  $10.6\\%$  better than PatchTST in MSE. We provide additional results in Appendix B.1 including a comparison with the S4 model (Gu et al.) in Table 6.\n\n# 5.2 Demand Forecasting\n\nIn order to showcase our model's ability to handle static attributes and complex dynamic covariates we use the M5 forecasting competition benchmarks (Makridakis et al., 2022). We follow the convention in the example notebook $^2$  released by the authors of (Alexandrov et al., 2020). The dataset consists of more than 30k time-series with static attributes like hierarchical categories and dynamic covariates like promotions. More details of the setup are available in Appendix B.1.\n\nWe present the competition metric (WRMSSE) results on the test set corresponding to the private leader-board in Table 3. We compare with DeepAR (Salinas et al., 2020) whose implementation can handle all covariates and also PatchTST (the best model from Table 2). Note that the implementation of PatchTST (Nie et al., 2022) does not handle covariates. We report the score over 3 independent runs along with the corresponding standard errors. We can see that PatchTST performs poorly as it does not use covariates. Our model using all the covariates outperforms DeepAR (that also uses all the covariates) by as much as  $20\\%$ . For the sake of ablation, we also provide the metric for our model that uses only date derived features as covariates. There is a degradation in performance from not using the dataset specific covariates but even so this version of the model also outperforms the other baselines.\n\n<table><tr><td>Model</td><td>Covariates</td><td>Test WRMSSE</td></tr><tr><td>TiDE</td><td>Static + Dynamic</td><td>0.611 ± 0.009</td></tr><tr><td>TiDE</td><td>Date only</td><td>0.637 ± 0.005</td></tr><tr><td>DeepAR</td><td>Static + Dynamic</td><td>0.789 ± 0.025</td></tr><tr><td>PatchTST</td><td>None</td><td>0.976 ± 0.014</td></tr></table>\n\nTable 3: M5 forecasting results on the private test set. We report the competition metric (averaged across three runs) for each model. We also list the covariates used by all models.\n\n# 5.3 Training and Inference Efficiency\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/017638d5-07ba-49e1-918d-804194503790/d1d0ab1c4e06ee86a6c9551b0889c8adbb32cbe927e811ef70ed577877a50299.jpg)  \n(a) Inference time per batch in microseconds\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/017638d5-07ba-49e1-918d-804194503790/43d1a86a570cda81d055c867b54b21bde0f12198f94ce2da16889e987cb3abd3.jpg)  \n(b) Training time for one epoch in seconds.  \nFigure 2: In (a) we show the inference time per batch on the electricity dataset. In (b) we show the corresponding training times for one epoch. In both the figures the y-axis is plotted in log-scale. Note that the PatchTST model ran out of GPU memory for look-back  $L \\geq 1440$ .\n\nIn the previous section we have seen that TiDE outperforms all methods except PatchTST by a large margin while it performs better or comparable to PatchTST in all datasets except Weather. Next, we would like to demonstrate that TiDE is much more efficient than PatchTST in terms of both training and inference times.\n\nFirstly, we would like to note that inference scales as  $\\tilde{O}(n_e h^2 + h L)$  for the encoder in TiDE, where  $n_e$  is the number of layers in the encoder,  $h$  is the hidden size of the internal layers and  $L$  is the look-back. On the other hand, inference in PatchTST encoder would scale as  $\\tilde{O}(K n_a L^2 / P^2)$ , where  $K$  is the size of the key in self-attention,  $P$  is the patch-size and  $n_a$  is the number of attention layers. The quadratic scaling in  $L$  can be prohibitive for very long contexts. Also, the amount of memory required is quadratic in  $L$  for the vanilla Transformer architecture used in PatchTST<sup>3</sup>.\n\nWe demonstrate these effects in practice in Figure 2. For the comparison to be fair we carry out the experiment using the data loader in the Autoformer (Wu et al., 2021) code base that was used in all subsequent papers. We use the electricity dataset with batch size 8, that is each batch has a shape of  $8 \\times 321 \\times L$  because the electricity dataset has 321 time-series. We report the inference time for one batch and the training time for one epoch for TiDE and PatchTST as the look-back ( $L$ ) is increased from 192 to 2880. We can see that there is an order of magnitude difference in inference time. The differences in training time is even more stark with PatchTST being much more sensitive to the look-back size. Further PatchTST runs out of memory for  $L \\geq 1440$ . Thus our model achieves better or similar accuracy while being much more computation and memory efficient. All the experiments in this section were performed using a single NVIDIA T4 GPU on the same machine with 64 core Intel(R) Xeon(R) CPU @ 2.30GHz.\n\n# 5.4 Ablation Study\n\nTemporal Decoder. The use of the temporal decoder for adaptation to future covariates is perhaps one of the most interesting components of our model. Therefore in this section we would like to show the usefulness of that component with a semi-synthetic example using the electricity dataset.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/017638d5-07ba-49e1-918d-804194503790/e5e8b6b485e25db9cf35a7b16012ea71453d7ede2a1c0a1a958122da5b3892ac.jpg)  \nFigure 3: We plot the actuals vs the predictions from TiDE with and without the temporal decoder after just one epoch of training on the modified electricity dataset. The red part of the horizontal line indicates an event of Type A occurring.\n\nWe derive a new dataset from the electricity dataset, where we add numerical features for two kinds of events. When an event of Type A occurs the value of a time-series is increased by a factor which is uniformly chosen between [3, 3.2]. When an event of Type B occurs the value of a time-series is decreased by a factor which is uniformly chosen between [2, 2.2]. Only  $80\\%$  of the time-series are affected by these events and the time-series id's that fall in this bracket are chosen randomly. There are 4 numerical covariates that indicate Type A and 4 that indicate Type B. When Type A event occurs the Type A covariates are drawn from an isotropic Gaussian with mean  $[1.0, 2.0, 2.0, 1.0]$  and variance 0.1 for every coordinate. On the other hand in the absence of Type A events Type A covariates are drawn from an isotropic Gaussian with mean  $[0.0, 0.0, 0.0, 0.0]$ . Thus these covariates serve as noisy indicators of the event. We follow a similar pattern for Type B events and covariates but with different means. Whenever these events occur they occur for 24 contiguous hours.\n\nIn order to showcase that the use of the temporal decoder can learn such patterns derived from the covariates faster, we plot the predictions from the TiDE model with and without the temporal decoder after just one epoch of training on the modified electricity dataset in Figure 3. The red part of the horizontal line indicates the occurrence of Type A events. We can see that the use of temporal decoder has a slight advantage during that time-period. But more importantly, in the time instances following the event the model without the temporal decoder is thrown off possibly because it has not yet readjusted its past to what it should have been without the event. This effect is negligible in the model which uses the temporal decoder, even after just one epoch of training.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/017638d5-07ba-49e1-918d-804194503790/fcde29964b23a78f818fa94137e0b991c0ebd345ecf0837d3034fb220d6c5f66.jpg)  \nFigure 4: We plot the Test MSE on the traffic dataset as a function of different context sizes for three different horizon length tasks. Each plot is an average of 5 runs with the 2 standard error interval plotted.  \nFigure 5: We perform an ablation study by presenting results from our model without any residual connections, on the electricity benchmark. We average over 5 runs for all the numbers and present the corresponding standard errors.\n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">TiDE</td><td colspan=\"2\">TiDE (no res.)</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.132 ± 0.003</td><td>0.229 ± 0.001</td><td>0.136 ± 0.001</td><td>0.235 ± 0.002</td></tr><tr><td>192</td><td>0.147 ± 0.003</td><td>0.243 ± 0.001</td><td>0.153 ± 0.001</td><td>0.253 ± 0.001</td></tr><tr><td>336</td><td>0.161 ± 0.001</td><td>0.261 ± 0.002</td><td>0.172 ± 0.003</td><td>0.274 ± 0.002</td></tr><tr><td>720</td><td>0.196 ± 0.002</td><td>0.294 ± 0.001</td><td>0.196 ± 0.003</td><td>0.295 ± 0.002</td></tr></table>\n\nContext Size. In Figure 4 we study the dependence of prediction accuracy with context size on the Traffic dataset. We plot the results for multiple horizon length tasks and show that in all cases our methods performance becomes better with increasing context size as expected. This is contrast to some of the transformer based methods like Fedformer, Informer as shown in (Zeng et al., 2023).\n\nResidual Connections. In Table 5 we perform an ablation study of the residual connections on the electricity dataset. In the model dubbed TiDE (no res) we remove all the residual connections including the ones in the residual block as well as the global linear residual connection. In horizons 96-336 we see a statistically significant drop in performance without the residual connections.\n",
  "hyperparameter": "Context length (look-back window L): 720 for all horizon lengths {96, 192, 336, 720}. Key architectural parameters include: hiddenSize (controls encoder/decoder internal layer sizes), numEncoderLayers (n_e), numDecoderLayers (n_d), temporalWidth (r̃, dimensionality of projected features, r̃ << r where r is original covariate dimension), decoderOutputDim (p, output dimension before reshape to H×p), temporalDecoderHidden (hidden size of temporal decoder). Training uses Adam optimizer with default settings, MSE loss, and mini-batch gradient descent. All hyperparameters tuned on validation set using rolling validation error. Models trained on normalized datasets using train:validation:test ratio of 7:1:2."
}