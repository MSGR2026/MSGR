{
  "id": "MICN_2023",
  "paper_title": "MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting",
  "alias": "MICN",
  "year": 2023,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "MICN proposes a multi-scale hybrid decomposition approach combined with multi-scale isometric convolution for long-term time series forecasting. The core innovation lies in: (1) Multi-scale Hybrid Decomposition (MHDecomp) that uses multiple averaging kernels to separate different trend-cyclical and seasonal patterns, (2) A novel Local-Global module using isometric convolution instead of self-attention to capture both local features through downsampling convolution and global correlations through isometric convolution on compressed sequences, and (3) Multi-scale branches with different kernel sizes to model different underlying temporal patterns, merged using Conv2d for weighted integration.",
  "introduction": "# 1 INTRODUCTION\n\nResearches related to time series forecasting are widely applied in the real world, such as sensor network monitoring (Papadimitriou & Yu., 2006), weather forecasting, economics and finance (Zhu & Shasha, 2002), and disease propagation analysis (Matsubara et al., 2014) and electricity forecasting. In particular, long-term time series forecasting is increasingly in demand in reality. Therefore, this paper focuses on the task of long-term forecasting. The problem to be solved is to predict values for a future period:  $X_{t + 1},X_{t + 2},\\dots,X_{t + T - 1},X_{t + T}$ , based on observations from a historical period:  $X_{1},X_{2},\\dots,X_{t - 1},X_{t}$ , and  $T\\gg t$ .\n\nAs a classic CNN-based model, TCN (Bai et al., 2018) uses causal convolution to model the temporal causality and dilated convolution to expand the receptive field. It can integrate the local information of the sequence better and achieve competitive results in short and medium-term forecasting (Sen et al., 2019) (Borovykh et al., 2017). However, limited by the receptive field size, TCN often needs many layers to model the global relationship of time series, which greatly increases the complexity of the network and the training difficulty of the model.\n\nTransformers (Vaswani et al., 2017) based on the attention mechanism shows great power in sequential data, such as natural language processing (Devlin et al., 2019) (Brown et al., 2020), audio processing (Huang et al., 2019) and even computer vision (Dosovitskiy et al., 2021) (Liu et al., 2021b). It has also recently been applied in long-term series forecasting tasks (Li et al., 2019b) (Wen et al., 2022) and can model the long-term dependence of sequences effectively, allowing leaps and bounds in the accuracy and length of time series forecasts (Zhu & Soricut, 2021) (Wu et al., 2021b) (Zhou et al., 2022). The learned attention matrix represents the correlations between different time points of the sequence and can explain relatively well how the model makes future predictions based on past information. However, it has a quadratic complexity, and many of the computations\n\nbetween token pairs are non-essential, so it is also an interesting research direction to reduce its computational complexity. Some notable models include: LogTrans (Li et al., 2019b), Informer (Zhou et al., 2021), Reformer (Kitaev et al., 2020), Autoformer Wu et al. (2021b), Pyraformer (Liu et al., 2021a), FEDformer (Zhou et al., 2022).\n\nHowever, as a special sequence, time series has not led to a unified modeling direction so far. In this paper, we combine the modeling perspective of CNNs with that of Transformers to build models from the realistic features of the sequences themselves, i.e., local features and global correlations. Local features represent the characteristics of a sequence over a small period  $T$ , and global correlations are the correlations exhibited between many periods  $T_{1}, T_{2}, \\ldots, T_{n-1}, T_{n}$ . For example, the temperature at a moment is not only influenced by the specific change during the day but may also be correlated with the overall trend of a period (e.g., week, month, etc.). We can identify the value of a time point more accurately by learning the overall characteristics of that period and the correlation among many periods before. Therefore, a good forecasting method should have the following two properties: (1) The ability to extract local features to measure short-term changes. (2) The ability to model the global correlations to measure the long-term trend.\n\nBased on this, we propose Multi-scale Isometric Convolution Network (MICN). We use multiple branches of different convolution kernels to model different potential pattern information of the sequence separately. For each branch, we extract the local features of the sequence using a local module based on downsampling convolution, and on top of this, we model the global correlation using a global module based on isometric convolution. Finally, Merge operation is adopted to fuse information about different patterns from several branches. This design reduces the time and space complexity to linearity, eliminating many unnecessary and redundant calculations. MICN achieves state-of-the-art accuracy on five real-world benchmarks. The contributions are summarized as follows:\n\n- We propose MICN based on convolution structure to efficiently replace the self-attention, and it achieves linear computational complexity and memory cost.  \n- We propose a multiple branches framework to deeply mine the intricate temporal patterns of time series, which validates the need and validity for separate modeling when the input data is complex and variable.  \n- We propose a local-global structure to implement information aggregation and long-term dependency modeling for time series, which outperforms the self-attention family and Auto-correlation mechanism. We adopt downsampling one-dimensional convolution for local features extraction and isometric convolution for global correlations discovery.  \n- Our empirical studies show that the proposed model improves the performance of state-of-the-art method by  $17.2\\%$  and  $21.6\\%$  for multivariate and univariate forecasting, respectively.\n",
  "method": "# 3 MODEL\n\n# 3.1 MICN FRAMEWORK\n\nThe overall structure of MICN is shown in Figure 1. The long time series prediction task is to predict a future series of length  $O$  based on a past series of length  $I$ , which can be expressed as input  $-I - \\text{predict} - O$ , where  $O$  is much larger than  $I$ . Inspired by traditional time series decomposition algorithms (Robert et al., 1990) (Wu et al., 2021b), we design a multi-scale hybrid decomposition (MHDecomp) block to separate complex patterns of input series. Then we use Seasonal Prediction Block to predict seasonal information and Trend-cyclical Prediction Block to predict trend-cyclical information. Then add the prediction results up to get the final prediction  $Y_{pred}$ . We donate  $d$  as the number of variables in multivariate time series and  $D$  as the hidden state of the series. The details will be given in the following sections.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/cc06ae2c7bb7b188df5e0f5be546680747ade00f7784750a95e89ca4512be364.jpg)  \nFigure 1: MICN overall architecture.\n\n# 3.2 MULTI-SCALE HYBRID DECOMPOSITION\n\nPrevious series decomposition algorithms (Wu et al., 2021b) adopt the moving average to smooth out periodic fluctuations and highlight the long-term trends. For the input series  $X \\in R^{I \\times d}$ , the process is:\n\n$$\n\\begin{array}{l} X _ {t} = \\operatorname {A v g P o o l} (\\text {P a d d i n g} (X)) _ {\\text {k e r n e l}} \\\\ X _ {1} = X _ {2} = X _ {3} \\end{array} \\tag {1}\n$$\n\n$$\nX _ {s} = X - X _ {t},\n$$\n\nwhere:  $X_{t}, X_{s} \\in R^{I \\times d}$  denote the trend-cyclical and seasonal parts, respectively. The use of the Avgpool(\\cdot) with the padding operation keeps the series length unchanged. But the parameter kernel of the Avgpool(\\cdot) is artificially set and there are often large differences in trend-cyclical series and seasonal series obtained from different kernels. Therefore, we design a multi-scale hybrid decomposition block that uses several different kernels of the Avgpool(\\cdot) and can separate several different patterns of trend-cyclical and seasonal parts purposefully. Different from the MOEDecomp block of FEDformer (Zhou et al., 2022), we use simple mean operation to integrate these different patterns because we cannot determine the weight of each pattern before learning its features. Correspondingly, we put this weighting operation in the Merge part of Seasonal Prediction block after the representation of the features. Concretely, for the input series  $X \\in R^{I \\times d}$ , the process is:\n\n$$\n\\begin{array}{l} X _ {t} = \\operatorname {m e a n} \\left(\\operatorname {A v g P o o l} \\left(\\operatorname {P a d d i n g} (X)\\right) _ {\\text {k e r n e l} _ {1}}, \\dots , \\operatorname {A v g P o o l} \\left(\\operatorname {P a d d i n g} (X)\\right) _ {\\text {k e r n e l} _ {n}}\\right) \\\\ X _ {0} = X _ {1} = X _ {2} \\end{array} \\tag {2}\n$$\n\n$$\nX _ {s} = X - X _ {t},\n$$\n\nwhere  $X_{t}, X_{s} \\in R^{I \\times d}$  denote the trend-cyclical and seasonal part, respectively. The different kernels are consistent with multi-scale information in Seasonal Prediction block. The effectiveness is demonstrated experimentally in Appendix B.1.\n\n# 3.3 TREND-CYCLICAL PREDICTION BLOCK\n\nCurrently, Autoformer (Wu et al., 2021b) concatenates the mean of the original series and then accumulates it with the trend-cyclical part obtained from the inner series decomposition block. But there is no explanation of this and no proof of its effectiveness. In this paper, we use a simple linear regression strategy to make a prediction about trend-cyclical, demonstrating that simple modeling of trend-cyclical is also necessary for non-stationary series forecasting tasks (See Section 4.2). Concretely, for the trend-cyclical series  $X_{t} \\in R^{I \\times d}$  obtained with MHDecomp block, the process is:\n\n$$\nY _ {t} ^ {\\text {r e g r e}} = \\operatorname {r e g r e s s i o n} \\left(X _ {t}\\right) \\tag {3}\n$$\n\nwhere  $Y_{t}^{\\text{regre}} \\in R^{O \\times d}$  denotes the prediction of the trend part using the linear regression strategy. And we use MICN-regre to represent MICN model with this trend-cyclical prediction method.\n\nFor comparison, we use the mean of  $X_{t}$  to cope with the series where the trend-cyclical keeps constant:\n\n$$\nY _ {t} ^ {\\text {m e a n}} = \\operatorname {m e a n} \\left(X _ {t}\\right) \\tag {4}\n$$\n\nwhere  $Y_{t}^{mean} \\in R^{O \\times d}$  denotes the prediction of the trend part. And we use MICN-mean to represent MICN model with this trend-cyclical prediction method.\n\n# 3.4 SEASONAL PREDICTION BLOCK\n\nAs shown in Figure 2, the Seasonal Prediction Block focuses on the more complex seasonal part modeling. After embedding the input sequence  $X_{s}$ , we adopt multi-scale isometric convolution to capture the local features and global correlations, and branches of different scales model different underlying patterns of the time series. We then merge the results from different branches to complete comprehensive information utilization of the sequence. It can be summarised as follows:\n\n$$\nX _ {s} ^ {\\text {e m b}} = \\operatorname {E m b d i n g} \\left(\\operatorname {C o n c a t} \\left(X _ {s}, X _ {\\text {z e r o}}\\right)\\right)\n$$\n\n$$\nY _ {s} ^ {0} = X _ {s} ^ {\\text {e m b}} \\tag {5}\n$$\n\n$$\nY _ {s, l} = M I C \\left(Y _ {s, l - 1}\\right), \\quad l \\in \\{1, 2, \\dots , N \\}\n$$\n\n$$\nY _ {s} = \\text {T r u n c a t e} \\left(\\text {P r o j e c t i o n} \\left(Y _ {s, N}\\right)\\right),\n$$\n\nwhere  $X_{zero} \\in R^{O \\times d}$  denotes the placeholders filled with zero, and  $X_s^{emb} \\in R^{(I + O) \\times D}$  denotes the embedded representation of  $X_s$ .  $Y_{s,l} \\in R^{(I + O) \\times D}$  represents the output of  $l - th$  multi-scale isometric\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/c6eaa88865eee02215337a9a52e919ddda21749a18af51414175497e8d6c45ff.jpg)  \nFigure 2: Seasonal Prediction Block.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/65e6a726bac6571ec7c8981815f3eaaf74b25c3f511027dc76bda0f42a105215.jpg)  \nFigure 3: Local-Global module architecture.\n\nconvolution (MIC) layer, and  $Y_{s} \\in R^{O \\times d}$  represents the final prediction of the seasonal part after a linear function Projection with  $Y_{s,N} \\in R^{(I + O) \\times D}$  and Truncate operation. The detailed description of Embedding and MIC will be given as follows.\n\n**Embedding** The decoder of the latest Transformer-based models such as Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021b) and FEDformer (Zhou et al., 2022) contain the latter half of the encoder's input with the length  $\\frac{I}{2}$  and placeholders with length  $O$  filled by scalars, which may lead to redundant calculations. To avoid this problem and adapt to the prediction length  $O$ , we replace the traditional encoder-decoder style input with a simpler complementary  $O$  strategy. Meanwhile, we follow the setting of FEDformer and adopt three parts to embed the input. The process is:\n\n$$\nX _ {s} ^ {\\text {e m b}} = \\operatorname {s u m} \\left(T F E + P E + V E \\left(\\operatorname {C o n c a t} \\left(X _ {s}, X _ {\\text {z e r o}}\\right)\\right)\\right) \\tag {6}\n$$\n\nwhere  $X_{s}^{emb} \\in R^{(I + O) \\times D}$ . TFE represents time features encoding (e.g., MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, and MonthOfYear), PE represents positional encoding and VE represents value embedding.\n\nMulti-scale isometric Convolution(MIC) Layer MIC layer contains several branches, with different scale sizes used to model potentially different temporal patterns. In each branch, as shown in Figure 3, the local-global module extracts the local features and the global correlations of the sequence (See Appendix B.7 for more detailed description). Concretely, after obtaining the corresponding single pattern by avgpool, the local module adopts one-dimensional convolution to implement downsampling. The process is:\n\n$$\nY _ {s, l} = Y _ {s, l - 1}\n$$\n\n$$\nY _ {s, l} ^ {\\text {l o c a l}, i} = \\operatorname {C o n v 1 d} \\left(\\operatorname {A v g p o o l} \\left(\\operatorname {P a d d i n g} \\left(Y _ {s, l}\\right)\\right) _ {\\text {k e r n e l} = i}\\right) _ {\\text {k e r n e l} = i}, \\tag {7}\n$$\n\nwhere  $Y_{s,l-1}$  denotes the output of  $(l-1) - th$  MIC layer and  $Y_{s,0} = X_s^{emb}$ .  $i \\in \\left\\{\\frac{l}{4}, \\frac{l}{8}, \\ldots \\right\\}$  denote the different scale sizes corresponding to the different branches in Figure 2. For Conv1d, we set stride = kernel = i, which serves as compression of local features.  $Y_{s,l}^{local,i} \\in R^{\\frac{(l+O)}{i} \\times D}$  represents the result obtained by compressing local features, which is a short sequence.\n\nAnd furthermore, the global module is designed to model the global correlations of the output of the local module. A commonly used method for modeling global correlations is the self-attention mechanism. But in this paper, we use a variant of casual convolution, isometric convolution, as an alternative. As shown in Figure 4, isometric convolution pads the sequence of length  $S$  with\n\nplaceholders zero of length  $S - 1$ , and its kernel is equal to  $S$ . It means that we can use a large convolution kernel to measure the global correlation of the whole series. The current generative prediction approach is to add placeholder to the input sequence, which has no actual sequence information in the second half. The Isometric Convolution can enable sequential inference of sequences by fusing local features information. Moreover, the kernel of Isometric convolution is determined by all the training data, which can introduces a global temporal inductive bias (translation equivariance, etc.) and achieve better generalization than self-attention (the correlations are obtained from the product between different elements). Meanwhile, we demonstrate that for a shorter sequence, isometric convolution is superior to self-attention. The detailed experiments of the proof are in Appendix B.3. And to keep the sequence length constant, we upsample the result of the isometric convolution using transposed convolution. The global module can be formalized as follows:\n\n$$\nY _ {s, l} ^ {\\prime, i} = \\operatorname {N o r m} \\left(Y _ {s, l} ^ {\\text {l o c a l}, i} + \\operatorname {D r o p o u t} \\left(\\operatorname {T a n h} (\\operatorname {I s o m e t r i c C o n v} \\left(Y _ {s, l} ^ {\\text {l o c a l}, i}\\right))\\right)\\right)\n$$\n\n$$\nY _ {s, l} ^ {\\text {g l o b a l}, i} = \\operatorname {N o r m} \\left(Y _ {s, l - 1} + \\operatorname {D r o p o u t} \\left(\\operatorname {T a n h} \\left(\\operatorname {C o n v 1 d T r a n s p o s e} \\left(Y _ {s, l} ^ {', i}\\right) _ {\\text {k e r n e l} = i}\\right)\\right)\\right), \\tag {8}\n$$\n\nwhere  $Y_{s,l}^{local,i} \\in R^{\\frac{(l + O)}{i} \\times D}$  denote the result after the global correlations modeling.  $Y_{s,l-1}$  is the output of  $l-1$  MIC layer.  $Y_{s,l}^{global,i} \\in R^{(I + O) \\times D}$  represents the result of this pattern (i.e., this branch).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/094f6d0891c62b967b943c1ae1bbdcd0f5b1534003629d1943b7ab88fdfa6977.jpg)  \nIsometric Convolution\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/a60f83c56dc8e0dacc8464f023448d1ed43fe4056d05de2985ae8452fc161137.jpg)  \nMasked self-attention  \nFigure 4: Isometric Convolution architecture vs. Masked self-attention architecture\n\nThen we propose to use Conv2d to merge the different patterns with different weights instead of the traditional Concat operation. The validity of this weighting approach is verified in Appendix B.4. The process is:\n\n$$\nY _ {s, l} ^ {\\text {m e r g e}} = \\left(\\operatorname {C o n v 2 d} \\left(Y _ {s, l} ^ {\\text {g l o b a l}, i}, i \\in \\left\\{\\frac {I}{4}, \\frac {I}{8}, \\dots \\right\\}\\right)\\right) \\tag {9}\n$$\n\n$$\nY _ {s, l} = \\operatorname {N o r m} \\left(Y _ {s, l} ^ {\\text {m e r g e}} + \\text {F e e d F o r w a r d} \\left(Y _ {s, l} ^ {\\text {m e r g e}}\\right)\\right),\n$$\n\nwhere  $Y_{s,l} \\in R^{(l + O) \\times D}$  represents the result of  $l - th$  MIC layer.\n\nTo get the final prediction of the seasonal part, we use the projection and truncate operations:\n\n$$\nY _ {s} = \\text {T r u n c a t e} \\left(\\text {P r o j e c t i o n} \\left(Y _ {s, N}\\right)\\right) \\tag {10}\n$$\n\nwhere  $Y_{s,N} \\in R^{(I + O) \\times D}$  represents the output of N-th MIC layer, and  $Y_{s} \\in R^{O \\times d}$  represents the final prediction about the seasonal part.\n",
  "experiments": "# 4 EXPERIMENTS\n\nDataset To evaluate the proposed MICN, we conduct extensive experiments on six popular real-world datasets, covering many aspects of life: energy, traffic, economics, and weather. We follow standard protocol (Zhou et al., 2021) and split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets. More details about the datasets and implementation are described in Appendix A.1 and A.2.\n\nBaselines We include four transformer-based models: FEDformer (Zhou et al., 2022), Autoformer (Wu et al., 2021b), Informer (Zhou et al., 2021), LogTrans (Li et al., 2019b), two RNN-based models: LSTM (Hochreiter & Schmidhuber, 1997), LSTMNet (Lai et al., 2018b) and CNN-based model TCN (Bai et al., 2018) as baselines. For the univariate setting, we mainly compare transformer-based models. For the state-of-the-art model FEDformer, we compare the better one (FEDformer-f).\n\nTable 1: Multivariate long-term series forecasting results with input length  $I = 96$  and prediction length  $O \\in \\{96, 192, 336, 720\\}$  (for ILI, the input length  $I = 36$ ). A lower MSE or MAE indicates a better prediction, and the best results are highlighted in bold.  \n\n<table><tr><td colspan=\"2\">Methods</td><td colspan=\"2\">MICN-regre</td><td colspan=\"2\">MICN-mean</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">LogTrans</td><td colspan=\"2\">LSTNet</td><td colspan=\"2\">LSTM</td><td colspan=\"2\">TCN</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.179</td><td>0.275</td><td>0.203</td><td>0.287</td><td>0.203</td><td>0.287</td><td>0.255</td><td>0.339</td><td>0.365</td><td>0.453</td><td>0.768</td><td>0.642</td><td>3.142</td><td>1.365</td><td>2.041</td><td>1.073</td><td>3.041</td><td>1.330</td></tr><tr><td>192</td><td>0.307</td><td>0.376</td><td>0.262</td><td>0.326</td><td>0.269</td><td>0.328</td><td>0.281</td><td>0.340</td><td>0.533</td><td>0.563</td><td>0.989</td><td>0.757</td><td>3.154</td><td>1.369</td><td>2.249</td><td>1.112</td><td>3.072</td><td>1.339</td></tr><tr><td>336</td><td>0.325</td><td>0.388</td><td>0.305</td><td>0.353</td><td>0.325</td><td>0.366</td><td>0.339</td><td>0.372</td><td>1.363</td><td>0.887</td><td>1.334</td><td>0.872</td><td>3.160</td><td>1.369</td><td>2.568</td><td>1.238</td><td>3.105</td><td>1.348</td></tr><tr><td>720</td><td>0.502</td><td>0.490</td><td>0.389</td><td>0.407</td><td>0.421</td><td>0.415</td><td>0.422</td><td>0.419</td><td>3.379</td><td>1.388</td><td>3.048</td><td>1.328</td><td>3.171</td><td>1.368</td><td>2.720</td><td>1.287</td><td>3.135</td><td>1.354</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.164</td><td>0.269</td><td>0.193</td><td>0.308</td><td>0.193</td><td>0.308</td><td>0.201</td><td>0.317</td><td>0.274</td><td>0.368</td><td>0.258</td><td>0.357</td><td>0.680</td><td>0.645</td><td>0.375</td><td>0.437</td><td>0.985</td><td>0.813</td></tr><tr><td>192</td><td>0.177</td><td>0.285</td><td>0.200</td><td>0.308</td><td>0.201</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.296</td><td>0.386</td><td>0.266</td><td>0.368</td><td>0.725</td><td>0.676</td><td>0.442</td><td>0.473</td><td>0.996</td><td>0.821</td></tr><tr><td>336</td><td>0.193</td><td>0.304</td><td>0.219</td><td>0.328</td><td>0.214</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.300</td><td>0.394</td><td>0.280</td><td>0.380</td><td>0.828</td><td>0.727</td><td>0.439</td><td>0.473</td><td>1.000</td><td>0.824</td></tr><tr><td>720</td><td>0.212</td><td>0.321</td><td>0.224</td><td>0.332</td><td>0.246</td><td>0.355</td><td>0.254</td><td>0.361</td><td>0.373</td><td>0.439</td><td>0.283</td><td>0.376</td><td>0.957</td><td>0.811</td><td>0.980</td><td>0.814</td><td>1.438</td><td>0.784</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.102</td><td>0.235</td><td>0.173</td><td>0.297</td><td>0.148</td><td>0.278</td><td>0.197</td><td>0.323</td><td>0.847</td><td>0.752</td><td>0.968</td><td>0.812</td><td>1.551</td><td>1.058</td><td>1.453</td><td>1.049</td><td>3.004</td><td>1.432</td></tr><tr><td>192</td><td>0.172</td><td>0.316</td><td>0.324</td><td>0.408</td><td>0.271</td><td>0.380</td><td>0.300</td><td>0.369</td><td>1.204</td><td>0.895</td><td>1.040</td><td>0.851</td><td>1.477</td><td>1.028</td><td>1.846</td><td>1.179</td><td>3.048</td><td>1.444</td></tr><tr><td>336</td><td>0.272</td><td>0.407</td><td>0.639</td><td>0.598</td><td>0.460</td><td>0.500</td><td>0.509</td><td>0.524</td><td>1.672</td><td>1.036</td><td>1.659</td><td>1.081</td><td>1.507</td><td>1.031</td><td>2.136</td><td>1.231</td><td>3.113</td><td>1.459</td></tr><tr><td>720</td><td>0.714</td><td>0.658</td><td>1.218</td><td>0.862</td><td>1.195</td><td>0.841</td><td>1.447</td><td>0.941</td><td>2.478</td><td>1.310</td><td>1.941</td><td>1.127</td><td>2.285</td><td>1.243</td><td>2.984</td><td>1.427</td><td>3.150</td><td>1.458</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.519</td><td>0.309</td><td>0.575</td><td>0.344</td><td>0.587</td><td>0.366</td><td>0.613</td><td>0.388</td><td>0.719</td><td>0.391</td><td>0.684</td><td>0.384</td><td>1.107</td><td>0.685</td><td>0.843</td><td>0.453</td><td>1.438</td><td>0.784</td></tr><tr><td>192</td><td>0.537</td><td>0.315</td><td>0.580</td><td>0.349</td><td>0.604</td><td>0.373</td><td>0.616</td><td>0.382</td><td>0.696</td><td>0.379</td><td>0.685</td><td>0.390</td><td>1.157</td><td>0.706</td><td>0.847</td><td>0.453</td><td>1.463</td><td>0.794</td></tr><tr><td>336</td><td>0.534</td><td>0.313</td><td>0.583</td><td>0.345</td><td>0.621</td><td>0.383</td><td>0.622</td><td>0.337</td><td>0.777</td><td>0.420</td><td>0.733</td><td>0.408</td><td>1.216</td><td>0.730</td><td>0.853</td><td>0.455</td><td>1.479</td><td>0.799</td></tr><tr><td>720</td><td>0.577</td><td>0.325</td><td>0.601</td><td>0.363</td><td>0.626</td><td>0.382</td><td>0.660</td><td>0.408</td><td>0.864</td><td>0.472</td><td>0.717</td><td>0.396</td><td>1.481</td><td>0.805</td><td>1.500</td><td>0.805</td><td>1.499</td><td>0.804</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.161</td><td>0.229</td><td>0.183</td><td>0.250</td><td>0.217</td><td>0.296</td><td>0.266</td><td>0.336</td><td>0.300</td><td>0.384</td><td>0.458</td><td>0.490</td><td>0.594</td><td>0.587</td><td>0.369</td><td>0.406</td><td>0.615</td><td>0.589</td></tr><tr><td>192</td><td>0.220</td><td>0.281</td><td>0.246</td><td>0.317</td><td>0.276</td><td>0.336</td><td>0.307</td><td>0.367</td><td>0.598</td><td>0.544</td><td>0.658</td><td>0.589</td><td>0.560</td><td>0.565</td><td>0.416</td><td>0.435</td><td>0.629</td><td>0.600</td></tr><tr><td>336</td><td>0.278</td><td>0.331</td><td>0.293</td><td>0.335</td><td>0.339</td><td>0.380</td><td>0.359</td><td>0.395</td><td>0.578</td><td>0.523</td><td>0.797</td><td>0.652</td><td>0.597</td><td>0.587</td><td>0.455</td><td>0.454</td><td>0.639</td><td>0.608</td></tr><tr><td>720</td><td>0.311</td><td>0.356</td><td>0.373</td><td>0.399</td><td>0.403</td><td>0.428</td><td>0.419</td><td>0.428</td><td>1.059</td><td>0.741</td><td>0.869</td><td>0.675</td><td>0.618</td><td>0.599</td><td>0.535</td><td>0.520</td><td>0.639</td><td>0.610</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>2.684</td><td>1.112</td><td>3.029</td><td>1.180</td><td>3.228</td><td>1.260</td><td>3.483</td><td>1.287</td><td>5.764</td><td>1.677</td><td>4.480</td><td>1.444</td><td>6.026</td><td>1.770</td><td>5.914</td><td>1.734</td><td>6.624</td><td>1.830</td></tr><tr><td>36</td><td>2.667</td><td>1.068</td><td>2.507</td><td>1.013</td><td>2.679</td><td>1.080</td><td>3.103</td><td>1.148</td><td>4.755</td><td>1.467</td><td>4.799</td><td>1.467</td><td>5.340</td><td>1.668</td><td>6.631</td><td>1.845</td><td>6.858</td><td>1.879</td></tr><tr><td>48</td><td>2.558</td><td>1.052</td><td>2.423</td><td>1.012</td><td>2.622</td><td>1.078</td><td>2.669</td><td>1.085</td><td>4.763</td><td>1.469</td><td>4.800</td><td>1.468</td><td>6.080</td><td>1.787</td><td>6.736</td><td>1.857</td><td>6.968</td><td>1.892</td></tr><tr><td>60</td><td>2.747</td><td>1.110</td><td>2.653</td><td>1.085</td><td>2.857</td><td>1.157</td><td>2.770</td><td>1.125</td><td>5.264</td><td>1.564</td><td>5.278</td><td>1.560</td><td>5.548</td><td>1.720</td><td>6.870</td><td>1.879</td><td>7.127</td><td>1.918</td></tr></table>\n\nTable 2: Univariate long-term series forecasting results with input length  $I = {96}$  and prediction length  $O \\in  \\{ {96},{192},{336},{720}\\}$  (for ILI,the input length  $I = {36}$  ). A lower MSE or MAE indicates a better prediction, and the best results are highlighted in bold.  \n\n<table><tr><td rowspan=\"2\" colspan=\"2\">MethodsMetric</td><td colspan=\"2\">MICN-regre</td><td colspan=\"2\">MICN-mean</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">LogTrans</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>96</td><td>0.059</td><td>0.176</td><td>0.074</td><td>0.206</td><td>0.072</td><td>0.206</td><td>0.065</td><td>0.189</td><td>0.088</td><td>0.225</td><td>0.075</td><td>0.208</td></tr><tr><td>192</td><td>0.100</td><td>0.234</td><td>0.098</td><td>0.238</td><td>0.102</td><td>0.245</td><td>0.118</td><td>0.256</td><td>0.132</td><td>0.283</td><td>0.129</td><td>0.275</td></tr><tr><td>336</td><td>0.153</td><td>0.301</td><td>0.135</td><td>0.282</td><td>0.130</td><td>0.279</td><td>0.154</td><td>0.305</td><td>0.180</td><td>0.336</td><td>0.154</td><td>0.302</td></tr><tr><td>720</td><td>0.210</td><td>0.354</td><td>0.175</td><td>0.326</td><td>0.178</td><td>0.325</td><td>0.182</td><td>0.335</td><td>0.300</td><td>0.435</td><td>0.160</td><td>0.321</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.310</td><td>0.398</td><td>0.326</td><td>0.418</td><td>0.253</td><td>0.370</td><td>0.341</td><td>0.438</td><td>0.484</td><td>0.538</td><td>0.288</td><td>0.393</td></tr><tr><td>192</td><td>0.300</td><td>0.394</td><td>0.317</td><td>0.410</td><td>0.282</td><td>0.386</td><td>0.345</td><td>0.428</td><td>0.557</td><td>0.558</td><td>0.432</td><td>0.483</td></tr><tr><td>336</td><td>0.323</td><td>0.413</td><td>0.376</td><td>0.450</td><td>0.346</td><td>0.431</td><td>0.406</td><td>0.470</td><td>0.636</td><td>0.613</td><td>0.430</td><td>0.483</td></tr><tr><td>720</td><td>0.364</td><td>0.449</td><td>0.417</td><td>0.479</td><td>0.422</td><td>0.484</td><td>0.565</td><td>0.581</td><td>0.819</td><td>0.682</td><td>0.491</td><td>0.531</td></tr><tr><td rowspan=\"4\">Exchange</td><td>96</td><td>0.099</td><td>0.240</td><td>0.179</td><td>0.312</td><td>0.154</td><td>0.304</td><td>0.241</td><td>0.387</td><td>0.591</td><td>0.615</td><td>0.237</td><td>0.377</td></tr><tr><td>192</td><td>0.198</td><td>0.354</td><td>0.304</td><td>0.420</td><td>0.286</td><td>0.420</td><td>0.300</td><td>0.369</td><td>1.183</td><td>0.912</td><td>0.738</td><td>0.619</td></tr><tr><td>336</td><td>0.302</td><td>0.447</td><td>0.711</td><td>0.651</td><td>0.511</td><td>0.555</td><td>0.509</td><td>0.524</td><td>1.367</td><td>0.984</td><td>2.018</td><td>1.070</td></tr><tr><td>720</td><td>0.738</td><td>0.662</td><td>1.416</td><td>0.918</td><td>1.301</td><td>0.879</td><td>1.260</td><td>0.867</td><td>1.872</td><td>1.072</td><td>2.405</td><td>1.175</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.158</td><td>0.241</td><td>0.214</td><td>0.324</td><td>0.207</td><td>0.312</td><td>0.246</td><td>0.346</td><td>0.257</td><td>0.353</td><td>0.226</td><td>0.317</td></tr><tr><td>192</td><td>0.154</td><td>0.236</td><td>0.228</td><td>0.336</td><td>0.205</td><td>0.312</td><td>0.266</td><td>0.370</td><td>0.299</td><td>0.376</td><td>0.314</td><td>0.408</td></tr><tr><td>336</td><td>0.165</td><td>0.243</td><td>0.217</td><td>0.337</td><td>0.219</td><td>0.323</td><td>0.263</td><td>0.371</td><td>0.312</td><td>0.387</td><td>0.387</td><td>0.453</td></tr><tr><td>720</td><td>0.182</td><td>0.264</td><td>0.225</td><td>0.339</td><td>0.244</td><td>0.344</td><td>0.269</td><td>0.372</td><td>0.366</td><td>0.436</td><td>0.491</td><td>0.437</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.0029</td><td>0.039</td><td>0.0038</td><td>0.052</td><td>0.0062</td><td>0.062</td><td>0.011</td><td>0.081</td><td>0.0038</td><td>0.044</td><td>0.0046</td><td>0.052</td></tr><tr><td>192</td><td>0.0021</td><td>0.034</td><td>0.0015</td><td>0.029</td><td>0.0060</td><td>0.062</td><td>0.0075</td><td>0.067</td><td>0.0023</td><td>0.040</td><td>0.0056</td><td>0.060</td></tr><tr><td>336</td><td>0.0023</td><td>0.034</td><td>0.0039</td><td>0.053</td><td>0.0041</td><td>0.050</td><td>0.0063</td><td>0.062</td><td>0.0041</td><td>0.049</td><td>0.0060</td><td>0.054</td></tr><tr><td>720</td><td>0.0048</td><td>0.054</td><td>0.0024</td><td>0.037</td><td>0.0055</td><td>0.059</td><td>0.0085</td><td>0.070</td><td>0.0031</td><td>0.042</td><td>0.0071</td><td>0.063</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>0.674</td><td>0.671</td><td>0.607</td><td>0.587</td><td>0.708</td><td>0.627</td><td>0.948</td><td>0.732</td><td>5.282</td><td>2.050</td><td>3.607</td><td>1.662</td></tr><tr><td>36</td><td>0.712</td><td>0.733</td><td>0.551</td><td>0.604</td><td>0.584</td><td>0.617</td><td>0.634</td><td>0.650</td><td>4.554</td><td>1.916</td><td>2.407</td><td>1.363</td></tr><tr><td>48</td><td>0.823</td><td>0.803</td><td>0.693</td><td>0.704</td><td>0.717</td><td>0.697</td><td>0.791</td><td>0.752</td><td>4.273</td><td>1.846</td><td>3.106</td><td>1.575</td></tr><tr><td>60</td><td>0.992</td><td>0.892</td><td>0.816</td><td>0.779</td><td>0.855</td><td>0.774</td><td>0.874</td><td>0.797</td><td>5.214</td><td>2.057</td><td>3.698</td><td>1.733</td></tr></table>\n\n# 4.1 MAIN RESULTS\n\nMultivariate results For multivariate long-term series forecasting, MICN achieves the state-of-the-art performance in all benchmarks and all prediction length settings (Table 1). Compared to the previous best model FEDformer, MICN yields an  $17.2\\%$  averaged MSE reduction. Especially, under the input-96-predict-96 setting, MICN gives  $12\\%$  relative MSE reduction in ETTm2,  $14\\%$  relative MSE reduction in Electricity,  $31\\%$  relative MSE reduction in Exchange,  $12\\%$  relative MSE reduction in Traffic,  $26\\%$  relative MSE reduction in Weather,  $17\\%$  relative MSE reduction in ILI, and  $18.6\\%$  average MSE reduction in this setting. We can also find that MICN makes consistent improvements as the prediction increases, showing its competitiveness in terms of long-term time-series forecasting. Note that MICN still provides remarkable improvements with a  $51\\%$  averaged MSE reduction in the Exchange dataset that is without obvious periodicity. All above shows that MICN can cope well with a variety of time-series forecasting tasks in real-world applications. More results about other ETT benchmarks are provided in Appendix A.3. See Appendix C.3 for detailed showcases.\n\nUnivariate results We also show the univariate time-series forecasting results in Table 2. Significantly, MICN achieves a  $21.6\\%$  averaged MSE reduction compared to FEDformer. Especially for the Weather dataset, MICN gives  $53\\%$  relative MSE reduction under the predict-96 setting,  $75\\%$  relative MSE reduction under the predict-192 setting,  $44\\%$  relative MSE reduction under the predict-336 setting, and  $56\\%$  relative MSE reduction under the predict-720 setting. It again verifies the greater time-series forecasting capacity. More results about other ETT benchmarks are provided in Appendix A.3. See Appendix C.2 for detailed showcases.\n\n# 4.2 ABLATION STUDIES\n\nTrend-cyclical Prediction Block We attempt to verify the necessity of modeling the trend-cyclical part when using a decomposition-based structure. Like Autoformer (Wu et al., 2021b), previous methods decompose the time series and then take the mean prediction of the trend information, which is then added to the other trend information obtained from the decomposition module in the model. However, the reasons and rationality are not argued in the relevant papers. In this paper, we use simple linear regression to predict the trend-cyclical part and we also record the results of the mean prediction for comparison. Note that with different trend-cyclical prediction blocks, we have different models named MICN-regre and MICN-mean. As shown in Table 3, MICN-regre performs better than MICN-mean overall. Because all the datasets are non-stationary, a simple modeling for trend-cyclical to give model a holistic view of the trend direction is necessary. See Appendix B.2 for more visualization results and analysis.\n\nTable 3: Comparison of sample linear regression prediction and mean prediction in multivariate datasets. The better results are highlighted in bold.  \n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Datasets Prediction Length O</td><td colspan=\"4\">ETTm2</td><td colspan=\"4\">Electricity</td><td colspan=\"4\">Exchange</td><td colspan=\"4\">Traffic</td><td colspan=\"4\">WTH</td></tr><tr><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td></tr><tr><td rowspan=\"2\">MICN - regre</td><td>MSE</td><td>0.179</td><td>0.307</td><td>0.325</td><td>0.502</td><td>0.164</td><td>0.177</td><td>0.193</td><td>0.212</td><td>0.102</td><td>0.172</td><td>0.272</td><td>0.714</td><td>0.519</td><td>0.537</td><td>0.534</td><td>0.577</td><td>0.161</td><td>0.220</td><td>0.278</td><td>0.311</td></tr><tr><td>MAE</td><td>0.275</td><td>0.376</td><td>0.388</td><td>0.490</td><td>0.269</td><td>0.285</td><td>0.304</td><td>0.321</td><td>0.235</td><td>0.316</td><td>0.407</td><td>0.658</td><td>0.309</td><td>0.315</td><td>0.313</td><td>0.325</td><td>0.229</td><td>0.281</td><td>0.331</td><td>0.356</td></tr><tr><td rowspan=\"2\">MICN - mean</td><td>MSE</td><td>0.200</td><td>0.262</td><td>0.305</td><td>0.389</td><td>0.188</td><td>0.200</td><td>0.219</td><td>0.224</td><td>0.173</td><td>0.324</td><td>0.639</td><td>1.218</td><td>0.575</td><td>0.580</td><td>0.583</td><td>0.601</td><td>0.183</td><td>0.246</td><td>0.293</td><td>0.373</td></tr><tr><td>MAE</td><td>0.287</td><td>0.326</td><td>0.353</td><td>0.407</td><td>0.302</td><td>0.308</td><td>0.328</td><td>0.332</td><td>0.297</td><td>0.408</td><td>0.598</td><td>0.862</td><td>0.344</td><td>0.349</td><td>0.345</td><td>0.363</td><td>0.250</td><td>0.317</td><td>0.335</td><td>0.399</td></tr></table>\n\nLocal-Global Structure vs. Auto-correlation, self-attention In this work, we propose the local-global module to model the underlying pattern of time series, including local features and global correlations, while the previous outstanding model Autoformer uses auto-correlation. We replace the auto-correlation module in the original Autoformer with our proposed local-global module (we set  $i \\in \\{12, 16\\}$ ) for training, and the results are shown in Table 4. Also, We replace the Local-Global module in MICN-regre with the Auto-Correlation module and self-attention module for training, and the results are shown in Table 5. They all demonstrate that modeling time series in terms of local features and global correlations is better and more realistic.\n\nTable 4: Ablation of Local-global structure in other models. We replace the Auto-Correlation in Autoformer with our local-global module and implement it in the multivariate Electricity, Exchange and Traffic. The better results are highlighted in bold.  \n\n<table><tr><td colspan=\"2\">Datasets</td><td colspan=\"4\">Electricity</td><td colspan=\"4\">Exchange</td><td colspan=\"4\">Traffic</td></tr><tr><td colspan=\"2\">Prediction Length O</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td></tr><tr><td rowspan=\"2\">Autoformer-Local-Global</td><td>MSE</td><td>0.192</td><td>0.204</td><td>0.223</td><td>0.238</td><td>0.194</td><td>0.293</td><td>1.012</td><td>1.289</td><td>0.572</td><td>0.580</td><td>0.587</td><td>0.601</td></tr><tr><td>MAE</td><td>0.314</td><td>0.323</td><td>0.339</td><td>0.352</td><td>0.338</td><td>0.416</td><td>0.766</td><td>0.928</td><td>0.352</td><td>0.351</td><td>0.353</td><td>0.359</td></tr><tr><td rowspan=\"2\">Autoformer Auto-correlation</td><td>MSE</td><td>0.207</td><td>0.236</td><td>0.275</td><td>0.289</td><td>0.160</td><td>0.327</td><td>0.509</td><td>1.133</td><td>0.675</td><td>0.666</td><td>0.765</td><td>1.098</td></tr><tr><td>MAE</td><td>0.323</td><td>0.343</td><td>0.372</td><td>0.380</td><td>0.292</td><td>0.415</td><td>0.527</td><td>0.825</td><td>0.406</td><td>0.425</td><td>0.487</td><td>0.647</td></tr></table>\n\n# 4.3 MODEL ANALYSIS\n\nImpact of input length In time series forecasting tasks, the size of the input length indicates how much historical information the algorithm can utilize. In general, a model that has a strong ability to model long-term temporal dependency should perform better as the input length increases. Therefore, we conduct experiments with different input lengths and the same prediction length to validate our model. As shown in Figure 5, when the input length is relatively long, the performance of Transformer-based models becomes worse because of repeated short-term patterns as stated in (Zhou et al., 2021). Relatively, the overall performance of MICN prediction gradually gets better as\n\nTable 5: Ablation of Local-global structure in our model. We replace the Local-Global module in MICN-regre with Auto-correlation and self-attention and implement it in the multivariate Electricity, Exchange and Traffic. The better results are highlighted in bold.  \n\n<table><tr><td rowspan=\"2\" colspan=\"2\">Datasets Prediction Length O</td><td colspan=\"4\">Electricity</td><td colspan=\"4\">Exchange</td><td colspan=\"4\">Traffic</td></tr><tr><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td></tr><tr><td rowspan=\"2\">MICN-Local-Global</td><td>MSE</td><td>0.164</td><td>0.177</td><td>0.193</td><td>0.212</td><td>0.102</td><td>0.172</td><td>0.272</td><td>0.714</td><td>0.519</td><td>0.537</td><td>0.534</td><td>0.577</td></tr><tr><td>MAE</td><td>0.269</td><td>0.285</td><td>0.304</td><td>0.321</td><td>0.235</td><td>0.316</td><td>0.407</td><td>0.658</td><td>0.309</td><td>0.315</td><td>0.313</td><td>0.325</td></tr><tr><td rowspan=\"2\">MICN-Auto-Correlation</td><td>MSE</td><td>0.205</td><td>0.209</td><td>0.229</td><td>0.260</td><td>0.111</td><td>0.178</td><td>0.331</td><td>0.804</td><td>0.596</td><td>0.613</td><td>0.609</td><td>0.635</td></tr><tr><td>MAE</td><td>0.299</td><td>0.305</td><td>0.327</td><td>0.353</td><td>0.255</td><td>0.311</td><td>0.440</td><td>0.718</td><td>0.366</td><td>0.386</td><td>0.379</td><td>0.381</td></tr><tr><td rowspan=\"2\">MICN self-attention</td><td>MSE</td><td>0.181</td><td>0.194</td><td>0.216</td><td>0.271</td><td>0.147</td><td>0.290</td><td>0.480</td><td>1.578</td><td>0.612</td><td>0.642</td><td>0.622</td><td>0.656</td></tr><tr><td>MAE</td><td>0.289</td><td>0.304</td><td>0.321</td><td>0.362</td><td>0.291</td><td>0.402</td><td>0.549</td><td>0.978</td><td>0.357</td><td>0.376</td><td>0.374</td><td>0.382</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/084aa08b1df8d203675ebdd99e39725bc6397487d62ebee9f3ee174307e7257d.jpg)  \nElectricity\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/619cfea8277235759161f46d18ca5c465962e7dc47139a4e38a931a905bdfcc9.jpg)  \nExchange  \nFigure 5: The MSE results with different input lengths and same prediction lengths (192 time steps).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/2d7435138852d567ede2e3d2c52bc33473a810a27a2bd3f1ee1f5e13daa3a2bb.jpg)  \nTraffic\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/7831c404-23b6-4efb-a67e-87b0d58a8b15/4139203855e377a6d2fd77e598f3374d2b38b7f198aeab931725b3d41ea09db5.jpg)  \nWTH\n\nthe input length increases, indicating that MICN can capture the long-term temporal dependencies well and extract useful information deeply.\n\nRobustness analysis We use a simple noise injection to demonstrate the robustness of our model. Concretely, we randomly select data with proportion  $\\varepsilon$  in the original input sequence and randomly perturb the selected data in the range  $[-2X_i, 2X_i]$ , where  $X_i$  denotes the original data. The data after noise injection is then trained, and the MSE and MAE metrics are recorded. The results are shown in Table 6. As the proportion of perturbations  $\\varepsilon$  increases, the MSE and MAE metrics of the predictions increase by a small amount. It indicates that MICN exhibits good robustness in response to less noisy data (up to  $10\\%$ ) and has a great advantage in dealing with many data abnormal fluctuations (e.g. abnormal power data caused by equipment damage).\n\nTable 6: Robustness analysis of multivariate results. Different  $\\varepsilon$  indicates different proportions of noise injection. And MICN-regre is used as the base model.  \n\n<table><tr><td colspan=\"2\">Datasets</td><td colspan=\"4\">Electricity</td><td colspan=\"4\">Exchange</td><td colspan=\"4\">Traffic</td></tr><tr><td colspan=\"2\">Prediction Length O</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td></tr><tr><td rowspan=\"2\">MICN - \nregre</td><td>MSE</td><td>0.164</td><td>0.177</td><td>0.193</td><td>0.212</td><td>0.102</td><td>0.172</td><td>0.272</td><td>0.714</td><td>0.519</td><td>0.537</td><td>0.534</td><td>0.577</td></tr><tr><td>MAE</td><td>0.269</td><td>0.285</td><td>0.304</td><td>0.321</td><td>0.235</td><td>0.316</td><td>0.407</td><td>0.658</td><td>0.309</td><td>0.315</td><td>0.313</td><td>0.325</td></tr><tr><td rowspan=\"2\">ε = 1%</td><td>MSE</td><td>0.163</td><td>0.179</td><td>0.192</td><td>0.217</td><td>0.103</td><td>0.172</td><td>0.289</td><td>0.691</td><td>0.518</td><td>0.530</td><td>0.535</td><td>0.575</td></tr><tr><td>MAE</td><td>0.270</td><td>0.288</td><td>0.303</td><td>0.325</td><td>0.237</td><td>0.316</td><td>0.424</td><td>0.652</td><td>0.321</td><td>0.312</td><td>0.315</td><td>0.323</td></tr><tr><td rowspan=\"2\">ε = 5%</td><td>MSE</td><td>0.164</td><td>0.181</td><td>0.192</td><td>0.218</td><td>0.104</td><td>0.167</td><td>0.296</td><td>1.742</td><td>0.518</td><td>0.541</td><td>0.558</td><td>0.585</td></tr><tr><td>MAE</td><td>0.272</td><td>0.289</td><td>0.303</td><td>0.328</td><td>0.239</td><td>0.308</td><td>0.413</td><td>1.009</td><td>0.313</td><td>0.327</td><td>0.330</td><td>0.328</td></tr><tr><td rowspan=\"2\">ε = 10%</td><td>MSE</td><td>0.171</td><td>0.189</td><td>0.202</td><td>0.220</td><td>0.136</td><td>0.181</td><td>0.402</td><td>0.944</td><td>0.538</td><td>0.557</td><td>0.561</td><td>0.605</td></tr><tr><td>MAE</td><td>0.281</td><td>0.297</td><td>0.311</td><td>0.328</td><td>0.273</td><td>0.324</td><td>0.497</td><td>0.771</td><td>0.332</td><td>0.324</td><td>0.325</td><td>0.335</td></tr></table>\n",
  "hyperparameter": "Input length I=96 (I=36 for ILI dataset); Prediction length O∈{96,192,336,720} (O∈{24,36,48,60} for ILI); Hidden dimension D (not explicitly specified); Number of MIC layers N (not explicitly specified); Multi-scale kernel sizes i∈{I/4, I/8, ...} for different branches in MIC layers; Dropout applied in Local-Global module; Trend-cyclical decomposition kernels: multiple kernels (kernel₁, ..., kernelₙ) for MHDecomp; Conv1d stride=kernel=i for local feature compression; Isometric convolution kernel=S (sequence length) with padding of S-1; Training/validation/test split: 6:2:2 for ETT datasets, 7:1:2 for other datasets"
}