[
  {
    "source": "Informer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer encoder-decoder paradigm for long sequence time-series forecasting. The source implementation's base architecture (Encoder, EncoderLayer, Decoder, DecoderLayer classes) can be directly reused as the structural skeleton. The initialization, forward propagation logic, and layer stacking mechanisms are transferable.\n\n2. **Embedding and Positional Encoding**: Both use DataEmbedding for input representation with temporal encoding (configs.embed, configs.freq). Autoformer can reuse Informer's DataEmbedding implementation completely, including the temporal feature embedding and positional encoding strategies, since both handle similar input formats (x_enc, x_mark_enc for encoder; x_dec, x_mark_dec for decoder).\n\n3. **Multi-head Attention Framework**: Both implement multi-head attention mechanisms with query-key-value projections. Informer's AttentionLayer class structure (query_projection, key_projection, value_projection, out_projection) provides the template. Autoformer's Auto-Correlation can follow the same projection pattern, only replacing the inner_attention mechanism while keeping the multi-head projection framework intact.\n\n4. **Feedforward Network Design**: Both use identical feedforward networks with Conv1d layers and activation functions between attention blocks. Informer's implementation in DecoderLayer (self.conv1, self.conv2 with activation and dropout) can be directly copied to Autoformer's encoder and decoder layers.\n\n5. **Layer Normalization and Residual Connections**: Both employ LayerNorm and residual connections after each sub-block. The pattern 'x = x + dropout(sublayer(x))' followed by 'x = norm(x)' in Informer's implementation is identical to Autoformer's requirements in Equations 3 and 4.\n\n6. **Training Configuration and Loss Function**: Both use MSE loss for forecasting tasks and share similar hyperparameters (d_model, n_heads, d_ff, dropout). The training loop, optimizer setup, and evaluation metrics from Informer's codebase can be adapted with minimal changes.\n\n7. **Decoder Input Strategy**: Both use a similar decoder initialization strategy with historical information concatenated with future placeholders. Informer's generative inference approach (Equation 6: concat of X_token and X_0) aligns with Autoformer's decoder input design (Equation 2), allowing code reuse for input preparation.",
    "differences": "1. **CORE INNOVATION - Auto-Correlation Mechanism (NEW IMPLEMENTATION REQUIRED)**: Autoformer replaces Informer's ProbSparse self-attention with Auto-Correlation mechanism. This requires implementing: (a) Autocorrelation computation via FFT (Equation 8: Wiener-Khinchin theorem with Fast Fourier Transform), (b) Period-based dependency discovery using argTopk on autocorrelation values (Equation 6: selecting top-k time delays τ₁,...,τₖ), (c) Time delay aggregation with Roll operation to align sub-series at the same phase positions (Equation 6: Roll(V, τᵢ) weighted by softmax-normalized autocorrelations). The implementation must replace ProbAttention's _prob_QK and _update_context methods with FFT-based autocorrelation calculation and rolling aggregation.\n\n2. **Series Decomposition Block (NEW IMPLEMENTATION REQUIRED)**: Autoformer introduces series decomposition as a built-in operation throughout the architecture. Must implement: (a) SeriesDecomp module using AvgPool with padding to extract trend-cyclical components (Equation 1: X_t = AvgPool(Padding(X)), X_s = X - X_t), (b) Integration of decomposition blocks after EVERY attention and feedforward operation in both encoder and decoder (Equations 3-4), (c) The decomposition block should output both seasonal and trend components, unlike Informer which only processes single representations. This fundamentally changes the data flow through the network.\n\n3. **Trend-Cyclical Accumulation in Decoder (NEW IMPLEMENTATION REQUIRED)**: Autoformer's decoder progressively accumulates trend components, requiring: (a) Separate tracking of trend variables T_de^l across decoder layers (Equation 4: T_de^l = T_de^(l-1) + W_l,1*T_de^(l,1) + W_l,2*T_de^(l,2) + W_l,3*T_de^(l,3)), (b) Learnable projection weights W_l,i for each extracted trend component, (c) Modified decoder forward pass to maintain parallel seasonal and trend paths, (d) Final prediction combines both paths (W_S * X_de^M + T_de^M), unlike Informer's single output path from the decoder projection layer.\n\n4. **Encoder Distilling vs. Seasonal Focus (ARCHITECTURAL CHANGE)**: Informer uses self-attention distilling with ConvLayer and MaxPool to progressively reduce sequence length (Equation 5), creating a pyramid structure with halving inputs. Autoformer's encoder eliminates this distilling mechanism entirely, instead focusing on seasonal pattern modeling by decomposing and discarding trend components at each layer (Equation 3: only seasonal parts S_en^(l,i) are passed forward). The encoder output dimension handling differs: Informer concatenates multi-scale features from pyramid stacks, while Autoformer outputs only the final seasonal representation X_en^N.\n\n5. **Decoder Initialization Strategy (MODIFIED IMPLEMENTATION)**: While both use historical information, Autoformer requires dual initialization: (a) Seasonal component X_des initialized with decomposed recent history and zero placeholders (Equation 2: Concat(X_ens, X_0)), (b) Trend component X_det initialized with decomposed recent trend and mean placeholders (Equation 2: Concat(X_ent, X_Mean)), (c) SeriesDecomp must be applied to encoder input's latter half (X_en[I/2:I]) before decoder initialization. Informer's simpler concatenation of start tokens with zeros needs extension to handle dual-component initialization.\n\n6. **Attention Mask Handling (IMPLEMENTATION MODIFICATION)**: Informer uses ProbMask for sparse attention with specific masking strategies for causal relationships. Autoformer's Auto-Correlation uses different masking: (a) Period-based masking focuses on sub-series alignment rather than point-wise causality, (b) The Roll operation inherently handles temporal dependencies differently than masked attention, (c) Encoder-decoder Auto-Correlation resizes K,V to length-O (prediction length) rather than using traditional cross-attention masking, requiring new mask generation logic.\n\n7. **Complexity and Efficiency Trade-offs (ALGORITHMIC CHANGE)**: Informer achieves O(L log L) through query sparsity measurement and selective attention (Equation 2-4: max-mean measurement with Top-u query selection). Autoformer achieves O(L log L) through FFT-based autocorrelation and aggregating O(log L) series. The efficiency mechanisms are fundamentally different: Informer reduces computation by selecting important queries, while Autoformer reduces computation by limiting the number of aggregated time delays. Implementation must replace Informer's sampling and sparsity measurement with FFT computation and top-k delay selection.\n\n8. **Output Projection and Final Prediction (STRUCTURAL CHANGE)**: Informer uses a single linear projection from decoder output to target dimension (self.projection in Decoder). Autoformer requires: (a) Separate projection W_S for seasonal component transformation, (b) Accumulated trend component T_de^M from all decoder layers, (c) Element-wise addition of projected seasonal and accumulated trend for final prediction. The final forward pass must be restructured to handle dual-path outputs and their combination."
  },
  {
    "source": "Reformer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the Transformer architecture with encoder-decoder structure. The Reformer implementation provides the basic Transformer components (EncoderLayer, multi-head mechanism, feed-forward networks) that can be directly reused. The `Encoder` class with `EncoderLayer` stacking in Reformer's code can serve as a template for Autoformer's encoder structure, requiring only modifications to replace LSH attention with Auto-Correlation.\n\n2. **Embedding and Positional Encoding**: Both use `DataEmbedding` for input representation combining token embeddings with temporal features. Reformer's `DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)` can be directly reused in Autoformer implementation, as both papers need to encode time series with temporal markers (x_mark_enc).\n\n3. **Layer Normalization and Residual Connections**: Both architectures employ residual connections with layer normalization. Reformer's `norm_layer=torch.nn.LayerNorm(configs.d_model)` and the residual structure in EncoderLayer can be reused. Autoformer's equations show `Auto-Correlation(...) + X` patterns identical to Reformer's residual design.\n\n4. **Multi-head Mechanism**: Both implement multi-head attention patterns. Reformer's multi-head structure with `configs.n_heads` and dimension splitting logic can guide Autoformer's multi-head Auto-Correlation implementation in Equation 7, where heads are concatenated and projected.\n\n5. **Efficient Computation via FFT**: Both papers leverage Fast Fourier Transform for efficiency. Reformer uses LSH with random projections, while Autoformer uses FFT for autocorrelation computation (Equation 8). The FFT-based computational pattern in both achieves O(L log L) complexity, suggesting similar optimization strategies can be applied.\n\n6. **Sequence Length Handling**: Both deal with sequence length constraints. Reformer's `fit_length()` method that pads sequences to match bucket_size requirements provides a template for Autoformer's sequence padding needs, especially for the decoder initialization with length I/2 + O.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Reformer uses Locality-Sensitive Hashing (LSH) attention with hash bucketing and sorting (Equations 2-6 in Reformer), while Autoformer introduces Auto-Correlation mechanism (Equation 6 in Autoformer). **Must implement from scratch**: (a) Autocorrelation computation using FFT based on Wiener-Khinchin theorem (Equation 8), (b) Period-based dependency discovery via `argTopk` on autocorrelation values, (c) Time delay aggregation with `Roll(V, τ)` operation to align sub-series, (d) Softmax-weighted aggregation of rolled series. This completely replaces Reformer's `LSHSelfAttention` module.\n\n2. **Series Decomposition Architecture - NEW IMPLEMENTATION REQUIRED**: Autoformer's core innovation is the built-in series decomposition block (Equation 1) that separates trend-cyclical and seasonal components. **Must implement**: (a) `SeriesDecomp` block using moving average (`AvgPool` with padding) to extract trend Xt, (b) Seasonal component Xs = X - Xt extraction, (c) Integration into every encoder/decoder layer (Equations 3-4), (d) Dual-path processing where encoder focuses on seasonal modeling while decoder accumulates trend progressively. Reformer has no decomposition concept.\n\n3. **Decoder Architecture and Initialization - NEW IMPLEMENTATION REQUIRED**: Autoformer uses a fundamentally different decoder design. **Must implement**: (a) Dual-component decoder inputs (Xdes and Xdet) initialized from the latter half of encoder input plus placeholders (Equation 2), (b) Trend accumulation structure: `Tde^l = Tde^(l-1) + W*Tde^(l,1) + ...` that progressively refines trend across layers (Equation 4), (c) Three decomposition blocks per decoder layer extracting trends after each sub-block, (d) Final prediction as sum of seasonal and trend components: `Ws*Xde^M + Tde^M`. Reformer's decoder is standard Transformer-style.\n\n4. **Temporal Dependency Discovery - CONCEPTUAL DIFFERENCE**: Reformer discovers dependencies through point-wise similarity in hash space (nearby vectors in high-dimensional space via random projections), while Autoformer discovers period-based dependencies through series-wise connections. **Implementation impact**: Autoformer's `argTopk` selects k=⌊c×log L⌋ dominant periods based on autocorrelation peaks, fundamentally different from Reformer's bucket-based nearest neighbor search. This requires implementing period length estimation and phase-aligned aggregation.\n\n5. **Information Aggregation Strategy - NEW IMPLEMENTATION REQUIRED**: Reformer aggregates information from hash buckets using standard attention weights (softmax of dot-products within buckets), while Autoformer aggregates via time delay alignment. **Must implement**: The `Roll(V, τi)` operation that cyclically shifts value sequences by delay τi to align similar sub-processes at the same phase position, then weighted sum by autocorrelation confidence. This sub-series-level aggregation differs fundamentally from Reformer's point-wise aggregation.\n\n6. **Encoder-Decoder Cross-Attention - MODIFICATION REQUIRED**: In Autoformer, encoder-decoder Auto-Correlation resizes K,V from encoder output to length-O for prediction (mentioned after Equation 6), while Q comes from decoder. This differs from standard cross-attention and requires implementing the resizing logic and period-based cross-correlation between encoder's past seasonal information and decoder's future prediction.\n\n7. **Task Objective - FUNDAMENTAL DIFFERENCE**: Reformer implementation targets classification (outputs `configs.num_class` via projection layer), while Autoformer targets long-term forecasting (outputs future sequence of length O). **Must implement**: Replace classification head with sequence prediction head that outputs (I/2 + O) × d dimensional predictions, with proper handling of the seasonal and trend components' projection and summation."
  },
  {
    "source": "DLinear_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers utilize the series decomposition technique from Autoformer to handle trend and seasonal components. DLinear explicitly implements `series_decomp(configs.moving_avg)` to separate trend and seasonal parts, applying separate linear transformations to each. PatchTST can leverage this same decomposition approach as a preprocessing step before patching, reusing the `series_decomp` class from `layers.Autoformer_EncDec`. The implementation shows initialization of seasonal/trend components via `seasonal_init, trend_init = self.decompsition(x)` followed by permutation for temporal processing.\n\n2. **Instance Normalization Strategy**: Both methods employ normalization techniques to handle distribution shifts across different time series domains. DLinear's NLinear variant subtracts the last value of the sequence for normalization and adds it back after prediction. PatchTST uses zero-mean unit-variance instance normalization before patching with mean/deviation added back to output. The core pattern of `normalize → process → denormalize` is shared, and DLinear's normalization logic can be adapted for PatchTST's instance normalization layer.\n\n3. **Channel-Independence Processing**: Both architectures process each variate independently without modeling cross-channel correlations. DLinear's implementation shows `self.individual` flag controlling whether to use separate linear layers per channel (`nn.ModuleList()` with per-channel parameters) or shared weights. PatchTST follows the same channel-independence philosophy where each univariate series is \"fed independently into the Transformer backbone\". The code pattern of iterating through channels and applying transformations independently can be directly reused.\n\n4. **Linear Projection Layers**: Both use linear transformations as core building blocks. DLinear applies `nn.Linear(self.seq_len, self.pred_len)` for temporal mapping. PatchTST uses trainable linear projection `W_p ∈ R^(D×P)` to map patches to latent space and final flatten+linear head for predictions. The initialization pattern `nn.Parameter((1/self.seq_len) * torch.ones([self.pred_len, self.seq_len]))` from DLinear can inform weight initialization strategies for PatchTST's projection layers.\n\n5. **Task-Agnostic Encoder-Decoder Pattern**: Both implement an encoder-decoder structure adaptable to multiple tasks. DLinear's code shows `self.task_name` configuration and separate methods (`encoder()`, `classification()`) for different tasks. The classification method flattens encoder output (`enc_out.reshape(enc_out.shape[0], -1)`) before projection, a pattern applicable to PatchTST's classification head design when adapting the forecasting model to classification tasks.",
    "differences": "1. **Core Architecture Paradigm**: DLinear uses purely linear layers without any attention mechanism, operating directly on raw temporal sequences with complexity O(L). The implementation shows simple matrix multiplication via `nn.Linear` layers. PatchTST requires implementing a full **Transformer encoder with multi-head self-attention** (scaled dot-product attention with Q, K, V matrices), feed-forward networks, residual connections, and BatchNorm layers. NEW IMPLEMENTATION NEEDED: `Attention(Q, K, V) = Softmax(QK^T/√d_k)V`, multi-head attention aggregation, positional encoding addition, and layer normalization components not present in DLinear.\n\n2. **Input Tokenization Strategy**: DLinear processes entire sequences as continuous vectors with shape transformations via `permute(0, 2, 1)` for temporal axis alignment. PatchTST introduces **patch-based tokenization** requiring NEW IMPLEMENTATION: (a) Patching logic to divide sequences into overlapping/non-overlapping segments with configurable stride S and patch length P, generating N = ⌊(L-P)/S⌋ + 2 patches; (b) Padding logic to append repeated last values before patching; (c) Patch embedding via linear projection W_p; (d) Learnable additive positional encoding W_pos ∈ R^(D×N) specific to patch positions, not time steps.\n\n3. **Temporal Complexity and Memory Management**: DLinear operates with O(L) complexity through direct linear transformations on full sequences. PatchTST achieves quadratic complexity reduction from O(L²) to O((L/S)²) through patching, enabling longer lookback windows. NEW IMPLEMENTATION NEEDED: Dynamic patch number calculation based on sequence length, stride configuration management, and memory-efficient attention computation over reduced token count. The code must handle variable-length sequences and adjust patch counts accordingly, unlike DLinear's fixed-size linear layers.\n\n4. **Self-Supervised Pre-training Framework**: DLinear is purely supervised with no pre-training mechanism. PatchTST introduces **masked autoencoder pre-training** requiring entirely NEW COMPONENTS: (a) Random patch masking logic selecting subset of patch indices and zeroing corresponding patches; (b) Reconstruction head (D×P linear layer) replacing forecasting head; (c) Masked patch reconstruction loss computing MSE only on masked regions; (d) Transfer learning pipeline to load pre-trained weights and fine-tune on downstream tasks; (e) Handling different numbers of variates between pre-training and fine-tuning datasets.\n\n5. **Output Head Architecture**: DLinear uses a simple flattening + linear projection: `output = enc_out.reshape(enc_out.shape[0], -1)` followed by `self.projection(output)` with dimensions `(enc_in * seq_len, num_class)`. PatchTST requires NEW IMPLEMENTATION of: (a) Flatten layer processing patch representations z^(i) ∈ R^(D×N) from Transformer encoder; (b) Linear head mapping latent representations to prediction horizon T for each channel; (c) Separate head architectures for forecasting (predicting future T steps) vs. classification (mapping to class labels); (d) Aggregation mechanism to combine predictions from M independent channels when needed for multivariate forecasting."
  },
  {
    "source": "FEDformer_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both models utilize the Transformer encoder as their core backbone architecture. FEDformer's implementation includes complete encoder-decoder structures with multi-head attention mechanisms, layer normalization (my_Layernorm), and feed-forward networks. PatchTST can directly reuse the basic Transformer encoder components including the EncoderLayer structure, attention mechanism framework, and normalization layers from FEDformer's codebase.\n\n2. **Embedding and Positional Encoding Infrastructure**: Both models employ embedding layers to project input time series into latent representations. FEDformer's DataEmbedding class handles temporal and positional encodings which can be adapted for PatchTST's patch-level positional encoding (W_pos). The embedding dimension transformation logic (configs.d_model) and dropout mechanisms are directly transferable.\n\n3. **Instance Normalization for Distribution Shift**: Both papers recognize and address distribution shift between training and testing data. FEDformer uses series_decomp for normalization purposes, while PatchTST explicitly employs instance normalization. The normalization infrastructure from FEDformer (mean/std computation and restoration) can be adapted for PatchTST's instance normalization requirements.\n\n4. **Multi-head Attention Mechanism**: Both models leverage multi-head attention with configurable heads (configs.n_heads). FEDformer's attention implementation with query-key-value projections and scaled dot-product attention can serve as the foundation for PatchTST's attention layers, requiring only modifications to handle patch-level tokens instead of time-step tokens.\n\n5. **Channel-wise Processing Capability**: While FEDformer processes multivariate series jointly, its architecture supports per-channel operations through dimension handling. PatchTST's channel-independence design can leverage FEDformer's dimension management code (handling B, L, H, E shapes) by processing each channel separately through the same Transformer backbone with shared weights.\n\n6. **Training Infrastructure and Loss Functions**: Both models use MSE loss for optimization and share similar training configurations (dropout, activation functions, learning rate scheduling). FEDformer's training loop, loss computation (MSE over predictions), and optimization infrastructure can be directly reused for PatchTST's supervised and self-supervised learning tasks.",
    "differences": "1. **Patching Mechanism (NEW IMPLEMENTATION REQUIRED)**: PatchTST's core innovation is dividing time series into patches with configurable patch length P and stride S, reducing input tokens from L to approximately L/S. This requires implementing: (a) A patching function that segments univariate series into overlapping/non-overlapping patches with padding, (b) Patch embedding via trainable linear projection W_p ∈ R^(D×P), and (c) Patch-level positional encoding. FEDformer operates on raw time-step level without any patching, so this entire patching pipeline must be built from scratch.\n\n2. **Frequency Domain Processing vs. Time Domain Patches**: FEDformer's fundamental approach uses Fourier/Wavelet transforms (FourierBlock, MultiWaveletTransform) to process signals in frequency domain with modes selection and complex-valued operations. PatchTST operates entirely in time domain on segmented patches without any frequency transformation. The entire frequency enhancement infrastructure (FEB-f, FEB-w, FEA-f, FEA-w, DFT/DWT operations, complex multiplication) is NOT needed for PatchTST and must be replaced with simple patch-based processing.\n\n3. **Channel-Independence Architecture (NEW DESIGN REQUIRED)**: PatchTST explicitly processes each channel independently through the same Transformer backbone with shared weights, where each univariate series x^(i) is fed separately and produces independent predictions. This requires: (a) Implementing a loop or parallel processing mechanism to handle M channels independently, (b) Ensuring weight sharing across channels while maintaining separate forward passes, and (c) Aggregating channel-specific losses. FEDformer processes all channels jointly in a single forward pass, requiring significant architectural restructuring.\n\n4. **Encoder-Only vs. Encoder-Decoder Structure**: PatchTST uses only a Transformer encoder with a simple linear prediction head (flatten + linear layer for forecasting, D×P linear layer for reconstruction). FEDformer employs a full encoder-decoder architecture with cross-attention between encoder and decoder. The entire decoder component (DecoderLayer, cross-attention mechanisms, decoder embedding, trend aggregation in decoder) must be removed, and a NEW simple prediction head must be implemented.\n\n5. **Masked Autoencoder for Self-Supervised Learning (NEW IMPLEMENTATION REQUIRED)**: PatchTST introduces masked patch reconstruction for pre-training where random patches are masked with zeros and the model reconstructs them. This requires implementing: (a) Random patch masking strategy with configurable masking ratio, (b) Reconstruction head (D×P linear layer) separate from forecasting head, (c) Masked MSE loss computation only on masked patches, and (d) Pre-training and fine-tuning pipeline. FEDformer has no self-supervised learning component.\n\n6. **Seasonal-Trend Decomposition Removal**: FEDformer extensively uses seasonal-trend decomposition throughout its architecture (MOEDecomp blocks, series_decomp with moving average filters, separate processing of seasonal and trend components). PatchTST does NOT employ any decomposition strategy, treating the time series as a whole after instance normalization. All decomposition-related code (MOEDecomp, mixture of experts, trend extraction and aggregation) must be completely removed.\n\n7. **Simplified Prediction Head Design**: PatchTST uses a straightforward flatten + linear layer to map patch representations (D×N) to forecasting outputs (1×T), with parameter matrix of size (D·N)×T. This is much simpler than FEDformer's complex decoder with multiple decomposition blocks and trend aggregation. The prediction head must be redesigned to: (a) Flatten the patch representation sequence, (b) Apply a single linear projection to forecast horizon T, and (c) Handle the denormalization to restore original scale.\n\n8. **Lookback Window Handling**: PatchTST explicitly defines lookback window L and forecast horizon T with clear separation, using only historical data for prediction. FEDformer uses a more complex setup with label_len (decoder input from encoder) and pred_len, requiring careful handling of encoder-decoder information flow. The input preparation logic must be simplified to only use the lookback window L without any label sequence."
  },
  {
    "source": "Pyraformer_2021",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture**: Both papers use Transformer encoders as their core architecture. Pyraformer's `EncoderLayer` with self-attention and feed-forward networks can be directly reused as the base Transformer block for PatchTST. The `PositionwiseFeedForward` module with GELU activation, dropout, and layer normalization is identical in both approaches.\n\n2. **Embedding and Position Encoding**: Both models embed input sequences and add positional information. Pyraformer's `DataEmbedding` approach (combining observation, covariate, and positional embeddings) provides a foundation, though PatchTST uses learnable additive position encoding specifically for patches. The linear projection mechanism in Pyraformer can be adapted for PatchTST's patch projection layer.\n\n3. **Multi-head Attention Mechanism**: Both implement standard multi-head attention with Q-K-V transformations. Pyraformer's `AttentionLayer` with `FullAttention` (scaled dot-product attention with softmax) is the same mechanism used in PatchTST's Transformer encoder. The attention computation formula is identical: Softmax(QK^T/√d_k)V.\n\n4. **Normalization and Regularization**: Both use LayerNorm and Dropout for regularization. Pyraformer's implementation of these components in `EncoderLayer` and `PositionwiseFeedForward` can be directly reused. PatchTST additionally uses BatchNorm and Instance Normalization, which need to be added.\n\n5. **Channel-wise Processing**: Both models process multivariate time series in a channel-independent manner. Pyraformer's architecture already supports processing each channel separately through its encoder, which aligns with PatchTST's design philosophy of splitting input into M univariate series.",
    "differences": "1. **Core Innovation - Patching vs Pyramidal Attention**: Pyraformer reduces complexity through pyramidal multi-scale attention (PAM) with O(L) complexity, using coarser-scale construction (CSCM with ConvLayer) to build a C-ary tree structure. PatchTST achieves efficiency through patching - dividing sequences into non-overlapping/overlapping segments of length P with stride S, reducing tokens from L to ~L/S. This requires implementing: (a) A patching module that segments input sequences, (b) Padding logic to handle sequence length, (c) Patch projection layer W_p ∈ R^(D×P).\n\n2. **Attention Pattern and Complexity**: Pyraformer uses sparse pyramidal attention where each node attends to limited neighbors (A adjacent nodes, C children, 1 parent) via custom CUDA kernels with get_mask() and refer_points() functions. PatchTST uses standard full attention over patches with complexity O(N²) where N=L/S << L. Need to implement: (a) Remove pyramidal masking logic, (b) Use vanilla attention without custom masks, (c) Remove CSCM convolution layers entirely.\n\n3. **Temporal Encoding Strategy**: Pyraformer creates multi-resolution representations through bottleneck convolutions (Bottleneck_Construct with multiple ConvLayer modules, stride-C convolutions) building a hierarchical tree. PatchTST treats patches as flat tokens with learnable positional embeddings W_pos ∈ R^(D×N). Need to implement: (a) Simple linear patch embedding, (b) Learnable position encoding for N patches (not hierarchical), (c) Remove all convolution-based coarsening.\n\n4. **Output Head Design**: Pyraformer uses scale-aware prediction - gathering features from all pyramid scales via refer_points(), concatenating multi-scale representations, then projecting through FC layers. For classification, it flattens encoder output and projects to num_classes. PatchTST uses: (a) For forecasting: flatten layer + linear head mapping D×N → T, (b) For self-supervised: D×P linear layer for patch reconstruction. Need to implement: (a) Simple flatten + linear projection without multi-scale gathering, (b) Masked autoencoder head for pre-training.\n\n5. **Training Paradigm**: Pyraformer is trained end-to-end for supervised tasks only. PatchTST introduces self-supervised pre-training with masked patch reconstruction: (a) Random masking of patches (not single time steps), (b) MSE loss to reconstruct masked patches, (c) Transfer learning capability where pre-training data can have different number of channels than downstream tasks. Need to implement: (a) Random patch masking logic, (b) Reconstruction head, (c) Pre-training and fine-tuning pipeline.\n\n6. **Instance Normalization**: PatchTST applies instance normalization to each univariate series x^(i) before patching (zero mean, unit std), then denormalizes predictions. Pyraformer doesn't use this. Need to implement: (a) Per-channel normalization before patching, (b) Store mean/std statistics, (c) Denormalization after prediction.\n\n7. **Loss Function and Aggregation**: Pyraformer uses task-specific losses (e.g., classification cross-entropy). PatchTST uses MSE loss averaged over M channels: L = E_x (1/M)Σ||x̂^(i) - x^(i)||². For forecasting, this channel-wise MSE aggregation needs to be implemented explicitly."
  },
  {
    "source": "Informer_2020",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer Encoder Architecture**: Both papers utilize vanilla Transformer encoder as the core backbone. PatchTST can directly reuse Informer's EncoderLayer implementation including multi-head attention, feed-forward networks, and residual connections. The AttentionLayer class with query/key/value projections and the basic encoder structure from Informer's code can serve as the foundation.\n\n2. **Embedding and Positional Encoding**: Both employ learnable positional encodings to capture temporal order. PatchTST can adapt Informer's DataEmbedding class, though it needs modification since PatchTST applies position encoding to patches rather than individual time steps. The linear projection mechanism in Informer's embedding layer is conceptually similar to PatchTST's patch projection.\n\n3. **MSE Loss Function**: Both papers use Mean Squared Error (MSE) loss for training. PatchTST can directly reuse Informer's loss calculation approach, though PatchTST averages loss across channels in the channel-independent setting.\n\n4. **Normalization Techniques**: Both use normalization strategies. While Informer uses LayerNorm in the encoder, PatchTST additionally employs instance normalization before patching. The LayerNorm implementation in Informer's encoder layers can be reused.\n\n5. **Feed-Forward Network Design**: Both use similar FFN structures with Conv1d layers or linear projections. PatchTST can adapt Informer's feed-forward network design with activation functions (GELU/ReLU) and dropout.",
    "differences": "1. **Core Innovation - Patching Mechanism (NEW IMPLEMENTATION REQUIRED)**: PatchTST's fundamental contribution is segmenting time series into patches as input tokens, reducing sequence length from L to approximately L/S. This requires implementing: (a) A patching module that divides univariate series into overlapping/non-overlapping patches with configurable patch length P and stride S; (b) Padding mechanism to append S repeated values of the last element before patching; (c) Patch projection layer W_p ∈ R^(D×P) to map patches to latent space. Informer processes individual time steps, so this entire patching pipeline is completely new.\n\n2. **Channel-Independence vs. Multivariate Processing (NEW IMPLEMENTATION REQUIRED)**: PatchTST treats each channel independently with shared weights, feeding M univariate series separately through the same backbone. This requires: (a) Input splitting logic to separate multivariate series into M univariate channels; (b) Independent forward passes for each channel with shared Transformer weights; (c) Aggregation of M independent predictions. Informer processes multivariate series jointly through encoder-decoder, so this channel-wise processing paradigm needs complete reimplementation.\n\n3. **Attention Mechanism - Standard vs. ProbSparse (REMOVE COMPLEXITY)**: Informer uses ProbSparse self-attention with query sparsity measurement M(q_i, K) and Top-u query selection to achieve O(L log L) complexity. PatchTST uses vanilla scaled dot-product attention O(N²) where N = L/S << L due to patching. PatchTST should REMOVE Informer's complex ProbAttention class including _prob_QK, _get_initial_context, _update_context methods, and use standard PyTorch attention or simple scaled dot-product implementation.\n\n4. **Encoder-Decoder vs. Encoder-Only Architecture (REMOVE DECODER)**: Informer uses full encoder-decoder with: (a) Decoder with masked self-attention and cross-attention; (b) Start token mechanism for generative inference; (c) ConvLayer distilling operations in encoder. PatchTST uses encoder-only architecture with a simple flatten + linear head. The entire Decoder, DecoderLayer, and ConvLayer classes from Informer should be REMOVED. Replace with a lightweight prediction head: flatten(z^(i)) → Linear(D×N, T).\n\n5. **Self-Supervised Pre-training with Masked Autoencoding (NEW IMPLEMENTATION REQUIRED)**: PatchTST introduces masked patch reconstruction for representation learning: (a) Random patch-level masking (not time-step level); (b) Reconstruction head (D×P linear layer) instead of forecasting head; (c) Separate pre-training and fine-tuning workflows. This requires implementing: masking logic to select and zero-out random patches uniformly; reconstruction loss to recover masked patches; dual-mode architecture supporting both supervised forecasting and self-supervised pre-training. Informer has no self-supervised learning component.\n\n6. **Instance Normalization (NEW IMPLEMENTATION REQUIRED)**: PatchTST applies instance normalization to each univariate series x^(i) before patching (zero mean, unit std), then denormalizes predictions. This requires: (a) Computing per-series statistics (mean, std); (b) Normalizing input before patching; (c) Storing statistics and applying inverse transformation to predictions. Informer doesn't use instance normalization.\n\n7. **Sequence Length Reduction Strategy**: Informer reduces memory via ProbSparse attention and distilling (halving sequence length with MaxPool). PatchTST reduces sequence length directly through patching (L → N where N ≈ L/S), achieving quadratic memory reduction by factor S². The distilling mechanism with ConvLayer and MaxPool from Informer should be removed as patching provides more efficient length reduction.\n\n8. **Forecasting Paradigm**: Informer uses generative inference with start tokens (L_token-length historical slice + placeholder X_0), predicting via one forward pass. PatchTST uses direct forecasting: encoder representations → flatten → linear projection to horizon T. The start token mechanism and generative decoder logic should be completely removed and replaced with the simpler direct forecasting head."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the Transformer architecture for time series modeling, utilizing self-attention mechanisms as the core component. The basic building blocks like multi-head attention, feed-forward networks, and layer normalization from Pyraformer's `EncoderLayer` and `PositionwiseFeedForward` classes can be reused as foundational components. The attention computation framework (query-key-value mechanism) is shared between both approaches.\n\n2. **Embedding Layer Strategy**: Both models employ similar embedding strategies that combine multiple types of information. Pyraformer's `DataEmbedding` module that fuses observation embeddings, covariate embeddings, and positional embeddings can serve as a template. The Nonstationary Transformer would similarly need to embed input sequences with temporal and positional information, making this component highly reusable with potential modifications for handling non-stationarity.\n\n3. **Encoder-Decoder Framework**: Both architectures follow an encoder-based paradigm for feature extraction from time series. Pyraformer's `Encoder` class structure with stacked layers can provide the skeleton for implementing the Nonstationary Transformer's encoder. The layer stacking mechanism, residual connections, and normalization strategies are transferable design patterns.\n\n4. **Task-Agnostic Feature Extraction**: Both models extract rich temporal representations that can be adapted for various downstream tasks (forecasting, classification). The feature extraction pipeline from Pyraformer's encoder, which processes sequential data through multiple transformation layers, provides a reusable pattern for the Nonstationary Transformer's feature learning component.",
    "differences": "1. **Core Innovation - Non-stationarity Handling**: The fundamental difference is that Nonstationary Transformer explicitly addresses non-stationary time series through de-stationary attention mechanisms, while Pyraformer focuses on computational efficiency via pyramidal attention. **NEW IMPLEMENTATION REQUIRED**: A de-stationary attention module that normalizes statistics (mean/variance) of queries and keys before attention computation, and a re-stationary module to restore original statistics after processing. This requires implementing statistical normalization layers and learnable projection parameters for series decomposition.\n\n2. **Attention Mechanism Architecture**: Pyraformer uses pyramidal attention (PAM) with multi-resolution C-ary tree structure to achieve O(L) complexity, implementing sparse attention through custom CUDA kernels. In contrast, Nonstationary Transformer uses standard full attention but modifies it with statistical normalization. **NEW IMPLEMENTATION REQUIRED**: Replace Pyraformer's `get_mask()`, pyramidal graph construction, and CSCM (Coarser-Scale Construction Module) with a de-stationary attention layer that computes statistics-normalized attention scores. The custom CUDA kernel for pyramidal attention is not needed; instead, implement differentiable statistical normalization operations.\n\n3. **Multi-scale Feature Representation**: Pyraformer explicitly constructs a multi-resolution pyramid using `Bottleneck_Construct` and `ConvLayer` modules with downsampling convolutions (stride=window_size) to create coarser scales. **NEW IMPLEMENTATION REQUIRED**: Nonstationary Transformer doesn't use explicit pyramidal structures. Instead, implement a series decomposition module (likely trend-seasonal decomposition using moving average) to handle non-stationarity at different temporal scales. Remove the bottleneck convolution layers and replace with decomposition blocks.\n\n4. **Temporal Dependency Modeling Strategy**: Pyraformer models long-range dependencies through inter-scale and intra-scale connections in the pyramid (defined in `refer_points()` function), limiting each node's attention to neighbors, children, and parent nodes. **NEW IMPLEMENTATION REQUIRED**: Nonstationary Transformer needs full attention with modified attention scores based on de-stationarized representations. Implement a projection mechanism that separates stationary and non-stationary components, requiring new learnable parameters for statistical projection and restoration.\n\n5. **Normalization and Regularization Approach**: Pyraformer uses standard LayerNorm and BatchNorm within its architecture. **NEW IMPLEMENTATION REQUIRED**: Nonstationary Transformer requires instance-level normalization for handling distribution shifts and a specialized normalization strategy within the de-stationary attention. Implement adaptive normalization layers that compute and apply statistics at the instance level rather than batch level, with learnable affine parameters for re-stationarization.\n\n6. **Classification Head Design**: Pyraformer's classification module concatenates features from all pyramid scales and flattens them (`output.reshape(output.shape[0], -1)`) before projection. **NEW IMPLEMENTATION REQUIRED**: Nonstationary Transformer would likely use a simpler classification head that operates on de-stationarized representations, potentially with a global pooling strategy over the sequence dimension followed by a linear classifier, without the multi-scale concatenation logic."
  },
  {
    "source": "Informer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers adopt the encoder-decoder Transformer architecture as their backbone. The Nonstationary Transformer can directly reuse Informer's basic building blocks including the multi-head attention mechanism structure (AttentionLayer class), layer normalization, feed-forward networks, and the overall encoder-decoder framework. The DataEmbedding module for temporal encoding (positional + temporal features) can be largely preserved.\n\n2. **Time Series Embedding Strategy**: Both utilize similar embedding approaches combining value embedding with temporal features (time stamps). The enc_embedding and dec_embedding components from Informer that integrate learnable embeddings with frequency-based positional encodings can serve as the foundation. The target paper would maintain this multi-faceted embedding strategy but may need modifications for handling non-stationarity.\n\n3. **Encoder-Decoder Paradigm for Sequence Processing**: Both employ the standard encoder-decoder structure where the encoder processes input sequences and the decoder generates predictions. The basic forward pass logic, masking mechanisms (TriangularCausalMask for decoder), and the interaction between encoder and decoder through cross-attention can be reused. The DecoderLayer structure with self-attention and cross-attention is a shared component.\n\n4. **Layer Stacking and Normalization**: Both papers use multiple stacked encoder/decoder layers with layer normalization. The modular design where EncoderLayer and DecoderLayer are repeated can be directly adopted, though the internal attention mechanisms may differ.",
    "differences": "1. **Core Innovation - Non-stationarity Handling**: The target paper's main contribution is addressing non-stationary time series through De-stationary Attention and Series Stationarization modules. This requires implementing: (a) A stationarization layer that removes trend and seasonality before attention computation, (b) A de-stationarization layer that restores statistics after processing, (c) Modified attention mechanisms that operate on stationary representations. These components are entirely absent in Informer and need to be built from scratch.\n\n2. **Attention Mechanism Design**: Informer uses ProbSparse attention with query sparsity measurement (M(q,K)) and selective top-u query sampling for O(L log L) complexity. The target paper likely replaces this with a different attention strategy that incorporates non-stationarity awareness. NEW IMPLEMENTATION NEEDED: De-stationary Attention that applies normalization statistics (mean/std) within the attention computation, potentially using learnable projections to model time-varying statistics rather than Informer's sparsity-based pruning.\n\n3. **Statistical Modeling of Time Series**: While Informer treats sequences uniformly, the Nonstationary Transformer requires explicit statistical modeling. NEW COMPONENTS: (a) Modules to estimate and track time-varying mean and variance across the sequence, (b) Normalization layers that apply instance-wise or window-wise standardization, (c) Parameter networks that learn to predict normalization statistics, (d) Mechanisms to propagate these statistics through the network and apply inverse transformations at the output.\n\n4. **Distilling Operation Removal**: Informer's encoder uses ConvLayer with max-pooling for self-attention distilling to reduce sequence length progressively (Equation 5). The Nonstationary Transformer likely removes this distilling mechanism as it may interfere with preserving non-stationary statistics across different time scales. Instead, it may use full-length attention or different downsampling strategies that preserve statistical properties.\n\n5. **Loss Function and Training Objective**: While Informer uses standard MSE loss on predictions, the Nonstationary Transformer may incorporate additional regularization terms related to stationarity constraints or distribution matching. NEW IMPLEMENTATION: Potentially a compound loss that includes reconstruction error plus terms that encourage proper stationarization/de-stationarization, or losses that measure distribution shift between normalized and original data.\n\n6. **Generative Inference Strategy**: Informer uses a specific start token strategy with known historical slices (L_token) concatenated with zero placeholders. The target paper may modify this to better handle distribution shifts in non-stationary data, potentially using adaptive token generation or different initialization strategies that account for changing statistics in the forecast horizon."
  },
  {
    "source": "Reformer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Informer_2020",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Pyraformer_2021",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Linear Projection Philosophy**: Both papers advocate for simple linear transformations as powerful baselines for time series tasks. DLinear uses temporal linear layers (nn.Linear) that can be directly reused in TiDE's residual blocks. The weight initialization strategy `(1/self.seq_len) * torch.ones([pred_len, seq_len])` from DLinear provides a good starting point for TiDE's dense layers.\n\n2. **Decomposition-based Processing**: DLinear employs series_decomp for trend-seasonal separation, which aligns with TiDE's philosophy of handling different temporal components. The `series_decomp(configs.moving_avg)` module from DLinear can be adapted for TiDE's preprocessing stage to separate low-frequency and high-frequency components before encoder-decoder processing.\n\n3. **Channel-wise Independence Option**: DLinear's `individual` parameter that creates separate linear layers per channel (ModuleList approach) mirrors TiDE's capability to process variates independently or jointly. The implementation pattern `nn.ModuleList()` with per-channel linear layers can be reused for TiDE's encoder when implementing channel-independent mode.\n\n4. **Direct Multi-step Forecasting (DMS)**: Both avoid autoregressive decoding and predict all future timesteps simultaneously. DLinear's direct mapping from `seq_len` to `pred_len` demonstrates the DMS strategy that TiDE also employs, eliminating error accumulation in long-horizon forecasting.\n\n5. **Reshape and Projection Pattern**: DLinear's classification head uses `enc_out.reshape(enc_out.shape[0], -1)` followed by `nn.Linear` projection, which is conceptually similar to TiDE's global pooling and dense layer approach for aggregating temporal information before final prediction.",
    "differences": "1. **Encoder-Decoder Architecture with Residual Connections**: TiDE introduces a sophisticated encoder-decoder structure with skip connections across time, whereas DLinear uses only direct linear mappings. NEW IMPLEMENTATION NEEDED: (a) Multi-layer dense encoder that projects lookback window to latent representation, (b) Dense decoder that expands latent space to forecast horizon, (c) Residual connection mechanism that adds encoded past directly to decoded future, bypassing the bottleneck.\n\n2. **Temporal Feature Encoding Layer**: TiDE incorporates explicit feature projection layers that transform covariates (time features, static attributes) into the same embedding space before concatenation. NEW IMPLEMENTATION NEEDED: (a) Separate dense layers for temporal covariates (past and future), (b) Static covariate encoder, (c) Feature concatenation logic along channel dimension, (d) Layer normalization after feature fusion.\n\n3. **Bottleneck Latent Representation**: Unlike DLinear's direct temporal mapping, TiDE compresses the time series into a fixed-size latent vector (independent of input/output lengths) before decoding. NEW IMPLEMENTATION NEEDED: (a) Dimension reduction layers in encoder (e.g., seq_len → hidden_dim → latent_dim), (b) Dimension expansion layers in decoder (latent_dim → hidden_dim → pred_len), (c) This creates an information bottleneck that forces learning of compressed representations.\n\n4. **Hierarchical Dense Layer Stack**: TiDE uses multiple stacked dense layers with ReLU activations and dropout for both encoder and decoder, while DLinear uses single-layer linear transformations. NEW IMPLEMENTATION NEEDED: (a) Configurable depth for encoder/decoder (e.g., 2-4 layers each), (b) Activation functions (ReLU/GELU) between layers, (c) Dropout layers for regularization, (d) Optional batch normalization or layer normalization between dense layers.\n\n5. **Covariate Integration Strategy**: TiDE explicitly handles three types of inputs (past observations, past covariates, future covariates) with separate processing pathways and late fusion, whereas DLinear only processes the main time series. NEW IMPLEMENTATION NEEDED: (a) Input parsing logic to separate x_enc (past values), x_mark_enc (past time features), x_mark_dec (future time features), (b) Covariate encoders with appropriate dimensions, (c) Concatenation strategy at encoder input and decoder output stages.\n\n6. **Residual Bypass Mechanism**: TiDE's key innovation is the temporal residual connection that adds encoded historical information directly to the decoded output, which is absent in DLinear. NEW IMPLEMENTATION NEEDED: (a) Linear projection layer to match encoder output dimensions to decoder output (if seq_len ≠ pred_len), (b) Temporal alignment logic (e.g., taking last pred_len steps from encoded sequence), (c) Element-wise addition of residual to decoder output before final projection.\n\n7. **Task-Agnostic Design**: While DLinear has task-specific branches (classification vs forecasting), TiDE is designed primarily for forecasting with a unified architecture. NEW IMPLEMENTATION NEEDED: Remove the classification-specific projection layer and reshape operations, focus on (batch, pred_len, channels) output format for forecasting tasks.\n\n8. **Global Temporal Pooling**: TiDE may employ global average/max pooling over the temporal dimension in the latent space for certain variants, while DLinear maintains temporal structure until the final classification layer. NEW IMPLEMENTATION NEEDED: Optional pooling operations in the bottleneck layer, configurable pooling strategies (mean, max, attention-based)."
  },
  {
    "source": "PatchTST_2022",
    "target": "TiDE_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "PatchTST_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Tokenization Foundation**: Both papers utilize patch-based segmentation as the fundamental tokenization strategy for time series. The PatchTST implementation's `PatchEmbedding` module (with configurable `patch_len`, `stride`, and `padding_patch_layer`) can be directly reused as the base patching mechanism. The unfold operation `x.unfold(dimension=-1, size=self.patch_len, step=self.stride)` provides the core patching logic that MultiPatchFormer likely extends to multiple scales.\n\n2. **Transformer Encoder Architecture**: Both employ vanilla Transformer encoders with multi-head self-attention as the backbone for learning temporal representations. The PatchTST's `Encoder` and `EncoderLayer` implementations with `FullAttention`, BatchNorm normalization layers, and feed-forward networks can serve as the foundational architecture. The attention mechanism `torch.einsum(\"blhe,bshe->bhls\", queries, keys)` and softmax operation are reusable components.\n\n3. **Channel-Independence Processing**: Both papers process multivariate time series in a channel-independent manner where each univariate series is processed separately through shared weights. PatchTST's reshape operation `torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))` that flattens batch and channel dimensions, then later reconstructs them, provides the implementation pattern for channel-wise processing.\n\n4. **Instance Normalization for Distribution Shift**: Both apply instance normalization (zero mean, unit variance) to handle non-stationarity. The PatchTST classification code shows: `means = x_enc.mean(1, keepdim=True).detach(); x_enc = x_enc - means; stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5); x_enc /= stdev`. This exact normalization logic can be reused.\n\n5. **Positional Encoding Strategy**: Both use learnable/sinusoidal positional embeddings to preserve temporal order. The `PositionalEmbedding` module with sine/cosine encoding `pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)` provides a ready-to-use implementation for encoding patch positions.",
    "differences": "1. **Multi-Scale Patch Hierarchy (CORE INNOVATION)**: MultiPatchFormer's primary contribution is processing time series at multiple patch scales simultaneously, whereas PatchTST uses a single fixed patch length. NEW IMPLEMENTATION REQUIRED: Create a multi-branch patching module that generates patches at different granularities (e.g., patch_len=[8, 16, 32]) in parallel. Each scale needs its own `PatchEmbedding` instance, requiring a `nn.ModuleList` to store multiple patch embeddings and potentially multiple encoders or a shared encoder with scale-specific tokens.\n\n2. **Cross-Scale Feature Fusion Mechanism**: While PatchTST has a simple flatten-and-project head, MultiPatchFormer needs a sophisticated fusion strategy to combine representations from different patch scales. NEW IMPLEMENTATION REQUIRED: Design a fusion module (possibly cross-attention between scales, concatenation with gating, or hierarchical aggregation) that merges multi-scale patch representations before classification. This could involve attention-based weighting: `fusion_weights = softmax(W_gate @ [features_scale1, features_scale2, ...])` or learnable scale importance parameters.\n\n3. **Scale-Adaptive Patch Number Handling**: Different patch lengths produce different numbers of patches (N varies across scales). PatchTST handles one N value, but MultiPatchFormer must align or handle variable-length sequences from different scales. NEW IMPLEMENTATION REQUIRED: Implement padding/truncation strategies or use attention masks to handle heterogeneous patch sequence lengths. May need scale-specific positional encodings with different maximum lengths.\n\n4. **Multi-Scale Attention Patterns**: PatchTST uses standard self-attention within a single patch scale. MultiPatchFormer may employ cross-scale attention where coarse patches attend to fine patches and vice versa. NEW IMPLEMENTATION REQUIRED: Extend the attention mechanism to support cross-scale queries and keys, potentially requiring a hierarchical attention module: `CrossScaleAttention(queries_scale_i, keys_scale_j, values_scale_j)` with appropriate dimension matching through projection layers.\n\n5. **Enhanced Classification Head for Multi-Scale**: PatchTST's head performs `flatten → dropout → linear(head_nf * enc_in, num_class)`. MultiPatchFormer needs a head that processes concatenated or fused multi-scale features. NEW IMPLEMENTATION REQUIRED: Design a classification head that takes multiple feature tensors with potentially different dimensions (since head_nf varies with patch_len), possibly using: `output = projection([flatten(scale1_features), flatten(scale2_features), ...])` with dimension calculations: `total_dim = sum([d_model * num_patches_i for each scale_i])`.\n\n6. **Computational Efficiency Trade-offs**: While PatchTST optimizes for memory by reducing tokens via patching, MultiPatchFormer multiplies computational cost by processing multiple scales. NEW IMPLEMENTATION REQUIRED: Implement efficient multi-scale processing, possibly with: (a) shared encoder weights across scales with scale embeddings, (b) progressive refinement where coarse scales guide fine scales, or (c) dynamic scale selection based on input characteristics. May need custom training strategies to balance scale-specific losses."
  },
  {
    "source": "DLinear_2022",
    "target": "MultiPatchFormer_2025",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Crossformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Embedding Strategy**: Both papers utilize segmentation/patching of time series data. Crossformer's DSW (Dimension-Segment-Wise) embedding divides each dimension into segments of length L_seg, which is conceptually similar to patch-based approaches. The `PatchEmbedding` module in Crossformer's implementation (using `unfold` operation with stride and padding) can be directly reused or adapted for MultiPatchFormer's multi-patch extraction mechanism. The core patching logic including `self.padding_patch_layer = nn.ReplicationPad1d((0, padding))` and `x.unfold(dimension=-1, size=self.patch_len, step=self.stride)` provides a solid foundation.\n\n2. **Hierarchical Feature Processing**: Both architectures employ multi-scale/hierarchical processing. Crossformer's hierarchical encoder with segment merging (`SegMerging` module) progressively reduces temporal resolution while increasing receptive field. This hierarchical structure (encoder layers with different scales via `scale_block`) can inform MultiPatchFormer's multi-scale patch processing. The `scale_block` implementation with configurable `win_size` and `depth` parameters demonstrates how to build scalable hierarchical architectures.\n\n3. **Transformer-based Architecture**: Both leverage Transformer attention mechanisms for capturing dependencies. Crossformer's `AttentionLayer` with `FullAttention` module, including the query/key/value projection mechanism and multi-head attention computation, provides reusable components. The normalization patterns (`LayerNorm`), residual connections, and MLP blocks (2-layer feedforward networks with GELU activation) are standard Transformer components that can be directly adopted.\n\n4. **Position Encoding**: Crossformer implements learnable position embeddings (`self.enc_pos_embedding = nn.Parameter(torch.randn(...))`) for both encoder and decoder, which can be adapted for MultiPatchFormer's positional encoding needs across multiple patch scales. The `PositionalEmbedding` class with sinusoidal encoding also provides an alternative implementation option.\n\n5. **Classification Head Design**: Both papers target time series classification. Crossformer's classification head uses flattening followed by dropout and linear projection (`self.flatten`, `self.dropout`, `self.projection`), which provides a baseline implementation for the final classification layer that can be extended for multi-patch fusion.",
    "differences": "1. **Multi-Patch Extraction vs. Single Segmentation**: CORE INNOVATION - MultiPatchFormer likely extracts multiple patches at different scales/positions simultaneously, whereas Crossformer uses uniform segmentation with fixed L_seg. NEW IMPLEMENTATION NEEDED: A multi-scale patch extraction module that generates patches of varying lengths (e.g., small, medium, large patches) from the same input sequence, possibly with overlapping windows. This requires implementing parallel patching streams with different patch_len and stride parameters, unlike Crossformer's single `PatchEmbedding` module.\n\n2. **Cross-Dimension vs. Cross-Patch Attention**: Crossformer's TSA (Two-Stage Attention) focuses on cross-time and cross-dimension dependencies using a router mechanism for dimension interaction. MultiPatchFormer likely needs CROSS-PATCH attention mechanisms to fuse information from different patch scales. NEW IMPLEMENTATION NEEDED: A patch fusion attention module that attends across patches of different granularities, potentially replacing or augmenting Crossformer's `dim_sender` and `dim_receiver` router mechanism with patch-level routers that aggregate multi-scale patch representations.\n\n3. **Patch Fusion Strategy**: Crossformer aggregates multi-scale features through hierarchical encoding and decoder layer summation (Eq. 8: summing predictions from all decoder layers). MultiPatchFormer requires a more sophisticated MULTI-PATCH FUSION mechanism. NEW IMPLEMENTATION NEEDED: An explicit fusion module (e.g., attention-based fusion, learnable weighted combination, or cross-patch Transformer layers) that combines features from patches of different scales before classification, rather than simple summation. This might involve creating separate embedding streams for each patch scale and a dedicated fusion Transformer block.\n\n4. **Temporal Resolution Handling**: Crossformer uses segment merging to progressively reduce temporal resolution in the encoder (`SegMerging` with `win_size`). MultiPatchFormer likely maintains multiple temporal resolutions in parallel throughout the network. NEW IMPLEMENTATION NEEDED: Parallel processing branches that independently encode patches at different scales without merging, requiring separate encoder stacks or shared encoders with scale-specific parameters. This contrasts with Crossformer's sequential hierarchical reduction.\n\n5. **Classification Token vs. Global Pooling**: Crossformer flattens all encoder outputs and projects to class logits. MultiPatchFormer may employ CLS tokens for each patch scale or patch-specific pooling strategies. NEW IMPLEMENTATION NEEDED: Implement learnable CLS tokens for each patch scale (similar to ViT), or design a multi-scale pooling mechanism that aggregates patch representations before fusion. This requires modifying the input embedding to include CLS tokens and the attention mechanism to process them, which is absent in Crossformer's current implementation.\n\n6. **Patch-Level Position Encoding**: While Crossformer uses segment-level position encoding, MultiPatchFormer needs HIERARCHICAL position encoding that captures both within-patch positions and cross-patch relationships. NEW IMPLEMENTATION NEEDED: A multi-level positional encoding scheme that encodes (1) position within each patch, (2) patch position in the sequence, and (3) patch scale information. This requires extending Crossformer's single-level `enc_pos_embedding` to a multi-scale positional encoding system, possibly with separate encodings for each patch granularity that are combined additively or concatenatively."
  },
  {
    "source": "FEDformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "PatchTST_2022",
    "target": "SegRNN_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "TimesNet_2022",
    "target": "SegRNN_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition Philosophy**: Both papers recognize the importance of decomposing time series into interpretable components. DLinear uses series_decomp to extract trend and seasonal components via moving average kernels. SegRNN likely adopts a segmentation-based decomposition approach where the time series is divided into meaningful segments. The series_decomp module from DLinear's implementation can serve as a reference for implementing preprocessing decomposition in SegRNN, particularly the moving average kernel logic for trend extraction.\n\n2. **Direct Mapping Architecture**: Both models favor simple, direct mapping approaches over complex attention mechanisms. DLinear uses linear layers (nn.Linear) to directly map from input length to prediction length along the temporal dimension. SegRNN similarly employs RNN-based direct mapping within segments. The weight initialization strategy in DLinear ((1/self.seq_len) * torch.ones) demonstrates a principled approach to temporal weight initialization that can be adapted for SegRNN's segment-level processing.\n\n3. **Channel-wise Processing**: DLinear's 'individual' parameter enables independent processing of each variate/channel through separate linear layers (nn.ModuleList). This design pattern is highly relevant for SegRNN, which processes segments independently. The implementation pattern of iterating through channels (for i in range(self.channels)) and maintaining separate parameters can be directly reused for SegRNN's segment-wise RNN processing.\n\n4. **Permutation Operations for Dimension Management**: DLinear extensively uses .permute(0, 2, 1) operations to switch between (batch, channels, time) and (batch, time, channels) formats. This dimension management strategy is crucial for SegRNN when handling segmented inputs, as segments need to be reshaped and processed through RNN layers. The pattern of permuting before and after processing can be directly adopted.\n\n5. **Classification Head Design**: Both papers need to aggregate temporal features for final classification. DLinear's classification method flattens the encoder output (enc_out.reshape(enc_out.shape[0], -1)) and applies a projection layer (nn.Linear). This flattening and projection pattern provides a template for SegRNN's aggregation of segment-level RNN outputs into a final classification decision.",
    "differences": "1. **Core Architecture - Linear vs. RNN**: DLinear uses purely linear transformations (nn.Linear layers) operating on the entire time series, while SegRNN requires implementing RNN cells (LSTM/GRU) that operate on individual segments. NEW IMPLEMENTATION NEEDED: (a) Segment division logic to split input time series into fixed-length or adaptive segments, (b) RNN module instantiation (nn.LSTM or nn.GRU) for each segment, (c) Hidden state initialization and management across segments, (d) Sequential processing logic for segment-by-segment RNN computation.\n\n2. **Segmentation Strategy**: DLinear processes the entire input sequence as a whole (self.seq_len), whereas SegRNN divides the time series into non-overlapping or overlapping segments. NEW IMPLEMENTATION NEEDED: (a) Segmentation function to partition input [batch, seq_len, channels] into [batch, num_segments, segment_len, channels], (b) Segment length determination logic (fixed vs. adaptive), (c) Overlap handling if using sliding windows, (d) Padding strategy for sequences that don't divide evenly into segments.\n\n3. **Temporal Dependency Modeling**: DLinear captures temporal patterns through weighted combinations (matrix multiplication in linear layers), which is inherently limited to linear relationships. SegRNN needs to capture non-linear temporal dependencies within and across segments through recurrent connections. NEW IMPLEMENTATION NEEDED: (a) Intra-segment RNN processing to model local temporal patterns, (b) Inter-segment information propagation mechanism (e.g., passing final hidden states between segments or using a higher-level RNN), (c) Bidirectional processing if needed for better context capture, (d) Attention mechanism over segments for weighted aggregation.\n\n4. **Feature Aggregation Mechanism**: DLinear simply adds trend and seasonal outputs (seasonal_output + trend_output) and flattens for classification. SegRNN requires sophisticated segment-level feature aggregation. NEW IMPLEMENTATION NEEDED: (a) Segment-level feature extraction (final hidden states or all hidden states from each segment's RNN), (b) Cross-segment aggregation strategy (max pooling, average pooling, attention-based pooling, or another RNN layer over segment features), (c) Multi-scale feature fusion if using hierarchical segment structures, (d) Learnable aggregation weights or gating mechanisms.\n\n5. **Decomposition Approach**: DLinear uses Autoformer's series_decomp with moving average for trend-seasonal decomposition applied to the entire series. SegRNN's decomposition is segment-based, where each segment may represent a local pattern or regime. NEW IMPLEMENTATION NEEDED: (a) Segment-level statistics computation (mean, variance per segment for normalization), (b) Local trend extraction within each segment, (c) Segment boundary handling to avoid discontinuities, (d) Optional learnable segment embeddings to encode segment position or characteristics, (e) Segment-specific normalization or standardization strategies.\n\n6. **Parameter Sharing Strategy**: DLinear offers shared vs. individual parameters across channels but shares parameters across time steps within each linear layer. SegRNN needs to decide parameter sharing at the segment level. NEW IMPLEMENTATION NEEDED: (a) Shared RNN weights across all segments vs. segment-specific RNN parameters, (b) Position-aware segment encoding if using shared RNNs, (c) Segment-type embeddings if different segments have different characteristics, (d) Efficient memory management for storing segment-level parameters or hidden states."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-step (DMS) Forecasting Strategy**: Both papers adopt the DMS forecasting paradigm where the entire future sequence is predicted in one forward pass, avoiding the error accumulation problem of autoregressive IMS (Iterative Multi-step) methods. FEDformer's decoder architecture with its generative-style prediction can inform the implementation of DLinear's direct prediction mechanism, though DLinear simplifies this significantly.\n\n2. **Time Series Decomposition**: Both methods leverage seasonal-trend decomposition as a core preprocessing/architectural component. FEDformer uses the MOEDecomp (Mixture of Experts Decomposition) block with multiple average pooling filters of different sizes, while DLinear uses a simpler moving average kernel for decomposition. The decomposition implementation in FEDformer (specifically the `series_decomp` class from Autoformer) can be directly reused or simplified for DLinear's decomposition component. The mathematical formulation is similar: extract trend via moving average, obtain seasonal component as the residual.\n\n3. **Sequence-to-Sequence Framework**: Both adopt an encoder-decoder or input-output mapping framework for LTSF. FEDformer uses explicit encoder-decoder with attention mechanisms, while DLinear simplifies this to direct linear mapping. The overall pipeline structure (input embedding → processing → output projection) in FEDformer's code can guide the basic structure of DLinear, though DLinear eliminates the complex attention layers.\n\n4. **Multi-variate Handling**: Both methods process multiple time series variates. FEDformer's channel-wise processing in the frequency domain (dimension D in their notation) parallels DLinear's approach of applying separate linear layers per variate. The data handling and batching logic from FEDformer can be reused for DLinear's implementation.\n\n5. **Normalization Considerations**: Both papers implicitly handle data distribution issues. FEDformer uses standard normalization in embeddings, while NLinear (DLinear variant) explicitly uses last-value subtraction normalization. The embedding layer's normalization logic in FEDformer can inform DLinear's preprocessing pipeline.",
    "differences": "1. **Core Architecture Complexity**: FEDformer uses sophisticated Frequency Enhanced Blocks (FEB) with Fourier/Wavelet transforms, multi-head attention mechanisms, and complex encoder-decoder stacks with O(L) complexity through frequency domain operations. DLinear completely eliminates these components, using only simple one-layer temporal linear transformations (W ∈ R^(T×L)) without any attention mechanism. NEW IMPLEMENTATION NEEDED: Simple linear layers without frequency transforms, attention, or complex neural blocks.\n\n2. **Temporal Modeling Approach**: FEDformer operates in frequency domain using DFT/DWT with mode selection (randomly selecting M=64 modes), implementing complex multiplication operations and inverse transforms. It explicitly models frequency components through learnable kernels in frequency space. DLinear operates purely in time domain with direct weighted sum operations across temporal dimension, sharing weights across variates without modeling frequency or spatial correlations. NEW IMPLEMENTATION NEEDED: Pure time-domain linear regression without any frequency domain transformations or complex-valued operations.\n\n3. **Model Depth and Parameter Complexity**: FEDformer has deep multi-layer architecture (N encoder layers, M decoder layers) with FEB/FEA blocks, feedforward networks, layer normalization, and residual connections at each layer. It includes sophisticated components like MultiWaveletTransform with recursive decomposition (L levels), sparseKernelFT1d, and mixture of experts. DLinear is extremely shallow with only 1-2 linear layers total (one for trend, one for seasonal in DLinear; single layer in vanilla Linear). NEW IMPLEMENTATION NEEDED: Minimal architecture with no stacking, no residual connections, no layer normalization beyond input preprocessing.\n\n4. **Decomposition Strategy Sophistication**: FEDformer uses MOEDecomp with mixture of experts - multiple average filters with different kernel sizes combined via learnable data-dependent weights: X_trend = Softmax(L(x)) * F(x), where F(·) is a set of pooling filters. DLinear uses single fixed moving average kernel for trend extraction with no learnable mixing. NEW IMPLEMENTATION NEEDED: Simple single-kernel moving average decomposition without mixture of experts or learnable weights.\n\n5. **Input Processing and Embeddings**: FEDformer uses rich DataEmbedding with multiple components: channel projection embedding, fixed positional encoding, and learnable temporal embeddings (week, month, year hierarchies). It processes inputs through embedding layers with dimension d_model before main processing. DLinear variants (NLinear) use minimal preprocessing: only last-value subtraction for normalization, no positional encodings, no temporal embeddings, operating directly on raw normalized sequences. NEW IMPLEMENTATION NEEDED: Minimal embedding - either direct input (vanilla Linear/DLinear) or simple last-value normalization (NLinear) without learnable embeddings.\n\n6. **Prediction Head Design**: FEDformer's decoder outputs through complex multi-stage process: seasonal component X_de^M projected via W_S plus accumulated trend T_de^M across all decoder layers with weighted aggregation. Final prediction combines decomposed components after deep transformation. DLinear directly sums outputs from two independent linear layers (trend and seasonal) without intermediate transformations: Ŷ = Linear_trend(X_trend) + Linear_seasonal(X_seasonal). NEW IMPLEMENTATION NEEDED: Simple additive combination of two linear layer outputs without complex aggregation or deep decoder structure.\n\n7. **Computational Paradigm**: FEDformer achieves O(L) complexity through clever frequency domain operations with pre-selected Fourier/Wavelet basis, requiring FFT/IFFT operations and complex number arithmetic. DLinear has O(L) complexity through straightforward matrix multiplication in time domain with real-valued operations only. NEW IMPLEMENTATION NEEDED: Standard PyTorch linear layers (nn.Linear) without any FFT operations or complex number handling - dramatically simpler computational graph."
  },
  {
    "source": "Autoformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Architecture**: Both papers utilize the same seasonal-trend decomposition approach. Autoformer's `series_decomp` module with moving average kernel (Equation 1) can be directly reused in DLinear. The implementation code shows `moving_avg` and `series_decomp` classes that extract trend-cyclical components via AvgPool with padding, which is exactly what DLinear's decomposition variant requires. The source code's decomposition block can be imported as-is: `self.decomp = series_decomp(kernel_size)`.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both methods adopt DMS forecasting rather than autoregressive approaches. Autoformer's decoder generates the entire future sequence at once (length O) rather than iteratively, avoiding error accumulation. This architectural philosophy aligns with DLinear's core design principle. The source implementation's decoder structure demonstrates how to handle input initialization (Equation 2) with concatenation of historical components and placeholders, which provides guidance for DLinear's input handling.\n\n3. **Normalization and Preprocessing Pipeline**: Both papers emphasize proper data preprocessing. Autoformer's `DataEmbedding_wo_pos` class includes dropout and handles temporal features, providing a template for input normalization. DLinear's NLinear variant uses subtraction normalization (last value subtraction), which can be implemented as a lightweight preprocessing step before the linear layer, similar to how Autoformer applies decomposition before attention mechanisms.\n\n4. **Handling Multiple Variates Independently**: Autoformer processes each channel/variate through shared operations (as seen in the encoder-decoder architecture), and DLinear explicitly shares weights across variates without modeling spatial correlations. The source code's channel-wise processing in `TokenEmbedding` and projection layers demonstrates this pattern, which can guide DLinear's implementation of per-variate linear layers.",
    "differences": "1. **Core Architecture - Attention vs. Linear**: Autoformer's fundamental innovation is the Auto-Correlation mechanism (Equations 5-7) replacing self-attention with period-based dependencies via FFT and time delay aggregation. This involves complex components: `AutoCorrelation`, `AutoCorrelationLayer`, multi-head mechanism, and O(LlogL) FFT computations. **DLinear completely eliminates this** - it requires implementing simple temporal linear layers `W ∈ R^(T×L)` that directly regress historical series via weighted sum (`Ŷ_i = WX_i`). NEW implementation needed: Single-layer `nn.Linear(seq_len, pred_len)` for each component, no attention mechanism, no FFT operations.\n\n2. **Encoder-Decoder vs. Direct Mapping**: Autoformer uses a sophisticated encoder-decoder architecture with N encoder layers and M decoder layers (Equations 3-4), progressive trend accumulation (`T_de^l = T_de^(l-1) + W_l,1*T_de^l,1 + ...`), and cross-attention between encoder-decoder. **DLinear removes all of this** - it needs only direct linear mapping from input to output. NEW implementation: For DLinear variant, apply decomposition, then two parallel linear layers (one for trend, one for seasonal), sum outputs. For NLinear, subtract last value, apply linear layer, add back. No multi-layer stacking, no cross-attention, no progressive refinement.\n\n3. **Embedding Strategy**: Autoformer employs complex embeddings: `TokenEmbedding` with Conv1d kernels, `PositionalEmbedding` with sinusoidal encoding, `TemporalEmbedding` with learnable timestamps (minute/hour/day/month), and projects to d_model dimension. **DLinear eliminates all embeddings** - it works directly on raw numerical sequences. NEW implementation: Remove all embedding layers; input shape is simply [batch, seq_len, variates], output is [batch, pred_len, variates]. Only optional preprocessing is decomposition or last-value normalization.\n\n4. **Model Complexity and Components**: Autoformer includes: (a) LayerNorm with special `my_Layernorm` for seasonal parts, (b) FeedForward layers with Conv1d, (c) Multi-head mechanism splitting d_model across h heads, (d) Dropout at multiple stages, (e) Activation functions (ReLU/GELU). **DLinear strips to bare minimum** - no normalization layers, no feedforward networks, no multi-head splitting, minimal dropout. NEW implementation: Single weight matrix per component, optional batch normalization for stability, but fundamentally just `nn.Linear` without intermediate transformations.\n\n5. **Computational Paradigm**: Autoformer discovers temporal dependencies through learned attention weights, FFT-based autocorrelation for period detection, and aggregates sub-series via time delay operations. **DLinear's paradigm is pure linear regression** - it assumes temporal patterns can be captured by direct weighted combinations of historical points. NEW implementation: No FFT, no autocorrelation calculation, no top-k delay selection, no rolling operations. Just matrix multiplication: `output = W @ input`.\n\n6. **Handling Trends**: Autoformer accumulates trends progressively through decoder layers with learned projection matrices `W_l,i` for each decomposed trend component. **DLinear handles trends via either**: (a) separate linear layer for decomposed trend in DLinear variant, or (b) last-value normalization in NLinear variant. NEW implementation: For DLinear, two independent linear layers; for NLinear, explicit subtraction/addition operations around the linear layer, no learned trend accumulation."
  },
  {
    "source": "Informer_2020",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-step (DMS) Forecasting Strategy**: Both papers employ a non-autoregressive DMS forecasting approach where the entire prediction horizon is generated in one forward pass. Informer's decoder uses a generative-style inference with start tokens (Eq. 6: X_de = Concat(X_token, X_0)) to predict all future steps simultaneously, avoiding error accumulation. DLinear inherits this DMS paradigm as its core strategy. The target paper can reuse Informer's decoder structure and forward pass logic, particularly the mechanism of concatenating historical context with placeholder targets.\n\n2. **Temporal Decomposition Paradigm**: Both methods incorporate seasonal-trend decomposition as a preprocessing/processing step. Informer's encoder applies self-attention distilling with Conv1d layers (Eq. 5) that implicitly captures temporal patterns, while DLinear explicitly uses Autoformer's moving average kernel decomposition to separate trend and seasonal components. The decomposition logic from Informer's distilling operation (ConvLayer with kernel_size=3, MaxPool with stride=2) provides a foundation for implementing DLinear's explicit decomposition module. The target paper can adapt Informer's convolutional components for trend extraction.\n\n3. **Input Embedding and Normalization Infrastructure**: Both papers require sophisticated input preprocessing. Informer uses DataEmbedding with positional encoding, temporal embeddings, and value embeddings (enc_embedding and dec_embedding modules). DLinear's NLinear variant performs sequence normalization (subtract last value, add back after prediction) which is conceptually similar to Informer's embedding normalization strategies. The embedding infrastructure from Informer (DataEmbedding class with freq-based temporal encoding) can be simplified and reused for DLinear's input preprocessing, particularly for handling multi-variate time series with temporal context.\n\n4. **Multi-variate Handling with Channel Independence**: Both architectures process multiple variates with weight sharing but maintain channel independence. Informer's encoder processes all channels through shared attention layers (configs.enc_in, configs.dec_in dimensions), while DLinear explicitly shares weights across variates (W ∈ R^(T×L) applied to each X_i independently). The channel projection and batching logic from Informer's implementation can be directly adapted for DLinear's linear layer application across variates.\n\n5. **Loss Function and Training Infrastructure**: Both use MSE loss for time series forecasting optimization. Informer's training loop, data loading pipeline, and evaluation metrics (MSE, MAE as shown in Table 1) provide a complete framework that can be reused for DLinear. The target paper only needs to replace the model architecture while keeping the training infrastructure, data preprocessing pipelines, and evaluation protocols intact.",
    "differences": "1. **Core Architecture Replacement - From Complex Transformer to Simple Linear Layers**: Informer uses a sophisticated encoder-decoder architecture with ProbSparse self-attention (O(L log L) complexity), multi-head attention mechanisms, and attention distilling layers. The implementation includes ProbAttention with query sparsity measurement (Eq. 2-4), _prob_QK sampling, and _update_context operations. DLinear completely eliminates this complexity, replacing it with simple one-layer linear transformations (W ∈ R^(T×L)). NEW IMPLEMENTATION REQUIRED: (a) Remove all attention mechanisms (ProbAttention, AttentionLayer, multi-head logic); (b) Implement basic nn.Linear(L, T) layers for temporal projection; (c) For DLinear variant: add moving average decomposition (torch.nn.AvgPool1d or manual cumsum-based implementation) before linear layers; (d) For NLinear variant: implement normalization via last-value subtraction/addition wrapper.\n\n2. **Elimination of Positional and Temporal Encodings**: Informer heavily relies on multiple embedding strategies including fixed positional encoding, learnable temporal embeddings based on timestamps (week, month, year hierarchies), and channel projection embeddings (DataEmbedding class with configs.embed, configs.freq parameters). These embeddings inject temporal context that the self-attention mechanism cannot inherently capture. DLinear's premise is that temporal relations are naturally encoded in the sequential ordering of the input, making explicit positional encodings unnecessary. NEW IMPLEMENTATION REQUIRED: Strip out DataEmbedding entirely; implement simple input reshaping from [batch, seq_len, features] to direct linear layer input without any embedding transformations. The linear weights themselves implicitly learn positional patterns through their connection to specific time steps.\n\n3. **Decoder Architecture Simplification**: Informer uses a complex decoder with masked self-attention (DecoderLayer with TriangularCausalMask), cross-attention between encoder and decoder, feed-forward networks with Conv1d, and layer normalization at multiple stages. The decoder processes start tokens concatenated with target placeholders through multiple layers. DLinear has NO decoder - it directly outputs predictions through a single linear projection. NEW IMPLEMENTATION REQUIRED: Replace the entire Decoder module with either (a) one nn.Linear layer for vanilla Linear/NLinear, or (b) two parallel nn.Linear layers (one for trend, one for seasonal) that sum their outputs for DLinear variant. The projection layer should map from input length L to prediction length T.\n\n4. **Explicit Decomposition vs. Implicit Feature Learning**: Informer's self-attention distilling (Eq. 5) implicitly learns multi-scale temporal patterns through Conv1d + ELU + MaxPool operations within the encoder, with the distilling progressively halving the sequence length. This is implicit feature extraction through learned convolutional filters. DLinear's decomposition is explicit and non-learnable: it uses a fixed moving average kernel (typically AvgPool1d with kernel_size=25 as in Autoformer) to extract trend, with seasonal = input - trend. NEW IMPLEMENTATION REQUIRED: Implement explicit decomposition module: (a) Create a moving average function (torch.nn.AvgPool1d with appropriate padding to maintain length); (b) Compute seasonal component as residual; (c) Apply separate linear layers to trend and seasonal; (d) Sum outputs. This is fundamentally different from Informer's learned convolutions.\n\n5. **Computational Complexity and Memory Footprint**: Informer's ProbSparse attention achieves O(L log L) complexity through query sampling (U_part = factor * ceil(log(L_K)) samples, Lemma 1 and Eq. 4 for max-mean measurement), attention distilling with progressive downsampling, and sparse query-key pair selection. The implementation includes complex indexing operations (M_top selection, K_sample gathering). DLinear achieves O(L) complexity with a single matrix multiplication (no attention, no sampling). NEW IMPLEMENTATION REQUIRED: Remove all complexity-reduction mechanisms (sampling, top-k selection, prob_QK calculations); implement straightforward nn.Linear forward pass. Memory usage drops from O(L log L) for attention matrices to O(L*T) for weight matrices only. This requires completely different memory management - no attention score storage, no intermediate attention maps.\n\n6. **Validation of Transformer Necessity**: The fundamental difference is philosophical - Informer assumes complex attention mechanisms are necessary for capturing long-range dependencies in time series, investing significant engineering in efficient attention variants. DLinear challenges this assumption, demonstrating that simple linear projections can outperform complex Transformers. NEW IMPLEMENTATION REQUIRED: Create a minimal baseline that intentionally avoids all Transformer components (no self-attention, no positional encoding, no multi-head mechanisms, no layer normalization beyond basic batch norm if needed). The implementation should be deliberately simple to prove the point that complexity is unnecessary for LTSF tasks."
  },
  {
    "source": "Pyraformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers address the long-term time series forecasting (LTSF) problem using DMS approach rather than autoregressive methods. Pyraformer uses either a fully connected layer or a decoder with attention layers to directly predict all M future time steps in a batch, avoiding error accumulation. DLinear similarly predicts all future time steps simultaneously through linear transformations. The source implementation's `self.projection` layer in the classification module demonstrates this batch prediction approach, which can be adapted for DLinear's direct forecasting mechanism.\n\n2. **Time Series Decomposition Awareness**: Both papers recognize the importance of decomposing time series into trend and seasonal components, though they apply it differently. Pyraformer's architecture implicitly handles multi-resolution patterns through its pyramidal structure (coarse scales capturing trends, fine scales capturing seasonal patterns). DLinear explicitly uses seasonal-trend decomposition from Autoformer with moving average kernels. The source code's multi-scale representation through `Bottleneck_Construct` and `ConvLayer` modules provides insights into handling decomposed components separately, which aligns with DLinear's dual-branch architecture for trend and seasonal components.\n\n3. **Embedding and Normalization Strategies**: Both papers employ data preprocessing techniques before the main model. Pyraformer uses `DataEmbedding` (combining observation, covariate, and positional embeddings) with normalization layers (`nn.LayerNorm`, `nn.BatchNorm1d`). DLinear's NLinear variant uses normalization by subtracting the last value of the sequence. The source implementation's embedding pipeline and normalization components (particularly in `Bottleneck_Construct` and `PositionwiseFeedForward`) can be referenced for implementing DLinear's preprocessing steps.\n\n4. **Handling Multiple Variates Independently**: Both architectures process different variates (channels) of multivariate time series. Pyraformer's implementation shows channel-wise operations in convolution layers and attention mechanisms. DLinear explicitly shares weights across different variates without modeling spatial correlations. The source code's handling of the channel dimension (`configs.enc_in`, `d_model` transformations) provides guidance on implementing DLinear's variate-independent linear transformations.\n\n5. **Task-Agnostic Framework Design**: Both papers present flexible frameworks that can be adapted to different time series tasks. Pyraformer's code includes a `task_name` parameter and modular design (encoder, decoder, projection layers). DLinear is presented as a baseline applicable across domains. The source implementation's modular structure (separate encoder, embedding, and projection modules) demonstrates how to build adaptable architectures, useful for implementing DLinear variants (Vanilla Linear, DLinear, NLinear).",
    "differences": "1. **Core Architecture Philosophy - Complexity vs. Simplicity**: Pyraformer introduces a sophisticated pyramidal attention mechanism with O(L) complexity to capture long-range dependencies through multi-scale hierarchical representations (C-ary tree structure with inter-scale and intra-scale connections). In stark contrast, DLinear deliberately abandons all attention mechanisms and complex architectures in favor of embarrassingly simple one-layer or two-layer linear models. **NEW IMPLEMENTATION REQUIRED**: For DLinear, you need to implement simple `nn.Linear` layers that operate directly on the temporal dimension without any attention mechanism, pyramidal structure, or hierarchical representations. Remove all attention-related code (PAM, query-key-value transformations, attention masks) and replace with straightforward matrix multiplication: `W @ X` where W ∈ R^(T×L).\n\n2. **Feature Extraction Mechanism - Attention vs. Direct Linear Mapping**: Pyraformer relies on the attention mechanism (query-key-value paradigm) to extract temporal dependencies, using customized CUDA kernels for sparse pyramidal attention patterns. The source code shows complex attention computation with `FullAttention`, `AttentionLayer`, and mask-based selective attention. DLinear completely eliminates this, using only weighted sums through linear layers. **NEW IMPLEMENTATION REQUIRED**: Implement pure linear transformations without any attention computation. For DLinear, create two separate linear branches: one for trend component and one for seasonal component, each being a simple `nn.Linear(L, T)` layer. No query-key dot products, no attention scores, no masking operations are needed.\n\n3. **Multi-Scale Temporal Modeling - Hierarchical vs. Single-Scale Decomposition**: Pyraformer constructs a multi-resolution C-ary tree through the Coarser-Scale Construction Module (CSCM) using stacked convolution layers with stride C, creating nodes at multiple scales (daily, weekly, monthly features). The implementation shows `Bottleneck_Construct` with multiple `ConvLayer` modules building this hierarchy. DLinear uses only a simple two-component decomposition (trend via moving average kernel + seasonal as remainder) without any hierarchical structure. **NEW IMPLEMENTATION REQUIRED**: Implement a simple moving average pooling operation (AvgPool1d with appropriate kernel size) to extract the trend component, then compute seasonal component as the difference. No multi-scale convolutions, no bottleneck structures, no concatenation of features from different scales are needed.\n\n4. **Positional and Temporal Encoding - Rich Embeddings vs. None**: Pyraformer employs sophisticated embedding strategies through `DataEmbedding`, combining observation embeddings, covariate embeddings (hour-of-day, day-of-week), and positional encodings to inject temporal context. The source code shows complex embedding pipelines. DLinear deliberately avoids any positional encoding or temporal embeddings, arguing that the permutation-invariant nature of attention mechanisms causes temporal information loss. **NEW IMPLEMENTATION REQUIRED**: For DLinear, implement NO embedding layers at all. The input raw time series values directly feed into linear layers without any positional encoding, temporal embedding, or covariate injection. Only NLinear variant needs a simple subtraction-addition normalization operation.\n\n5. **Model Complexity and Parameter Count - Heavy vs. Ultra-Lightweight**: Pyraformer has substantial parameters including multi-head attention weights (W_Q, W_K, W_V for each head), position-wise feed-forward networks (two linear layers with dimension expansion), convolution layers in CSCM, layer normalization parameters, and multiple encoder layers (N layers stacked). The source code shows parameter count O(N(HD_K D_K + DD_F) + (S-1)CD_K^2). DLinear has minimal parameters: just one or two linear transformation matrices. **NEW IMPLEMENTATION REQUIRED**: Implement an extremely lightweight architecture. For vanilla Linear: single `nn.Linear(L, T)` layer. For DLinear: two `nn.Linear(L, T)` layers (one for trend, one for seasonal). For NLinear: single `nn.Linear(L, T)` with normalization operations. Total parameter count should be O(LT) or O(2LT) at most, orders of magnitude smaller than Pyraformer.\n\n6. **Computational Graph and Operations - Complex Pipeline vs. Direct Transformation**: Pyraformer's forward pass involves multiple stages: embedding → CSCM convolutions → pyramidal attention computation with custom masks → feature gathering from all scales → decoder with attention layers → final projection. The source code shows this complex pipeline with multiple module calls. DLinear's forward pass is trivial: optional decomposition → linear transformation(s) → optional denormalization. **NEW IMPLEMENTATION REQUIRED**: Implement a minimal forward pass. For DLinear: (1) Apply moving average to get trend, (2) Compute seasonal = input - trend, (3) trend_output = Linear_trend(trend), (4) seasonal_output = Linear_seasonal(seasonal), (5) output = trend_output + seasonal_output. No attention computation, no multi-stage encoding, no complex feature gathering needed.\n\n7. **Theoretical Motivation - Semantic Correlation vs. Temporal Ordering**: Pyraformer is motivated by capturing semantic correlations through attention mechanisms while reducing complexity via pyramidal structure. The paper focuses on efficient long-range dependency modeling. DLinear is motivated by the observation that time series lack point-wise semantic correlations and that attention's permutation-invariance loses temporal information. DLinear argues that simple linear models better preserve temporal ordering. **NEW IMPLEMENTATION REQUIRED**: The implementation philosophy shifts from 'how to make attention efficient' to 'why not use attention at all'. Code should reflect this by completely removing attention mechanisms and focusing on preserving temporal order through direct linear transformations on the time axis.\n\n8. **Handling Distribution Shifts - Implicit vs. Explicit Normalization**: Pyraformer handles data variations through layer normalization and batch normalization within its architecture but doesn't explicitly address distribution shifts. DLinear's NLinear variant explicitly addresses distribution shifts by subtracting the last value of the input sequence before linear transformation and adding it back afterward. **NEW IMPLEMENTATION REQUIRED**: For NLinear, implement explicit normalization: (1) last_value = input[:, -1:, :], (2) normalized_input = input - last_value, (3) output = Linear(normalized_input), (4) final_output = output + last_value. This simple normalization scheme is not present in Pyraformer's codebase and needs to be added."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": null,
    "differences": null
  },
  {
    "source": "DLinear_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers utilize time series decomposition as a core preprocessing technique. DLinear employs seasonal-trend decomposition with moving average kernels (borrowed from Autoformer), while Crossformer also references decomposition methods in Transformer-based architectures. The `series_decomp` module from DLinear's implementation can be directly reused or adapted for Crossformer's preprocessing pipeline, particularly for extracting trend and seasonal components before embedding.\n\n2. **Linear Projection for Feature Transformation**: Both methods heavily rely on linear layers for feature transformation. DLinear uses `nn.Linear` layers (`Linear_Seasonal` and `Linear_Trend`) to project temporal sequences, while Crossformer uses linear projection matrices (E ∈ ℝ^(d_model × L_seg)) for DSW embedding and W^l for final predictions. The linear projection implementation pattern in DLinear (weight initialization, forward pass) provides a direct template for implementing Crossformer's projection layers.\n\n3. **Multi-variate Processing Architecture**: Both handle multivariate time series by processing channels/dimensions. DLinear offers both shared (`individual=False`) and channel-independent (`individual=True`) processing modes using `nn.ModuleList`, which directly informs how to implement Crossformer's dimension-wise operations. The channel iteration pattern (`for i in range(self.channels)`) in DLinear can be adapted for Crossformer's cross-dimension stage operations.\n\n4. **Direct Multi-step (DMS) Forecasting Strategy**: Both papers adopt DMS forecasting rather than autoregressive approaches. DLinear directly predicts the entire forecast horizon through linear layers, while Crossformer uses hierarchical decoder outputs summed across scales. The output aggregation logic in DLinear (summing seasonal and trend components) provides implementation guidance for Crossformer's multi-scale prediction fusion (Eq. 8).\n\n5. **Normalization and Residual Connections**: Both employ normalization techniques. DLinear uses implicit normalization through subtraction (NLinear variant), while Crossformer explicitly uses LayerNorm. The residual connection pattern in neural architectures is common to both, though Crossformer implements it more extensively in TSA layers.",
    "differences": "1. **Core Architecture Paradigm**: DLinear is fundamentally a simple linear model without self-attention mechanisms, directly mapping input to output via weighted sums (W ∈ ℝ^(T×L)). Crossformer implements a sophisticated Transformer-based architecture with Two-Stage Attention (TSA) layers combining cross-time and cross-dimension attention. **NEW IMPLEMENTATION REQUIRED**: Complete TSA layer with (a) Cross-Time Stage using MSA on each dimension independently (Eq. 3), (b) Cross-Dimension Stage with router mechanism to reduce O(D²) to O(D) complexity (Eq. 4), including learnable router vectors R ∈ ℝ^(L×c×d_model) and dual MSA operations (MSA₁^dim and MSA₂^dim).\n\n2. **Embedding Strategy**: DLinear uses minimal embedding (direct linear projection on raw sequences), while Crossformer introduces Dimension-Segment-Wise (DSW) embedding that fundamentally restructures input representation. **NEW IMPLEMENTATION REQUIRED**: (a) Segment partitioning logic to divide each dimension into segments of length L_seg (Eq. 1), (b) 2D vector array structure H ∈ ℝ^(L×D×d_model) instead of 1D sequence, (c) Learnable position embeddings E^(pos)_{i,d} for each (segment, dimension) position (Eq. 2), (d) Projection matrix E ∈ ℝ^(d_model×L_seg) to embed segments into vectors.\n\n3. **Hierarchical Multi-Scale Processing**: DLinear operates at a single temporal scale (direct input-to-output mapping), while Crossformer implements a Hierarchical Encoder-Decoder (HED) capturing information at multiple temporal scales. **NEW IMPLEMENTATION REQUIRED**: (a) Encoder with N layers performing segment merging (Eq. 6) - concatenating adjacent temporal segments via learnable matrix M ∈ ℝ^(d_model×2d_model), creating coarser representations at each layer, (b) Decoder with N+1 layers (Eq. 7) operating at different scales with learnable position embeddings E^(dec), (c) Cross-attention mechanism between encoder and decoder at each scale level, (d) Multi-scale prediction fusion summing outputs from all decoder layers (Eq. 8).\n\n4. **Attention Mechanism Design**: DLinear has no attention mechanism, while Crossformer's attention is specifically designed for multivariate time series. **NEW IMPLEMENTATION REQUIRED**: (a) Multi-head self-attention (MSA) adapted for 2D vector arrays with separate processing for time and dimension axes, (b) Router mechanism with fixed number of learnable vectors (c << D) to aggregate and distribute information across dimensions efficiently, (c) Shared MSA parameters across dimensions in cross-time stage and across time steps in cross-dimension stage, (d) Integration of attention with LayerNorm and MLP in residual blocks.\n\n5. **Temporal Dependency Modeling**: DLinear captures temporal patterns through simple weighted averaging (linear weights), treating time series as continuous numerical sequences. Crossformer explicitly models both cross-time dependencies (through temporal self-attention within each dimension) and cross-dimension dependencies (through router-based attention across dimensions). **NEW IMPLEMENTATION REQUIRED**: (a) Two-stage attention flow where cross-time attention output Z^time becomes input to cross-dimension attention, (b) Dimension-specific attention in cross-time stage (separate attention for each of D dimensions), (c) Time-step-specific router aggregation in cross-dimension stage (separate routers for each of L time segments), (d) Explicit modeling of segment-to-segment relationships rather than point-to-point.\n\n6. **Complexity and Scalability**: DLinear has O(1) complexity relative to sequence length (just matrix multiplication), making it extremely efficient. Crossformer has O(DL²) complexity per TSA layer where L = T/L_seg, with additional O(D·τ(T+τ)/L_seg²) for decoder. **NEW IMPLEMENTATION REQUIRED**: (a) Efficient segment merging to reduce L at each encoder layer, (b) Router mechanism implementation to avoid O(D²) complexity in cross-dimension attention, (c) Hierarchical processing logic to balance computation across scales, (d) Memory-efficient 2D array operations for large D and T.\n\n7. **Output Generation Strategy**: DLinear generates output through simple addition of two linear transformations (seasonal + trend), while Crossformer aggregates predictions from multiple scales. **NEW IMPLEMENTATION REQUIRED**: (a) Scale-specific linear projection layers W^l for each decoder layer l ∈ {0,...,N}, (b) Segment-to-sequence reconstruction logic to convert predicted segment vectors back to continuous time series, (c) Multi-scale fusion summing predictions across all N+1 decoder layers with proper alignment."
  },
  {
    "source": "FEDformer_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture for Time Series**: Both papers employ Transformer architectures adapted for multivariate time series (MTS), sharing the fundamental encoder-decoder structure with multi-head attention mechanisms. The source implementation provides reusable components like `DataEmbedding`, `my_Layernorm`, `series_decomp`, and the basic encoder-decoder framework (`Encoder`, `Decoder`, `EncoderLayer`, `DecoderLayer`) that can be adapted for Crossformer's hierarchical structure.\n\n2. **Frequency Domain Processing**: Both leverage frequency domain transformations - FEDformer uses Fourier/Wavelet transforms in FEB and FEA blocks, while Crossformer's TSA layer could benefit from similar frequency analysis. The source's `FourierBlock` and `FourierCrossAttention` implementations (including FFT operations, mode selection, complex multiplication) provide a solid foundation for understanding frequency-based feature extraction that could inform Crossformer's cross-dimension attention.\n\n3. **Multi-scale Hierarchical Processing**: FEDformer's wavelet decomposition (`MWT_CZ1d`) implements recursive multi-scale analysis with segment merging, similar to Crossformer's hierarchical encoder-decoder that merges segments across layers. The source's `wavelet_transform` and `evenOdd` reconstruction methods demonstrate how to handle different temporal scales, which directly parallels Crossformer's scale-based forecasting approach.\n\n4. **Decomposition Strategy**: Both employ decomposition mechanisms - FEDformer uses `series_decomp` for seasonal-trend decomposition with mixture of experts (`MOEDecomp`), while Crossformer implicitly decomposes through segment-wise embedding and hierarchical processing. The source's decomposition blocks can be adapted for Crossformer's dimension-segment-wise (DSW) embedding initialization.\n\n5. **Attention Mechanism Optimization**: Both papers address computational complexity in attention - FEDformer achieves O(L) through mode selection in Fourier domain, while Crossformer uses router mechanism for O(D) complexity. The source's efficient attention implementations (sparse mode selection, parameter sharing across dimensions/time) provide patterns for implementing Crossformer's two-stage attention with reduced complexity.\n\n6. **Embedding and Projection**: Both use learnable linear projections for input embedding and output generation. FEDformer's `DataEmbedding` class (with position encoding) and final projection layers can be reused as templates for Crossformer's DSW embedding (Eq. 2) and layer-wise predictions (Eq. 8).",
    "differences": "1. **Core Innovation - Dimension-Segment-Wise (DSW) Embedding**: Crossformer's primary contribution is embedding time series as a 2D array where each vector represents a segment of single dimension (Eq. 1-2), fundamentally different from FEDformer's point-wise or wavelet-based embedding. **NEW IMPLEMENTATION REQUIRED**: Create a custom embedding module that segments each dimension independently (length L_seg), applies linear projection per segment, and outputs a 2D array (L×D×d_model) instead of FEDformer's 1D sequence. This requires reshaping input from (B, T, D) to (B, T/L_seg, D, L_seg) before projection.\n\n2. **Two-Stage Attention (TSA) Architecture**: Crossformer introduces a novel TSA layer with separate cross-time and cross-dimension stages (Eq. 3-4), where cross-time applies MSA to each dimension independently, then cross-dimension uses a router mechanism. **NEW IMPLEMENTATION REQUIRED**: \n   - Cross-time stage: Apply self-attention to Z[:,d] for each dimension d (complexity O(DL²))\n   - Router mechanism: Implement learnable routers R (L×c×d_model) that aggregate from all D dimensions via MSA₁, then distribute back via MSA₂ (complexity O(DL))\n   - This differs fundamentally from FEDformer's frequency-domain attention that operates on Fourier/Wavelet coefficients\n\n3. **Router Mechanism for Cross-Dimension Dependency**: Crossformer's router mechanism (Fig. 2c, Eq. 4) uses a small fixed number (c<<D) of learnable vectors to build all-to-all dimension connections with O(DL) complexity. **NEW IMPLEMENTATION REQUIRED**: Create a router module with two MSA layers (MSA₁_dim, MSA₂_dim) where routers serve as queries in first stage and keys/values in second stage. This is entirely absent in FEDformer, which handles dimensions through channel-wise operations or wavelet filters.\n\n4. **Hierarchical Encoder-Decoder with Segment Merging**: Crossformer's HED (Eq. 6-7) merges adjacent segments in time domain using learnable matrix M (d_model × 2d_model) at each encoder layer, creating coarser representations. **NEW IMPLEMENTATION REQUIRED**: \n   - Implement segment merging: concatenate Z[2i-1,d] and Z[2i,d], then apply matrix M\n   - Modify decoder to accept N+1 encoded arrays (one per scale) instead of single encoded output\n   - Implement scale-specific TSA and cross-attention for each decoder layer\n   - This differs from FEDformer's wavelet decomposition which uses fixed filters (H0, H1, G0, G1)\n\n5. **Multi-scale Prediction Aggregation**: Crossformer generates predictions at each hierarchical level (Eq. 8) and sums them for final output, where each scale's prediction captures different temporal granularities. **NEW IMPLEMENTATION REQUIRED**: \n   - Create N+1 projection heads (W^l) for each decoder layer\n   - Implement prediction summation across scales\n   - FEDformer only produces single-scale predictions from final decoder output, requiring architectural changes to support multi-scale forecasting\n\n6. **Explicit Cross-Dimension Modeling**: Crossformer explicitly models cross-dimension dependency through router mechanism in every TSA layer, treating time and dimension axes differently. **NEW IMPLEMENTATION REQUIRED**: Design asymmetric attention where time uses standard MSA (Eq. 3) but dimension uses router-based attention (Eq. 4). FEDformer treats dimensions as channels in frequency domain, not explicitly modeling dimension-to-dimension relationships, requiring new attention patterns.\n\n7. **Complexity Management Strategy**: While both optimize complexity, Crossformer uses router mechanism (O(DL)) for dimension axis and standard attention (O(DL²)) for time axis. **NEW IMPLEMENTATION REQUIRED**: Implement complexity-aware layer design where cross-time stage scales with L² but cross-dimension scales linearly with D through routers. FEDformer's approach (mode selection in frequency domain) cannot be directly reused; need to implement router-based linear scaling for large D scenarios.\n\n8. **Position Embedding Design**: Crossformer uses 2D position embeddings E^(pos)_{i,d} for each (time_segment, dimension) position (Eq. 2), plus separate learnable embeddings E^(dec) for decoder. **NEW IMPLEMENTATION REQUIRED**: Create 2D position embedding matrix indexed by both segment position and dimension, different from FEDformer's 1D temporal position encoding. This requires (L×D) position embeddings instead of just L temporal positions."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture for Time Series**: Both papers adopt Transformer architectures for time series modeling, sharing the fundamental attention mechanism framework. The source paper's implementation of multi-head attention (MSA), LayerNorm, and position-wise feed-forward networks can be directly reused. Specifically, the `PositionwiseFeedForward` module and basic attention computation logic from Pyraformer are applicable to Crossformer's TSA layers.\n\n2. **Hierarchical Multi-scale Processing**: Both employ hierarchical structures to capture temporal dependencies at different scales. Pyraformer uses a pyramidal graph with coarser-scale construction (CSCM with ConvLayer), while Crossformer uses segment merging in its hierarchical encoder. The source implementation's `ConvLayer` and `Bottleneck_Construct` modules demonstrate how to downsample sequences progressively, which can guide implementing Crossformer's segment merging operation (Eq. 6). The pattern of processing information at multiple resolutions with complexity reduction is shared.\n\n3. **Embedding Strategy with Position Information**: Both papers embed input sequences with positional information before feeding to attention layers. Pyraformer's `DataEmbedding` module (combining observation, covariate, and positional embeddings) provides a template for Crossformer's DSW embedding implementation. The learnable position embedding mechanism and linear projection approach can be adapted for Crossformer's segment-wise embedding (Eq. 2).\n\n4. **Complexity Reduction via Sparse Attention**: Both papers address the O(L²) complexity problem of full attention through sparse connectivity patterns. Pyraformer achieves O(L) complexity via pyramidal attention with limited neighbor connections, while Crossformer reduces dimension complexity from O(D²) to O(D) using router mechanisms. The source code's custom attention mask implementation (`get_mask`, `RegularMask`) demonstrates how to implement sparse attention patterns, which can guide Crossformer's router-based attention mechanism.\n\n5. **Layer Stacking and Skip Connections**: Both architectures stack multiple attention layers with residual connections and layer normalization. The source paper's `EncoderLayer` structure (attention + residual + FFN + residual) directly maps to Crossformer's TSA layer structure. The pattern of `LayerNorm(input + sublayer(input))` is identical and can be reused.\n\n6. **Final Prediction via Linear Projection**: Both papers use linear projection layers to map encoded features to final outputs. Pyraformer's projection module for classification tasks demonstrates the pattern, which can be adapted for Crossformer's multi-scale prediction aggregation (Eq. 8).",
    "differences": "1. **Core Innovation - Cross-Dimension Dependency vs Pyramidal Attention**: Pyraformer focuses on long-range temporal dependencies through pyramidal graph structures (C-ary tree with inter/intra-scale connections), while Crossformer's innovation is explicitly capturing cross-dimension dependency through Two-Stage Attention. NEW IMPLEMENTATION NEEDED: Crossformer requires a completely new TSA layer with separate Cross-Time Stage (Eq. 3) and Cross-Dimension Stage (Eq. 4), including the novel router mechanism with learnable router vectors R that aggregate and distribute information across dimensions.\n\n2. **Input Representation - Point-wise vs Segment-wise Embedding**: Pyraformer embeds individual time points and builds a pyramidal structure over them, while Crossformer proposes Dimension-Segment-Wise (DSW) embedding where segments of length L_seg in each dimension are embedded into vectors, creating a 2D array structure. NEW IMPLEMENTATION NEEDED: A DSW embedding module that partitions input x_{1:T} into segments x_{i,d}^{(s)} and embeds them with position-specific embeddings E_{i,d}^{pos} (Eq. 1-2). This fundamentally changes the input representation from 1D sequence to 2D array.\n\n3. **Attention Pattern - Pyramidal Graph vs Two-Stage Processing**: Pyraformer uses a fixed pyramidal graph topology where each node attends to adjacent nodes, children, and parent (Eq. 2-3), implemented via custom CUDA kernels. Crossformer uses a two-stage approach: first applying MSA along the time dimension for each dimension separately, then using router mechanism for cross-dimension communication. NEW IMPLEMENTATION NEEDED: (a) Cross-Time Stage that applies MSA independently to each dimension Z_{:,d} (Eq. 3), (b) Cross-Dimension Stage with router mechanism involving MSA_1^dim and MSA_2^dim (Eq. 4), where routers R_{i,:} aggregate from all dimensions then distribute back.\n\n4. **Hierarchical Structure - C-ary Tree vs Segment Merging**: Pyraformer constructs a C-ary tree using strided convolutions (window_size as stride) in CSCM, creating parent nodes that summarize C children. Crossformer merges adjacent segments in time using learnable matrix M (Eq. 6). NEW IMPLEMENTATION NEEDED: A segment merging operation that concatenates adjacent vectors Z_{2i-1,d}^{enc,l-1} and Z_{2i,d}^{enc,l-1} and projects via matrix M, rather than convolution-based downsampling.\n\n5. **Encoder-Decoder Architecture - Single Encoder vs Hierarchical Encoder-Decoder**: Pyraformer uses a single encoder with pyramidal attention followed by task-specific prediction modules. Crossformer employs a Hierarchical Encoder-Decoder (HED) where N+1 decoder layers correspond to N+1 encoder scales, with cross-attention between encoder and decoder at each scale (Eq. 7). NEW IMPLEMENTATION NEEDED: (a) Multi-scale decoder structure with learnable position embeddings E^{(dec)} for decoder input, (b) Cross-attention layers connecting decoder queries to encoder outputs at matching scales, (c) Multi-scale prediction aggregation where each decoder layer produces predictions that are summed (Eq. 8).\n\n6. **Feature Aggregation - Pyramid Node Gathering vs Multi-scale Summation**: Pyraformer gathers features from the last nodes at all pyramid scales using `refer_points` and `torch.gather` operations, then concatenates them for final prediction. Crossformer applies linear projections W^l to each decoder layer's output to get scale-specific predictions, then sums them. NEW IMPLEMENTATION NEEDED: A multi-scale prediction module with N+1 separate projection matrices W^l (one per scale) and summation-based aggregation rather than concatenation-based.\n\n7. **Complexity Focus - Temporal Length vs Dimension Count**: Pyraformer optimizes for long sequence length L (achieving O(L) from O(L²)), while Crossformer addresses both temporal and dimensional complexity, particularly optimizing for large dimension count D (reducing from O(D²L) to O(DL) via routers). NEW IMPLEMENTATION NEEDED: Router mechanism with fixed number c of router vectors (c << D) to achieve O(2cD) = O(D) complexity in cross-dimension attention, rather than Pyraformer's pyramid-based temporal complexity reduction.\n\n8. **Task Formulation - Classification vs Forecasting**: Pyraformer's provided implementation is adapted for classification (flattening encoder output and projecting to num_class), while Crossformer is designed for multivariate forecasting (predicting future values x_{T+1:T+τ}). NEW IMPLEMENTATION NEEDED: Forecasting-specific components including decoder input preparation with zero-padded future observations, multi-horizon prediction logic, and MSE/MAE loss computation for regression rather than cross-entropy for classification."
  },
  {
    "source": "Autoformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the Transformer architecture with encoder-decoder structures for time series processing. The basic building blocks (LayerNorm, MLP/FeedForward, residual connections) are shared. **Code Reuse**: The `my_Layernorm`, `series_decomp`, `moving_avg` modules from Autoformer can be directly reused. The encoder-decoder framework structure can serve as a template.\n\n2. **Embedding Strategy with Temporal Information**: Both use learnable embeddings combined with positional/temporal information. Autoformer uses `DataEmbedding_wo_pos` with `TokenEmbedding` (1D Conv) and `TemporalEmbedding`. Crossformer uses DSW embedding with linear projection and position embedding. **Code Reuse**: The embedding infrastructure (`TokenEmbedding`, `PositionalEmbedding`, `TemporalEmbedding` classes) can be adapted. The pattern of embedding + position encoding is directly transferable.\n\n3. **Multi-layer Hierarchical Processing**: Both employ multiple encoder/decoder layers to progressively refine representations. Autoformer uses N encoder layers and M decoder layers; Crossformer uses hierarchical encoder-decoder with segment merging. **Code Reuse**: The `Encoder` and `Decoder` wrapper classes that stack multiple layers can be reused with modifications to the layer definitions.\n\n4. **Cross-Attention Mechanism in Decoder**: Both use encoder-decoder attention where decoder queries attend to encoder outputs. Autoformer's decoder has encoder-decoder Auto-Correlation; Crossformer has MSA between decoder and encoder features. **Code Reuse**: The pattern of cross-attention in decoder layers (taking encoder output as key/value) can be adapted from Autoformer's `DecoderLayer` structure.\n\n5. **Decomposition for Trend-Seasonal Separation**: While Crossformer doesn't explicitly mention decomposition in the methodology, both architectures benefit from separating different components. Autoformer explicitly decomposes trend-cyclical and seasonal parts. **Code Reuse**: The `series_decomp` module with moving average could be explored for preprocessing in Crossformer implementation.\n\n6. **Linear Projection for Output**: Both use linear projections to map hidden representations to predictions. Autoformer uses `nn.Linear` for classification head; Crossformer uses learnable matrices W^l for segment projection. **Code Reuse**: The pattern of final linear projection can be directly adapted.",
    "differences": "1. **Core Innovation - Attention Mechanism**: \n   - **Autoformer**: Proposes Auto-Correlation mechanism using FFT to compute autocorrelation R(τ), discovering period-based dependencies via time-delay aggregation with Roll operations. Complexity O(L log L).\n   - **Crossformer**: Proposes Two-Stage Attention (TSA) with separate Cross-Time Stage (MSA per dimension) and Cross-Dimension Stage (router mechanism with learnable vectors). Complexity O(DL²).\n   - **NEW Implementation Needed**: Complete TSA layer with (1) Cross-Time Stage applying MSA to each dimension separately, (2) Router mechanism with learnable router vectors R for Cross-Dimension Stage, (3) Two sequential MSA operations (MSA₁^dim for aggregation, MSA₂^dim for distribution).\n\n2. **Embedding Philosophy and Structure**:\n   - **Autoformer**: Point-wise embedding - embeds all D dimensions at each time step t into a vector h_t, resulting in T vectors for sequence length T.\n   - **Crossformer**: Dimension-Segment-Wise (DSW) embedding - segments each dimension into segments of length L_seg, embeds each segment into a vector, resulting in a 2D array (T/L_seg × D) of vectors.\n   - **NEW Implementation Needed**: (1) Segmentation logic to divide each dimension into segments of length L_seg, (2) 2D array structure to maintain segment-dimension organization, (3) Learnable position embedding E^(pos)_{i,d} for each position (i,d) in the 2D array, (4) Linear projection matrix E for segment embedding.\n\n3. **Hierarchical Multi-scale Processing**:\n   - **Autoformer**: Single-scale processing - all layers operate at the same temporal resolution, no explicit multi-scale hierarchy.\n   - **Crossformer**: Hierarchical Encoder-Decoder (HED) with segment merging - each encoder layer (except first) merges adjacent segments using learnable matrix M, doubling the time coverage. Decoder has N+1 layers corresponding to N+1 scales from encoder.\n   - **NEW Implementation Needed**: (1) Segment merging operation with learnable matrix M ∈ R^(d_model × 2d_model) to concatenate and project adjacent segments, (2) Multi-scale decoder structure where each layer l takes encoder output from scale l, (3) Hierarchical prediction aggregation summing predictions from all N+1 decoder layers.\n\n4. **Cross-Dimension Dependency Modeling**:\n   - **Autoformer**: Implicit cross-dimension modeling through shared processing of all dimensions at each time step, no explicit mechanism to capture dimension interactions.\n   - **Crossformer**: Explicit Cross-Dimension Stage with router mechanism - uses c (constant, c << D) learnable router vectors to aggregate information from all D dimensions, then distributes back. Reduces complexity from O(D²L) to O(DL).\n   - **NEW Implementation Needed**: (1) Learnable router array R ∈ R^(L×c×d_model), (2) Two-step attention: MSA₁^dim(routers as Q, dimensions as K/V) for aggregation into B, MSA₂^dim(dimensions as Q, aggregated B as K/V) for distribution, (3) Separate MSA modules for cross-dimension stage.\n\n5. **Decoder Input and Initialization**:\n   - **Autoformer**: Decoder inputs initialized with decomposed components from latter half of encoder input (seasonal X_ens, trend X_ent) plus placeholders (zeros X_0 for seasonal, mean X_Mean for trend) for prediction horizon.\n   - **Crossformer**: Decoder uses learnable position embeddings E^(dec) ∈ R^(τ/L_seg × D × d_model) for the prediction horizon, no explicit trend-seasonal decomposition in initialization.\n   - **NEW Implementation Needed**: Learnable decoder position embedding matrix for all future segment positions, initialization strategy without decomposition-based components.\n\n6. **Output Prediction Strategy**:\n   - **Autoformer**: Single prediction path - accumulates trend components through decoder layers (T_de^l = T_de^(l-1) + projected trends), final output is sum of projected seasonal component and accumulated trend.\n   - **Crossformer**: Multi-scale prediction fusion - each decoder layer l produces a prediction x^(pred,l) via linear projection W^l, final prediction is sum of all N+1 layer predictions (Equation 8).\n   - **NEW Implementation Needed**: (1) Separate projection matrix W^l for each decoder layer, (2) Segment-to-prediction conversion for each layer, (3) Multi-scale prediction summation logic across all decoder layers.\n\n7. **Temporal Dependency Focus**:\n   - **Autoformer**: Period-based dependencies - discovers periodic patterns using autocorrelation, selects top-k time delays τ based on autocorrelation values, aggregates similar sub-processes from underlying periods.\n   - **Crossformer**: Segment-based dependencies - treats each segment as a unit, captures dependencies among segments within same dimension (Cross-Time) and across dimensions (Cross-Dimension), no explicit periodicity detection.\n   - **NEW Implementation Needed**: Segment-level attention computation without FFT-based autocorrelation, standard MSA applied to segment representations rather than time-delay aggregation."
  },
  {
    "source": "Informer_2020",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer encoder-decoder paradigm for time series processing. The basic building blocks including multi-head attention, layer normalization, feedforward networks (MLP), and residual connections can be directly reused from Informer's implementation. Specifically, the `LayerNorm`, `MLP/Conv1d` feedforward layers, and dropout mechanisms are identical structural components.\n\n2. **Embedding Strategy for Sequential Input**: Both models require embedding raw time series into high-dimensional vector representations before processing. Informer's `DataEmbedding` module (combining value embedding with positional encoding) provides a reusable template. While Crossformer uses Dimension-Segment-Wise (DSW) embedding which segments time series differently, the underlying linear projection mechanism (`nn.Linear` for embedding) and learnable position embedding (`nn.Parameter`) can be adapted from Informer's implementation.\n\n3. **Hierarchical Information Processing**: Both architectures employ hierarchical structures to capture multi-scale temporal patterns. Informer uses self-attention distilling with `MaxPool` and `Conv1d` to progressively reduce sequence length (Eq. 5), creating a pyramid structure. Crossformer's segment merging operation (Eq. 6) serves a similar purpose by merging adjacent segments. The implementation pattern of progressive dimension reduction through layers can be reused, though the specific merging mechanisms differ.\n\n4. **Attention Mechanism Core Components**: The fundamental attention computation pipeline (Q, K, V projections, scaled dot-product, softmax) is shared. Informer's `AttentionLayer` class with `query_projection`, `key_projection`, `value_projection` and `out_projection` provides the structural template. The multi-head mechanism splitting and concatenation logic can be directly adapted for Crossformer's cross-time and cross-dimension attention stages.\n\n5. **Decoder Cross-Attention Pattern**: Both models use cross-attention in the decoder to connect encoder outputs with decoder states. Informer's `DecoderLayer` implementation with separate `self_attention` and `cross_attention` modules (with proper masking) provides the blueprint. Crossformer's decoder also employs MSA between decoder queries and encoder keys/values (Eq. 7), following the same computational pattern.",
    "differences": "1. **Core Attention Innovation - ProbSparse vs Two-Stage Attention (TSA)**: Informer introduces ProbSparse self-attention to reduce complexity from O(L²) to O(L log L) by selecting top-u dominant queries based on sparsity measurement (Eq. 2-4). This requires implementing the `_prob_QK` sampling method and `_update_context` mechanism with sparse query selection. In contrast, Crossformer proposes Two-Stage Attention that explicitly separates cross-time and cross-dimension dependency modeling. **NEW IMPLEMENTATION REQUIRED**: (1) Cross-Time Stage applying MSA independently to each dimension (Eq. 3), (2) Cross-Dimension Stage with router mechanism using fixed number of learnable router vectors to aggregate and distribute information (Eq. 4), reducing complexity from O(D²L) to O(DL). The router-based attention with two sequential MSA operations (MSA₁ᵈⁱᵐ and MSA₂ᵈⁱᵐ) is entirely novel.\n\n2. **Input Representation - Point-wise vs Dimension-Segment-Wise (DSW) Embedding**: Informer embeds all dimensions at each time step into a single vector (xₜ → hₜ), treating time series as a 1D sequence of length T. Crossformer fundamentally redesigns this by segmenting each dimension independently into segments of length L_seg, creating a 2D array representation (Eq. 1-2). **NEW IMPLEMENTATION REQUIRED**: (1) Segment partitioning logic that divides each dimension into non-overlapping windows, (2) 2D position embedding indexed by (time_segment_index, dimension_index), (3) Reshape operations to maintain 2D array structure [L × D × d_model] throughout the network instead of flattening to 1D. This requires rethinking the entire data flow pipeline.\n\n3. **Hierarchical Structure Design - Distilling vs Segment Merging**: Informer's encoder uses self-attention distilling with ConvLayer (1D convolution + ELU + MaxPool with stride 2) to halve the sequence length at each layer, building multiple encoder stacks with progressively shorter inputs. Crossformer employs segment merging where adjacent segments in time dimension are concatenated and projected through a learnable matrix M (Eq. 6). **NEW IMPLEMENTATION REQUIRED**: (1) Segment merging operation `M[Z_{2i-1,d} · Z_{2i,d}]` that concatenates pairs of adjacent time segments within each dimension, (2) Maintaining 2D array structure during merging (only merging along time axis, not dimension axis), (3) Multi-scale decoder layers that take different encoder layer outputs and sum their predictions (Eq. 8), unlike Informer's single decoder output.\n\n4. **Decoder Initialization and Prediction Strategy**: Informer uses a generative inference approach with start tokens sampled from historical data (L_token length slice) concatenated with zero placeholders for target sequence (Eq. 6), enabling one-forward prediction. Crossformer initializes decoder with learnable position embeddings E^(dec) for the target sequence (Eq. 7), without using historical data as start tokens. **NEW IMPLEMENTATION REQUIRED**: (1) Learnable decoder position embedding initialization instead of data-driven tokens, (2) Multi-scale prediction aggregation where each decoder layer produces predictions via linear projection (W^l), and final output is the sum across all layers (Eq. 8), requiring N+1 separate projection heads and prediction summation logic.\n\n5. **Complexity Optimization Strategy - Query Sparsity vs Router Mechanism**: Informer reduces complexity through sparse query selection based on max-mean measurement M̄(qᵢ, K) (Eq. 4), sampling U = L_K ln L_Q dot-products and selecting top-u queries. This requires implementing probabilistic sampling and top-k selection. Crossformer addresses the D² complexity problem in cross-dimension attention through a router mechanism with fixed c learnable vectors that mediate information exchange between dimensions. **NEW IMPLEMENTATION REQUIRED**: (1) Learnable router vectors R ∈ ℝ^(L×c×d_model) as model parameters, (2) Two-step attention process: routers aggregate from all dimensions via MSA₁ᵈⁱᵐ(R, Z, Z), then distribute to dimensions via MSA₂ᵈⁱᵐ(Z, B, B) where B is aggregated messages (Eq. 4), (3) The router mechanism is fundamentally different from any sparse attention pattern, requiring separate implementation of the gather-distribute paradigm."
  },
  {
    "source": "Autoformer_2021",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Philosophy**: Both papers adopt decomposition-based architectures to separate time series into interpretable components. Autoformer decomposes into trend-cyclical and seasonal parts using moving average blocks, while ETSformer decomposes into level, growth (trend), and seasonal components inspired by exponential smoothing. **Code Reuse**: The `series_decomp` module and `moving_avg` block from Autoformer can be adapted as a baseline for ETSformer's decomposition operations, though ETSformer requires additional level extraction logic.\n\n2. **Encoder-Decoder Architecture**: Both utilize multi-layer encoder-decoder frameworks with residual connections and layer normalization. Autoformer's encoder focuses on seasonal modeling while accumulating trend progressively in the decoder. ETSformer follows a similar paradigm where encoders extract growth/seasonal representations and decoders generate forecasts. **Code Reuse**: The overall `Encoder` and `Decoder` class structures, including layer stacking (`nn.ModuleList`), residual connections, and `LayerNorm` operations can be directly reused with modifications to the attention mechanisms.\n\n3. **FFT-Based Efficient Computation**: Both papers leverage Fast Fourier Transform for O(L log L) complexity. Autoformer uses FFT in Auto-Correlation to compute autocorrelation via Wiener-Khinchin theorem (Equation 8), while ETSformer uses FFT in Frequency Attention for periodicity detection and DFT-based seasonal pattern extraction (Equation 4). **Code Reuse**: The FFT computation patterns (`torch.fft.rfft`, `torch.fft.irfft`) from Autoformer's `AutoCorrelation` module can be adapted for ETSformer's Frequency Attention implementation.\n\n4. **Multi-Head Attention Paradigm**: Both employ multi-head mechanisms to capture diverse temporal patterns. Autoformer's `AutoCorrelationLayer` uses multi-head Auto-Correlation with separate Q/K/V projections, while ETSformer uses Multi-Head Exponential Smoothing Attention (MH-ESA). **Code Reuse**: The multi-head projection structure (`query_projection`, `key_projection`, `value_projection`, `out_projection`) and concatenation logic from `AutoCorrelationLayer` can serve as templates for implementing MH-ESA.\n\n5. **Embedding Modules**: Both use convolutional token embeddings and temporal embeddings. Autoformer's `DataEmbedding_wo_pos` combines `TokenEmbedding` (Conv1d with kernel_size=3) and `TemporalEmbedding`/`TimeFeatureEmbedding`. ETSformer uses a similar Conv1d-based input embedding with kernel_size=3. **Code Reuse**: The `TokenEmbedding` class with circular padding can be directly reused, though ETSformer explicitly avoids time-dependent covariates, so temporal embedding logic should be excluded.\n\n6. **Progressive Refinement Strategy**: Both architectures progressively refine predictions through stacked layers. Autoformer extracts and accumulates trend components across decoder layers (Equation 4), while ETSformer cascades growth/seasonal extraction through encoder stacks. **Code Reuse**: The layer-wise progressive extraction pattern and residual update mechanisms can be adapted between implementations.",
    "differences": "1. **Core Attention Mechanism Innovation**: Autoformer introduces **Auto-Correlation** that discovers period-based dependencies by computing series autocorrelation and aggregates similar sub-series via time delay (Roll operation). ETSformer proposes two novel mechanisms: **Exponential Smoothing Attention (ESA)** - a non-adaptive attention with exponential decay based on relative time lag (not input content), and **Frequency Attention (FA)** - a non-learnable mechanism using DFT to select top-K dominant frequencies. **New Implementation Required**: (1) ESA with efficient O(L log L) cross-correlation implementation (Algorithm 1 in ETSformer), including learnable smoothing parameter α and initial state v0; (2) FA module with amplitude-based frequency selection (arg Top-K), phase/amplitude extraction, and inverse DFT for seasonal reconstruction (Equation 4); (3) Replace Autoformer's `AutoCorrelation` class entirely with `ExponentialSmoothingAttention` and `FrequencyAttention` classes.\n\n2. **Component Extraction Logic**: Autoformer uses **moving average** for trend extraction and residual for seasonal (Equation 1), applied as inner operations after attention blocks. ETSformer follows **exponential smoothing principles** with explicit level smoothing equations (weighted average with learnable α parameter), successive differencing for growth extraction, and Fourier-based seasonal extraction. **New Implementation Required**: (1) Level module with exponential smoothing formula (α * de-seasonalized_level + (1-α) * previous_level_growth); (2) Growth extraction via successive differencing in MH-ESA; (3) Replace `series_decomp` blocks with ETSformer's level/growth/seasonal extraction pipeline; (4) Learnable smoothing parameters (α, γ) as model parameters.\n\n3. **Decoder Forecast Composition**: Autoformer's decoder accumulates trend through weighted projections (W_{l,1}, W_{l,2}, W_{l,3}) and adds final seasonal projection (Equation 4). ETSformer uses **additive composition** of level, growth, and seasonal forecasts (Equation 3) with a **Growth Damping** module implementing trend damping (γ^i damping factor). **New Implementation Required**: (1) Growth Damping (TD) module with learnable multi-head damping parameters γ; (2) Level Stack using Repeat_H operation; (3) Final forecast composition as sum of level + Linear(growth + seasonal) instead of Autoformer's trend accumulation structure; (4) Remove Autoformer's trend projection weights and decoder trend accumulation logic.\n\n4. **Temporal Aggregation Strategy**: Autoformer uses **Roll operation** to align sub-series at same phase positions of estimated periods, then aggregates by softmax-normalized autocorrelation confidences (Equation 6). ETSformer's ESA uses **exponential decay weights** based purely on time lag without content-based similarity, and FA uses **amplitude-weighted frequency components** without rolling. **New Implementation Required**: (1) Remove Roll-based time delay aggregation; (2) Implement exponential decay weight computation (α(1-α)^j pattern); (3) Implement amplitude-based weighting for frequency components; (4) Cross-correlation based efficient ESA computation instead of attention matrix multiplication.\n\n5. **Input Representation Philosophy**: Autoformer initializes decoder with decomposed components from latter half of encoder input (I/2 length) plus placeholders (Equation 2), using both seasonal (X_0) and trend-mean (X_Mean) placeholders. ETSformer uses **only encoder outputs** (growth B_t, seasonal S_{t-L:t}) for decoder, with level extracted from lookback window and repeated for forecast horizon. **New Implementation Required**: (1) Remove decoder input initialization logic with placeholders; (2) Implement decoder that takes encoder's extracted components directly; (3) Level repetition mechanism (Repeat_H); (4) Modify decoder to generate H-step forecasts from single time step representations.\n\n6. **Periodicity Discovery Approach**: Autoformer discovers periods via **arg Top-K on autocorrelation R(τ)** across all possible lags {1,...,L}, selecting k=⌊c×log L⌋ periods. ETSformer discovers periods via **frequency domain analysis** - selecting top-K frequency components with largest amplitudes in DFT space, then reconstructing seasonal patterns via inverse DFT. **New Implementation Required**: (1) Replace autocorrelation-based period selection with amplitude-based frequency selection; (2) Implement DFT/IDFT pipeline with phase/amplitude extraction; (3) Conjugate frequency handling for real-valued signals; (4) Hyperparameter K for frequency selection instead of c×log L formula.\n\n7. **Normalization and Residual Design**: Autoformer uses custom `my_Layernorm` that subtracts mean bias for seasonal parts, and applies series decomposition after each attention+residual block. ETSformer uses standard LayerNorm but applies it **after removing growth/seasonal components** from residuals, with explicit residual updates (Z := Z - S, Z := LN(Z - B)). **New Implementation Required**: (1) Modify residual connection pattern to explicitly subtract extracted components before normalization; (2) Use standard LayerNorm instead of custom my_Layernorm; (3) Implement cascaded residual update logic (remove seasonal, then remove growth, then feedforward)."
  },
  {
    "source": "Informer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer-based encoder-decoder paradigm for time series forecasting. The source implementation's basic encoder-decoder structure (Encoder/Decoder classes with stacked layers) can be directly reused as the architectural skeleton. The target paper can leverage the same multi-layer stacking mechanism and residual connections.\n\n2. **Input Embedding Strategy**: Both use convolutional embedding to map raw time series to latent space. Informer's DataEmbedding with Conv1d operations can be adapted for ETSformer's input embedding module. The source code's Conv1d(kernel_size=3) in ConvLayer is nearly identical to ETSformer's Conv input embedding requirement.\n\n3. **Layer Normalization and Feed-Forward Networks**: Both architectures employ LayerNorm and position-wise FFN after attention mechanisms. The source implementation's norm layers (nn.LayerNorm) and FFN structure (Conv1d-based) in EncoderLayer/DecoderLayer can be directly reused, reducing implementation effort for basic layer operations.\n\n4. **Multi-Head Attention Framework**: While attention mechanisms differ fundamentally, both use multi-head structures. The source's AttentionLayer class with query/key/value projections and multi-head splitting logic provides a template. ETSformer can reuse the projection infrastructure (query_projection, key_projection, value_projection) and reshape operations for implementing MH-ESA.\n\n5. **Efficient Complexity Focus**: Both papers emphasize O(L log L) complexity. Informer's ProbSparse attention achieves this through sampling, while ETSformer uses FFT-based operations. The source code's complexity-aware design philosophy (factor-based sampling, efficient computation) provides implementation patterns for ETSformer's FFT-based FA mechanism.\n\n6. **Distillation/Residual Processing**: Informer's distilling operation (ConvLayer with MaxPool) shares conceptual similarity with ETSformer's residual learning approach. The source's progressive dimension reduction through Conv1d + MaxPool can inspire ETSformer's sequential decomposition of level/growth/seasonal components from residuals.",
    "differences": "1. **Core Attention Mechanism - MAJOR NEW IMPLEMENTATION**: \n   - **Informer**: Uses content-based ProbSparse attention with query sparsity measurement (M(q,K)) and Top-u query selection based on KL divergence approximation. Requires implementing sampling logic, LSE operations, and sparse matrix operations.\n   - **ETSformer**: Introduces two fundamentally different attention types:\n     a) **Exponential Smoothing Attention (ESA)**: Non-adaptive, time-lag-based attention with exponential decay (α(1-α)^j weighting). Requires implementing Algorithm 1 with FFT-based cross-correlation for O(L log L) complexity, learnable smoothing parameter α, and initial state v0.\n     b) **Frequency Attention (FA)**: Non-learnable, frequency-domain attention using DFT/IDFT. Requires implementing: (i) DFT along temporal dimension, (ii) amplitude-based Top-K frequency selection, (iii) phase/amplitude extraction, (iv) IDFT reconstruction with selected frequencies.\n   - **Implementation Gap**: Need to build from scratch: FFT-based ESA algorithm, DFT/IDFT operations for FA, frequency selection logic, and phase/amplitude handling. Cannot reuse Informer's ProbAttention code.\n\n2. **Component Decomposition Philosophy - NEW ARCHITECTURAL PARADIGM**:\n   - **Informer**: Single-path processing with distilling for dimension reduction. Encoder outputs single representation, decoder generates forecasts directly.\n   - **ETSformer**: Explicit multi-component decomposition at each layer:\n     a) **Level Module**: Implements exponential smoothing equation E_t^(n) = α*(E_t^(n-1) - Linear(S_t^(n))) + (1-α)*(E_{t-1}^(n) + Linear(B_{t-1}^(n))) with learnable α. Requires recurrent computation across time steps.\n     b) **Growth Module**: Extracts growth via MH-ESA on successive differences (Z - [Z_{shifted}, v0]). Requires implementing difference operation and growth damping (TD) with learnable γ parameter.\n     c) **Seasonal Module**: Extracts seasonality via FA, performs de-seasonalization on residuals.\n   - **Implementation Gap**: Need to implement: (i) Level smoothing recurrence with learnable α, (ii) Growth extraction via differencing, (iii) Sequential residual updates (Z := Z - S, Z := Z - B), (iv) Component-wise Linear projections from latent to observation space.\n\n3. **Decoder Forecasting Strategy - COMPLETELY DIFFERENT APPROACH**:\n   - **Informer**: Uses start token + placeholder (X_token concatenated with X_0), masked attention for autoregressive prevention, generates forecasts through single forward pass with generative inference.\n   - **ETSformer**: Compositional forecasting via Equation (3): ŷ = E_{t:t+H} + Linear(Σ(B^(n) + S^(n))). Components:\n     a) **Level Stack**: Simple repetition of last level: Repeat_H(E_t^(N)).\n     b) **Growth Damping (GD)**: Implements TD(B_t^(n))_j = Σ_{i=1}^j γ^i B_t^(n) with multi-head damping parameters.\n     c) **Frequency Attention for Seasonality**: Extrapolates seasonal patterns from lookback to forecast horizon using Equation (4).\n   - **Implementation Gap**: Need to implement: (i) Repeat operation for level, (ii) Growth damping with learnable γ and cumulative summation, (iii) Seasonal extrapolation via FA's cosine reconstruction, (iv) Final composition layer summing all components.\n\n4. **No Positional/Temporal Covariates - SIMPLIFIED INPUT**:\n   - **Informer**: Uses DataEmbedding with temporal features (x_mark_enc, x_mark_dec) including positional encodings, time stamps (month, day-of-week, etc.).\n   - **ETSformer**: Explicitly avoids manual temporal covariates, relies solely on FA to automatically discover seasonal patterns. Input is raw time series only.\n   - **Implementation Gap**: Need to remove temporal feature engineering from input pipeline, ensure FA mechanism alone handles periodicity detection.\n\n5. **Training Objective and Loss Propagation**:\n   - **Informer**: Direct MSE loss on decoder output predictions, standard backpropagation through encoder-decoder.\n   - **ETSformer**: MSE loss on compositional forecast (level + growth + seasonal). Loss gradients flow through multiple component-specific pathways (Level Module's α, GD's γ, ESA's α and v0, FA's amplitude selection).\n   - **Implementation Gap**: Need to ensure proper gradient flow through: (i) Level smoothing recurrence, (ii) Growth damping summation, (iii) FFT-based ESA (requires autograd-compatible FFT), (iv) DFT-based FA (gradient through Top-K selection and phase/amplitude).\n\n6. **Memory and Complexity Trade-offs**:\n   - **Informer**: Achieves O(L log L) via query sampling and distilling (ConvLayer + MaxPool). Memory: O((2-ε)L log L) with pyramid stacking.\n   - **ETSformer**: Achieves O(L log L) via FFT-based ESA and DFT-based FA. No explicit distilling, but sequential decomposition reduces residual complexity.\n   - **Implementation Gap**: Need to implement FFT-optimized algorithms (Algorithm 1 for ESA, DFT/IDFT for FA) ensuring O(L log L) complexity. Different memory management strategy without pyramid stacking."
  },
  {
    "source": "Reformer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the Transformer architecture for sequence modeling. Reformer introduces LSH attention to reduce O(L²) complexity to O(L log L), while ETSformer also achieves O(L log L) complexity through its Exponential Smoothing Attention (ESA) and Frequency Attention (FA) mechanisms. The source paper's basic Transformer encoder structure (EncoderLayer, multi-head attention framework) can be reused as the backbone, particularly the layer normalization, feedforward networks, and residual connections.\n\n2. **Efficient Attention Mechanisms with O(L log L) Complexity**: Both papers address the quadratic complexity bottleneck of standard attention. Reformer uses LSH hashing and bucketing (with FFT-based operations for efficient computation), while ETSformer leverages FFT for both ESA (cross-correlation via FFT) and FA (DFT/IDFT operations). The source implementation's efficient attention computation patterns, particularly the memory-efficient attention strategy and the use of chunking, provide valuable insights for implementing ETSformer's efficient algorithms (Algorithm 1 in ETSformer).\n\n3. **Multi-Head Attention Pattern**: Both architectures employ multi-head mechanisms to capture different aspects of the input. Reformer uses standard multi-head attention with LSH, while ETSformer uses Multi-Head Exponential Smoothing Attention (MH-ESA) with multiple smoothing parameters. The source paper's multi-head implementation framework (splitting dimensions, parallel computation, concatenation) can be directly adapted for ETSformer's MH-ESA module.\n\n4. **Layer Stacking and Residual Learning**: Both models use deep stacked architectures with residual connections. Reformer employs reversible layers to reduce memory (RevNet blocks), while ETSformer uses cascaded encoder layers with explicit residual decomposition (Z := Z - S, Z := Z - B). The source implementation's encoder stacking pattern and layer normalization placement can guide ETSformer's encoder implementation.\n\n5. **Embedding and Projection Layers**: Both architectures require input embedding and output projection. Reformer uses DataEmbedding for encoding inputs, while ETSformer uses temporal convolution for input embedding. The source paper's Linear projection layers and embedding infrastructure can be reused with modifications for ETSformer's Conv-based embedding and multi-component projection (level, growth, seasonal).",
    "differences": "1. **Core Task and Architecture Paradigm**: Reformer targets sequence classification/language modeling with a single-path encoder architecture, while ETSformer is specifically designed for time-series forecasting with an explicit encoder-decoder structure. NEW IMPLEMENTATION NEEDED: (a) Decoder architecture with Growth+Seasonal (G+S) Stacks and Level Stack; (b) Forecast composition mechanism combining level, growth, and seasonal components (Equation 3); (c) Growth Damping (GD) module with learnable damping parameter γ for multi-step forecasting.\n\n2. **Attention Mechanism Philosophy**: Reformer uses content-based LSH attention where attention weights depend on query-key similarity (adaptive, data-driven), while ETSformer introduces two fundamentally different attention types: (a) Exponential Smoothing Attention (ESA) - non-adaptive, position-based attention with exponential decay based on time lag, not content; (b) Frequency Attention (FA) - non-learnable, amplitude-based selection in frequency domain. NEW IMPLEMENTATION NEEDED: (a) ESA module with learnable smoothing parameter α and initial state v₀, implementing Algorithm 1 with FFT-based cross-correlation; (b) FA module with DFT, Top-K amplitude selection, phase/amplitude extraction, and IDFT reconstruction (Equation 4); (c) MH-ESA with successive differencing for growth extraction.\n\n3. **Component Decomposition vs. Unified Representation**: Reformer processes unified hidden representations through LSH attention, while ETSformer explicitly decomposes signals into interpretable components (level, growth, seasonal) at each layer. NEW IMPLEMENTATION NEEDED: (a) Level Module implementing exponential smoothing equation with learnable α parameter; (b) Sequential component extraction pipeline (seasonal → growth → residual update → feedforward); (c) Component-specific processing: FA for seasonal, MH-ESA for growth, exponential smoothing for level; (d) Multi-layer cascaded decomposition where each layer refines components from residuals.\n\n4. **Temporal Modeling Strategy**: Reformer uses causal masking and position-based bucketing for sequence modeling, while ETSformer employs time-series-specific inductive biases. NEW IMPLEMENTATION NEEDED: (a) Frequency-domain processing for seasonal pattern detection (DFT → Top-K selection → IDFT); (b) Exponential decay weighting for recency bias in growth modeling; (c) Growth damping mechanism for robust multi-step forecasting with learnable γ; (d) Seasonal pattern extrapolation from lookback window to forecast horizon using identified dominant frequencies.\n\n5. **Input/Output Design and Feature Engineering**: Reformer uses token embeddings with positional encodings and outputs class probabilities, while ETSformer uses raw time-series values without manual covariates and outputs multi-horizon forecasts. NEW IMPLEMENTATION NEEDED: (a) Temporal convolutional embedding (kernel size 3) replacing token embeddings; (b) Multi-component output projection: Linear(Σ(B + S)) for growth+seasonal, Repeat_H for level; (c) Forecast composition combining level forecasts E_{t:t+H}, growth representations B_{t:t+H}, and seasonal representations S_{t:t+H}; (d) Removal of positional encodings and time-dependent covariates, relying solely on FA for pattern discovery."
  },
  {
    "source": "Autoformer_2021",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Decomposition Architecture Foundation**: Both papers adopt a deep decomposition architecture with encoder-decoder structure. FEDformer directly builds upon Autoformer's decomposition framework, using similar encoder/decoder layer structures with series decomposition blocks. The existing Autoformer implementation's `Encoder`, `Decoder`, `EncoderLayer`, and `DecoderLayer` classes can be reused as structural templates, requiring only modifications to replace Auto-Correlation with FEB/FEA blocks.\n\n2. **Series Decomposition Block**: Both use series decomposition to separate seasonal and trend-cyclical components. Autoformer uses moving average (`series_decomp` with `moving_avg`), while FEDformer proposes MOEDecomp (Mixture of Experts Decomposition). The existing `series_decomp` and `moving_avg` classes provide the foundational structure - FEDformer needs to implement MOEDecomp as an enhanced version with multiple average pooling filters and learnable mixing weights, but the calling interface and position in the architecture remain identical.\n\n3. **Progressive Trend Accumulation in Decoder**: Both papers accumulate trend components progressively through decoder layers. The decoder structure with trend accumulation (`T_de^l = T_de^{l-1} + W_{l,1}*T_{de}^{l,1} + W_{l,2}*T_{de}^{l,2} + W_{l,3}*T_{de}^{l,3}`) is identical. The existing decoder implementation with trend projection layers (`self.projection` in DecoderLayer) can be directly reused without modification.\n\n4. **Embedding and Input Processing**: Both use similar embedding strategies with `DataEmbedding_wo_pos` (token embedding + temporal embedding without positional encoding). The existing `TokenEmbedding`, `TemporalEmbedding`, `TimeFeatureEmbedding`, and `DataEmbedding_wo_pos` classes can be reused entirely for FEDformer implementation.\n\n5. **Feedforward Network and Layer Normalization**: Both employ standard feedforward networks with Conv1d layers and similar normalization strategies. The existing `my_Layernorm` (seasonal-specific layer normalization) and feedforward Conv1d implementations in encoder/decoder layers can be directly reused.\n\n6. **Multi-head Mechanism**: Both use multi-head architecture for attention-like operations. While Autoformer has `AutoCorrelationLayer` with multi-head Auto-Correlation, FEDformer needs FEB/FEA with multi-head structure. The multi-head projection pattern (query/key/value projections, output projection) from `AutoCorrelationLayer` provides a template for implementing multi-head FEB/FEA blocks.",
    "differences": "1. **Core Attention Mechanism - Auto-Correlation vs Frequency Enhanced Blocks**: Autoformer uses Auto-Correlation mechanism based on time-delay aggregation in time domain (FFT for autocorrelation computation, then Roll operation for aggregation). FEDformer replaces this entirely with Frequency Enhanced Blocks (FEB) operating in frequency domain. **NEW IMPLEMENTATION REQUIRED**: (a) FEB-f: Implements random mode selection in frequency domain, parameterized kernel R for frequency domain multiplication, padding operations; (b) FEB-w: Implements recursive wavelet decomposition with Legendre wavelet basis, three separate FEB-f modules for high/low/remaining frequency processing, multi-scale ladder decomposition/reconstruction mechanism. The entire `AutoCorrelation` class and `AutoCorrelationLayer` need to be replaced with new `FEB_f`, `FEB_w`, and corresponding layer wrappers.\n\n2. **Cross-Attention Design - Auto-Correlation vs FEA**: Autoformer uses the same Auto-Correlation mechanism for encoder-decoder cross-attention (with K,V from encoder and Q from decoder, resized to length-O). FEDformer introduces Frequency Enhanced Attention (FEA) specifically for cross-attention. **NEW IMPLEMENTATION REQUIRED**: (a) FEA-f: Performs attention mechanism in frequency domain with randomly selected modes, using σ(Q̃·K̃ᵀ)·Ṽ pattern with activation functions (softmax or tanh); (b) FEA-w: Similar to FEB-w but with separate processing for Q, K, V signals and cross-attention operations replacing FEB-f modules. This requires implementing new `FEA_f` and `FEA_w` classes with different computation patterns from Auto-Correlation.\n\n3. **Frequency Domain Operations vs Time Domain Operations**: Autoformer primarily operates in time domain (uses FFT only for autocorrelation computation via Wiener-Khinchin theorem, then converts back). FEDformer fundamentally operates in frequency domain throughout FEB/FEA blocks. **NEW IMPLEMENTATION REQUIRED**: (a) Random mode selection strategy (`Select` operator) to keep only M<<N modes; (b) Frequency domain parameterized kernel multiplication (⊙ operator with R∈ℂ^{D×D×M}); (c) Padding operations in frequency domain before inverse transforms; (d) For wavelet version: fixed Legendre wavelet decomposition matrices (H^(0), H^(1), G^(0), G^(1)), recursive decomposition/reconstruction with scale parameter L.\n\n4. **Decomposition Block Enhancement - Moving Average vs MOEDecomp**: Autoformer uses simple moving average with fixed kernel size for trend extraction (`moving_avg` with AvgPool1d). FEDformer proposes Mixture of Experts Decomposition (MOEDecomp) with multiple average filters and data-dependent mixing weights. **NEW IMPLEMENTATION REQUIRED**: Implement `MOEDecomp` class with: (a) A set of average pooling filters F(·) with different kernel sizes; (b) Learnable mixing network L(x) that produces Softmax weights; (c) Weighted combination mechanism: X_trend = Softmax(L(x)) * F(x). This replaces all `series_decomp` calls in encoder/decoder.\n\n5. **Complexity and Efficiency Focus**: Autoformer achieves O(L log L) complexity through FFT-based autocorrelation and sparse time-delay aggregation. FEDformer achieves O(L) complexity through random mode selection (fixed M=64 modes) and pre-selected Fourier/Wavelet basis. **NEW IMPLEMENTATION REQUIRED**: (a) Mode selection mechanism that limits to fixed M modes before any frequency domain operations; (b) For FEB-w: fixed recursive decomposition steps (L=3) to ensure linear complexity; (c) Efficient implementation avoiding full FFT by operating only on selected modes.\n\n6. **Two-Version Architecture Design**: Autoformer has a single Auto-Correlation mechanism. FEDformer provides two versions: FEDformer-f (Fourier-based) and FEDformer-w (Wavelet-based). **NEW IMPLEMENTATION REQUIRED**: (a) Implement both FEB-f/FEA-f and FEB-w/FEA-w versions as separate classes; (b) Create a factory pattern or configuration flag to switch between Fourier and Wavelet versions; (c) For wavelet version: implement the complete recursive decomposition/reconstruction pipeline with proper tensor dimension handling (decimation by 1/2 per cycle, reconstruction by 2× per cycle).\n\n7. **Activation Function Flexibility in Cross-Attention**: Autoformer uses Softmax normalization for autocorrelation weights. FEDformer's FEA allows flexible activation functions (softmax or tanh) based on dataset characteristics. **NEW IMPLEMENTATION REQUIRED**: Add configurable activation function parameter in FEA implementation with support for both softmax and tanh, requiring dataset-specific tuning guidance."
  },
  {
    "source": "Informer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both models adopt the canonical Transformer encoder-decoder paradigm for long-term time series forecasting. FEDformer can directly reuse Informer's basic encoder-decoder structure, including the multi-layer stacking mechanism (Encoder layers with N stacks, Decoder layers with M stacks), residual connections, and layer normalization components. The implementation of `Encoder`, `Decoder`, `EncoderLayer`, `DecoderLayer` classes can serve as templates.\n\n2. **Embedding Layer Design**: Both papers use identical time series embedding strategies combining value embedding with temporal encoding. FEDformer can directly reuse Informer's `DataEmbedding` module which includes: (1) token embedding for raw values, (2) positional encoding for temporal information, (3) temporal feature embedding (hour, day, week, month). The dropout mechanism after embedding is also shared.\n\n3. **Decoder Input Construction**: Both employ the same generative inference strategy with start tokens. The decoder input concatenates historical context (label_len) with zero-padded future placeholders: `X_de = Concat(X_token, X_0)`. FEDformer can reuse this one-forward-pass prediction mechanism instead of autoregressive decoding, significantly improving inference efficiency.\n\n4. **Feed-Forward Network**: Both use identical position-wise feed-forward networks with two linear transformations and activation functions (ReLU/GELU) in between. The structure `Conv1d → Activation → Dropout → Conv1d → Dropout` from Informer can be directly adopted in FEDformer's encoder/decoder layers.\n\n5. **Training Paradigm**: Both use MSE loss for end-to-end training and share the same optimization strategy. The loss computation on prediction sequences and backpropagation mechanism are identical, allowing FEDformer to reuse Informer's training loop and loss calculation code.\n\n6. **Multi-Head Mechanism**: While attention mechanisms differ, both employ multi-head projections. FEDformer can reuse the query/key/value projection structure from Informer's `AttentionLayer`: linear projections to create multiple heads, reshape operations, and final output projection after concatenation.",
    "differences": "1. **Core Attention Mechanism - CRITICAL NEW IMPLEMENTATION**: Informer uses ProbSparse self-attention with query sparsity measurement (top-u selection based on KL divergence approximation), while FEDformer completely replaces this with Frequency Enhanced Blocks (FEB) and Frequency Enhanced Attention (FEA). **NEW COMPONENTS NEEDED**: (a) FEB-f: Fourier transform-based block with `Select(F(q))`, learnable frequency kernel R, and inverse transform `F^(-1)(Padding(Q⊙R))`. (b) FEB-w: Wavelet transform-based block with recursive decomposition using Legendre basis, three parallel FEB-f modules for high/low/remaining frequency processing. (c) FEA-f/FEA-w: Frequency domain attention replacing cross-attention, operating on transformed Q, K, V in frequency space. **CANNOT REUSE**: Informer's `ProbAttention`, `_prob_QK`, `_get_initial_context`, `_update_context` methods must be completely replaced.\n\n2. **Seasonal-Trend Decomposition Architecture - CRITICAL NEW IMPLEMENTATION**: Informer has no explicit decomposition mechanism, while FEDformer introduces deep decomposition at every layer. **NEW COMPONENTS NEEDED**: (a) MOEDecomp (Mixture of Experts Decomposition): implements `X_trend = Softmax(L(x)) * F(x)` with multiple average pooling filters of different sizes and learnable mixing weights. (b) Dual-path processing: seasonal component S flows through attention blocks, trend component T accumulates progressively with weighted projections `T_de^l = T_de^(l-1) + W_1·T_1 + W_2·T_2 + W_3·T_3`. (c) Integration into every residual connection: after each sub-block (FEB, FEA, FFN), apply MOEDecomp to extract and separate seasonal/trend. **CANNOT REUSE**: Informer's simple residual additions must be replaced with decomposition operations.\n\n3. **Encoder Distilling Strategy**: Informer uses self-attention distilling with Conv1d and MaxPooling to progressively halve sequence length (`MaxPool(ELU(Conv1d([X]_AB)))`) and builds pyramid-like replica stacks with halving inputs. FEDformer **REMOVES** this distilling mechanism entirely - no convolutional distilling layers, no max-pooling, no pyramid replicas. The encoder maintains full sequence length throughout all layers. **MUST DELETE**: `ConvLayer` class and distilling operations from Informer.\n\n4. **Frequency Domain Operations - CRITICAL NEW IMPLEMENTATION**: Informer operates purely in time domain, while FEDformer performs extensive frequency domain transformations. **NEW COMPONENTS NEEDED**: (a) DFT/IDFT operations: `Select(F(q))` selects M random modes from Fourier coefficients, operates in complex number space C^(M×D). (b) DWT operations: Legendre wavelet basis decomposition with fixed matrices (H^(0), H^(1), G^(0), G^(1)), recursive multi-scale processing with L decomposition levels. (c) Parameterized frequency kernels: learnable complex-valued tensor R ∈ C^(D×D×M) for frequency domain transformations. (d) Mode selection strategy: random sampling of frequency modes to reduce complexity from O(N²) to O(N). **COMPLETELY NEW**: No frequency domain code exists in Informer.\n\n5. **Complexity Optimization Strategy**: Informer achieves O(L log L) through sparse query selection (selecting top-u queries based on sparsity measurement M(q,K)). FEDformer achieves O(L) through fixed-mode frequency operations (pre-selecting M=64 modes regardless of sequence length). **NEW IMPLEMENTATION**: (a) Fixed mode selection instead of dynamic sparsity measurement. (b) Padding/truncation operations for frequency domain tensors. (c) Wavelet recursive decomposition with fixed depth L=3. **CANNOT REUSE**: Informer's sparsity measurement (`M(q,K) = LSE - mean`) and Top-u selection logic.\n\n6. **Cross-Attention Design**: Informer uses standard ProbSparse cross-attention between encoder output and decoder queries in time domain. FEDformer replaces this with FEA (Frequency Enhanced Attention) operating in frequency domain: `FEA-f(q,k,v) = F^(-1)(Padding(σ(Q̃·K̃^T)·Ṽ))` where Q̃, K̃, Ṽ are selected Fourier/Wavelet coefficients. **NEW COMPONENTS**: (a) Separate Fourier transforms for q, k, v. (b) Frequency domain attention computation with activation σ (softmax/tanh). (c) Wavelet version with recursive decomposition of all three inputs. **CANNOT REUSE**: Informer's cross-attention `AttentionLayer` with ProbAttention.\n\n7. **Output Projection Strategy**: Informer directly projects decoder output with single linear layer. FEDformer uses dual-component projection: `W_S·X_de^M + T_de^M`, combining projected seasonal component with accumulated trend component. **NEW IMPLEMENTATION**: (a) Separate projection for seasonal component W_S. (b) Trend accumulation across all decoder layers. (c) Final summation of two refined components. The trend pathway requires tracking across all layers, which doesn't exist in Informer."
  },
  {
    "source": "Reformer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer Architecture Foundation**: Both papers build upon the Transformer architecture with encoder-decoder structures. FEDformer can reuse Reformer's basic Transformer components including multi-head attention mechanism, feed-forward networks, layer normalization, and residual connections. The encoder-decoder framework in Reformer's implementation (EncoderLayer, Encoder structure) provides a solid foundation for FEDformer's multilayer encoder-decoder design.\n\n2. **Efficiency-Focused Design Philosophy**: Both papers aim to reduce computational complexity from O(L²) to more efficient alternatives. Reformer achieves O(L log L) through LSH attention, while FEDformer targets O(L) through frequency domain operations. The memory-efficient attention implementation strategy in Reformer (computing attention for each query separately to avoid materializing the full QK^T matrix) can inspire FEDformer's efficient frequency domain processing.\n\n3. **Embedding and Positional Encoding**: Both models use similar input embedding strategies. Reformer's DataEmbedding component (combining token embedding with positional encoding) can be directly reused in FEDformer for encoding time series data. The embedding layer that projects input sequences to d_model dimensional space is fundamental to both architectures.\n\n4. **Residual Connections and Layer Stacking**: Both employ residual connections around attention and feed-forward blocks, and stack multiple layers for deep feature extraction. FEDformer's encoder structure (Equation 1) with residual additions around FEB and FeedForward blocks mirrors Reformer's layer design, allowing reuse of the layer stacking and residual connection implementation.\n\n5. **Sequence-to-Sequence Framework**: Both handle sequential input-output mapping. While Reformer focuses on language modeling and FEDformer on forecasting, the basic sequence processing pipeline (input embedding → encoder processing → decoder processing → output projection) is conceptually similar.",
    "differences": "1. **Core Attention Mechanism - MAJOR NEW IMPLEMENTATION**: Reformer uses Locality-Sensitive Hashing (LSH) attention with hash bucketing, sorting, and chunking in the time domain. FEDformer completely replaces this with Frequency Enhanced Blocks (FEB) and Frequency Enhanced Attention (FEA) operating in the frequency domain. NEW COMPONENTS NEEDED: (a) FEB-f: Discrete Fourier Transform (DFT) based block with mode selection, learnable frequency kernels R∈C^(D×D×M), and inverse DFT; (b) FEB-w: Discrete Wavelet Transform (DWT) based block with recursive decomposition, Legendre wavelet basis, and reconstruction stages; (c) FEA-f: Fourier-based cross-attention computing attention in frequency domain with selected modes; (d) FEA-w: Wavelet-based cross-attention with decomposition/reconstruction. These require implementing FFT/IFFT operations, mode selection logic, and complex number operations absent in Reformer.\n\n2. **Decomposition Architecture - MAJOR NEW IMPLEMENTATION**: FEDformer introduces a deep seasonal-trend decomposition architecture with the MOEDecomp (Mixture Of Experts Decomposition) block throughout encoder and decoder layers. NEW COMPONENTS NEEDED: (a) MOEDecomp block implementing multiple average pooling filters with different window sizes and learnable data-dependent weights (Equation 10); (b) Seasonal-trend separation logic in both encoder (Equation 1) and decoder (Equation 2); (c) Trend accumulation mechanism in decoder across layers (T_de^l = T_de^(l-1) + W_(l,1)·T_de^(l,1) + ...). Reformer has no decomposition mechanism and processes sequences holistically.\n\n3. **Frequency Domain Operations vs Time Domain Hashing**: Reformer's LSH attention operates entirely in time domain using hash functions h(x) = argmax([xR; -xR]), bucket sorting, and chunk-based attention within buckets. FEDformer operates in frequency domain using: (a) DFT: X_l = Σx_n·e^(-iωln) for converting to frequency domain; (b) Mode selection: Select(Q) to keep only M<<N frequency modes; (c) Frequency domain multiplication: Q⊙R with learnable frequency kernels; (d) DWT: Multi-scale wavelet decomposition with fixed Legendre basis and recursive processing. These require completely different mathematical operations and implementation strategies.\n\n4. **Multi-Scale Processing**: FEDformer's FEB-w and FEA-w implement recursive multi-scale decomposition with L levels (typically L=3), processing high-frequency, low-frequency, and remaining components separately at each scale. Each decomposition cycle decimates signal by 1/2, with reconstruction doubling the length. Reformer has no multi-scale mechanism - it processes all positions at the same granularity level through hash bucketing.\n\n5. **Decoder Design and Prediction Strategy**: FEDformer's decoder performs progressive refinement with three decomposition blocks per layer (after FEB, after FEA, after FeedForward), accumulating trend components and outputting final prediction as W_S·X_de^M + T_de^M (sum of refined seasonal and accumulated trend). Reformer's decoder uses standard autoregressive generation with LSH attention and causal masking (m(j,P_i) to prevent attending to future), outputting class probabilities through projection layer. FEDformer needs NEW implementation for trend accumulation and dual-component prediction.\n\n6. **Reversible Layers vs Standard Layers**: Reformer implements reversible residual networks (RevNets) with paired computations Y₁=X₁+F(X₂), Y₂=X₂+G(Y₁) allowing backward pass without storing activations, and chunking for memory efficiency. FEDformer uses standard residual connections without reversibility, focusing on frequency domain efficiency instead. This represents fundamentally different memory optimization strategies.\n\n7. **Task and Output Structure**: Reformer is designed for classification (output: class logits via projection layer) with causal masking for autoregressive generation. FEDformer is designed for long-term forecasting (output: future sequence values) with encoder-decoder cross-attention and no causal constraints in encoder. The loss functions, evaluation metrics, and output dimensions differ completely, requiring task-specific implementation adjustments."
  },
  {
    "source": "Informer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "Informer_2020",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the standard Transformer encoder-decoder architecture with multi-head attention mechanisms. The source paper's implementation of basic Transformer components (multi-head attention structure, feed-forward networks, layer normalization, positional encoding) can be directly reused. The `EncoderLayer` structure with attention and FFN blocks in Reformer's code provides a solid foundation for Informer's encoder implementation.\n\n2. **Efficient Attention Mechanism Philosophy**: Both papers aim to reduce the quadratic complexity O(L²) of standard self-attention to handle longer sequences. Reformer uses LSH-based attention while Informer uses ProbSparse attention, but both share the core idea of selective attention computation. The source implementation's approach to computing attention efficiently (avoiding full materialization of QK^T matrix, memory-efficient attention calculation) provides valuable patterns for implementing Informer's selective attention mechanism.\n\n3. **Sequence Length Handling Strategy**: Both papers address the challenge of processing long sequences with limited memory. Reformer's `fit_length` function that pads sequences to multiples of bucket_size demonstrates practical sequence preprocessing. This padding and chunking strategy can be adapted for Informer's encoder input preparation, particularly for handling variable-length inputs in time series forecasting.\n\n4. **Multi-head Attention Framework**: Both maintain the multi-head attention mechanism from vanilla Transformer. The source code's multi-head structure (with configurable n_heads, d_model) can be reused as the backbone for Informer's ProbSparse multi-head attention. The head splitting and concatenation logic remains identical.\n\n5. **Layer Stacking and Normalization**: Both use stacked encoder layers with residual connections and layer normalization. Reformer's encoder stacking implementation (iterating through `e_layers` with `EncoderLayer` blocks) provides a direct template for Informer's encoder stack construction, though Informer adds the distilling operation between layers.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: \n   - **Reformer**: Uses Locality-Sensitive Hashing (LSH) attention with random projections, hash bucketing, and sorting queries/keys by hash values. Implements shared-QK attention where Q=K/||K||.\n   - **Informer**: Uses ProbSparse self-attention based on query sparsity measurement M(q_i, K) = LSE - mean. Requires implementing: (a) Sparsity measurement calculation with max-mean approximation, (b) Top-u query selection mechanism, (c) Random sampling of U=L_K*ln(L_Q) dot-products for efficiency.\n   - **Implementation Gap**: Need to build entirely new attention scoring, query selection, and sparse attention computation logic. Cannot reuse LSH hashing mechanism.\n\n2. **Encoder Architecture - Self-Attention Distilling (NEW)**: \n   - **Reformer**: Uses standard stacked encoder layers without dimension reduction between layers.\n   - **Informer**: Introduces self-attention distilling operation between encoder layers using Conv1d (kernel=3) + ELU activation + MaxPooling (stride=2) to progressively halve the sequence length. Implements pyramid structure with multiple encoder stacks processing different input slices.\n   - **Implementation Gap**: Must implement: (a) Distilling operation with 1D convolution and pooling, (b) Multi-stack pyramid encoder with replica stacks, (c) Feature map concatenation from all stacks. This is completely absent in Reformer.\n\n3. **Decoder Design and Inference Strategy - MAJOR NEW COMPONENT**: \n   - **Reformer**: Primarily designed for language modeling with causal masking, uses autoregressive generation.\n   - **Informer**: Implements generative-style decoder with start token strategy. Feeds decoder with Concat(X_token, X_0) where X_token is historical slice and X_0 is placeholder with target timestamps. Enables one-forward-pass prediction instead of iterative decoding.\n   - **Implementation Gap**: Need to implement: (a) Start token extraction from input sequence, (b) Placeholder generation with target timestamps, (c) Modified decoder forward pass for direct long-sequence prediction, (d) Different masking strategy for forecasting vs. language modeling.\n\n4. **Task Objective and Output Layer - FUNDAMENTAL DIFFERENCE**: \n   - **Reformer**: Designed for sequence classification tasks. Uses global pooling over sequence dimension followed by projection to num_classes. Output shape: [B, num_classes].\n   - **Informer**: Designed for time series forecasting (LSTF problem). Requires multi-step ahead prediction with output shape [B, L_y, d_y] where L_y is prediction horizon. Uses fully connected layer to generate forecasted values.\n   - **Implementation Gap**: Must replace classification head with forecasting head, implement MSE loss for regression, handle multivariate vs. univariate forecasting scenarios.\n\n5. **Memory Optimization Approach - DIFFERENT STRATEGIES**: \n   - **Reformer**: Uses reversible layers (RevNet) to eliminate activation storage across layers, implements chunking for feed-forward layers to handle large d_ff. Memory complexity: O(L*d_model) independent of n_layers.\n   - **Informer**: Uses self-attention distilling to reduce sequence length progressively, achieving O((2-ε)*L*log(L)) memory. Does not use reversible layers.\n   - **Implementation Gap**: Informer doesn't need Reformer's reversible architecture but requires implementing the distilling-based memory reduction, which is architecturally different.\n\n6. **Positional and Temporal Encoding - TIME SERIES SPECIFIC (NEW)**: \n   - **Reformer**: Uses standard learned or sinusoidal positional embeddings for sequence positions.\n   - **Informer**: Requires time series specific embeddings including temporal features (hour, day, week, month), position encoding, and value embedding. The `DataEmbedding` component needs to handle timestamp information.\n   - **Implementation Gap**: Must implement temporal feature extraction from timestamps, combine multiple embedding types (value + position + temporal), handle different time series frequencies (hourly, daily, etc.).\n\n7. **Complexity Analysis Focus - DIFFERENT BOTTLENECKS**: \n   - **Reformer**: Targets O(L*log(L)) attention complexity and O(1) memory per layer through LSH and reversibility. Hash computation adds overhead.\n   - **Informer**: Targets O(L*log(L)) through ProbSparse attention and O(L*log(L)) memory through distilling. Focuses on reducing both attention computation and memory for encoder-decoder forecasting.\n   - **Implementation Gap**: Different complexity-reduction strategies require different implementation optimizations and profiling approaches."
  },
  {
    "source": "PatchTST_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Transformer Encoder Backbone**: Both papers utilize vanilla Transformer encoder architecture as their core component. The PatchTST implementation's Encoder and EncoderLayer classes (with multi-head attention, feed-forward networks, and residual connections) can be directly reused for iTransformer. The attention mechanism (FullAttention class) and layer structure are compatible, requiring only dimensional adjustments.\n\n2. **Instance/Layer Normalization Philosophy**: Both papers employ normalization techniques to handle distribution shifts and non-stationarity. PatchTST uses instance normalization on each univariate series before processing, while iTransformer applies layer normalization on series representations. The normalization infrastructure in PatchTST (mean-std computation and denormalization) can be adapted, though iTransformer applies it differently (on variate tokens rather than temporal sequences).\n\n3. **Channel Independence Paradigm**: Both architectures process multivariate time series in a channel-independent manner initially. PatchTST explicitly splits input into M univariate series and processes them independently through shared Transformer backbone. iTransformer similarly treats each variate as an independent token. The PatchTST implementation's channel-wise processing loop and shared weight mechanism can inform iTransformer's variate-wise token processing.\n\n4. **Linear Projection Layers**: Both papers use linear layers for embedding and projection. PatchTST's linear projection from patches to latent space (W_p) and the final prediction head can guide iTransformer's Embedding and Projection MLPs, though iTransformer projects entire series (T→D) rather than patches (P→D).\n\n5. **Feed-Forward Network Design**: Both leverage identical feed-forward networks applied to tokens. PatchTST's FFN in EncoderLayer (with dropout and activation) provides a ready implementation template for iTransformer's FFN that processes variate tokens, requiring no structural changes.",
    "differences": "1. **Core Architectural Inversion (CRITICAL NEW COMPONENT)**: The fundamental innovation requires complete dimensional inversion. PatchTST treats time patches as tokens (N patches × D features), while iTransformer treats variates as tokens (N variates × D features where D encodes the entire time series). NEW IMPLEMENTATION: The embedding layer must map entire time series X_{:,n} ∈ ℝ^T to token h_n ∈ ℝ^D using MLP, not patch-wise linear projection. The attention operates on N variate tokens instead of temporal patches, requiring reshaping logic from (B, T, N) → (B, N, D).\n\n2. **Patching vs. Whole Series Embedding (NEW COMPONENT)**: PatchTST uses PatchEmbedding class with unfold operations to create overlapping/non-overlapping patches of length P with stride S, reducing sequence length from L to ~L/S tokens. iTransformer completely abandons patching and embeds the full lookback window T as a single representation per variate. NEW IMPLEMENTATION: Replace PatchEmbedding with a simple MLP that takes the entire series: Embedding: ℝ^T → ℝ^D. Remove all patching logic (unfold, padding_patch_layer, stride calculations).\n\n3. **Positional Encoding Removal (MODIFICATION)**: PatchTST explicitly uses PositionalEmbedding (sinusoidal encoding) added to patch embeddings to maintain temporal order. iTransformer states \"position embedding in vanilla Transformer is no longer needed\" because temporal order is implicitly stored in the neuron permutation of the feed-forward network processing the full series. NEW IMPLEMENTATION: Remove the PositionalEmbedding module entirely. The temporal structure is preserved through the MLP's learned weights that process the ordered time series.\n\n4. **Attention Semantic Interpretation (CONCEPTUAL SHIFT)**: In PatchTST, attention scores A ∈ ℝ^{N_patches × N_patches} capture temporal dependencies between different time segments. In iTransformer, attention scores A ∈ ℝ^{N_variates × N_variates} explicitly model multivariate correlations, with A_{i,j} ∝ q_i^T k_j revealing correlation between variate i and j after layer normalization. NEW IMPLEMENTATION: The attention mechanism code remains the same, but the interpretation and visualization logic differs—attention maps now represent variate-variate relationships rather than time-time dependencies.\n\n5. **Layer Normalization Application Dimension (MODIFICATION)**: PatchTST applies BatchNorm across the feature dimension of patches (Transpose(1,2) → BatchNorm1d(d_model) → Transpose(1,2)). iTransformer applies LayerNorm on the series representation dimension of each variate token individually: LayerNorm(H) normalizes h_n for each n separately. NEW IMPLEMENTATION: Modify the normalization in Encoder from BatchNorm1d applied after transposing to LayerNorm applied directly on token features, normalizing each h_n ∈ ℝ^D to zero mean and unit variance.\n\n6. **Projection Head Design (NEW COMPONENT)**: PatchTST uses FlattenHead that flattens (d_model × num_patches) and projects to prediction horizon T for each variate. iTransformer's projection is conceptually simpler: each variate token h_n^L ∈ ℝ^D is independently projected to future series Ŷ_{:,n} ∈ ℝ^S via MLP (Projection: ℝ^D → ℝ^S). NEW IMPLEMENTATION: Replace FlattenHead with a simple MLP projection layer that maps from representation dimension D to forecast horizon S for each variate independently, without flattening across patches.\n\n7. **Input/Output Reshaping Logic (CRITICAL MODIFICATION)**: PatchTST input flow: (B, L, M) → permute to (B, M, L) → patch to (B*M, N_patches, P) → embed → process → reshape back. iTransformer flow: (B, T, N) → transpose to (B, N, T) → embed each series → (B, N, D) → process → project → (B, N, S) → transpose to (B, S, N). NEW IMPLEMENTATION: Complete overhaul of data flow—remove all patching reshapes, implement direct series-to-token embedding with proper dimension handling for variate-as-token paradigm.\n\n8. **Training Objective Formulation (MINOR MODIFICATION)**: PatchTST averages MSE loss across M channels: L = (1/M)Σ||ŷ^(i) - y^(i)||². iTransformer similarly computes loss per variate but the prediction mechanism differs (no patch reconstruction, direct series prediction). The loss computation logic can be reused but must account for the different output shapes (no flattening from patches)."
  },
  {
    "source": "Crossformer_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Transformer-Based Architecture Foundation**: Both papers utilize Transformer architectures for multivariate time series processing, employing self-attention mechanisms, layer normalization, and feed-forward networks. The source implementation's `AttentionLayer`, `FullAttention`, `nn.LayerNorm`, and MLP components (e.g., `self.MLP1 = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model))`) can be directly reused as building blocks for iTransformer's encoder blocks.\n\n2. **Embedding Strategy with Linear Projections**: Both papers use linear projections for embedding time series data. Crossformer's `PatchEmbedding` with `self.value_embedding = nn.Linear(patch_len, d_model)` and iTransformer's MLP-based `Embedding: ℝ^T → ℝ^D` share the same principle of mapping time series segments/sequences to a latent space. The source's embedding logic can be adapted by replacing patch-based segmentation with full series embedding.\n\n3. **Layer Normalization on Feature Dimensions**: Both papers apply layer normalization, though with different semantic meanings. Crossformer's `LayerNorm(d_model)` normalizes across the model dimension, while iTransformer explicitly normalizes each variate token's representation (Equation 2). The source's `nn.LayerNorm` modules can be reused, but the application context needs adjustment to normalize across the series representation dimension rather than temporal segments.\n\n4. **Multi-Head Self-Attention for Dependency Modeling**: Both employ multi-head self-attention to capture dependencies. Crossformer's `AttentionLayer` with query/key/value projections (`self.query_projection`, `self.key_projection`, `self.value_projection`) and the `FullAttention` mechanism can be directly adapted for iTransformer's attention on variate tokens, since the mathematical formulation remains the same, only the semantic interpretation changes (cross-dimension vs. cross-variate).\n\n5. **Feed-Forward Network Design**: Both use identical feed-forward networks applied to tokens. Crossformer's `self.MLP1 = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model))` matches iTransformer's FFN structure. This component can be directly reused, as both papers apply FFN independently to each token (segment in Crossformer, variate in iTransformer).\n\n6. **Encoder-Based Architecture for Feature Learning**: Both papers employ encoder structures for representation learning. Crossformer's `Encoder` class with stacked `scale_block` layers and iTransformer's stacked `TrmBlock` layers follow similar hierarchical processing patterns. The source's encoder loop structure (`for block in self.encode_blocks: x, attns = block(x)`) can be adapted for iTransformer's encoder-only architecture.\n\n7. **Classification Head with Linear Projection**: Both papers use linear projections for final predictions/classification. Crossformer's `self.projection = nn.Linear(self.head_nf * configs.enc_in, configs.num_class)` and iTransformer's `Projection: ℝ^D → ℝ^S` demonstrate the same approach. The source's flattening and projection logic can be reused for iTransformer's output layer.",
    "differences": "1. **CORE INNOVATION - Inverted Dimension Philosophy**: iTransformer fundamentally inverts the token definition compared to Crossformer. Crossformer treats time segments as tokens (`x_{i,d}^{(s)}` - segment i in dimension d), while iTransformer treats entire variates as tokens (`X_{:,n}` - whole series of variate n). **NEW IMPLEMENTATION NEEDED**: Complete restructuring of input processing to embed full time series (ℝ^T) as tokens instead of segments, requiring a new embedding module that takes shape [B, N, T] and outputs [B, N, D] where N is the number of variates (not time steps).\n\n2. **Dimension-Segment-Wise (DSW) vs. Variate-Wise Embedding**: Crossformer's DSW embedding (Equation 1-2) segments time series into patches and embeds them with 2D position embeddings `E_{i,d}^{pos}`, creating a [L×D×d_model] array. iTransformer embeds the entire time series of each variate without segmentation or position embeddings. **NEW IMPLEMENTATION NEEDED**: Remove all segmentation logic (`self.seg_len`, `self.pad_in_len`, segment merging), eliminate 2D position embeddings (`self.enc_pos_embedding`), and implement a simple per-variate embedding that processes the full temporal sequence.\n\n3. **Two-Stage Attention vs. Standard Self-Attention**: Crossformer's `TwoStageAttentionLayer` explicitly separates cross-time and cross-dimension stages with a router mechanism (Equations 3-4), processing [L×D×d_model] arrays. iTransformer uses standard self-attention on variate tokens [N×D] without temporal attention within each variate. **NEW IMPLEMENTATION NEEDED**: Replace the complex two-stage attention with vanilla multi-head self-attention that operates on the variate dimension (N tokens), removing all router mechanisms (`self.router`, `dim_sender`, `dim_receiver`) and cross-time stage logic.\n\n4. **Hierarchical Encoder-Decoder vs. Encoder-Only Architecture**: Crossformer uses a hierarchical encoder-decoder with segment merging (`SegMerging`) across layers and cross-attention between encoder and decoder (Equations 6-7). iTransformer adopts an encoder-only architecture without any decoder or cross-attention. **NEW IMPLEMENTATION NEEDED**: Remove the entire decoder module (`self.decoder`, `DecoderLayer`), eliminate segment merging operations, and implement a simple stacked encoder with L identical blocks, each containing only self-attention and FFN without encoder-decoder cross-attention.\n\n5. **Multi-Scale Hierarchical Processing vs. Flat Token Processing**: Crossformer's `scale_block` progressively merges segments (`self.merge_layer = SegMerging(d_model, win_size)`) to capture information at different scales, creating a pyramid structure. iTransformer processes all variate tokens at the same scale throughout all layers. **NEW IMPLEMENTATION NEEDED**: Remove all hierarchical scaling logic, segment merging layers, and multi-scale prediction aggregation (Equation 8's sum over layers), replacing with a uniform processing of N variate tokens across all L layers.\n\n6. **Layer Normalization Semantics**: While both use LayerNorm, Crossformer normalizes across the model dimension for each segment-dimension pair, while iTransformer explicitly normalizes each variate token's series representation (Equation 2: normalizing `h_n` across its feature dimension). **NEW IMPLEMENTATION NEEDED**: Ensure LayerNorm is applied correctly to normalize each variate token independently across its D-dimensional representation, not across time or across variates.\n\n7. **Position Encoding Strategy**: Crossformer uses explicit learnable 2D position embeddings (`self.enc_pos_embedding = nn.Parameter(torch.randn(1, enc_in, in_seg_num, d_model))`) to encode both temporal and dimensional positions. iTransformer states \"the position embedding in the vanilla Transformer is no longer needed\" as temporal order is implicitly stored in FFN neuron permutation. **NEW IMPLEMENTATION NEEDED**: Remove all position embedding modules and ensure the FFN can implicitly capture temporal patterns through its learned weights.\n\n8. **Input/Output Shape Transformation**: Crossformer processes [B, T, D] → [B, D, L, d_model] (2D array of segments) → [B, (seg_num×seg_len), D]. iTransformer processes [B, T, N] → [B, N, D] (variate tokens) → [B, N, S] where each variate is independently projected. **NEW IMPLEMENTATION NEEDED**: Implement input reshaping from [B, T, N] to [B, N, T] for per-variate embedding, and output projection that independently maps each variate's D-dimensional representation to S future time steps, requiring shape [B, N, D] → [B, N, S].\n\n9. **Attention Score Interpretation**: Crossformer's attention scores represent cross-time dependencies (within dimensions) and cross-dimension dependencies (via routers). iTransformer's attention scores A_{i,j} = q_i^T k_j reveal variate-wise correlations between normalized variate tokens, providing interpretable multivariate correlation maps. **NEW IMPLEMENTATION NEEDED**: Ensure attention is computed on [B, N, D] tokens where N is the variate dimension, and optionally implement attention score visualization to reveal multivariate correlations.\n\n10. **Training Objective and Loss Computation**: Crossformer's classification task uses `self.projection = nn.Linear(self.head_nf * enc_in, num_class)` after flattening hierarchical outputs. iTransformer's forecasting task requires per-variate projection with shape [B, N, D] → [B, S, N] for time series prediction. **NEW IMPLEMENTATION NEEDED**: For forecasting, implement independent linear projections for each variate token (shared weights across variates) that map from representation space D to prediction horizon S, with appropriate loss functions (MSE/MAE) computed on the [B, S, N] output."
  },
  {
    "source": "TiDE_2023",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **MLP-based Feature Extraction Philosophy**: Both papers emphasize the effectiveness of Multi-Layer Perceptrons (MLPs) for time series representation learning. TiDE uses dense encoders/decoders, while iTransformer's feed-forward networks (FFNs) serve similar purposes for extracting series representations. The source paper's MLP implementation for temporal feature extraction can be directly adapted for iTransformer's embedding and projection layers (Embedding: ℝ^T → ℝ^D and Projection: ℝ^D → ℝ^S).\n\n2. **Channel-Independent Processing**: Both architectures process each variate independently through shared networks. TiDE applies dense layers to individual time series, and iTransformer's FFN identically processes each variate token. The source paper's implementation of channel-wise independent processing can be reused for iTransformer's per-variate token processing, where each series X_{:,n} is independently embedded and projected.\n\n3. **Layer Normalization for Stability**: Both employ layer normalization to handle non-stationarity and improve training stability. TiDE normalizes across features, while iTransformer normalizes each variate's series representation (Equation 2). The normalization implementation from TiDE can be adapted, though the normalization axis differs (iTransformer normalizes along the D-dimensional representation of each variate token).\n\n4. **Encoder-Decoder Separation of Concerns**: Both architectures separate representation learning from prediction generation. TiDE uses encoders for feature extraction and decoders for forecasting, while iTransformer uses Transformer blocks for representation and simple linear projection for prediction. The source paper's modular design philosophy (separate encoding/decoding stages) can guide the implementation of iTransformer's embedding → Transformer blocks → projection pipeline.\n\n5. **Residual Connections and Deep Architecture**: Both utilize deep networks with skip connections for gradient flow. TiDE's residual dense blocks and iTransformer's Transformer blocks with residual connections share this design principle. The residual connection implementation from TiDE can be reused in iTransformer's TrmBlock stacking.",
    "differences": "1. **Core Innovation - Inverted Attention Mechanism**: iTransformer's main contribution is inverting the Transformer architecture to treat each variate as a token rather than each timestamp. This requires NEW implementation of:\n   - Variate-wise tokenization where entire time series X_{:,n} becomes one token\n   - Self-attention applied across N variates (not T timestamps)\n   - Attention score map A ∈ ℝ^(N×N) revealing multivariate correlations\n   TiDE has no attention mechanism, so the entire multi-head self-attention module with Q, K, V projections needs to be implemented from scratch.\n\n2. **Temporal Encoding Strategy**: \n   - TiDE: Explicitly encodes time through dense layers operating on the temporal dimension T, with temporal features extracted via MLP operations on lookback windows\n   - iTransformer: Implicitly stores temporal order in neuron permutations of FFN, eliminating positional embeddings entirely. The sequence order is encoded through the feed-forward network's learned weights\n   NEW implementation needed: The implicit temporal encoding mechanism where FFN neurons learn to represent temporal properties (amplitude, periodicity, frequency spectrums) without explicit position embeddings.\n\n3. **Multivariate Correlation Modeling**:\n   - TiDE: Limited multivariate interaction, primarily through shared dense layers\n   - iTransformer: Explicit multivariate correlation learning via self-attention with interpretable attention maps showing variate dependencies (A_{i,j} ∝ q_i^T k_j)\n   NEW implementation needed: The correlation discovery mechanism through attention scores normalized by queries/keys from different variates, with visualization capabilities for the N×N correlation matrix.\n\n4. **Token Representation and Dimensionality**:\n   - TiDE: Works with fixed temporal windows, processing T×N matrices with temporal focus\n   - iTransformer: Flexible token representation H ∈ ℝ^(N×D) where N can vary between training/inference, enabling training on arbitrary numbers of variates\n   NEW implementation needed: Variable-length token processing allowing different numbers of variates at train/test time, with attention mechanisms that adapt to changing N.\n\n5. **Architecture Paradigm**:\n   - TiDE: Pure MLP-based architecture with temporal convolutions/dense layers\n   - iTransformer: Transformer encoder-only architecture requiring stacked blocks with attention + FFN + LayerNorm\n   NEW implementation needed: The complete Transformer block structure (TrmBlock in Equation 1) with proper ordering of LayerNorm → Attention → FFN, and the ability to plug in efficient attention variants (FlashAttention, etc.) for scalability when N is large.\n\n6. **Generalization Capability Design**:\n   - TiDE: Fixed to trained variate numbers\n   - iTransformer: Designed for zero-shot generalization to unseen variates due to shared FFN and flexible attention\n   NEW implementation needed: Mechanisms to handle variable N during inference, ensuring the model can process datasets with different numbers of variates than seen during training."
  },
  {
    "source": "TimesNet_2022",
    "target": "iTransformer_2023",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Foundation**: Both papers utilize the series_decomp module from Autoformer for temporal decomposition. DLinear explicitly uses moving average kernels to separate trend and seasonal components, which iTransformer can leverage in preprocessing. The DLinear implementation's `self.decompsition = series_decomp(configs.moving_avg)` can be directly reused as a preprocessing step before embedding in iTransformer.\n\n2. **Linear Projection Philosophy**: Both approaches recognize the effectiveness of simple linear layers for time series modeling. DLinear uses `nn.Linear(self.seq_len, self.pred_len)` for temporal mapping, while iTransformer employs MLP-based Embedding and Projection layers (`Embedding: R^T → R^D` and `Projection: R^D → R^S`). The weight initialization strategy in DLinear `(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])` provides a good starting point for iTransformer's projection layers.\n\n3. **Channel-wise Processing**: DLinear's `individual=True` mode processes each variate independently with separate linear layers (`nn.ModuleList()` for each channel), which directly aligns with iTransformer's core principle of treating each variate as an independent token. The loop structure `for i in range(self.channels)` in DLinear's encoder can guide the implementation of per-variate token processing in iTransformer.\n\n4. **Task Adaptation Architecture**: Both papers share the same classification task structure - encoding the input sequence and flattening to a feature vector before final projection. DLinear's `output = enc_out.reshape(enc_out.shape[0], -1)` followed by `self.projection(output)` provides the exact template for iTransformer's classification head implementation.\n\n5. **Normalization Strategy**: While DLinear uses implicit normalization through decomposition, both approaches recognize the importance of handling distribution shifts. DLinear's NLinear variant (subtract-normalize-add) and iTransformer's Layer Normalization on series representations (Equation 2) share the goal of reducing variate discrepancies, suggesting the normalization logic can be adapted from DLinear's preprocessing pipeline.",
    "differences": "1. **Core Architectural Paradigm - CRITICAL NEW IMPLEMENTATION**: DLinear operates on the temporal dimension (time points as features), while iTransformer inverts this to operate on the variate dimension (variates as tokens). This requires implementing:\n   - Token embedding where each variate's entire time series becomes one token: `h_n^0 = Embedding(X[:,n])` instead of DLinear's per-timestep processing\n   - Self-attention mechanism operating on N variate tokens rather than temporal sequences\n   - This inversion fundamentally changes the data flow: DLinear processes [Batch, Channels, Time] → [Batch, Channels, Pred_len], while iTransformer needs [Batch, Time, Channels] → [Batch, Channels, D] → [Batch, Channels, S]\n\n2. **Self-Attention Module - NEW COMPONENT**: DLinear has no attention mechanism, relying purely on linear layers. iTransformer requires implementing:\n   - Multi-head self-attention operating on variate tokens: `Attention(Q, K, V)` where Q, K, V ∈ R^(N×d_k)\n   - Attention score computation revealing multivariate correlations: `A_{i,j} ∝ q_i^T k_j`\n   - Value aggregation based on correlation weights\n   - This is entirely absent in DLinear and must be built from scratch, though standard PyTorch attention modules can be adapted\n\n3. **Feed-Forward Network on Series Representations - NEW IMPLEMENTATION**: DLinear uses simple linear layers on temporal dimension, but iTransformer requires:\n   - FFN operating on the D-dimensional series representation of each variate token\n   - Stacked inverted blocks with FFN extracting temporal features within each token independently\n   - The FFN serves as both encoder (observed series) and decoder (future series) through dense connections\n   - Implementation needs: `FFN(h_n) = W_2·ReLU(W_1·h_n + b_1) + b_2` applied per-variate\n\n4. **Layer Normalization Dimension - MODIFICATION REQUIRED**: DLinear normalizes across channels implicitly through decomposition, but iTransformer requires:\n   - Explicit LayerNorm on the feature dimension of each variate token: `LayerNorm(H) = {(h_n - Mean(h_n))/sqrt(Var(h_n))}`\n   - This normalizes the D-dimensional representation per variate, not across variates or time\n   - Requires modifying the normalization axis from DLinear's approach\n\n5. **Embedding and Projection Layers - DIMENSION TRANSFORMATION**: DLinear directly maps temporal dimensions [seq_len → pred_len], but iTransformer needs:\n   - Embedding layer: MLP transforming time series R^T → representation R^D per variate\n   - Projection layer: MLP transforming representation R^D → prediction R^S per variate\n   - These are conceptually similar to DLinear's linear layers but operate on different dimensions and require MLP depth (multi-layer) rather than single linear transformation\n\n6. **Stacked Transformer Blocks - NEW ARCHITECTURE**: DLinear has a single-pass encoder, but iTransformer requires:\n   - L stacked blocks: `H^(l+1) = TrmBlock(H^l)` for l = 0,...,L-1\n   - Each block contains: LayerNorm → Self-Attention → Residual → LayerNorm → FFN → Residual\n   - Residual connections and block stacking logic not present in DLinear\n   - Need to implement the iterative refinement of variate token representations\n\n7. **Position Encoding Removal - CONCEPTUAL DIFFERENCE**: DLinear doesn't use position encoding (operates on full sequence), and iTransformer explicitly removes it because \"order of sequence is implicitly stored in neuron permutation of FFN\". This means:\n   - No need to implement positional encoding (simplification from vanilla Transformer)\n   - But requires careful FFN design to capture temporal ordering through weight structure\n   - The absence is intentional and differs from DLinear's implicit temporal encoding through linear weights"
  },
  {
    "source": "TimesNet_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "KANAD_2024",
    "type": "insufficient-information",
    "relation": null
  },
  {
    "source": "Informer_2020",
    "target": "KANAD_2024",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Philosophy**: Both papers recognize the importance of decomposing time series into components. Autoformer explicitly uses series_decomp blocks with moving average to separate trend and seasonal components, while LightTS implicitly handles different temporal patterns through continuous and interval sampling. The source implementation's series_decomp module (moving_avg + residual extraction) provides a foundation for understanding temporal pattern separation, though LightTS achieves this through sampling rather than explicit decomposition.\n\n2. **Multi-scale Temporal Pattern Capture**: Both architectures capture patterns at different temporal scales. Autoformer uses Auto-Correlation to discover period-based dependencies at various lags (top-k delays via FFT), while LightTS uses continuous sampling (C consecutive tokens for local patterns) and interval sampling (tokens with fixed intervals for global patterns). The source's Auto-Correlation mechanism with FFT-based period discovery (torch.fft.rfft in implementation) demonstrates efficient multi-scale pattern extraction that conceptually aligns with LightTS's dual-sampling strategy.\n\n3. **Embedding and Feature Projection**: Both use embedding layers to transform raw input into learnable representations. Autoformer's DataEmbedding_wo_pos (TokenEmbedding via Conv1d + TemporalEmbedding) can be partially reused for LightTS's initial feature extraction. The Conv1d-based TokenEmbedding in the source code provides a starting point for temporal feature extraction before sampling operations.\n\n4. **Multi-head Architecture for Feature Diversity**: Autoformer's multi-head Auto-Correlation (splitting d_model into h heads) parallels LightTS's use of multiple IEBlocks with different sampling strategies. The source's multi-head projection pattern (query/key/value projections + concatenation) demonstrates how to extract diverse feature representations, which can guide implementing LightTS's parallel continuous/interval sampling branches.\n\n5. **Normalization and Regularization**: Both employ LayerNorm and Dropout for training stability. The source's my_Layernorm (special seasonal part normalization) and standard dropout usage can be directly reused in LightTS's IEBlocks for feature normalization between temporal and channel projections.",
    "differences": "1. **Core Architecture Paradigm - Transformer vs. Pure MLP**: Autoformer is fundamentally a Transformer variant with encoder-decoder architecture, attention mechanisms, and residual connections. LightTS is a pure MLP-based architecture with no attention mechanisms. NEW IMPLEMENTATION NEEDED: Replace the entire Transformer structure (AutoCorrelationLayer, Encoder, Decoder) with stacked IEBlocks that consist of three sequential MLPs: temporal projection (H→F'), channel projection (W→W), and output projection (F'→F). The bottleneck design (F' << F, H) must be implemented from scratch as it has no equivalent in Autoformer.\n\n2. **Sampling Strategy - Auto-Correlation vs. Explicit Sampling**: Autoformer discovers temporal patterns through Auto-Correlation mechanism using FFT to compute autocorrelation R(τ) and aggregate time-delayed series. LightTS uses explicit continuous and interval sampling to downsample sequences into non-overlapping sub-sequences. NEW IMPLEMENTATION NEEDED: Implement continuous sampling (Equation 1: consecutive C tokens) and interval sampling (Equation 2: C tokens with ⌊T/C⌋ interval) to transform input X_t ∈ R^T into matrices X_t^con, X_t^int ∈ R^(C×T/C). This requires custom tensor reshaping operations not present in Autoformer.\n\n3. **Information Flow - Series-wise Connection vs. Dimension-wise Projection**: Autoformer uses series-wise connections through time delay aggregation (Roll operation + weighted sum by softmax-normalized autocorrelation). LightTS uses dimension-wise projections where MLPs are applied alternately along temporal (columns) and channel (rows) dimensions with weight sharing. NEW IMPLEMENTATION NEEDED: Implement IEBlock with three projection stages: (1) MLP^(H→F') applied to each column with shared weights, (2) MLP^(W→W) applied to each row with shared weights, (3) MLP^(F'→F) applied to each column. This projection pattern with explicit weight sharing across dimensions is entirely different from Autoformer's attention-based aggregation.\n\n4. **Decomposition Architecture - Progressive Accumulation vs. Feature Concatenation**: Autoformer accumulates trend components progressively through decoder layers (T_de^l = T_de^(l-1) + W_l,1*T_de^l,1 + ...) while refining seasonal components. LightTS concatenates features from different sampling strategies and variables without explicit trend/seasonal separation. NEW IMPLEMENTATION NEEDED: Replace Autoformer's decomposition blocks and trend accumulation with simple feature concatenation (2F×N matrix from continuous/interval features) followed by final IEBlock-C for prediction. No progressive refinement or separate trend/seasonal modeling is required.\n\n5. **Forecasting Mechanism - Autoregressive Decoder vs. Direct Prediction**: Autoformer uses an autoregressive decoder with encoder-decoder cross-attention, taking X_des (seasonal initialization) and X_det (trend initialization) as inputs with length I/2+O. LightTS directly predicts the entire horizon L from extracted features without autoregressive steps or decoder. NEW IMPLEMENTATION NEEDED: Implement direct prediction by mapping concatenated features (2F×N) to output (L×N) through IEBlock-C, eliminating the need for decoder initialization, cross-attention, and iterative refinement. The prediction head is a simple dimension transformation rather than Autoformer's complex decoder with multiple decomposition stages.\n\n6. **Computational Efficiency Design - FFT-based Correlation vs. Bottleneck MLP**: Autoformer achieves O(L log L) complexity through FFT-based autocorrelation computation (Wiener-Khinchin theorem). LightTS achieves efficiency through bottleneck architecture (F' << H, F) in IEBlocks and downsampling (T→C×T/C). NEW IMPLEMENTATION NEEDED: Implement bottleneck MLPs where temporal projection reduces dimension from H to F' before expensive channel projection, then expands to F. This bottleneck design (reducing computation from H×W to F'×W projections) has no equivalent in Autoformer's FFT-based efficiency approach and must be carefully tuned for the speed-accuracy tradeoff."
  },
  {
    "source": "Informer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Forecasting Task Domain**: Both papers tackle multivariate time series forecasting (MTSF) with long sequence inputs. Informer focuses on Long Sequence Time-Series Forecasting (LSTF) with encoder-decoder architecture, while LightTS addresses the same LSTF problem but with a pure MLP approach. The source implementation's data loading, preprocessing pipelines (DataEmbedding with temporal/positional encoding), and evaluation metrics (MSE, MAE) can be directly reused for LightTS experiments.\n\n2. **Encoder-Decoder Paradigm Awareness**: Although LightTS uses pure MLP, both papers acknowledge the encoder-decoder structure for sequence-to-sequence prediction. Informer's encoder extracts robust long-range dependencies, and decoder generates predictions with start tokens. LightTS's two-part architecture (Part I: feature extraction per variable, Part II: inter-variable correlation learning) mirrors this encoder-decoder philosophy. The source's generative inference approach (feeding start tokens + target placeholders) can inform LightTS's input preparation, especially for multi-step forecasting where concatenating historical context is critical.\n\n3. **Multi-Scale Temporal Pattern Capture**: Informer uses self-attention distilling with Conv1D and max-pooling to progressively downsample feature maps, capturing hierarchical temporal patterns (see ConvLayer with kernel_size=3, stride=2). LightTS employs continuous sampling (local short-term patterns) and interval sampling (long-term global patterns) to achieve similar multi-scale feature extraction. The source's distilling operation (Eq. 5) conceptually aligns with LightTS's sampling strategies—both reduce temporal dimensionality while preserving critical information. The implementation of downsampling layers (ConvLayer) can guide LightTS's sampling module design, particularly the bottleneck architecture in IEBlock where temporal projection reduces dimension before channel projection.\n\n4. **Feature Dimension Reduction and Projection**: Informer's decoder uses linear projection (nn.Linear) to map d_model features to output dimension c_out. LightTS similarly uses linear mappings (temporal projection: R^H → R^F', output projection: R^F' → R^F) in IEBlock. The source's projection layer implementation (self.projection in Decoder) can be reused for LightTS's down-projection (R^(T/C) → R) and IEBlock's MLP layers.\n\n5. **Handling Variable-Length Sequences and Batching**: Both papers process batched multivariate time series with shape [B, L, N] (batch, length, variables). Informer's embedding layers (DataEmbedding) handle temporal encoding and normalization, which is reusable for LightTS preprocessing. The source's forward pass structure (x_enc, x_mark_enc for encoder input) can inform LightTS's input formatting, especially when integrating temporal features like time-of-day or day-of-week encodings.",
    "differences": "1. **Core Architecture Paradigm - Transformer vs. Pure MLP**: Informer's innovation lies in ProbSparse self-attention (Eq. 2-4) with O(L log L) complexity, using query sparsity measurement (KL divergence-based) to select dominant queries. The source implementation includes ProbAttention module with _prob_QK() for sampling and _update_context() for sparse attention computation. LightTS completely abandons attention mechanisms, instead using **pure MLP layers with weight sharing across temporal/channel dimensions**. For reproduction, the entire ProbAttention, AttentionLayer, and self-attention distilling components must be replaced with: (a) continuous/interval sampling functions (Eq. 1-2) to transform [B, T, N] → [B, C, T/C, N], (b) IEBlock modules with three sequential MLPs (temporal projection, channel projection, output projection) as shown in Figure 2, and (c) bottleneck design where F' << F to reduce computational cost.\n\n2. **Sampling Strategy - Self-Attention Distilling vs. Continuous/Interval Sampling**: Informer uses Conv1D + MaxPool (ConvLayer in source) to progressively halve sequence length across encoder layers, creating a pyramid structure. This is a **learned downsampling** where convolutional filters adaptively extract features. LightTS uses **fixed, non-learned sampling**: continuous sampling (Eq. 1) groups consecutive tokens [x_{t-T+(j-1)C+1}, ..., x_{t-T+jC}], while interval sampling (Eq. 2) selects tokens with fixed stride [x_{t-T+j}, x_{t-T+j+⌊T/C⌋}, ...]. These are **deterministic transformations** without trainable parameters. Implementation requires: (a) new sampling functions that reshape input tensors according to Eq. 1-2, (b) removal of ConvLayer modules from encoder, and (c) parallel processing of both sampling outputs (concatenated in Part II).\n\n3. **Information Exchange Mechanism - Multi-Head Attention vs. Row/Column MLP**: Informer's multi-head attention (AttentionLayer with n_heads) computes Q, K, V projections and performs scaled dot-product attention across all positions. The source's forward() in AttentionLayer shows query/key/value projections followed by inner_attention computation. LightTS's IEBlock exchanges information through: (a) **temporal projection** (MLP applied column-wise on R^H → R^F'), (b) **channel projection** (MLP applied row-wise on R^W → R^W), and (c) **output projection** (MLP applied column-wise on R^F' → R^F). This is fundamentally different—no query-key-value mechanism, no attention scores. Implementation needs: (a) three separate MLP modules per IEBlock, (b) weight sharing across columns (temporal/output projection) and rows (channel projection), (c) bottleneck architecture where middle layer dimension F' is much smaller than input H and output F.\n\n4. **Two-Part Architecture for Inter-Variable Correlation**: Informer processes all variables jointly from the start via encoder input [B, L, N*d_model] (where N variables are embedded together). The source's enc_embedding handles multivariate input as a whole. LightTS uses a **two-stage approach**: Part I treats each of N time series independently (applying IEBlock-A/B to each variable separately), extracting features R^F per variable; Part II concatenates all N variables' features into [B, 2F, N] and applies IEBlock-C to learn inter-variable correlations. This requires: (a) loop/vectorized operation over N variables in Part I, (b) concatenation along channel dimension after Part I, (c) IEBlock-C specifically designed for [2F × N] input (2F from continuous+interval sampling features), and (d) final output projection to [L × N] for multi-step forecasting.\n\n5. **Decoder and Prediction Strategy**: Informer uses a standard Transformer decoder with masked self-attention and cross-attention to encoder outputs. The source's Decoder includes DecoderLayer with self_attention (masked) and cross_attention mechanisms, plus generative inference with start tokens (Eq. 6). LightTS has **no explicit decoder**—Part II's IEBlock-C directly outputs predictions [L × N] without autoregressive generation or teacher forcing. Implementation changes: (a) remove entire Decoder module and masked attention, (b) IEBlock-C's output projection directly maps to forecasting horizon L, (c) no need for start tokens or target placeholders (x_dec, x_mark_dec in source), (d) single forward pass for all L future steps (vs. Informer's generative-style inference).\n\n6. **Temporal Encoding and Positional Information**: Informer's DataEmbedding includes learnable positional encoding and temporal feature embedding (time-of-day, day-of-week, etc.) via TemporalEmbedding. The source implementation shows positional encoding added to value embeddings. LightTS paper does not explicitly describe temporal encoding mechanisms—the focus is on raw value patterns captured by sampling and MLP. For reproduction, need to decide: (a) whether to add temporal features (if yes, how to integrate into IEBlock input), (b) whether positional encoding is needed (likely not, since sampling inherently preserves temporal order), (c) if temporal features are added, they should be concatenated to input before Part I processing."
  },
  {
    "source": "Reformer_2020",
    "target": "LightTS_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Architecture**: Both papers adopt a decomposition-based framework to separate complex time series patterns. FEDformer uses MOEDecomp (Mixture of Experts Decomposition) with multiple average pooling filters of different sizes to extract trend components, while MICN uses MHDecomp (Multi-scale Hybrid Decomposition) with similar multi-kernel averaging strategy. The core decomposition logic from FEDformer's `series_decomp` class can be directly reused as a foundation, requiring only modification to support MICN's mean-based aggregation instead of learned weights.\n\n2. **Multi-scale Feature Processing**: Both architectures explicitly model different temporal scales. FEDformer's frequency-based approach (selecting different Fourier/Wavelet modes) and MICN's multi-branch convolution with different kernel sizes (I/4, I/8, etc.) share the conceptual goal of capturing patterns at multiple resolutions. The multi-scale branching structure in FEDformer's implementation (particularly the wavelet decomposition's recursive mechanism) provides a template for implementing MICN's multi-branch MIC layer architecture.\n\n3. **Embedding Strategy**: Both papers use similar input embedding approaches combining temporal features, positional encoding, and value embedding. FEDformer's `DataEmbedding` class that sums TFE (time features), PE (positional encoding), and VE (value embedding) can be directly reused for MICN with minimal modifications, as MICN follows the same three-part embedding design (Equation 6 in MICN).\n\n4. **Encoder-Only Architecture for Forecasting**: While FEDformer uses encoder-decoder, both papers can leverage encoder-style stacked layers for feature extraction. The general layer stacking logic, normalization patterns, and residual connections from FEDformer's `EncoderLayer` provide structural guidance for implementing MICN's stacked MIC layers.\n\n5. **Trend-Seasonal Separation and Reconstruction**: Both architectures separate trend and seasonal components, process them differently, and recombine them for final predictions. FEDformer's trend accumulation mechanism in the decoder (Equation 2) provides implementation patterns for MICN's trend-cyclical prediction block, though MICN uses simpler linear regression or mean-based prediction instead of progressive accumulation.",
    "differences": "1. **Core Attention Mechanism vs. Convolution-Based Processing**: FEDformer's fundamental innovation is frequency-enhanced attention (FEB-f/FEB-w and FEA-f/FEA-w) operating in Fourier/Wavelet domains with O(L) complexity, using FFT/DWT transforms and learnable frequency mode selection. MICN completely abandons attention mechanisms in favor of **Isometric Convolution** - a novel causal convolution variant that pads sequences with (S-1) zeros and uses kernel=S to capture global correlations. This requires implementing: (a) Custom isometric convolution layers with specific padding logic (Figure 4), (b) Local-global module combining downsampling Conv1d and upsampling Conv1dTranspose, (c) Conv2d-based merging of multi-scale branches instead of attention-based fusion. The entire frequency domain processing pipeline from FEDformer (FFT, mode selection, complex multiplication) is NOT needed.\n\n2. **Decoder Architecture and Placeholder Strategy**: FEDformer uses a traditional encoder-decoder with the decoder input containing the latter half of encoder input (length I/2) plus O placeholders, requiring cross-attention between encoder and decoder outputs. MICN eliminates the decoder entirely, using a simpler **complementary O strategy** where input X_s is directly concatenated with X_zero (O-length zero placeholders) before embedding, then processed through stacked MIC layers. This requires: (a) Removing all decoder and cross-attention components, (b) Implementing direct concatenation of input with zero padding (Equation 5), (c) Designing MIC layers that process the full (I+O) length sequence simultaneously, (d) Adding truncation operation to extract final O-length predictions.\n\n3. **Multi-Scale Feature Extraction Method**: FEDformer's multi-scale comes from selecting different frequency modes (modes=64 by default) in Fourier/Wavelet domain with fixed mode selection. MICN implements multi-scale through **multiple parallel branches** with different downsampling rates (kernel sizes i ∈ {I/4, I/8, ...}) in the spatial domain. This requires implementing: (a) Multi-branch architecture where each branch has different Conv1d kernel sizes for local feature compression (Equation 7), (b) Branch-specific isometric convolution with kernel matching the compressed sequence length, (c) Conv2d-based weighted merging of branches (Equation 9) instead of simple concatenation or attention-based fusion, (d) Ensuring all branches maintain (I+O) output length through careful stride and padding settings.\n\n4. **Trend-Cyclical Prediction Approach**: FEDformer accumulates trend components progressively through the decoder layers using learnable projections (W_{l,i} in Equation 2) and does not explicitly predict trend independently. MICN proposes two explicit trend prediction methods: (a) **MICN-regre**: Linear regression on extracted trend-cyclical component X_t (Equation 3), requiring implementation of regression-based forecasting module, (b) **MICN-mean**: Simple mean-based constant trend prediction (Equation 4). Both are standalone prediction blocks that operate independently from the seasonal prediction, requiring separate implementation of these lightweight trend forecasting modules.\n\n5. **Decomposition Block Design Philosophy**: FEDformer's MOEDecomp uses Softmax(L(x)) to learn data-dependent weights for mixing multiple trend components from different kernel sizes, where weights are learned through a neural network. MICN's MHDecomp uses simple **mean operation** to aggregate trends from different kernels (Equation 2), arguing that pattern weights should be determined after feature learning rather than before. This requires: (a) Replacing FEDformer's learnable weight mechanism with simple averaging, (b) Moving the weighting logic to the Conv2d merge operation in the seasonal prediction block, (c) Implementing the mean-based aggregation for multi-kernel decomposition.\n\n6. **Global Correlation Modeling**: FEDformer models global dependencies through frequency domain attention (computing attention in Fourier/Wavelet space with selected modes), which inherently captures long-range dependencies through frequency components. MICN models global correlations through **spatial domain isometric convolution** with large kernels (kernel=S for sequence length S), claiming it provides better inductive bias (translation equivariance) and generalization than attention for shorter sequences. This requires: (a) Implementing isometric convolution with dynamic kernel size based on input length, (b) Proper zero-padding strategy (S-1 zeros) to maintain causality, (c) Residual connections and normalization specific to isometric convolution (Equation 8), (d) No frequency domain transformations or complex number operations."
  },
  {
    "source": "Autoformer_2021",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Philosophy**: Both papers adopt decomposition-based architectures to separate complex temporal patterns into trend-cyclical and seasonal components. Autoformer uses `series_decomp` with moving average (Equation 1), while MICN extends this with Multi-scale Hybrid Decomposition (MHDecomp, Equation 2). The core `moving_avg` and `series_decomp` classes from Autoformer can be directly reused as foundation for MICN's MHDecomp - only need to extend it to support multiple kernels and mean aggregation instead of single kernel.\n\n2. **Progressive Decomposition Strategy**: Both employ decomposition as inner blocks throughout the architecture rather than one-time preprocessing. Autoformer's encoder/decoder layers (Equations 3-4) apply `SeriesDecomp()` after each attention/feedforward block. MICN similarly applies decomposition progressively in its Seasonal Prediction Block. The implementation pattern of wrapping decomposition around transformation blocks can be reused, though MICN replaces attention with convolution-based Local-Global modules.\n\n3. **Embedding Architecture**: Both use similar multi-component embedding strategies. Autoformer's `DataEmbedding_wo_pos` combines value embedding (TokenEmbedding with Conv1d), temporal embedding, and positional encoding. MICN's Equation 6 uses TFE + PE + VE with the same conceptual structure. The `TokenEmbedding`, `TemporalEmbedding`, `PositionalEmbedding` classes from Autoformer can be directly adapted for MICN's embedding layer, requiring only minor modifications to handle the concatenated input format (Concat(X_s, X_zero)).\n\n4. **Trend Modeling Approach**: Both models handle trend-cyclical components separately from seasonal patterns. Autoformer accumulates trend components through decoder layers (Equation 4, last line with W_l weights). MICN simplifies this with linear regression or mean strategies (Equations 3-4). The trend accumulation logic and linear projection layers from Autoformer's decoder can be adapted for MICN's simpler Trend-cyclical Prediction Block.\n\n5. **Normalization and Residual Connections**: Both employ LayerNorm and residual connections extensively. Autoformer's `my_Layernorm` for seasonal parts and residual additions in encoder/decoder layers parallel MICN's Norm operations in Equations 8-9. The normalization infrastructure and residual connection patterns from Autoformer can be directly reused in MICN's MIC layers.",
    "differences": "1. **Core Mechanism - Auto-Correlation vs. Isometric Convolution**: Autoformer's fundamental innovation is the Auto-Correlation mechanism (Equations 5-7) using FFT-based autocorrelation to discover period-based dependencies and time delay aggregation. This requires implementing `AutoCorrelation`, `AutoCorrelationLayer` classes with FFT operations and Roll aggregation. MICN completely replaces this with **Isometric Convolution** (Figure 4, Equation 8) - a causal convolution variant that pads sequence with S-1 zeros and uses kernel=S for global correlation modeling. NEW IMPLEMENTATION NEEDED: Custom IsometricConv layer with zero-padding logic, different from standard Conv1d. This is MICN's core contribution and has no equivalent in Autoformer.\n\n2. **Multi-scale Architecture - Series-wise vs. Multi-branch Convolution**: Autoformer achieves series-wise connections through autocorrelation across full sequences with O(L log L) complexity. MICN introduces **Multi-scale Isometric Convolution (MIC)** with parallel branches using different scale sizes (i ∈ {I/4, I/8, ...}, Equation 7). NEW IMPLEMENTATION NEEDED: (a) Multi-branch architecture where each branch has different avgpool kernel sizes, (b) Local module with downsampling Conv1d (stride=kernel=i), (c) Global module with IsometricConv followed by Conv1dTranspose upsampling, (d) Conv2d-based merge layer (Equation 9) to weight different patterns. Autoformer has no multi-scale branching structure.\n\n3. **Input Strategy - Encoder-Decoder vs. Zero-Padding**: Autoformer uses traditional encoder-decoder with latter half of encoder input (I/2) + placeholders (Equation 2: Concat(X_ens, X_0) and Concat(X_ent, X_Mean)). MICN simplifies to **direct zero-padding** (Equation 5: Concat(X_s, X_zero)) without encoder-decoder separation. NEW IMPLEMENTATION NEEDED: Remove Autoformer's Encoder/Decoder classes and implement single-path architecture with zero-padded input directly fed to stacked MIC layers. This eliminates cross-attention and simplifies the forward pass significantly.\n\n4. **Attention vs. Pure Convolution**: Autoformer relies on attention mechanism (AutoCorrelationLayer with Q/K/V projections, multi-head structure in Equation 7). MICN uses **pure convolution-based Local-Global module** (Figure 3) without any attention mechanism. NEW IMPLEMENTATION NEEDED: (a) Local module: Avgpool → Conv1d downsampling pipeline, (b) Global module: IsometricConv → Tanh activation → Conv1dTranspose upsampling pipeline, (c) Remove all attention-related components (query_projection, key_projection, value_projection, multi-head logic) from Autoformer. This represents a fundamental architectural shift from attention-based to convolution-based modeling.\n\n5. **Seasonal Prediction - Stacked Decomposition vs. MIC Layers**: Autoformer's decoder stacks decomposition blocks after self-attention and cross-attention (Equation 4 with three decomposition stages). MICN's Seasonal Prediction Block stacks **N MIC layers** (Equation 5) where each layer contains multi-branch Local-Global modules + Conv2d merge + FeedForward. NEW IMPLEMENTATION NEEDED: (a) MIC layer class encapsulating multi-branch structure with different scales, (b) Local-Global module as described above, (c) Conv2d merge operation to combine branches (replacing simple concatenation), (d) Stacking logic for N MIC layers with residual connections. The entire seasonal modeling pipeline is fundamentally different from Autoformer's attention-based decoder.\n\n6. **Trend Prediction Simplification**: Autoformer accumulates trend through all decoder layers with learnable projectors W_l (Equation 4, last line). MICN offers two simpler alternatives: **linear regression** (Equation 3, MICN-regre) or **mean strategy** (Equation 4, MICN-mean). NEW IMPLEMENTATION NEEDED: Implement simple linear regression layer or mean operation for trend prediction, removing Autoformer's complex trend accumulation with multiple projectors across decoder layers. This is a deliberate simplification in MICN's design.\n\n7. **Multi-scale Hybrid Decomposition**: Autoformer uses single-kernel moving average decomposition. MICN extends to **MHDecomp with multiple kernels** (Equation 2) using mean aggregation across different kernel sizes (kernel_1, ..., kernel_n). NEW IMPLEMENTATION NEEDED: Extend Autoformer's `series_decomp` to support multiple kernels, compute trend-cyclical for each kernel, then average them. The multi-kernel strategy aligns with multi-scale information in MIC layers and requires coordinating kernel sizes across decomposition and convolution branches."
  },
  {
    "source": "Informer_2020",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Time Series Forecasting Task Framework**: Both papers address long-term time series forecasting (LSTF) problems with the same input-output paradigm (predict future O points based on past I points). The basic data flow structure from Informer (input embedding → feature extraction → output projection) can be reused as the skeleton for MICN implementation.\n\n2. **Embedding Layer Architecture**: Both models use similar multi-component embedding strategies. Informer's DataEmbedding includes value embedding, positional encoding, and temporal features encoding (MinuteOfHour, HourOfDay, etc.). MICN explicitly uses the same three components (VE, PE, TFE) as shown in Equation 6. The embedding implementation code from Informer can be directly reused with minimal modifications.\n\n3. **Series Decomposition Philosophy**: Both adopt time series decomposition to separate trend and seasonal components. Informer uses moving average in its encoder layers, while MICN uses MHDecomp block with multiple kernels. The core AvgPool operation with padding from Informer's implementation provides the foundation for MICN's multi-scale decomposition.\n\n4. **Normalization and Residual Connections**: Both architectures heavily use LayerNorm and residual connections throughout their networks. Informer's encoder/decoder layers include norm layers after attention blocks, and MICN uses Norm operations in its local-global modules (Equation 8-9). The normalization patterns from Informer can be adapted.\n\n5. **Placeholder Strategy for Future Prediction**: Both use zero-filled placeholders for future time steps. Informer's decoder uses X_0 placeholders (Equation 6 in Informer), and MICN concatenates X_zero with input series (Equation 5 in MICN). This generative prediction approach is shared and Informer's implementation pattern can guide MICN's input construction.\n\n6. **Multi-layer Stacking Architecture**: Both employ multiple layers of feature extraction blocks. Informer stacks EncoderLayers with self-attention distilling, while MICN stacks N MIC layers. The layer stacking logic and forward propagation pattern from Informer's Encoder class can be adapted for MICN's MIC layer stacking.",
    "differences": "1. **Core Attention Mechanism vs. Convolution-Based Feature Extraction**: Informer's innovation is ProbSparse self-attention with O(L log L) complexity using query sparsity measurement (Equation 2-4), requiring implementation of custom attention masks, Top-u query selection, and max-mean measurement. MICN completely abandons attention mechanisms and uses **isometric convolution** as the core operation for global correlation modeling (Figure 4, Equation 8). NEW implementation needed: (a) Isometric convolution layer with padding of S-1 length and kernel=S; (b) Local module with Conv1d downsampling at different scales; (c) Global module with IsometricConv + Conv1dTranspose upsampling.\n\n2. **Multi-Scale Feature Extraction Strategy**: Informer uses self-attention distilling with MaxPool (stride=2) to progressively halve sequence length (Equation 5), creating a pyramid structure. MICN uses **multi-scale isometric convolution (MIC)** with parallel branches at different scales {I/4, I/8, ...} (Equation 7). NEW implementation needed: (a) Parallel branch architecture with different kernel sizes; (b) AvgPool with multiple kernels for pattern separation; (c) Conv2d-based merge operation to weight different scale features (Equation 9) instead of simple concatenation.\n\n3. **Trend-Cyclical Prediction Approach**: Informer doesn't explicitly model trend separately; it relies on encoder-decoder architecture for full prediction. MICN introduces a **dedicated Trend-Cyclical Prediction Block** with two variants: (a) Linear regression strategy (Equation 3, MICN-regre); (b) Mean strategy (Equation 4, MICN-mean). NEW implementation needed: Separate trend prediction module that operates on decomposed X_t and generates Y_t independently from seasonal prediction.\n\n4. **Decoder Architecture and Prediction Generation**: Informer uses a standard Transformer decoder with masked multi-head attention, cross-attention with encoder outputs, and iterative token generation (though optimized with start tokens). MICN eliminates the encoder-decoder structure entirely, using a **single-path seasonal prediction block** that directly outputs O-length predictions through MIC layers + Projection + Truncate (Equation 5, 10). NEW implementation needed: Direct prediction path without cross-attention or decoder layers.\n\n5. **Multi-Scale Hybrid Decomposition (MHDecomp)**: Informer uses single-kernel moving average for decomposition. MICN's MHDecomp uses **multiple AvgPool kernels simultaneously** and averages the results (Equation 2). NEW implementation needed: Multi-kernel decomposition block that applies different kernel sizes {kernel_1, ..., kernel_n} and computes mean of multiple trend-cyclical patterns.\n\n6. **Local-Global Module Design**: Informer processes sequences globally through attention mechanisms. MICN's innovation is the **local-global module** (Figure 3, Equation 7-8) that explicitly separates local feature extraction (Conv1d with stride=kernel for compression) from global correlation modeling (isometric convolution on compressed features). NEW implementation needed: Two-stage processing where local Conv1d reduces sequence length by factor i, then isometric convolution models global dependencies, followed by Conv1dTranspose to restore original length.\n\n7. **Feature Merging Strategy**: Informer concatenates multi-head attention outputs and applies linear projection. MICN uses **Conv2d to merge multi-scale branch outputs with learnable weights** (Equation 9) instead of fixed concatenation. NEW implementation needed: Conv2d-based weighted merging layer that learns importance of different scale patterns during training.\n\n8. **Memory Efficiency Approach**: Informer achieves O((2-ε)L log L) memory through attention sparsity and distilling layers that progressively downsample. MICN achieves efficiency through **convolution operations** which have better memory characteristics than attention for long sequences, with local compression reducing intermediate sequence lengths to (I+O)/i. The memory optimization strategies are fundamentally different and require separate implementation approaches."
  },
  {
    "source": "Reformer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanism Foundation**: Both papers address the O(L²) complexity problem of standard Transformer attention. Reformer's LSH attention reduces complexity to O(L log L) while Pyraformer achieves O(L). The source implementation's `LSHSelfAttention` module and the concept of restricting attention to subsets of keys (rather than all keys) provides a foundational understanding for implementing Pyraformer's sparse pyramidal attention. The bucketing/hashing logic in Reformer (grouping similar queries/keys) conceptually parallels Pyraformer's graph-based attention where nodes only attend to neighbors in the pyramidal structure.\n\n2. **Multi-Head Attention Architecture**: Both utilize multi-head attention mechanisms where queries, keys, and values are linearly projected through learned transformations (W_Q, W_K, W_V). The source code's `ReformerLayer` wrapping the attention mechanism within encoder layers can be directly adapted - Pyraformer similarly needs encoder layers that wrap its pyramidal attention module. The implementation pattern of `EncoderLayer` containing attention + feed-forward + normalization is reusable.\n\n3. **Embedding and Positional Encoding**: Both papers embed input sequences with learned embeddings plus positional information. The source implementation's `DataEmbedding` class that combines data embedding with positional encoding can be directly reused for Pyraformer. Both models require encoding temporal positions to maintain sequence order awareness, though Pyraformer adds covariate embeddings (hour-of-day, etc.).\n\n4. **Encoder-Decoder Structure with Normalization**: Both employ stacked encoder layers with LayerNorm. The source code's `Encoder` class with multiple `EncoderLayer` instances and final normalization layer provides the exact structural template needed for Pyraformer's encoder stack. The activation functions (GELU), dropout, and feed-forward network structure are identical design patterns.\n\n5. **Reversible/Memory-Efficient Design Philosophy**: While Reformer uses reversible layers (RevNet) to reduce memory from O(n_l * b * l * d_model) to O(b * l * d_model), Pyraformer achieves memory efficiency through sparse attention patterns. Both share the goal of enabling longer sequence processing. The chunking strategy in Reformer (processing feed-forward in chunks) conceptually relates to Pyraformer's scale-wise processing of different temporal resolutions.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: \n   - **Reformer**: Uses Locality-Sensitive Hashing (LSH) to cluster similar queries/keys into buckets, then performs attention within buckets. Requires implementing hash functions h(x) = argmax([xR; -xR]), bucket sorting, and chunk-based attention.\n   - **Pyraformer**: Implements Pyramidal Attention Module (PAM) with explicit multi-scale graph structure. **MUST IMPLEMENT**: (a) Pyramidal graph construction with C-ary tree structure where each node attends to neighbors N_ℓ^(s) = A_ℓ^(s) ∪ C_ℓ^(s) ∪ P_ℓ^(s) (adjacent nodes, children, parent), (b) Custom sparse attention pattern following Equation (3) that only computes attention for nodes in the neighborhood set, (c) Custom CUDA kernel using TVM for efficient sparse attention (as standard PyTorch doesn't support this pattern efficiently), (d) Scale-specific attention masks to enforce pyramidal connectivity.\n\n2. **Multi-Resolution Feature Extraction - NEW MODULE REQUIRED**:\n   - **Reformer**: Operates on single-scale sequence representations. No explicit multi-resolution processing.\n   - **Pyraformer**: **MUST IMPLEMENT** Coarser-Scale Construction Module (CSCM): (a) Sequential 1D convolutions with kernel size C and stride C to create coarser scales, (b) Bottleneck structure with dimension reduction before convolutions and restoration after, (c) Construction of C-ary tree where scale s has length L/C^s, (d) Concatenation of all scales (fine-to-coarse) before feeding to PAM. This requires implementing convolution layers that progressively downsample the sequence to create the pyramid structure shown in Figure 3.\n\n3. **Sequence Length Handling and Padding**:\n   - **Reformer**: Requires sequence length divisible by (bucket_size * 2). The `fit_length` method pads sequences with zeros to meet this constraint.\n   - **Pyraformer**: **MUST IMPLEMENT**: Padding logic to ensure L is divisible by C^(S-1) where S is number of scales and C is the arity of the tree. The constraint from Lemma 1 (Equation 4) must be satisfied: L/C^(S-1) - 1 ≤ (A-1)N/2, where A is adjacent nodes, N is number of layers.\n\n4. **Prediction/Decoding Module - NEW ARCHITECTURE REQUIRED**:\n   - **Reformer**: Uses simple linear projection from flattened encoder output for classification: `output.reshape(B, -1)` → `projection(output)`.\n   - **Pyraformer**: **MUST IMPLEMENT** two distinct prediction modules: (a) **Single-step**: Add end token (z_{t+1}=0), gather last nodes from all scales, concatenate, and predict through FC layer, (b) **Multi-step**: Either batch prediction of all M steps from last nodes, OR decoder with two full attention layers where prediction tokens F_p query encoder output F_e. The decoder architecture requires implementing cross-attention between future time steps and multi-scale encoder features.\n\n5. **Graph-Based vs Hash-Based Information Flow**:\n   - **Reformer**: Information flows through hash bucket assignments. Similar queries attend to similar keys based on LSH clustering. The `n_hashes` parameter controls multiple rounds of hashing to reduce collision probability.\n   - **Pyraformer**: **MUST IMPLEMENT**: Explicit graph-based message passing where: (a) Inter-scale connections form C-ary tree (parent-child relationships), (b) Intra-scale connections link adjacent nodes (within distance (A-1)/2), (c) Maximum path length is O(S + L/C^(S-1)/A) as per Proposition 2, (d) Node features must be aggregated across scales. This requires implementing graph adjacency matrices or efficient indexing to determine which nodes can attend to which based on pyramidal structure.\n\n6. **Complexity Guarantees and Hyperparameter Constraints**:\n   - **Reformer**: Complexity O(L log L) with hyperparameters bucket_size and n_hashes controlling accuracy-efficiency tradeoff.\n   - **Pyraformer**: **MUST IMPLEMENT**: Complexity O(AL) where A is constant adjacent nodes (3 or 5). Requires enforcing constraints from Equations (4) and (5) to ensure global receptive field and O(1) maximum path length. Hyperparameters S (scales), C (arity), A (adjacency), N (layers) must satisfy mathematical relationships for theoretical guarantees.\n\n7. **Covariate Integration**:\n   - **Reformer**: No explicit covariate handling in the implementation.\n   - **Pyraformer**: **MUST IMPLEMENT**: Embedding and integration of covariates x_{t-L+1:t+M} (e.g., hour-of-day, day-of-week) alongside observations, requiring separate embedding layers and fusion strategy with temporal and positional embeddings."
  },
  {
    "source": "Informer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the encoder-decoder paradigm from Transformer for time series forecasting. The Informer implementation provides a complete encoder-decoder structure with embedding layers (DataEmbedding), encoder stacks, and decoder layers that can be directly reused. The embedding mechanism (combining value embedding, temporal embedding, and positional embedding) is identical in both approaches.\n\n2. **Multi-Head Attention Mechanism**: Both utilize multi-head attention as the core building block. Informer's AttentionLayer class with query/key/value projections and multi-head structure can be partially reused. The fundamental attention computation framework (Q, K, V transformations and scaled dot-product) remains the same, though the specific attention pattern differs.\n\n3. **Hierarchical Feature Extraction**: Both papers employ hierarchical/multi-scale feature extraction strategies. Informer uses ConvLayer with MaxPooling for distilling operations to create a pyramid-like structure in the encoder, reducing sequence length progressively (stride-2 pooling). This hierarchical downsampling concept directly aligns with Pyraformer's multi-resolution C-ary tree structure, and the ConvLayer implementation can serve as a reference for CSCM.\n\n4. **Feed-Forward Networks and Normalization**: Both use identical feed-forward network designs (two Conv1d layers with activation) and layer normalization. The DecoderLayer implementation in Informer with its conv1/conv2 structure and normalization layers can be directly reused with minimal modification.\n\n5. **Batch Processing and Training Pipeline**: Both papers handle batched time series data with the same input format (batch_size, seq_len, features). The overall training loop, loss computation (MSE), and data flow through embedding → encoder → decoder → projection can be reused from Informer's implementation.\n\n6. **Generative Inference Strategy**: Both avoid autoregressive decoding by using start tokens and predicting all future steps in one forward pass. Informer's generative inference approach with concatenated start tokens (X_token) and placeholder zeros (X_0) directly applies to Pyraformer's prediction module design.",
    "differences": "1. **Core Attention Mechanism - ProbSparse vs Pyramidal Attention**: \n   - **Informer**: Uses ProbSparse attention that selects Top-u queries based on sparsity measurement M(q,K), maintaining O(L log L) complexity through query sampling. The implementation includes _prob_QK for sparse query selection and _update_context for selective attention computation.\n   - **Pyraformer**: Introduces Pyramidal Attention Module (PAM) with a multi-resolution C-ary tree structure where each node attends to A adjacent nodes, C children, and 1 parent (Equation 2-3). This requires **NEW IMPLEMENTATION**: (a) Custom CUDA kernel using TVM for sparse attention pattern on pyramidal graph, (b) Neighborhood definition logic for three types of connections (adjacent, children, parent), (c) Scale-aware attention computation that operates across multiple resolutions simultaneously.\n   - **Key Implementation Gap**: The pyramidal graph attention pattern is fundamentally different from ProbSparse sampling and requires specialized sparse attention kernels that cannot be directly adapted from Informer's implementation.\n\n2. **Multi-Resolution Construction - Distilling vs CSCM**:\n   - **Informer**: Uses self-attention distilling with Conv1d (kernel=3, stride=2) and MaxPooling to progressively halve sequence length, creating replica stacks with different input lengths. The distilling operation is applied after attention blocks.\n   - **Pyraformer**: Employs Coarser-Scale Construction Module (CSCM) that explicitly builds a C-ary tree by applying convolutions with kernel size C and stride C sequentially to create scales. **NEW IMPLEMENTATION NEEDED**: (a) Bottleneck structure with dimension reduction before convolutions, (b) Scale-by-scale construction from bottom to top (not just halving), (c) Concatenation of all scales (not just replica outputs), (d) Explicit parent-child relationship maintenance for the C-ary tree structure.\n   - **Key Difference**: Pyraformer constructs explicit multi-resolution representations before attention, while Informer distills after attention.\n\n3. **Graph Structure and Connectivity**:\n   - **Informer**: Uses full attention (all-to-all) with sparse query sampling, operating on a single-scale sequence with progressively reduced length through distilling layers.\n   - **Pyraformer**: Operates on an explicit pyramidal graph with defined inter-scale (C-ary tree) and intra-scale (adjacent nodes) connections. **NEW IMPLEMENTATION**: (a) Graph construction logic defining neighborhoods N_ℓ^(s) for each node at each scale, (b) Scale indexing and node positioning system, (c) Message passing mechanism that respects the pyramidal graph topology, (d) Handling of variable-length sequences at different scales simultaneously.\n\n4. **Complexity Reduction Strategy**:\n   - **Informer**: Achieves O(L log L) through: (a) ProbSparse attention selecting u = c·ln(L_Q) queries, (b) Random sampling of U = L_K·ln(L_Q) key-value pairs for sparsity measurement, (c) Max-mean measurement for efficient top-u selection.\n   - **Pyraformer**: Achieves O(L) complexity through: (a) Fixed local connectivity (A+C+1 neighbors per node regardless of L), (b) Multi-scale decomposition reducing long-range to short-range at coarser scales, (c) O(1) maximum path length when C satisfies Equation 5. **NEW IMPLEMENTATION**: The complexity analysis and parameter selection (A, C, S, N) based on Lemma 1, Proposition 1-2 need to be implemented to ensure theoretical guarantees.\n\n5. **Prediction Module Design**:\n   - **Informer**: Uses a standard decoder with masked multi-head attention and cross-attention, with a single fully connected layer for final projection. The decoder processes the concatenated start token and placeholder sequence.\n   - **Pyraformer**: Proposes two distinct prediction modules: **NEW IMPLEMENTATION NEEDED**: (a) Single/Multi-step module: gathers features from last nodes at ALL scales, concatenates them, and uses FC layer for prediction, (b) Decoder module: uses two full attention layers where the first takes prediction tokens as query and encoder output as key/value, the second takes first layer output as query but concatenated first layer output and encoder output as key/value. This dual-attention design with explicit historical information feeding is fundamentally different.\n\n6. **Scale Management and Feature Aggregation**:\n   - **Informer**: Concatenates outputs from replica stacks (different input lengths) at the final encoder output, with all stacks having aligned output dimensions through progressive distilling.\n   - **Pyraformer**: **NEW IMPLEMENTATION**: (a) Maintains explicit scale indices (s=1 to S) throughout the network, (b) Performs attention computation scale-aware (nodes know their scale and neighbors' scales), (c) Aggregates features from last nodes at each scale for prediction (not just final layer output), (d) Requires bookkeeping of node positions and scale memberships throughout forward pass.\n\n7. **Positional and Temporal Encoding**:\n   - **Informer**: Uses standard sinusoidal positional encoding and temporal feature encoding (hour, day, week, month) combined with value embedding.\n   - **Pyraformer**: While using similar embedding strategy, the positional encoding must be **NEWLY ADAPTED** to: (a) Account for multi-resolution structure where coarser scales represent aggregated time periods, (b) Encode parent-child relationships in the C-ary tree, (c) Distinguish between nodes at different scales representing different temporal granularities (hourly vs daily vs weekly).\n\n8. **Parameter Complexity**:\n   - **Informer**: O(N(HD·D_K + D·D_F)) parameters, primarily from attention and feed-forward layers.\n   - **Pyraformer**: O(N(HD·D_K + D·D_F) + (S-1)C·D_K²) parameters, with additional parameters from CSCM convolutions across scales. **NEW IMPLEMENTATION**: The bottleneck structure in CSCM adds unique parameter requirements not present in Informer's distilling layers."
  },
  {
    "source": "DLinear_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Foundation**: Both papers leverage time series decomposition as a core preprocessing technique. DLinear uses the `series_decomp` module with moving average kernels to extract trend and seasonal components, which TimesNet can directly reuse for its preprocessing pipeline. The decomposition logic (seasonal_init, trend_init = self.decomposition(x)) provides a proven implementation that TimesNet can adopt before its 2D transformation step.\n\n2. **Classification Task Architecture**: Both implement time series classification through encoder-projection paradigm. DLinear's classification method shows the standard pattern: (1) encode input via `self.encoder(x_enc)`, (2) flatten to (batch_size, seq_length * d_model), (3) project to classes via `self.projection(output)`. TimesNet can reuse this exact classification head structure after its TimesBlock feature extraction, requiring only to replace the encoder with TimesBlock layers.\n\n3. **Temporal Feature Processing**: Both operate on multivariate time series with shape (batch, seq_len, channels) and preserve channel independence during processing. DLinear's permutation operations (`.permute(0, 2, 1)`) for channel-wise linear transformations demonstrate the pattern TimesNet needs when processing multiple variates through its 2D transformation - each variate can be reshaped independently while maintaining batch structure.\n\n4. **Parameter Initialization Strategy**: DLinear's weight initialization `nn.Parameter((1/self.seq_len) * torch.ones([self.pred_len, self.seq_len]))` provides a principled approach for temporal linear layers. While TimesNet uses 2D convolutions, this initialization philosophy (uniform distribution scaled by sequence length) can inform the initialization of its inception block kernels.\n\n5. **Modular Design Philosophy**: Both adopt modular architectures where core operations are encapsulated (DLinear's Linear_Seasonal/Linear_Trend, TimesNet's TimesBlock). DLinear's `individual` parameter pattern (separate parameters per channel vs shared) demonstrates a reusable design pattern that TimesNet could adopt for channel-specific vs shared 2D convolutions.",
    "differences": "1. **Core Architecture Paradigm** [NEW IMPLEMENTATION REQUIRED]: DLinear uses simple 1D temporal linear layers operating directly on flattened time series, while TimesNet requires implementing a complete 1D-to-2D transformation pipeline. This involves: (a) FFT-based period detection (`Period()` function with `torch.fft.fft`, amplitude calculation, top-k frequency selection), (b) 2D reshaping logic (`Reshape_{p_i,f_i}` with padding to make sequences compatible), (c) 2D inception blocks with multi-scale convolutions, (d) 2D-to-1D inverse transformation with truncation. None of these components exist in DLinear.\n\n2. **Multi-Periodicity Modeling** [NEW IMPLEMENTATION REQUIRED]: TimesNet's fundamental innovation is discovering and processing multiple periodicities simultaneously. This requires implementing: (a) FFT amplitude analysis across all frequencies `A = Avg(Amp(FFT(X_1D)))`, (b) Top-k frequency selection with corresponding period lengths `{f_1,...,f_k}` and `{p_1,...,p_k}`, (c) Multiple parallel 2D transformations (k different reshaped tensors), (d) Adaptive aggregation using softmax-normalized amplitudes as attention weights. DLinear has no concept of periodicity or multi-branch processing.\n\n3. **Feature Extraction Mechanism** [NEW IMPLEMENTATION REQUIRED]: DLinear uses decomposed linear transformations (separate weights for seasonal and trend), while TimesNet requires implementing the Inception block architecture with: (a) Multi-scale 2D convolutional kernels operating on (p_i, f_i, d_model) tensors, (b) Parallel conv branches with different kernel sizes, (c) Concatenation and fusion of multi-scale features, (d) Shared parameters across k different period-based 2D tensors. The 2D convolution operations fundamentally differ from DLinear's 1D linear layers.\n\n4. **Residual Connection Strategy** [PARTIAL NEW IMPLEMENTATION]: While DLinear implicitly uses residual connections through component addition (seasonal + trend), TimesNet requires explicit residual connections around each TimesBlock: `X_1D^l = TimesBlock(X_1D^{l-1}) + X_1D^{l-1}`. This requires implementing: (a) Stacked TimesBlock layers (l=1 to L), (b) Skip connections preserving original 1D structure through 2D transformation and back, (c) Embedding layer at input `X_1D^0 = Embed(X_1D)` to project to d_model dimensions.\n\n5. **Temporal Representation Philosophy** [CONCEPTUAL DIFFERENCE]: DLinear treats time series as 1D sequences where temporal relationships are captured through weighted sums along the time axis (permutation-invariant with learned weights). TimesNet fundamentally restructures time series into 2D space where rows represent interperiod-variation and columns represent intraperiod-variation, enabling 2D spatial convolutions to capture both types of locality simultaneously. This requires implementing the complete 2D vision backbone integration (inception/ResNet/ConvNeXt options) versus DLinear's simple linear algebra operations.\n\n6. **Aggregation and Fusion** [NEW IMPLEMENTATION REQUIRED]: DLinear simply adds decomposed components (seasonal_output + trend_output), while TimesNet requires sophisticated adaptive aggregation: (a) Computing normalized amplitude weights via Softmax, (b) Weighted summation of k different 1D representations from different periods, (c) Ensuring gradient flow through the amplitude-based attention mechanism. This attention-like aggregation mechanism is entirely absent in DLinear's additive approach."
  },
  {
    "source": "FEDformer_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Frequency Domain Analysis for Periodicity**: Both papers leverage frequency domain transformations to understand time series structure. FEDformer uses Discrete Fourier Transform (DFT) via FFT to select important frequency modes (Equation 3-4), while TimesNet uses FFT to discover dominant periods (Equation 1-2). The FFT computation and mode selection logic from FEDformer's `get_frequency_modes()` function can be adapted for TimesNet's period discovery module. Specifically, FEDformer's approach of selecting top-k modes by amplitude (`np.random.shuffle(index)` or `list(range(0, modes))`) is similar to TimesNet's `argTopk` operation on amplitude values.\n\n2. **Embedding Layer for Feature Projection**: Both models use embedding layers to project raw time series into higher-dimensional feature spaces before main processing. FEDformer employs `DataEmbedding` (combining value embedding, positional encoding, and temporal encoding) to transform input from `enc_in` to `d_model` dimensions. TimesNet similarly uses an embedding layer `Embed()` to project raw inputs `X_1D ∈ R^(T×C)` to deep features `X_1D^0 ∈ R^(T×d_model)`. The entire `DataEmbedding` class from FEDformer (including `TokenEmbedding`, `PositionalEmbedding`, `TemporalEmbedding`) can be reused with minimal modifications for TimesNet's embedding requirements.\n\n3. **Residual Connection Architecture**: Both models adopt residual connections for stable training. FEDformer uses residual connections in encoder/decoder layers (Equation 1-2: `MOEDecomp(FEB(X) + X)`), while TimesNet explicitly uses residual structure in TimesBlock (Equation 4: `X_1D^l = TimesBlock(X_1D^(l-1)) + X_1D^(l-1)`). The residual connection pattern and layer normalization components (`my_Layernorm`) from FEDformer can be directly reused in TimesNet's block design.\n\n4. **Multi-Scale Feature Processing**: Both architectures process time series at multiple scales. FEDformer's Wavelet Enhanced Structure (FEB-w) performs recursive multi-scale decomposition with `L` levels (Section 3.3), using high-frequency and low-frequency components. TimesNet processes multiple periods simultaneously (k different 2D tensors from different period lengths). The multi-scale processing philosophy and the parameter-efficient design (sharing parameters across scales) from FEDformer's wavelet implementation can inform TimesNet's multi-period processing strategy.\n\n5. **Decomposition-Based Processing**: Both papers employ decomposition strategies. FEDformer uses seasonal-trend decomposition via `series_decomp` with moving average pooling (Section 3.4, MOEDecomp with multiple average filters). TimesNet decomposes 1D time series into multiple 2D representations based on different periodicities. The decomposition philosophy and the moving average pooling implementation (`series_decomp` class) from FEDformer can potentially be adapted for TimesNet's preprocessing or as an auxiliary component.\n\n6. **Classification Task Adaptation**: Both models can be adapted for time series classification. FEDformer's classification method (in `classification()` function) uses: (1) encoder to extract features, (2) activation and dropout for regularization, (3) flattening with attention masking (`output * x_mark_enc.unsqueeze(-1)`), and (4) linear projection to class logits. This entire classification pipeline structure can be reused for TimesNet, only replacing the encoder with TimesNet's TimesBlock-based feature extractor.",
    "differences": "1. **Core Innovation - Temporal 2D-Variation vs Frequency Enhancement**: FEDformer's innovation is frequency-enhanced decomposed Transformer using Fourier/Wavelet basis for efficient attention in frequency domain (O(L) complexity). It processes time series as 1D sequences throughout, performing attention mechanisms in frequency space via DFT/DWT. TimesNet's core innovation is transforming 1D time series into 2D space based on discovered periodicities, then applying 2D computer vision techniques (inception blocks with 2D convolutions). **NEW IMPLEMENTATION NEEDED**: (1) Period-based 2D reshaping module (`Reshape_{p_i,f_i}()` in Equation 3) that converts 1D tensor `R^(T×C)` to 2D tensor `R^(p_i×f_i×C)` based on period length, (2) Adaptive padding strategy to make sequence length compatible with discovered periods, (3) Reverse transformation from 2D back to 1D (`Trunc()` operation in Equation 5).\n\n2. **Attention Mechanism vs 2D Convolutional Processing**: FEDformer uses attention-based mechanisms (FEB, FEA) operating in frequency domain with learnable frequency modes. The `FourierBlock` performs complex multiplication between Fourier coefficients and learnable weights (Equation 4: `compl_mul1d`). TimesNet completely abandons attention mechanisms and instead uses **parameter-efficient inception blocks** with multi-scale 2D convolutions. **NEW IMPLEMENTATION NEEDED**: (1) Inception block architecture adapted for time series (multi-scale 2D kernels processing both intraperiod and interperiod variations), (2) Shared inception block mechanism that processes k different 2D tensors with the same parameters for efficiency, (3) 2D convolutional layers with appropriate kernel sizes for capturing temporal 2D-variations (e.g., 3×3, 5×5 kernels for different scales).\n\n3. **Period Discovery vs Mode Selection**: FEDformer selects frequency modes either randomly or by lowest frequencies (`get_frequency_modes()` with `mode_select_method`), focusing on computational efficiency. TimesNet discovers **dominant periods** by selecting top-k frequencies based on amplitude values (Equation 1: `argTopk(A)`), where each frequency corresponds to a specific period length `p_i = ⌈T/f_i⌉`. **NEW IMPLEMENTATION NEEDED**: (1) Top-k amplitude selection algorithm that returns both frequencies `{f_1,...,f_k}` and corresponding period lengths `{p_1,...,p_k}`, (2) Period length calculation from frequency values, (3) Amplitude-based importance weighting for adaptive aggregation (Equation 6: `Softmax(A_{f_1},...,A_{f_k})`), which differs from FEDformer's fixed mode selection.\n\n4. **Multi-Period Aggregation vs Decomposition**: FEDformer uses Mixture of Experts Decomposition (MOEDecomp) with multiple average pooling filters of different sizes to extract trend components, combining them with learned weights (Equation 10). TimesNet uses **amplitude-based adaptive aggregation** to fuse representations from k different periods (Equation 6). **NEW IMPLEMENTATION NEEDED**: (1) Softmax-based amplitude normalization across selected frequencies, (2) Weighted sum aggregation of k different 1D representations using normalized amplitudes as weights, (3) Dynamic aggregation that adapts to the importance of each discovered period, unlike FEDformer's fixed decomposition strategy.\n\n5. **Encoder-Decoder vs Stacked Blocks**: FEDformer follows Transformer's encoder-decoder architecture with cross-attention between encoder and decoder (FEA module), suitable for sequence-to-sequence forecasting. TimesNet uses a **homogeneous stacked architecture** of TimesBlocks in residual fashion (Equation 4), without encoder-decoder separation. **NEW IMPLEMENTATION NEEDED**: (1) TimesBlock module that combines 2D transformation, inception processing, and adaptive aggregation in a single block, (2) Stacking mechanism for multiple TimesBlocks (typically 2-4 layers) without encoder-decoder distinction, (3) End-to-end architecture where each block processes the full temporal sequence without separate encoding/decoding phases.\n\n6. **Frequency Space Operations vs Spatial Convolutions**: FEDformer performs all main operations in frequency domain (Fourier/Wavelet space), using complex-valued operations (`compl_mul1d` for complex multiplication in Equations 4, 7). TimesNet operates in **spatial domain** after 2D transformation, using standard real-valued convolutions. **NEW IMPLEMENTATION NEEDED**: (1) Standard 2D convolutional layers (nn.Conv2d) instead of frequency-domain operations, (2) Batch normalization and activation functions suitable for 2D spatial features, (3) No complex number handling or FFT/IFFT operations during main processing (only used for initial period discovery), (4) Integration with standard computer vision backbones (ResNet, ResNeXt, ConvNeXt options mentioned in Section 3.2).\n\n7. **Classification Strategy**: FEDformer's classification flattens encoder output with attention masking before projection: `output.reshape(B, -1)` creates a vector of size `(d_model × seq_len)`. TimesNet would need a different strategy suitable for multi-period 2D representations. **NEW IMPLEMENTATION NEEDED**: (1) Global pooling strategy across 2D representations from multiple periods (e.g., adaptive average pooling over spatial dimensions), (2) Feature aggregation across k different period-based representations before final classification, (3) Classification head that takes aggregated multi-period features and projects to class logits, potentially with different dimensionality than FEDformer's flattened approach."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Multi-scale Temporal Representation**: Both papers recognize the importance of capturing temporal patterns at different scales. Pyraformer uses a pyramidal multi-resolution structure with C-ary tree (coarse-to-fine scales), while TimesNet discovers multiple periodicities via FFT. The CSCM (Coarser-Scale Construction Module) in Pyraformer with its hierarchical convolution layers can inspire the multi-period processing in TimesNet. The bottleneck convolution structure in `Bottleneck_Construct` class could be adapted for efficient feature extraction.\n\n2. **Embedding and Feature Projection**: Both architectures start with embedding layers that combine data embedding, positional encoding, and covariate information. Pyraformer's `DataEmbedding` module can be directly reused or adapted for TimesNet's initial feature projection (`Embed()` operation). The dimension projection mechanism (`self.down` and `self.up` in Bottleneck_Construct) provides a parameter-efficient design pattern applicable to TimesNet.\n\n3. **Hierarchical Feature Aggregation**: Pyraformer aggregates features from multiple scales (gathering from all pyramid levels via `refer_points` and `torch.gather`), similar to TimesNet's adaptive aggregation of representations from different periods using amplitude-based weighting. The aggregation logic in Pyraformer's prediction module (concatenating features from different scales) can guide TimesNet's period-based aggregation implementation.\n\n4. **Residual Connections**: Both employ residual connections for stable training. Pyraformer uses residual connections in its encoder layers (`x = x + residual` in PositionwiseFeedForward), and TimesNet explicitly uses residual structure in TimesBlock (Equation 4). The residual implementation pattern can be directly transferred.\n\n5. **Normalization and Regularization**: Both use LayerNorm and Dropout for regularization. Pyraformer's `self.layer_norm` and `self.dropout` in PositionwiseFeedForward can be reused in TimesNet's architecture. The normalize_before strategy in Pyraformer provides guidance on where to place normalization layers.\n\n6. **Classification Head Design**: For classification tasks, Pyraformer flattens multi-scale features and projects them through a linear layer (`self.projection`). This pattern of `output.reshape(output.shape[0], -1)` followed by projection can be adapted for TimesNet's final classification layer, though TimesNet may need to handle 2D-to-1D transformation first.",
    "differences": "1. **Core Temporal Modeling Paradigm**: Pyraformer uses attention-based mechanisms on a pyramidal graph structure with O(L) complexity, while TimesNet fundamentally transforms 1D time series into 2D tensors based on discovered periodicities. NEW IMPLEMENTATION NEEDED: (a) FFT-based period discovery module (`Period()` function in Equation 1-2) with amplitude calculation and Top-k frequency selection; (b) 1D-to-2D reshaping logic (`Reshape_{p_i,f_i}()` in Equation 3) that converts temporal sequences into structured 2D tensors with rows representing periods and columns representing intraperiod variations; (c) Padding and Truncation operations specific to period-based transformation.\n\n2. **Attention vs. Convolution**: Pyraformer relies on pyramidal attention mechanism (PAM) with customized sparse attention patterns (inter-scale and intra-scale connections defined in Equation 2-3), implemented via custom CUDA kernels. TimesNet uses 2D convolutional inception blocks for spatial feature extraction. NEW IMPLEMENTATION NEEDED: (a) 2D Inception block with multi-scale convolutional kernels operating on (period × frequency) dimensions; (b) Parameter-sharing mechanism across different reshaped 2D tensors from multiple periods; (c) 2D-to-1D transformation back after convolution (`Trunc(Reshape_{1,(p_i×f_i)})` in Equation 5). The entire ConvLayer and Inception architecture needs to be built from scratch as Pyraformer only has 1D convolutions in CSCM.\n\n3. **Multi-Resolution Construction**: Pyraformer constructs coarser scales through hierarchical 1D convolutions with stride C (CSCM module, `ConvLayer` with `kernel_size=window_size, stride=window_size`), creating a fixed C-ary tree. TimesNet dynamically discovers k different periods through FFT analysis and creates k different 2D views. NEW IMPLEMENTATION NEEDED: (a) Dynamic period selection based on amplitude spectrum rather than fixed hierarchical downsampling; (b) Multiple parallel 2D transformations for each discovered period; (c) Adaptive aggregation using softmax-normalized amplitudes (Equation 6) as importance weights, fundamentally different from Pyraformer's fixed concatenation of pyramid levels.\n\n4. **Graph Structure vs. 2D Spatial Structure**: Pyraformer explicitly models temporal dependencies as a pyramidal graph with defined neighbor sets (N_ℓ^(s) including adjacent nodes A, children C, and parent P), using masked attention. TimesNet implicitly encodes temporal relationships through 2D spatial locality (adjacent columns for intraperiod, adjacent rows for interperiod). NEW IMPLEMENTATION NEEDED: (a) 2D spatial convolution kernels that naturally capture both types of variations simultaneously; (b) No need for attention masks or graph connectivity; (c) The entire 2D vision backbone integration framework allowing plug-and-play of ResNet, ResNeXt, ConvNeXt, etc.\n\n5. **Feature Aggregation Strategy**: Pyraformer uses `refer_points()` to gather specific nodes from each pyramid level and concatenates them (indexes-based gathering via `torch.gather`), treating all scales equally. TimesNet performs amplitude-weighted summation across different period representations. NEW IMPLEMENTATION NEEDED: (a) Softmax normalization over selected frequency amplitudes; (b) Weighted sum aggregation (∑_{i=1}^k Â_{f_i} × X̂_{1D}^{l,i}) instead of concatenation; (c) The amplitude values serve as learnable attention weights reflecting period importance.\n\n6. **Complexity and Scalability**: Pyraformer achieves O(L) complexity through sparse pyramidal attention with maximum path length O(1), while TimesNet's complexity depends on the 2D convolution operations on reshaped tensors (O(k × p_i × f_i × kernel_size²)). NEW IMPLEMENTATION NEEDED: (a) Efficient FFT computation for period discovery at each layer; (b) Dynamic tensor reshaping and padding operations that maintain computational efficiency; (c) Shared inception block parameters across k periods to maintain parameter efficiency regardless of k value; (d) The entire computational flow is fundamentally different - Pyraformer's custom CUDA kernels for sparse attention vs. TimesNet's standard 2D convolution operations."
  }
]
