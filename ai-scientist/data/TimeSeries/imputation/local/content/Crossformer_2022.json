{
  "id": "Crossformer_2022",
  "paper_title": "Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting",
  "alias": "Crossformer",
  "year": 2022,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "Crossformer introduces three key innovations for multivariate time series forecasting: (1) Dimension-Segment-Wise (DSW) embedding that embeds segments from each dimension separately rather than all dimensions at each time step, explicitly capturing cross-dimension dependency; (2) Two-Stage Attention (TSA) layer that separately captures cross-time and cross-dimension dependencies with a router mechanism to reduce complexity from O(D²) to O(D); (3) Hierarchical Encoder-Decoder (HED) that uses segment merging to capture information at different temporal scales and combines predictions from multiple scales for final forecasting.",
  "introduction": "# 1 INTRODUCTION\n\nMultivariate time series (MTS) are time series with multiple dimensions, where each dimension represents a specific univariate time series (e.g. a climate feature of weather). MTS forecasting aims to forecast the future value of MTS using their historical values. MTS forecasting benefits the decision-making of downstream tasks and is widely used in many fields including weather (Angryk et al., 2020), energy (Demirel et al., 2012), finance (Patton, 2013), etc. With the development of deep learning, many models have been proposed and achieved superior performances in MTS forecasting (Lea et al., 2017; Qin et al., 2017; Flunkert et al., 2017; Rangapuram et al., 2018; Li et al., 2019a; Wu et al., 2020; Li et al., 2021). Among them, the recent Transformer-based models (Li et al., 2019b; Zhou et al., 2021; Wu et al., 2021a; Liu et al., 2021a; Zhou et al., 2022; Chen et al., 2022) show great potential thanks to their ability to capture long-term temporal dependency (cross-time dependency).\n\nBesides cross-time dependency, the cross-dimension dependency is also critical for MTS forecasting, i.e. for a specific dimension, information from associated series in other dimensions may improve prediction. For example, when predicting future temperature, not only the historical temperature, but also historical wind speed helps to forecast. Some previous neural models explicitly capture the cross-dimension dependency, i.e. preserving the information of dimensions in the latent feature space and using convolution neural network (CNN) (Lai et al., 2018) or graph neural network (GNN) (Wu et al., 2020; Cao et al., 2020) to capture their dependency. However, recent Transformer-based models only implicitly utilize this dependency by embedding. In general, Transformer-based models embed data points in all dimensions at the same time step into a feature vector and try to capture dependency among different time steps (like Fig. 1 (b)). In this way, cross-time dependency is well captured, but cross-dimension dependency is not, which may limit their forecasting capability.\n\nTo fill the gap, we propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for MTS forecasting. Specifically, we devise Dimension-Segment-Wise (DSW) embedding to process the historical time series. In DSW embedding, the series in each dimension is first partitioned into segments and then embedded into feature vectors. The output of DSW embedding is a 2D vector array where the two axes correspond to time and dimension. Then we propose the Two-Stage-Attention (TSA) layer to efficiently capture the cross-time and cross-dimension dependency among the 2D vector array. Using DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) for forecasting. In HED, each layer corresponds to a scale. The encoder's upper layer merges adjacent segments output by the lower layer to capture the dependency at a coarser scale. Decoder layers generate predictions at different scales and add them up as the final prediction. The contributions of this paper are:\n\n1) We dive into the existing Transformer-based models for MTS forecasting and figure out that the cross-dimension dependency is not well utilized: these models simply embed data points of all dimensions at a specific time step into a single vector and focus on capturing the cross-time dependency among different time steps. Without adequate and explicit mining and utilization of cross-dimension dependency, their forecasting capability is empirically shown limited.  \n2) We develop Crossformer, a Transformer model utilizing cross-dimension dependency for MTS forecasting. This is one of the few transformer models (perhaps the first to our best knowledge) that explicitly explores and utilizes cross-dimension dependency for MTS forecasting.  \n3) Extensive experimental results on six real-world benchmarks show the effectiveness of our Crossformer against previous state-of-the-arts. Specifically, Crossformer ranks top-1 among the 9 models for comparison on 36 out of the 58 settings of varying prediction lengths and metrics and ranks top-2 on 51 settings.",
  "method": "# 3 METHODOLOGY\n\nIn multivariate time series forecasting, one aims to predict the future value of time series  $\\mathbf{x}_{T + 1:T + \\tau} \\in \\mathbb{R}^{\\tau \\times D}$  given the history  $\\mathbf{x}_{1:T} \\in \\mathbb{R}^{T \\times D}$ , where  $\\tau, T$  is the number of time steps in the future and past, respectively<sup>2</sup>.  $D > 1$  is the number of dimensions. A natural assumption is that these  $D$  series are associated (e.g. climate features of weather), which helps to improve the forecasting accuracy. To utilize the cross-dimension dependency, in Section 3.1, we embed the MTS using Dimension-Segment-Wise (DSW) embedding. In Section 3.2, we propose a Two-Stage Attention (TSA) layer to efficiently capture the dependency among the embedded segments. In Section 3.3, using DSW embedding and TSA layer, we construct a hierarchical encoder-decoder (HED) to utilize information at different scales for final forecasting.\n\n# 3.1 DIMENSION-SEGMENT-WISE EMBEDDING\n\nTo motivate our approach, we first analyze the embedding methods of the previous Transformer-based models for MTS forecasting (Zhou et al., 2021; Wu et al., 2021a; Liu et al., 2021a; Zhou et al., 2022). As shown in Fig. 1 (b), existing methods embed data points at the same time step into a vector:  $\\mathbf{x}_t \\rightarrow \\mathbf{h}_t$ ,  $\\mathbf{x}_t \\in \\mathbb{R}^D$ ,  $\\mathbf{h}_t \\in \\mathbb{R}^{d_{model}}$ , where  $\\mathbf{x}_t$  represents all the data points in  $D$  dimensions at step  $t$ . In this way, the input  $\\mathbf{x}_{1:T}$  is embedded into  $T$  vectors  $\\{\\mathbf{h}_1, \\mathbf{h}_2, \\dots, \\mathbf{h}_T\\}$ . Then the dependency among the  $T$  vectors is captured for forecasting. Therefore, previous Transformer-based models mainly capture cross-time dependency, while the cross-dimension dependency is not explicitly captured during embedding, which limits their forecasting capability.\n\nTransformer was originally developed for NLP (Vaswani et al., 2017), where each embedded vector represents an informative word. For MTS, a single value at a step alone provides little information.\n\nWhile it forms informative pattern with nearby values in time domain. Fig. 1 (a) shows a typical attention score map of original Transformer for MTS forecasting. We can see that attention values have a tendency to segment, i.e. close data points have similar attention weights.\n\nBased on the above two points, we argue that an embedded vector should represent a series segment of single dimension (Fig. 1 (c)), rather than the values of all dimensions at single step (Fig. 1 (b)). To this end, we propose Dimension-Segment-Wise (DSW) embedding where the points in each dimension are divided into segments of length  $L_{seg}$  and then embedded:\n\n$$\n\\mathbf {x} _ {1: T} = \\left\\{\\mathbf {x} _ {i, d} ^ {(s)} \\mid 1 \\leq i \\leq \\frac {T}{L _ {\\text {s e g}}}, 1 \\leq d \\leq D \\right\\} \\tag {1}\n$$\n\n$$\n\\mathbf {x} _ {i, d} ^ {(s)} = \\left\\{x _ {t, d} | (i - 1) \\times L _ {s e g} <   t \\leq i \\times L _ {s e g} \\right\\}\n$$\n\nwhere  $\\mathbf{x}_{i,d}^{(s)}\\in \\mathbb{R}^{L_{seg}}$  is the  $i$ -th segment in dimension  $d$  with length  $L_{seg}$ . For convenience, we assume that  $T,\\tau$  are divisible by  $L_{seg}^3$ . Then each segment is embedded into a vector using linear projection added with a position embedding:\n\n$$\n\\mathbf {h} _ {i, d} = \\mathbf {E x} _ {i, d} ^ {(s)} + \\mathbf {E} _ {i, d} ^ {(p o s)} \\tag {2}\n$$\n\nwhere  $\\mathbf{E} \\in \\mathbb{R}^{d_{model} \\times L_{seg}}$  denotes the learnable projection matrix, and  $\\mathbf{E}_{i,d}^{(pos)} \\in \\mathbb{R}^{d_{model}}$  denotes the learnable position embedding for position  $(i, d)$ . After embedding, we obtain a 2D vector array  $\\mathbf{H} = \\left\\{\\mathbf{h}_{i,d} | 1 \\leq i \\leq \\frac{T}{L_{seg}}, 1 \\leq d \\leq D\\right\\}$ , where each  $\\mathbf{h}_{i,d}$  represents a univariate time series segment. The idea of segmentation is also used in Du et al. (2022), which splits the embedded 1D vector sequence into segments to compute the Segment-Correlation in order to enhance locality and reduce computation complexity. However, like other Transformers for MTS forecasting, it does not explicitly capture cross-dimension dependency.\n\n# 3.2 TWO-STAGE ATTENTION LAYER\n\nFor the obtained 2D array  $\\mathbf{H}$ , one can flatten it into a 1D sequence so that it can be input to a canonical Transformer like ViT (Dosovitskiy et al., 2021) does in vision. While we have specific considerations: 1) Different from images where the axes of height and width are interchangeable, the axes of time and dimension for MTS have different meanings and thus should be treated differently. 2) Directly applying self-attention on 2D array will cause the complexity of  $O\\left(D^{2} \\frac{T^{2}}{L_{seg}^{2}}\\right)$ , which is unaffordable for large  $D$ . Therefore, we propose the Two-Stage Attention (TSA) Layer to capture cross-time and cross-dimension dependency among the 2D vector array, as sketched in Fig. 2 (a).\n\nCross-Time Stage Given a 2D array  $\\mathbf{Z} \\in \\mathbb{R}^{L \\times D \\times d_{model}}$  as the input of the TSA Layer, where  $L$  and  $D$  are the number of segments and dimensions, respectively.  $\\mathbf{Z}$  here can be the output of DSW embedding or lower TSA layers. For convenience, in the following, we use  $\\mathbf{Z}_{i,:}$  to denote the vectors of all dimensions at time step  $i$ ,  $\\mathbf{Z}_{:,d}$  for those of all time steps in dimension  $d$ . In the cross-time stage, we directly apply multi-head self-attention (MSA) to each dimension:\n\n$$\n\\hat {\\mathbf {Z}} _ {:, d} ^ {\\text {t i m e}} = \\operatorname {L a y e r N o r m} \\left(\\mathbf {Z} _ {:, d} + \\operatorname {M S A} ^ {\\text {t i m e}} \\left(\\mathbf {Z} _ {:, d}, \\mathbf {Z} _ {:, d}, \\mathbf {Z} _ {:, d}\\right)\\right) \\tag {3}\n$$\n\n$$\n\\mathbf {Z} ^ {\\text {t i m e}} = \\operatorname {L a y e r N o r m} \\left(\\hat {\\mathbf {Z}} ^ {\\text {t i m e}} + \\operatorname {M L P} (\\hat {\\mathbf {Z}} ^ {\\text {t i m e}})\\right)\n$$\n\nwhere  $1 \\leq d \\leq D$  and LayerNorm denotes layer normalization as widely adopted in Vaswani et al. (2017); Dosovitskiy et al. (2021); Zhou et al. (2021), MLP denotes a multi-layer (two in this paper) feedforward network,  $\\mathrm{MSA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})$  denotes the multi-head self-attention (Vaswani et al., 2017) layer where  $\\mathbf{Q},\\mathbf{K},\\mathbf{V}$  serve as queries, keys and values. All dimensions ( $1 \\leq d \\leq D$ ) share the same MSA layer.  $\\hat{\\mathbf{Z}}^{time},\\mathbf{Z}^{time}$  denotes the output of the MSA and MLP.\n\nThe computation complexity of cross-time stage is  $O(DL^2)$ . After this stage, the dependency among time segments in the same dimension is captured in  $\\mathbf{Z}^{time}$ . Then  $\\mathbf{Z}^{time}$  becomes the input of Cross-Dimension Stage to capture cross-dimension dependency.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/eab24676c8c707bb04cccd8169189af85620f05faf096156d03dd2af7fc20d19.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/a5034383276648701e8a63510dacb253a8192bd77b6c586112778b8f46cd85ad.jpg)  \n(b)  \nFigure 2: The TSA layer. (a) Two-Stage Attention Layer to process a 2D vector array representing multivariate time series: each vector refers to a segment of the original series. The whole vector array goes through the Cross-Time Stage and Cross-Dimension Stage to get corresponding dependency. (b) Directly using MSA in Cross-Dimension Stage to build the  $D$ -to- $D$  connection results in  $O(D^2)$  complexity. (c) Router mechanism for Cross-Dimension Stage: a small fixed number ( $c$ ) of \"routers\" gather information from all dimensions and then distribute the gathered information. The complexity is reduced to  $O(2cD) = O(D)$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/cf04f662ceac7be08ddbd69f097ca9b95f70a5c9e81936cfeb398feaa0db82e1.jpg)  \n(c)\n\nCross-Dimension Stage We can use a large  $L_{seg}$  for long sequence in DSW Embedding to reduce the number of segments  $L$  in cross-time stage. While in Cross-Dimension Stage, we can not partition dimensions and directly apply MSA will cause the complexity of  $O(D^2)$  (as shown in Fig. 2 (b)), which is unaffordable for datasets with large  $D$ . Instead, we propose the router mechanism for potentially large  $D$ . As shown in Fig. 2 (c), we set a small fixed number  $(c << D)$  of learnable vectors for each time step  $i$  as routers. These routers first aggregate messages from all dimensions by using routers as query in MSA and vectors of all dimensions as key and value. Then routers distribute the received messages among dimensions by using vectors of dimensions as query and aggregated messages as key and value. In this way, the all-to-all connection among  $D$  dimensions are built:\n\n$$\n\\mathbf {B} _ {i,:} = \\operatorname {M S A} _ {1} ^ {d i m} \\left(\\mathbf {R} _ {i,:}, \\mathbf {Z} _ {i,:} ^ {\\text {t i m e}}, \\mathbf {Z} _ {i,:} ^ {\\text {t i m e}}\\right), 1 \\leq i \\leq L\n$$\n\n$$\n\\overline {{\\mathbf {Z}}} _ {i,:} ^ {d i m} = \\mathrm {M S A} _ {2} ^ {d i m} \\left(\\mathbf {Z} _ {i,:} ^ {\\text {t i m e}}, \\mathbf {B} _ {i,:}, \\mathbf {B} _ {i,:}\\right), 1 \\leq i \\leq L\n$$\n\n$$\n\\hat {\\mathbf {Z}} ^ {d i m} = \\text {L a y e r N o r m} \\left(\\mathbf {Z} ^ {\\text {t i m e}} + \\overline {{\\mathbf {Z}}} ^ {d i m}\\right) \\tag {4}\n$$\n\n$$\n\\mathbf {Z} ^ {d i m} = \\text {L a y e r N o r m} \\left(\\hat {\\mathbf {Z}} ^ {d i m} + \\operatorname {M L P} (\\hat {\\mathbf {Z}} ^ {d i m})\\right)\n$$\n\nwhere  $\\mathbf{R} \\in \\mathbb{R}^{L \\times c \\times d_{model}}$  ( $c$  is a constant) is the learnable vector array serving as routers.  $\\mathbf{B} \\in \\mathbb{R}^{L \\times c \\times d_{model}}$  is the aggregated messages from all dimensions.  $\\overline{\\mathbf{Z}}^{dim}$  denotes output of the router mechanism. All time steps ( $1 \\leq i \\leq L$ ) share the same  $\\mathrm{MSA}_1^{dim}, \\mathrm{MSA}_2^{dim}$ .  $\\hat{\\mathbf{Z}}^{dim}, \\mathbf{Z}^{dim}$  denote output of skip connection and MLP respectively. The router mechanism reduces the complexity from  $O(D^2 L)$  to  $O(DL)$ .\n\nAdding up Eq. 3 and Eq. 4, we model the two stages as:\n\n$$\n\\mathbf {Y} = \\mathbf {Z} ^ {d i m} = \\operatorname {T S A} (\\mathbf {Z}) \\tag {5}\n$$\n\nwhere  $\\mathbf{Z},\\mathbf{Y}\\in \\mathbb{R}^{L\\times D\\times d_{model}}$  denotes the input and output vector array of TSA layer, respectively. Note that the overall computation complexity of the\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/a3bcbb9286736b46c341e3896c8210aac9352afa46314cbbd61263c0850afe9a.jpg)  \nFigure 3: Architecture of the Hierarchical Encoder-Decoder in Crossformer with 3 encoder layers. The length of each vector denotes the covered time range. The encoder (left) uses TSA layer and segment merging to capture dependency at different scales: a vector in upper layer covers a longer range, resulting in dependency at a coarser scale. Exploring different scales, the decoder (right) makes the final prediction by forecasting at each scale and adding them up.\n\nTSA layer is  $O(DL^{2} + DL) = O(DL^{2})$ . After the Cross-Time and Cross-Dimension Stages, every two segments (i.e.  $\\mathbf{Z}_{i_1,d_1}, \\mathbf{Z}_{i_2,d_2}$ ) in  $\\mathbf{Z}$  are connected, as such both cross-time and cross-dimension dependencies are captured in  $\\mathbf{Y}$ .\n\n# 3.3 HIERARCHICAL ENCODER-DECODER\n\nHierarchical structures are widely used in Transformers for MTS forecasting to capture information at different scales (Zhou et al., 2021; Liu et al., 2021a). In this section, we use the proposed DSW embedding, TSA layer and segment merging to construct a Hierarchical Encoder-Decoder (HED). As shown in Fig. 3, the upper layer utilizes information at a coarser scale for forecasting. Forecasting values at different scales are added to output the final result.\n\nEncoder In each layer of the encoder (except the first layer), every two adjacent vectors in time domain are merged to obtain the representation at a coarser level. Then a TSA layer is applied to capture dependency at this scale. This process is modeled as  $\\mathbf{Z}^{enc,l} = \\mathrm{Encoder}(\\mathbf{Z}^{enc,l-1})$ :\n\n$$\n\\left\\{ \\begin{array}{l l} l = 1: & \\hat {\\mathbf {Z}} ^ {e n c, l} = \\mathbf {H} \\\\ l > 1: & \\hat {\\mathbf {Z}} _ {i, d} ^ {e n c, l} = \\mathbf {M} \\left[ \\mathbf {Z} _ {2 i - 1, d} ^ {e n c, l - 1} \\cdot \\mathbf {Z} _ {2 i, d} ^ {e n c, l - 1} \\right], 1 \\leq i \\leq \\frac {L _ {l - 1}}{2}, 1 \\leq d \\leq D \\end{array} \\right. \\tag {6}\n$$\n\n$$\n\\mathbf {Z} ^ {e n c, l} = \\mathrm {T S A} (\\hat {\\mathbf {Z}} ^ {e n c, l})\n$$\n\nwhere  $\\mathbf{H}$  denotes the 2D array obtained by DSW embedding;  $\\mathbf{Z}^{enc,l}$  denotes the output of the  $l$ -th encoder layer;  $\\mathbf{M} \\in \\mathbb{R}^{d_{model} \\times 2d_{model}}$  denotes a learnable matrix for segment merging;  $[\\cdot]$  denotes the concatenation operation;  $L_{l-1}$  denotes the number of segments in each dimension in layer  $l-1$ , if it is not divisible by 2, we pad  $\\mathbf{Z}^{enc,l-1}$  to the proper length;  $\\hat{\\mathbf{Z}}^{enc,l}$  denotes the array after segment merging in the  $i$ -th layer. Suppose there are  $N$  layers in the encoder, we use  $\\mathbf{Z}^{enc,0}, \\mathbf{Z}^{enc,1}, \\ldots, \\mathbf{Z}^{enc,N}, (\\mathbf{Z}^{enc,0} = \\mathbf{H})$  to represent the  $N+1$  outputs of the encoder. The complexity of each encoder layer is  $O(D\\frac{T^2}{L_{seg}^2})$ .\n\nDecoder Obtaining the  $N + 1$  feature arrays output by the encoder, we use  $N + 1$  layers (indexed by  $0,1,\\ldots ,N$ ) in decoder for forecasting. Layer  $l$  takes the  $l$ -th encoded array as input, then outputs a decoded 2D array of layer  $l$ . This process is summarized as  $\\mathbf{Z}^{dec,l} = \\mathrm{Decoder}(\\mathbf{Z}^{dec,l - 1},\\mathbf{Z}^{enc,l})$ :\n\n$$\n\\left\\{ \\begin{array}{l l} l = 0: & \\tilde {\\mathbf {Z}} ^ {d e c, l} = \\operatorname {T S A} (\\mathbf {E} ^ {(d e c)}) \\\\ l > 0: & \\tilde {\\mathbf {Z}} ^ {d e c, l} = \\operatorname {T S A} (\\mathbf {Z} ^ {d e c, l - 1}) \\end{array} \\right.\n$$\n\n$$\n\\overline {{\\mathbf {Z}}} _ {:, d} ^ {d e c, l} = \\operatorname {M S A} \\left(\\tilde {\\mathbf {Z}} _ {:, d} ^ {d e c, l}, \\mathbf {Z} _ {:, d} ^ {e n c, l}, \\mathbf {Z} _ {:, d} ^ {e n c, l}\\right), 1 \\leq d \\leq D \\tag {7}\n$$\n\n$$\n\\hat {\\mathbf {Z}} ^ {d e c, l} = \\text {L a y e r N o r m} \\left(\\tilde {\\mathbf {Z}} ^ {d e c, l} + \\overline {{\\mathbf {Z}}} ^ {d e c, l}\\right)\n$$\n\n$$\n\\mathbf {Z} ^ {d e c, l} = \\operatorname {L a y e r N o r m} \\left(\\hat {\\mathbf {Z}} ^ {d e c, l} + \\operatorname {M L P} (\\hat {\\mathbf {Z}} ^ {d e c, l})\\right)\n$$\n\nwhere  $\\mathbf{E}^{(dec)}\\in \\mathbb{R}^{\\frac{\\tau}{L_{seg}}\\times D\\times d_{model}}$  denotes the learnable position embedding for decoder.  $\\tilde{\\mathbf{Z}}^{dec,l}$  is the output of TSA. The MSA layer takes  $\\tilde{\\mathbf{Z}}_{:,d}^{dec,l}$  as query and  $\\mathbf{Z}_{:,d}^{enc,l}$  as the key and value to build the connection between encoder and decoder. The output of MSA is denoted as  $\\overline{\\mathbf{Z}}_{:,d}^{dec,l}$ .  $\\hat{\\mathbf{Z}}^{dec,l},\\mathbf{Z}^{dec,l}$  denote the output of skip connection and MLP respectively. We use  $\\mathbf{Z}^{dec,0},\\mathbf{Z}^{enc,1},\\ldots ,\\mathbf{Z}^{dec,N}$  to represent the decoder output. The complexity of each decoder layer is  $O\\left(D\\frac{\\tau(T + \\tau)}{L_{seg}^2}\\right)$ .\n\nLinear projection is applied to each layer's output to yield the prediction of this layer. Layer predictions are summed to make the final prediction (for  $l = 0, \\dots, N$ ):\n\n$$\n\\text {f o r} l = 0, \\dots , N: \\mathbf {x} _ {i, d} ^ {(s), l} = \\mathbf {W} ^ {l} \\mathbf {Z} _ {i, d} ^ {d e c, l} \\quad \\mathbf {x} _ {T + 1: T + \\tau} ^ {p r e d, l} = \\left\\{\\mathbf {x} _ {i, d} ^ {(s), l} | 1 \\leq i \\leq \\frac {\\tau}{L _ {s e g}}, 1 \\leq d \\leq D \\right\\} \\tag {8}\n$$\n\n$$\n\\mathbf {x} _ {T + 1: T + \\tau} ^ {p r e d} = \\sum_ {l = 0} ^ {N} \\mathbf {x} _ {T + 1: T + \\tau} ^ {p r e d, l}\n$$\n\nwhere  $\\mathbf{W}^l\\in \\mathbb{R}^{L_{seg}\\times d_{model}}$  is a learnable matrix to project a vector to a time series segment.  $\\mathbf{x}_{i,d}^{(s),l}\\in \\mathbb{R}^{L_{seg}}$  denotes the  $i$ -th segment in dimension  $d$  of the prediction. All the segments in layer\n\nTable 1: MSE/MAE with different prediction lengths. Bold/underline indicates the best/second. Results of LSTMa, LSTnet, Transformer, Informer on the first 4 datasets are from Zhou et al. (2021).  \n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">LSTMA</td><td colspan=\"2\">LSTnet</td><td colspan=\"2\">MTGNN</td><td colspan=\"2\">Transformer</td><td colspan=\"2\">Informer</td><td colspan=\"2\">Autoformer</td><td colspan=\"2\">Pyraformer</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Crossformer</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"5\">ETh1</td><td>24</td><td>0.650</td><td>0.624</td><td>1.293</td><td>0.901</td><td>0.336</td><td>0.393</td><td>0.620</td><td>0.577</td><td>0.577</td><td>0.549</td><td>0.439</td><td>0.440</td><td>0.493</td><td>0.507</td><td>0.318</td><td>0.384</td><td>0.305</td><td>0.367</td></tr><tr><td>48</td><td>0.720</td><td>0.675</td><td>1.456</td><td>0.960</td><td>0.386</td><td>0.429</td><td>0.692</td><td>0.671</td><td>0.685</td><td>0.625</td><td>0.429</td><td>0.442</td><td>0.554</td><td>0.544</td><td>0.342</td><td>0.396</td><td>0.352</td><td>0.394</td></tr><tr><td>168</td><td>1.212</td><td>0.867</td><td>1.997</td><td>1.214</td><td>0.466</td><td>0.474</td><td>0.947</td><td>0.797</td><td>0.931</td><td>0.752</td><td>0.493</td><td>0.479</td><td>0.781</td><td>0.675</td><td>0.412</td><td>0.449</td><td>0.410</td><td>0.441</td></tr><tr><td>336</td><td>1.424</td><td>0.994</td><td>2.655</td><td>1.369</td><td>0.736</td><td>0.643</td><td>1.094</td><td>0.813</td><td>1.128</td><td>0.873</td><td>0.509</td><td>0.492</td><td>0.912</td><td>0.747</td><td>0.456</td><td>0.474</td><td>0.440</td><td>0.461</td></tr><tr><td>720</td><td>1.960</td><td>1.322</td><td>2.143</td><td>1.380</td><td>0.916</td><td>0.750</td><td>1.241</td><td>0.917</td><td>1.215</td><td>0.896</td><td>0.539</td><td>0.537</td><td>0.993</td><td>0.792</td><td>0.521</td><td>0.515</td><td>0.519</td><td>0.524</td></tr><tr><td rowspan=\"5\">ETm1</td><td>24</td><td>0.621</td><td>0.629</td><td>1.968</td><td>1.170</td><td>0.260</td><td>0.324</td><td>0.306</td><td>0.371</td><td>0.323</td><td>0.369</td><td>0.410</td><td>0.428</td><td>0.310</td><td>0.371</td><td>0.290</td><td>0.364</td><td>0.211</td><td>0.293</td></tr><tr><td>48</td><td>1.392</td><td>0.939</td><td>1.999</td><td>1.215</td><td>0.386</td><td>0.408</td><td>0.465</td><td>0.470</td><td>0.494</td><td>0.503</td><td>0.485</td><td>0.464</td><td>0.465</td><td>0.464</td><td>0.342</td><td>0.396</td><td>0.300</td><td>0.352</td></tr><tr><td>96</td><td>1.339</td><td>0.913</td><td>2.762</td><td>1.542</td><td>0.428</td><td>0.446</td><td>0.681</td><td>0.612</td><td>0.678</td><td>0.614</td><td>0.502</td><td>0.476</td><td>0.520</td><td>0.504</td><td>0.366</td><td>0.412</td><td>0.320</td><td>0.373</td></tr><tr><td>288</td><td>1.740</td><td>1.124</td><td>1.257</td><td>2.076</td><td>0.469</td><td>0.488</td><td>1.162</td><td>0.879</td><td>1.056</td><td>0.786</td><td>0.604</td><td>0.522</td><td>0.729</td><td>0.657</td><td>0.398</td><td>0.433</td><td>0.404</td><td>0.427</td></tr><tr><td>672</td><td>2.736</td><td>1.555</td><td>1.917</td><td>2.941</td><td>0.620</td><td>0.571</td><td>1.231</td><td>1.103</td><td>1.192</td><td>0.926</td><td>0.607</td><td>0.530</td><td>0.980</td><td>0.678</td><td>0.455</td><td>0.464</td><td>0.569</td><td>0.528</td></tr><tr><td rowspan=\"5\">WTH</td><td>24</td><td>0.546</td><td>0.570</td><td>0.615</td><td>0.545</td><td>0.307</td><td>0.356</td><td>0.349</td><td>0.397</td><td>0.335</td><td>0.381</td><td>0.363</td><td>0.396</td><td>0.301</td><td>0.359</td><td>0.357</td><td>0.412</td><td>0.294</td><td>0.343</td></tr><tr><td>48</td><td>0.829</td><td>0.677</td><td>0.660</td><td>0.589</td><td>0.388</td><td>0.422</td><td>0.386</td><td>0.433</td><td>0.395</td><td>0.459</td><td>0.456</td><td>0.462</td><td>0.376</td><td>0.421</td><td>0.428</td><td>0.458</td><td>0.370</td><td>0.411</td></tr><tr><td>168</td><td>1.038</td><td>0.835</td><td>0.748</td><td>0.647</td><td>0.498</td><td>0.512</td><td>0.613</td><td>0.582</td><td>0.608</td><td>0.567</td><td>0.574</td><td>0.548</td><td>0.519</td><td>0.521</td><td>0.564</td><td>0.541</td><td>0.473</td><td>0.494</td></tr><tr><td>336</td><td>1.657</td><td>1.059</td><td>0.782</td><td>0.683</td><td>0.506</td><td>0.523</td><td>0.707</td><td>0.634</td><td>0.702</td><td>0.620</td><td>0.600</td><td>0.571</td><td>0.539</td><td>0.543</td><td>0.533</td><td>0.536</td><td>0.495</td><td>0.515</td></tr><tr><td>720</td><td>1.536</td><td>1.109</td><td>0.851</td><td>0.757</td><td>0.510</td><td>0.527</td><td>0.834</td><td>0.741</td><td>0.831</td><td>0.731</td><td>0.587</td><td>0.570</td><td>0.547</td><td>0.553</td><td>0.562</td><td>0.557</td><td>0.526</td><td>0.542</td></tr><tr><td rowspan=\"5\">ECL</td><td>48</td><td>0.486</td><td>0.572</td><td>0.369</td><td>0.445</td><td>0.173</td><td>0.280</td><td>0.334</td><td>0.399</td><td>0.344</td><td>0.393</td><td>0.241</td><td>0.351</td><td>0.478</td><td>0.471</td><td>0.229</td><td>0.338</td><td>0.156</td><td>0.255</td></tr><tr><td>168</td><td>0.574</td><td>0.602</td><td>0.394</td><td>0.476</td><td>0.236</td><td>0.320</td><td>0.353</td><td>0.420</td><td>0.368</td><td>0.424</td><td>0.299</td><td>0.387</td><td>0.452</td><td>0.455</td><td>0.263</td><td>0.361</td><td>0.231</td><td>0.309</td></tr><tr><td>336</td><td>0.886</td><td>0.795</td><td>0.419</td><td>0.477</td><td>0.328</td><td>0.373</td><td>0.381</td><td>0.439</td><td>0.381</td><td>0.431</td><td>0.375</td><td>0.428</td><td>0.463</td><td>0.456</td><td>0.305</td><td>0.386</td><td>0.323</td><td>0.369</td></tr><tr><td>720</td><td>1.676</td><td>1.095</td><td>0.556</td><td>0.565</td><td>0.422</td><td>0.410</td><td>0.391</td><td>0.438</td><td>0.406</td><td>0.443</td><td>0.377</td><td>0.434</td><td>0.480</td><td>0.461</td><td>0.372</td><td>0.434</td><td>0.404</td><td>0.423</td></tr><tr><td>960</td><td>1.591</td><td>1.128</td><td>0.605</td><td>0.599</td><td>0.471</td><td>0.451</td><td>0.492</td><td>0.550</td><td>0.460</td><td>0.548</td><td>0.366</td><td>0.426</td><td>0.550</td><td>0.489</td><td>0.393</td><td>0.449</td><td>0.433</td><td>0.438</td></tr><tr><td rowspan=\"4\">ILI</td><td>24</td><td>4.220</td><td>1.335</td><td>4.975</td><td>1.660</td><td>4.265</td><td>1.387</td><td>3.954</td><td>1.323</td><td>4.588</td><td>1.462</td><td>3.101</td><td>1.238</td><td>3.970</td><td>1.338</td><td>2.687</td><td>1.147</td><td>3.041</td><td>1.186</td></tr><tr><td>36</td><td>4.771</td><td>1.427</td><td>5.322</td><td>1.659</td><td>4.777</td><td>1.496</td><td>4.167</td><td>1.360</td><td>4.845</td><td>1.496</td><td>3.397</td><td>1.270</td><td>4.377</td><td>1.410</td><td>2.887</td><td>1.160</td><td>3.406</td><td>1.232</td></tr><tr><td>48</td><td>4.945</td><td>1.462</td><td>5.425</td><td>1.632</td><td>5.333</td><td>1.592</td><td>4.746</td><td>1.463</td><td>4.865</td><td>1.516</td><td>2.947</td><td>1.203</td><td>4.811</td><td>1.503</td><td>2.797</td><td>1.155</td><td>3.459</td><td>1.221</td></tr><tr><td>60</td><td>5.176</td><td>1.504</td><td>5.477</td><td>1.675</td><td>5.070</td><td>1.552</td><td>5.219</td><td>1.553</td><td>5.212</td><td>1.576</td><td>3.019</td><td>1.202</td><td>5.204</td><td>1.588</td><td>2.809</td><td>1.163</td><td>3.640</td><td>1.305</td></tr><tr><td rowspan=\"5\">Traffic</td><td>24</td><td>0.668</td><td>0.378</td><td>0.648</td><td>0.403</td><td>0.506</td><td>0.278</td><td>0.597</td><td>0.332</td><td>0.608</td><td>0.334</td><td>0.550</td><td>0.363</td><td>0.606</td><td>0.338</td><td>0.562</td><td>0.375</td><td>0.491</td><td>0.274</td></tr><tr><td>48</td><td>0.709</td><td>0.400</td><td>0.709</td><td>0.425</td><td>0.512</td><td>0.298</td><td>0.658</td><td>0.369</td><td>0.644</td><td>0.359</td><td>0.595</td><td>0.376</td><td>0.619</td><td>0.346</td><td>0.567</td><td>0.374</td><td>0.519</td><td>0.295</td></tr><tr><td>168</td><td>0.900</td><td>0.523</td><td>0.713</td><td>0.435</td><td>0.521</td><td>0.319</td><td>0.664</td><td>0.363</td><td>0.660</td><td>0.391</td><td>0.649</td><td>0.407</td><td>0.635</td><td>0.347</td><td>0.607</td><td>0.385</td><td>0.513</td><td>0.289</td></tr><tr><td>336</td><td>1.067</td><td>0.599</td><td>0.741</td><td>0.451</td><td>0.540</td><td>0.335</td><td>0.654</td><td>0.358</td><td>0.747</td><td>0.405</td><td>0.624</td><td>0.388</td><td>0.641</td><td>0.347</td><td>0.624</td><td>0.389</td><td>0.530</td><td>0.300</td></tr><tr><td>720</td><td>1.461</td><td>0.787</td><td>0.768</td><td>0.474</td><td>0.557</td><td>0.343</td><td>0.685</td><td>0.370</td><td>0.792</td><td>0.430</td><td>0.674</td><td>0.417</td><td>0.670</td><td>0.364</td><td>0.623</td><td>0.378</td><td>0.573</td><td>0.313</td></tr></table>\n\n$l$  are rearranged to get the layer prediction  $\\mathbf{x}_{T + 1:T + \\tau}^{pred,l}$ . Predictions of all the layers are summed to obtain the final forecasting  $\\mathbf{x}_{T + 1:T + \\tau}^{pred}$ .\n",
  "experiments": "# 4 EXPERIMENTS\n\n# 4.1 PROTOCOLS\n\nDatasets We conduct experiments on six real-world datasets following Zhou et al. (2021); Wu et al. (2021a). 1) ETTh1 (Electricity Transformer Temperature-hourly), 2) ETTm1 (Electricity Transformer Temperature-minutely), 3) WTH (Weather), 4) ECL (Electricity Consuming Load), 5) ILI (Influenza-Like Illness), 6) Traffic. The train/val/test splits for the first four datasets are same as Zhou et al. (2021), the last two are split by the ratio of 0.7:0.1:0.2 following Wu et al. (2021a).\n\nBaselines We use the following popular models for MTS forecasting as baselines:1) LSTMa (Bahdanau et al., 2015), 2) LSTnet (Lai et al., 2018), 3) MTGNN (Wu et al., 2020), and recent Transformer-based models for MTS forecasting: 4) Transformer (Vaswani et al., 2017), 5) Informer (Zhou et al., 2021), 6) Autoformer (Wu et al., 2021a), 7) Pyraformer (Liu et al., 2021a) and 8) FEDformer (Zhou et al., 2022).\n\nSetup We use the same setting as in Zhou et al. (2021): train/val/test sets are zero-mean normalized with the mean and std of training set. On each dataset, we evaluate the performance over the changing future window size  $\\tau$ . For each  $\\tau$ , the past window size  $T$  is regarded as a hyper-parameter to search which is a common protocol in recent MTS transformer literature (Zhou et al., 2021; Liu et al., 2021a). We roll the whole set with stride  $= 1$  to generate different input-output pairs. The Mean Square Error (MSE) and Mean Absolute Error (MAE) are used as evaluation metrics. All experiments are repeated for 5 times and the mean of the metrics reported. Our Crossformer only utilize the past series to forecast the future, while baseline models use additional covariates such as hour-of-the-day. Details about datasets, baselines, implementation, hyper-parameters are shown in Appendix A.\n\n# 4.2 MAIN RESULTS\n\nAs shown in Table 1, Crossformer shows leading performance on most datasets, as well as on different prediction length settings, with the 36 top-1 and 51 top-2 cases out of 58 in total. It is worth noting that, perhaps due to the explicit use of cross-dimension dependency via GNN, MTGNN outperforms many Transformer-based baselines. While MTGNN has been rarely compared in existing\n\nTable 2: Component ablation of Crossformer: DSW embedding, TSA layer and HED on ETTh1.  \n\n<table><tr><td>Models</td><td colspan=\"2\">Transformer</td><td colspan=\"2\">DSW</td><td colspan=\"2\">DSW+TSA</td><td colspan=\"2\">DSW+HED</td><td colspan=\"2\">DSW+TSA+HED</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>24</td><td>0.620</td><td>0.577</td><td>0.373</td><td>0.418</td><td>0.322</td><td>0.373</td><td>0.406</td><td>0.454</td><td>0.305</td><td>0.367</td></tr><tr><td>48</td><td>0.692</td><td>0.671</td><td>0.456</td><td>0.479</td><td>0.365</td><td>0.403</td><td>0.493</td><td>0.512</td><td>0.352</td><td>0.394</td></tr><tr><td>168</td><td>0.947</td><td>0.797</td><td>0.947</td><td>0.731</td><td>0.473</td><td>0.479</td><td>0.614</td><td>0.583</td><td>0.410</td><td>0.441</td></tr><tr><td>336</td><td>1.094</td><td>0.813</td><td>0.969</td><td>0.752</td><td>0.553</td><td>0.534</td><td>0.788</td><td>0.676</td><td>0.440</td><td>0.461</td></tr><tr><td>720</td><td>1.241</td><td>0.971</td><td>1.086</td><td>0.814</td><td>0.636</td><td>0.599</td><td>0.841</td><td>0.717</td><td>0.519</td><td>0.524</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/85c36d5c226392b274484758e93a712e0be424ce4311dfff6739dbe281569a03.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/f7bb730c12dfe0144cf9dd63b9f9a01de9b0ab389f900284e7cb1319c5a418ca.jpg)  \n(b)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/b26a4343293811da4ce63db6800688e658d7a7b7fca5e665e4aebb2ebd8793c4.jpg)  \n(c)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-13/c9dee6de-8314-4af0-8041-b5d462c28eaf/a0153be14d5d923342d23e2bb784f10951d3e425c14866d50b6d3c32dde4455c.jpg)  \n(d)  \nFigure 4: Evaluation on hyper-parameter impact and computational efficiency. (a) MSE against hyperparameter segment length  $L_{seg}$  in DSW embedding on ETTh1. (b) MSE against hyper-parameter number of routers  $c$  in the Cross-Dimension Stage of TSA layer on ETTh1. (c) Memory occupation against the input length  $T$  on ETTh1. (d) Memory occupation against number of dimensions  $D$  on synthetic datasets with different number of dimensions.\n\ntransformers for MTS forecasting literatures. FEDformer and Autoformer outperform our model on ILI. We conjecture this is because the size of dataset ILI is small and these two models introduce the prior knowledge of sequence decomposition into the network structure which makes them perform well when the data is limited. Crossformer still outperforms other baselines on this dataset.\n\n# 4.3 ABLATION STUDY\n\nIn our approach, there are three components: DSW embedding, TSA layer and HED. We perform ablation study on the ETTh1 dataset in line with Zhou et al. (2021); Liu et al. (2021a). We use Transformer as the baseline and  $\\mathbf{DSW} + \\mathbf{TSA} + \\mathbf{HED}$  to denote Crossformer without ablation. Three ablation versions are compared: 1) DSW 2)  $\\mathbf{DSW} + \\mathbf{TSA}3)$  DSW+HED.\n\nWe analyze the results shown in Table 2. 1) DSW performs better than Transformer on most settings. The only difference between DSW and Transformer is the embedding method, which indicates the usefulness of DSW embedding and the importance of cross-dimension dependency. 2) TSA constantly improves the forecasting accuracy. This suggests that it is reasonable to treat time and dimension differently. Moreover, TSA makes it possible to use Crossformer on datasets where the number of dimensions is large (e.g.  $D = 862$  for dataset Traffic). 3) Comparing DSW+HED with DSW, HED decreases the forecasting accuracy when prediction length is short but increases it for long term prediction. The possible reason is that information at different scales is helpful to long term prediction. 4) Combining DSW, TSA and HED, our Crossformer yields best results on all settings.\n\n# 4.4 EFFECT OF HYPER-PARAMETERS\n\nWe evaluate the effect of two hyper-parameters: segment length ( $L_{seg}$  in Eq. 1) and number of routers in TSA ( $c$  in Cross-Dimension Stage of TSA) on the ETTh1 dataset. Segment Length: In Fig. 4(a), we prolong the segment length from 4 to 24 and evaluate MSE with different prediction windows. For short-term forecasting ( $\\tau = 24, 48$ ), smaller segment yields relevantly better results, but the prediction accuracy is stable. For long-term forecasting ( $\\tau \\geq 168$ ), prolonging the segment length from 4 to 24 causes the MSE to decrease. This indicates that long segments should be used for long-term forecasting. We further prolong the segment length to 48 for  $\\tau = 336, 720$ , the MSE is slightly larger than that of 24. The possible reason is that 24 hours exactly matches the daily period of this dataset, while 48 is too coarse to capture fine-grained information. Number of Routers in TSA Layer: Number of Routers  $c$  controls the information bandwidth among all dimensions. As Fig. 4(b) shows, the performance of Crossformer is stable w.r.t to  $c$  for  $\\tau \\leq 336$ . For  $\\tau = 720$ , the MSE is\n\nlarge when  $c = 3$  but decreases and stabilizes when  $c \\geq 5$ . In practice, we set  $c = 10$  to balance the prediction accuracy and computation efficiency.\n\n# 4.5 COMPUTATIONAL EFFICIENCY ANALYSIS\n\nThe theoretical complexity per layer of Transformer-based models is compared in Table 3. The complexity of Crossformer encoder is quadratic w.r.t  $T$ . However, for long-term prediction where large  $L_{seq}$  is used, the coefficient  $\\frac{1}{L_{seq}^2}$  term can significantly reduce its practical complexity. We evaluate the memory occupation of these models on ETTh1.4 We set the prediction window  $\\tau = 336$  and prolong input length  $T$ . For Crossformer,  $L_{seg}$  is set to 24, which is the best value for  $\\tau \\geq 168$\n\nTable 3: Computation complexity per layer of Transformer-based models.  $T$  denotes the length of past series,  $\\tau$  denotes the length of prediction window,  $D$  denotes the number of dimensions,  ${L}_{seg}$  denotes the segment length of DSW embedding in Crossformer.  \n\n<table><tr><td>Method</td><td>Encoder layer</td><td>Decoder layer</td></tr><tr><td>Transformer (Vaswani et al., 2017)</td><td>O(T2)</td><td>O(τ(τ+T))</td></tr><tr><td>Informer (Zhou et al., 2021)</td><td>O(T log T)</td><td>O(τ(τ+logT))</td></tr><tr><td>Autoformer (Wu et al., 2021a)</td><td>O(T log T)</td><td>O((T/2+τ) log(T/2+τ))</td></tr><tr><td>Pyraformer (Liu et al., 2021a)</td><td>O(T)</td><td>O(τ(τ+T))</td></tr><tr><td>FEDformer (Zhou et al., 2022)</td><td>O(T)</td><td>O(T/2+τ)</td></tr><tr><td>Crossformer (Ours)</td><td>O(D/T2)</td><td>O(D/T2τ(τ+T))</td></tr></table>\n\n(see Fig. 4 (a)). The result in Fig. 4 (c) shows that Crossformer achieves the best efficiency among the five methods within the tested length range. Theoretically, Informer, Autoformer and FEDformer are more efficient when  $T$  approaches infinity. In practice, Crossformer performs better when  $T$  is not extremely large (e.g.  $T \\leq 10^4$ ).\n\nWe also evaluate the memory occupation w.r.t the number of dimensions  $D$ . For baseline models where cross-dimension dependency is not modeled explicitly,  $D$  has little effect. Therefore, we compare Crossformer with its ablation versions in Section 4.3. We also evaluate the TSA layers that directly use MSA in Cross-Dimension Stage without the Router mechanism, denoted as TSA(w/o Router). Fig. 4 (d) shows that Crossformer without TSA layer (DSW and DSW+HED) has quadratic complexity w.r.t  $D$ . TSA(w/o Router) helps to reduce complexity and the Router mechanism further makes the complexity linear, so that Crossformer can process data with  $D = 300$ . Moreover, HED can slightly reduce the memory cost and we analyze this is because there are less vectors in upper layers after segment merging (see Fig. 3). Besides memory occupation, the actual running time evaluation is shown in Appendix B.6.\n",
  "hyperparameter": "Segment length (L_seg): 24 for long-term forecasting (τ≥168), smaller values like 4-12 for short-term forecasting (τ≤48); Number of routers (c): 10 (stable performance when c≥5 for long-term prediction); Model dimension (d_model): not explicitly specified but standard Transformer dimensions used; Number of encoder layers (N): 3 layers shown in architecture; Input length (T): treated as searchable hyperparameter for each prediction window τ; Prediction window (τ): evaluated at {24, 36, 48, 60, 96, 168, 288, 336, 672, 720, 960} depending on dataset"
}