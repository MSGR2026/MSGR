{
  "id": "iTransformer_2023",
  "paper_title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
  "alias": "iTransformer",
  "year": 2023,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "iTransformer inverts the conventional Transformer architecture for time series forecasting by treating each individual time series (variate) as a token rather than time points. This allows self-attention to capture multivariate correlations while feed-forward networks extract temporal representations from entire series. The inverted design enables better interpretability, generalization to unseen variates, and improved utilization of longer lookback windows, addressing fundamental limitations of previous Transformer-based forecasters.",
  "introduction": "# 1 INTRODUCTION\n\nTransformer (Vaswani et al., 2017) has achieved tremendous success in natural language processing (Brown et al., 2020) and computer vision (Dosovitskiy et al., 2021), growing into the foundation model that follows the scaling law (Kaplan et al., 2020). Inspired by the immense success in extensive fields, Transformer with strong capabilities of depicting pairwise dependencies and extracting multi-level representations in sequences is emerging in time series forecasting (Wu et al., 2021; Nie et al., 2023).\n\nHowever, researchers have recently begun to question the validity of Transformer-based forecasters, which typically embed multiple variates of the same timestamp into indistinguishable channels and apply attention on these temporal tokens to capture temporal dependencies. Considering the numerical but less semantic relationship among time points, researchers find that simple linear layers, which can be traced back to statistical forecasters (Box & Jenkins, 1968), have exceeded complicated Transformers on both et al., 2023; Das et al., 2023). Meanwhile, ensuring the independ\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/328fb172c5a3f2200215e36261f66b283d9d39b456dcea71fa572248de1c7305.jpg)  \nFigure 1: Performance of iTransformer. Average results (MSE) are reported following TimesNet (2023).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/0dd919f57b01a2071ddbb51e3d630fdf2a8360ac2cdc914da8ab1ccba017a0f5.jpg)  \nFigure 2: Comparison between the vanilla Transformer (top) and the proposed iTransformer (bottom). Transformer embeds the temporal token, which contains the multivariate representation of each time step. iTransformer embeds each series independently to the variate token, such that the attention module depicts the multivariate correlations and the feed-forward network encodes series representations.\n\ninformation is ever more highlighted by recent research that explicitly models multivariate correlations to achieve accurate forecasting (Zhang & Yan, 2023; Ekambaram et al., 2023), but this goal can be hardly achieved without subverting the vanilla Transformer architecture.\n\nConsidering the disputes of Transformer-based forecasters, we reflect on why Transformers perform even worse than linear models in time series forecasting while acting predominantly in many other fields. We notice that the existing structure of Transformer-based forecasters may be not suitable for multivariate time series forecasting. As shown on the top of Figure 2, it is notable that the points of the same time step that basically represent completely different physical meanings recorded by inconsistent measurements are embedded into one token with wiped-out multivariate correlations. And the token formed by a single time step can struggle to reveal beneficial information due to excessively local receptive field and time-unaligned events represented by simultaneous time points. Besides, while series variations can be greatly influenced by the sequence order, permutation-invariant attention mechanisms are improperly adopted on the temporal dimension (Zeng et al., 2023). Consequently, Transformer is weakened to capture essential series representations and portray multivariate correlations, limiting its capacity and generalization ability on diverse time series data.\n\nConcerning the potential risks of embedding multivariate points of a timestamp as a (temporal) token, we take an inverted view on time series and embed the whole time series of each variate independently into a (variate) token, the extreme case of Patching (Nie et al., 2023) that enlarges local receptive field. By inverting, the embedded token aggregates the global representations of series that can be more variate-centric and better leveraged by booming attention mechanisms for multivariate correlating. Meanwhile, the feed-forward network can be proficient enough to learn generalizable representations for distinct variates encoded from arbitrary lookback series and decoded to predict future series.\n\nBased on the above motivations, we believe it is not that Transformer is ineffective for time series forecasting, but rather it is improperly used. In this paper, we revisit the structure of Transformer and advocate Transformer as a fundamental backbone for time series forecasting. Technically, we embed each time series as variate tokens, adopt the attention for multivariate correlations, and employ the feed-forward network for series representations. Experimentally, the proposed Transformer achieves state-of-the-art performance on real-world forecasting benchmarks shown in Figure 1 and surprisingly tackles the pain points of Transformer-based forecasters. Our contributions lie in three aspects:\n\n- We reflect on the architecture of Transformer and refine that the competent capability of native Transformer components on multivariate time series is underexplored.  \n- We propose iTransformer that regards independent time series as tokens to capture multivariate correlations by self-attention and utilize layer normalization and feed-forward network modules to learn better series-global representations for time series forecasting.  \n- Experimentally, iTransformer achieves comprehensive state-of-the-art on real-world benchmarks. We extensively analyze the inverted modules and architecture choices, indicating a promising direction for the future improvement of Transformer-based forecasters.\n",
  "method": "# 3 ITRANSFORMER\n\nIn multivariate time series forecasting, given historical observations  $\\mathbf{X} = \\{\\mathbf{x}_1,\\dots ,\\mathbf{x}_T\\} \\in \\mathbb{R}^{T\\times N}$  with  $T$  time steps and  $N$  variates, we predict the future  $S$  time steps  $\\mathbf{Y} = \\{\\mathbf{x}_{T + 1},\\ldots ,\\mathbf{x}_{T + S}\\} \\in \\mathbb{R}^{S\\times N}$ . For convenience, we denote  $\\mathbf{X}_{t,:}$  as the simultaneously recorded time points at the step  $t$ , and  $\\mathbf{X}_{:,n}$  as the whole time series of each variate indexed by  $n$ . It is notable that  $\\mathbf{X}_{t,:}$  may not contain time points that essentially reflect the same event in real-world scenarios because of the systematical time lags among variates in the dataset. Besides, the elements of  $\\mathbf{X}_{t,:}$  can be distinct from each other in physical measurements and statistical distributions, for which a variate  $\\mathbf{X}_{:,n}$  generally shares.\n\n# 3.1 STRUCTURE OVERVIEW\n\nOur proposed iTransformer illustrated in Figure 4 adopts the encoder-only architecture of Transformer (Vaswani et al., 2017), including the embedding, projection, and Transformer blocks.\n\nEmbedding the whole series as the token Most Transformer-based forecasters typically regard multiple variates of the same time as the (temporal) token and follow the generative formulation of forecasting tasks. However, we find the approach on the numerical modality can be less instructive for\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/bb28fee96f4eabb1645ec89c3e9944ccfcf24e9430758b7d6a19f3b0e6305bec.jpg)  \nFigure 4: Overall structure of iTransformer, which shares the same modular arrangement with the encoder of Transformer. (a) Raw series of different variates are independently embedded as tokens. (b) Self-attention is applied to embedded variate tokens with enhanced interpretability revealing multivariate correlations. (c) Series representations of each token are extracted by the shared feedforward network. (d) Layer normalization is adopted to reduce the discrepancies among variates.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/e159c9ca213ee049c32880aae97cdbb4427da66aa6284da9755c4b5c5277c9f9.jpg)\n\nlearning attention maps, which is supported by increasing applications of Patching (Dosovitskiy et al., 2021; Nie et al., 2023) that broadens the respective field. Meanwhile, the triumph of linear forecasters also challenges the necessity of adopting a heavy encoder-decoder Transformer for generating tokens. Instead, our proposed encoder-only iTransformer focuses on representation learning and adaptive correlating of multivariate series. Each time series driven by the underlying complicated process is firstly tokenized to describe the properties of the variate, applied by self-attention for mutual interactions, and individually processed by feed-forward networks for series representations. Notably, the task to generate the predicted series is essentially delivered to linear layers, which has been proven competent by previous work (Das et al., 2023) and we provide a detailed analysis in the next section.\n\nBased on the above considerations, in iTransformer, the process of predicting future series of each specific variate  $\\hat{\\mathbf{Y}}_{:,n}$  based on the lookback series  $\\mathbf{X}_{:,n}$  is simply formulated as follows:\n\n$$\n\\mathbf {h} _ {n} ^ {0} = \\operatorname {E m b e d d i n g} (\\mathbf {X} _ {:; n}),\n$$\n\n$$\n\\mathbf {H} ^ {l + 1} = \\operatorname {T r m B l o c k} \\left(\\mathbf {H} ^ {l}\\right), l = 0, \\dots , L - 1, \\tag {1}\n$$\n\n$$\n\\hat {\\mathbf {Y}} _ {:, n} = \\operatorname {P r o j e c t i o n} (\\mathbf {h} _ {n} ^ {L}),\n$$\n\nwhere  $\\mathbf{H} = \\{\\mathbf{h}_1, \\dots, \\mathbf{h}_N\\} \\in \\mathbb{R}^{N \\times D}$  contains  $N$  embedded tokens of dimension  $D$  and the superscript denotes the layer index. Embedding:  $\\mathbb{R}^T \\mapsto \\mathbb{R}^D$  and Projection:  $\\mathbb{R}^D \\mapsto \\mathbb{R}^S$  are both implemented by multi-layer perceptron (MLP). The obtained variate tokens interact with each other by self-attention and are independently processed by the shared feed-forward network in each TrmBlock. Specifically, as the order of sequence is implicitly stored in the neuron permutation of the feed-forward network, the position embedding in the vanilla Transformer is no longer needed here.\n\niTransformers The architecture essentially presupposes no more specific requirements on Transformer variants, other than the attention is applicable for multivariate correlation. Thus, a bundle of efficient attention mechanisms (Li et al., 2021; Wu et al., 2022; Dao et al., 2022) can be the plugins, reducing the complexity when the variate number grows large. Besides, with the input flexibility of attention, the token number can vary from training to inference, and the model is allowed to be trained on arbitrary numbers of variates. The inverted Transformers, named iTransformers, are extensively evaluated in experiments of Section 4.2 and demonstrate advantages on time series forecasting.\n\n# 3.2 INVERTED TRANSFORMER COMPONENTS\n\nWe organize a stack of  $L$  blocks composed of the layer normalization, feed-forward network, and self-attention modules. But their duties on the inverted dimension are carefully reconsidered.\n\nLayer normalization Layer normalization (Ba et al., 2016) is originally proposed to increase the convergence and training stability of deep networks. In typical Transformer-based forecasters, the module normalizes the multivariate representation of the same timestamp, gradually fusing the variates with each other. Once the collected time points do not represent the same event, the operation will also introduce interaction noises between noncausal or delayed processes. In our inverted version, the normalization is applied to the series representation of individual variate as Equation 2, which has been studied and proved effective in tackling non-stationary problems (Kim et al., 2021; Liu et al., 2022b). Besides, since all series as (variate) tokens are normalized to a Gaussian distribution, the discrepancies caused by inconsistent measurements can be diminished. By contrast, in previous architecture, different tokens of time steps will be normalized, leading to oversmooth time series.\n\n$$\n\\operatorname {L a y e r N o r m} (\\mathbf {H}) = \\left\\{\\frac {\\mathbf {h} _ {n} - \\operatorname {M e a n} (\\mathbf {h} _ {n})}{\\sqrt {\\operatorname {V a r} (\\mathbf {h} _ {n})}} \\mid n = 1, \\dots , N \\right\\} \\tag {2}\n$$\n\nFeed-forward network Transformer adopts the feed-forward network (FFN) as the basic building block for encoding token representation and it is identically applied to each token. As aforementioned, in the vanilla Transformer, multiple variates of the same timestamp that form the token can be malpositioned and too localized to reveal enough information for predictions. In the inverted version, FFN is leveraged on the series representation of each variate token. By the universal approximation theorem (Hornik, 1991), they can extract complicated representations to describe a time series. With the stacking of inverted blocks, they are devoted to encoding the observed time series and decoding the representations for future series using dense non-linear connections, which work effectively as the recent works completely built on MLPs (Tolstikhin et al., 2021; Das et al., 2023).\n\nMore interestingly, the identical linear operation on independent time series, which serves as the combination of the recent linear forecasters (Zeng et al., 2023) and Channel Independence (Nie et al., 2023), can be instructive for us to understand the series representations. Recent revisiting on linear forecasters (Li et al., 2023) highlights that temporal features extracted by MLPs are supposed to be shared within distinct time series. We propose a rational explanation that the neurons of MLP are taught to portray the intrinsic properties of any time series, such as the amplitude, periodicity, and even frequency spectrums (neuron as a filter), serving as a more advantageous predictive representation learner than the self-attention applied on time points. Experimentally, we validate that the division of labor helps enjoy the benefits of linear layers in Section 4.3, such as the promoted performance if providing enlarged lookback series, and the generalization ability on unseen variates.\n\nSelf-attention While the attention mechanism is generally adopted for facilitating the temporal dependencies modeling in previous forecasters, the inverted model regards the whole series of one variate as an independent process. Concretely, with comprehensively extracted representations of each time series  $\\mathbf{H} = \\{\\mathbf{h}_0,\\dots ,\\mathbf{h}_N\\} \\in \\mathbb{R}^{N\\times D}$ , the self-attention module adopts linear projections to get queries, keys, and values  $\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in \\mathbb{R}^{N\\times d_k}$ , where  $d_{k}$  is the projected dimension.\n\nWith denotation of  $\\mathbf{q}_i, \\mathbf{k}_j \\in \\mathbb{R}^{d_k}$  as the specific query and key of one (variate) token, we notice that each entry of the pre-Softmax scores is formulated as  $\\mathbf{A}_{i,j} = (\\mathbf{QK}^\\top / \\sqrt{d_k})_{i,j} \\propto \\mathbf{q}_i^\\top \\mathbf{k}_j$ . Since each token is previously normalized on its feature dimension, the entries can somewhat reveal the variate-wise correlation, and the whole score map  $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$  exhibits the multivariate correlations between paired variate tokens. Consequently, highly correlated variate will be more weighted for the next representation interaction with values  $\\mathbf{V}$ . Based on this intuition, the proposed mechanism is believed to be more natural and interpretable for multivariate series forecasting. We further provide the visualization analysis of the score map in Section 4.3 and Appendix E.1.",
  "experiments": "# 4 EXPERIMENTS\n\nWe thoroughly evaluate the proposed iTransformer on various time series forecasting applications, validate the generality of the proposed framework and further dive into the effectiveness of applying the Transformer components on the inverted dimensions of time series.\n\nDatasets We extensively include 7 real-world datasets in our experiments, including ECL, ETT (4 subsets), Exchange, Traffic, Weather used by Autoformer (Wu et al., 2021), Solar-Energy datasets\n\nproposed in LSTNet (Lai et al., 2018), and PEMS (4 subsets) evaluated in SCINet (Liu et al., 2022a). We also provide the experiments on Market (6 subsets) in Appendix F.4. It records the minute-sampled server load of Alipay online transaction application with hundreds of variates, where we consistently outperform other baselines. Detailed dataset descriptions are provided in Appendix A.1.\n\n# 4.1 FORECASTING RESULTS\n\nIn this section, we conduct extensive experiments to evaluate the forecasting performance of our proposed model together with advanced deep forecasters.\n\nBaselines We carefully choose 10 well-acknowledged forecasting models as our benchmark, including (1) Transformer-based methods: Autoformer (Wu et al., 2021), FEDformer (Zhou et al., 2022), Stationary (Liu et al., 2022b), Crossformer (Zhang & Yan, 2023), PatchTST (Nie et al., 2023); (2) Linear-based methods: DLinear (Zeng et al., 2023), TiDE (Das et al., 2023), RLinear (Li et al., 2023); and (3) TCN-based methods: SCINet (Liu et al., 2022a), TimesNet (Wu et al., 2023).\n\nMain results Comprehensive forecasting results are listed in Table 1 with the best in red and the second underlined. The lower MSE/MAE indicates the more accurate prediction result. Compared with other forecasters, iTransformer is particularly good at forecasting high-dimensional time series. Besides, PatchTST as the previous state-of-the-art, fails in many cases of PEMS, which can stem from the extremely fluctuating series of the dataset, and the patching mechanism of PatchTST may lose focus on specific locality to handle rapid fluctuation. By contrast, the proposed model aggregating the whole series variations for series representations can better cope with this situation. Notably, as the representative that explicitly captures multivariate correlations, the performance of Crossformer is still subpar to iTransformer, indicating the interaction of time-unaligned patches from different multivariate will bring about unnecessary noise for forecasting. Therefore, the native Transformer components are competent for temporal modeling and multivariate correlating, and the proposed inverted architecture can effectively tackle real-world time series forecasting scenarios.\n\nTable 1: Multivariate forecasting results with prediction lengths  $S \\in \\{ {12},{24},{36},{48}\\}$  for PEMS and  $S \\in  \\{ {96},{192},{336},{720}\\}$  for others and fixed lookback length  $T = {96}$  . Results are averaged from all prediction lengths. Avg means further averaged by subsets. Full results are listed in Appendix F.4.  \n\n<table><tr><td>Models</td><td colspan=\"2\">iTransformer (Ours)</td><td colspan=\"2\">RLinear (2023)</td><td colspan=\"2\">PatchTST (2023)</td><td colspan=\"2\">Crossformer (2023)</td><td colspan=\"2\">TiDE (2023)</td><td colspan=\"2\">TimesNet (2023)</td><td colspan=\"2\">DLinear (2023)</td><td colspan=\"2\">SCINet (2022a)</td><td colspan=\"2\">FEDformer (2022)</td><td colspan=\"2\">Stationary (2022b)</td><td>Autoformer (2021)</td><td></td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td></td><td></td></tr><tr><td>ECL</td><td>0.178</td><td>0.270</td><td>0.219</td><td>0.298</td><td>0.205</td><td>0.290</td><td>0.244</td><td>0.334</td><td>0.251</td><td>0.344</td><td>0.192</td><td>0.295</td><td>0.212</td><td>0.300</td><td>0.268</td><td>0.365</td><td>0.214</td><td>0.327</td><td>0.193</td><td>0.296</td><td>0.227</td><td>0.338</td></tr><tr><td>ETT (Avg)</td><td>0.383</td><td>0.399</td><td>0.380</td><td>0.392</td><td>0.381</td><td>0.397</td><td>0.685</td><td>0.578</td><td>0.482</td><td>0.470</td><td>0.391</td><td>0.404</td><td>0.442</td><td>0.444</td><td>0.689</td><td>0.597</td><td>0.408</td><td>0.428</td><td>0.471</td><td>0.464</td><td>0.465</td><td>0.459</td></tr><tr><td>Exchange</td><td>0.360</td><td>0.403</td><td>0.378</td><td>0.417</td><td>0.367</td><td>0.404</td><td>0.940</td><td>0.707</td><td>0.370</td><td>0.413</td><td>0.416</td><td>0.443</td><td>0.354</td><td>0.414</td><td>0.750</td><td>0.626</td><td>0.519</td><td>0.429</td><td>0.461</td><td>0.454</td><td>0.613</td><td>0.539</td></tr><tr><td>Traffic</td><td>0.428</td><td>0.282</td><td>0.626</td><td>0.378</td><td>0.481</td><td>0.304</td><td>0.550</td><td>0.304</td><td>0.760</td><td>0.473</td><td>0.620</td><td>0.336</td><td>0.625</td><td>0.383</td><td>0.804</td><td>0.509</td><td>0.610</td><td>0.376</td><td>0.624</td><td>0.340</td><td>0.628</td><td>0.379</td></tr><tr><td>Weather</td><td>0.258</td><td>0.278</td><td>0.272</td><td>0.291</td><td>0.259</td><td>0.281</td><td>0.259</td><td>0.315</td><td>0.271</td><td>0.320</td><td>0.259</td><td>0.287</td><td>0.265</td><td>0.317</td><td>0.292</td><td>0.363</td><td>0.309</td><td>0.360</td><td>0.288</td><td>0.314</td><td>0.338</td><td>0.382</td></tr><tr><td>Solar-Energy</td><td>0.233</td><td>0.262</td><td>0.369</td><td>0.356</td><td>0.270</td><td>0.307</td><td>0.641</td><td>0.639</td><td>0.347</td><td>0.417</td><td>0.301</td><td>0.319</td><td>0.330</td><td>0.401</td><td>0.282</td><td>0.375</td><td>0.291</td><td>0.381</td><td>0.261</td><td>0.381</td><td>0.885</td><td>0.711</td></tr><tr><td>PEMS (Avg)</td><td>0.119</td><td>0.218</td><td>0.514</td><td>0.482</td><td>0.217</td><td>0.305</td><td>0.220</td><td>0.304</td><td>0.375</td><td>0.440</td><td>0.148</td><td>0.246</td><td>0.320</td><td>0.394</td><td>0.121</td><td>0.222</td><td>0.224</td><td>0.327</td><td>0.151</td><td>0.249</td><td>0.614</td><td>0.575</td></tr></table>\n\n# 4.2 ITRANSFORMERS GENERALITY\n\nIn this section, we evaluate iTransformers by applying our framework to Transformer and its variants, which generally address the quadratic complexity of the self-attention mechanism, including Reformer (Kitaev et al., 2020), Informer (Li et al., 2021), Flowformer (Wu et al., 2022) and FlashAttention (Dao et al., 2022). Surprising and promising discoveries are exhibited, indicating the simple inverted perspective can enhance Transformer-based forecasters with promoted performance with efficiency, generalization on unseen variates, and better utilization of historical observations.\n\nPerformance promotion We evaluate Transformers and the corresponding iTransformers with the reported performance promotions in Table 2. It is notable that the framework consistently improves various Transformers. Overall, it achieves averaged  $38.9\\%$  promotion on Transformer,  $36.1\\%$  on Reformer,  $28.5\\%$  on Informer,  $16.8\\%$  on Flowformer and  $32.2\\%$  on Flashformer, revealing the previous improper usage of the Transformer architecture on time series forecasting. Moreover, since the attention mechanism is adopted on the variate dimension in our inverted structure, the introduction of efficient attentions with linear complexity essentially addresses the computational problem due to\n\nnumerous variates, which is prevalent in real-world applications but can be resource-consuming for Channel Independence (Nie et al., 2023). Therefore, the idea of iTransformer can be widely practiced on Transformer-based forecasters to take advantage of booming efficient attention mechanisms.\n\nTable 2: Performance promotion obtained by our inverted framework. Flashformer means Transformer equipped with hardware-accelerated FlashAttention (Dao et al., 2022). We report the average performance and the relative MSE reduction (Promotion). Full results can be found in Appendix F.2.  \n\n<table><tr><td colspan=\"2\">Models</td><td colspan=\"2\">Transformer (2017)</td><td colspan=\"2\">Reformer (2020)</td><td colspan=\"2\">Informer (2021)</td><td colspan=\"2\">Flowformer (2022)</td><td colspan=\"2\">Flashformer (2022)</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"3\">ECL</td><td>Original</td><td>0.277</td><td>0.372</td><td>0.338</td><td>0.422</td><td>0.311</td><td>0.397</td><td>0.267</td><td>0.359</td><td>0.285</td><td>0.377</td></tr><tr><td>+Inverted</td><td>0.178</td><td>0.270</td><td>0.208</td><td>0.301</td><td>0.216</td><td>0.311</td><td>0.210</td><td>0.293</td><td>0.206</td><td>0.291</td></tr><tr><td>Promotion</td><td>35.6%</td><td>27.4%</td><td>38.4%</td><td>28.7%</td><td>30.5%</td><td>21.6%</td><td>21.3%</td><td>18.6%</td><td>27.8%</td><td>22.9%</td></tr><tr><td rowspan=\"3\">Traffic</td><td>Original</td><td>0.665</td><td>0.363</td><td>0.741</td><td>0.422</td><td>0.764</td><td>0.416</td><td>0.750</td><td>0.421</td><td>0.658</td><td>0.356</td></tr><tr><td>+Inverted</td><td>0.428</td><td>0.282</td><td>0.647</td><td>0.370</td><td>0.662</td><td>0.380</td><td>0.524</td><td>0.355</td><td>0.492</td><td>0.333</td></tr><tr><td>Promotion</td><td>35.6%</td><td>22.3%</td><td>12.7%</td><td>12.3%</td><td>13.3%</td><td>8.6%</td><td>30.1%</td><td>15.6%</td><td>25.2%</td><td>6.4%</td></tr><tr><td rowspan=\"3\">Weather</td><td>Original</td><td>0.657</td><td>0.572</td><td>0.803</td><td>0.656</td><td>0.634</td><td>0.548</td><td>0.286</td><td>0.308</td><td>0.659</td><td>0.574</td></tr><tr><td>+Inverted</td><td>0.258</td><td>0.279</td><td>0.248</td><td>0.292</td><td>0.271</td><td>0.330</td><td>0.266</td><td>0.285</td><td>0.262</td><td>0.282</td></tr><tr><td>Promotion</td><td>60.2%</td><td>50.8%</td><td>69.2%</td><td>55.5%</td><td>57.3%</td><td>39.8%</td><td>7.2%</td><td>7.7%</td><td>60.2%</td><td>50.8%</td></tr></table>\n\nVariate generalization By inverting vanilla Transformers, it is notable that the models are empowered with the generalization capability on unseen variates. Firstly, benefiting from the flexibility of the number of input tokens, the amount of variate channels is no longer restricted and thus feasible to vary from training and inference. Besides, feed-forward networks are identically applied on independent variate tokens in iTransformer. As aforementioned, the neurons as filters learn the intrinsic patterns of any time series, which are inclined to be shared and transferable among distinct variates.\n\nTo verify the hypothesis, we compare inverting with another generalizing strategy: Channel Independence, training a shared backbone to forecast all variates. We partition the variates of each dataset into five folders, train models with only  $20\\%$  of variates of one folder, and directly forecast all variates without fine-tuning. We compare the performance in Figure 5 and each bar presents the averaged results of all folders to avoid the randomness of partition. CI-Transformers take a long time to predict each variate one by one during inference while iTransformers directly predict all variates and generally present smaller increases, indicating FFN is competent to learn transferable time series representations. It leaves a potential direction to build a foundation model upon iTransformer, where diverse multivariate time series with different numbers of variates can be feasibly trained together.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/9bdf46aa82a709582712c694a8b714ae57f33a33547b7416b6849adcd5f4638e.jpg)  \nFigure 5: Performance of generalization on unseen variates. We partition the variates of each dataset into five folders, train models with  $20\\%$  variates, and use the partially trained model to forecast all varieties. iTransformers can be trained efficiently and forecast with good generalizability.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/1bc8410c59efbd275516b13040ac6f2cb84eff4c1fc9ecd07c0e67e03d524aa1.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/61541d6543b0742f60185f7945b54634513405027edc72723d7c3b734966333c.jpg)\n\nIncreasing lookback length Previous works have witnessed the phenomenon that the forecasting performance does not necessarily improve with the increase of lookback length on Transformers (Nie et al., 2023; Zeng et al., 2023), which can be attributed to the distracted attention on the growing input. However, the desired performance improvement is generally held on linear forecasts, theoretically supported by statistical methods (Box & Jenkins, 1968) with enlarged historical information to be\n\nutilized. As the working dimensions of attention and feed-forward network are inverted, we evaluate the performance of Transformers and iTransformer in Figure 6 with increased lookback length. The results surprisingly verify the rationality of leveraging MLPs on the temporal dimension such that Transformers can benefit from the extended lookback window for more precise predictions.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/5870f239790bffa4e9e25b7fee9a7f267067712d3a3a720e7dd63184215af8be.jpg)  \nFigure 6: Forecasting performance with the lookback length  $T \\in \\{48,96,192,336,720\\}$  and fixed prediction length  $S = 96$ . While the performance of Transformer-based forecasters does not necessarily benefit from the increased lookback length, the inverted framework empowers the vanilla Transformer and its variants with improved performance on the enlarged lookback window.\n\n# 4.3 MODEL ANALYSIS\n\nAblation study To verify the rational business of Transformer components, we provide detailed ablations covering both replacing components (Replace) and removing components (w/o) experiments. The results are listed in Table 3. iTransformer that utilizes attention on the variate dimension and feed-forward on the temporal dimension generally achieves the best performance. Notably, the performance of vanilla Transformer (the third row) performs the worst among these designs, revealing the potential risks of the conventional architecture, which we describe in detail in Appendix E.3.\n\nTable 3: Ablations on iTransformer. We replace different components on the respective dimension to learn multivariate correlations (Variate) and series representations (Temporal), in addition to component removal. The average results of all predicted lengths are listed here.  \n\n<table><tr><td rowspan=\"2\">Design</td><td rowspan=\"2\">Variate</td><td rowspan=\"2\">Temporal</td><td colspan=\"2\">ECL</td><td colspan=\"2\">Traffic</td><td colspan=\"2\">Weather</td><td colspan=\"2\">Solar-Energy</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>iTransformer</td><td>Attention</td><td>FFN</td><td>0.178</td><td>0.270</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.278</td><td>0.233</td><td>0.262</td></tr><tr><td rowspan=\"3\">Replace</td><td>Attention</td><td>Attention</td><td>0.193</td><td>0.293</td><td>0.913</td><td>0.500</td><td>0.255</td><td>0.280</td><td>0.261</td><td>0.291</td></tr><tr><td>FFN</td><td>Attention</td><td>0.202</td><td>0.300</td><td>0.863</td><td>0.499</td><td>0.258</td><td>0.283</td><td>0.285</td><td>0.317</td></tr><tr><td>FFN</td><td>FFN</td><td>0.182</td><td>0.287</td><td>0.599</td><td>0.348</td><td>0.248</td><td>0.274</td><td>0.269</td><td>0.287</td></tr><tr><td rowspan=\"2\">w/o</td><td>Attention</td><td>w/o</td><td>0.189</td><td>0.278</td><td>0.456</td><td>0.306</td><td>0.261</td><td>0.281</td><td>0.258</td><td>0.289</td></tr><tr><td>w/o</td><td>FFN</td><td>0.193</td><td>0.276</td><td>0.461</td><td>0.294</td><td>0.265</td><td>0.283</td><td>0.261</td><td>0.283</td></tr></table>\n\nAnalysis of series representations To further validate the claim that feed-forward networks are more favored to extract the series representations. We conduct representation analysis based on the centered kernel alignment (CKA) similarity (Kornblith et al., 2019). A higher CKA indicates more similar representations. For Transformer variants and iTransformers, we calculate the CKA between the output features of the first and the last block. Notably, previous works have demonstrated that time series forecasting, as a low-level generative task, prefers the higher CKA similarity (Wu et al., 2023; Dong et al., 2023) for the better performance. As shown in Figure 7, a clear division line is exhibited, implying that iTransformers have learned more appropriate series representations by inverting the dimension and thus achieve more accurate predictions. The results also advocate inverting Transformer deserves a fundamental renovation of the forecasting backbone.\n\nAnalysis of multivariate correlations By assigning the duty of multivariate correlation to the attention mechanism, the learned map enjoys enhanced interpretability. We present the case visualization on series from Solar-Energy in Figure 7, which has distinct correlations in the lookback and future windows. It can be observed that in the shallow attention layer, the learned map shares lots of similarities to the correlations of raw input series. As it dives into deeper layers, the learned map becomes gradually alike to the correlations of future series, which validates the inverted operation empowers interpretable attention for correlating, and the processes of encoding the past and decoding for the future are essentially conducted in series representations during feed-forwarding.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/74ebf28e360e09808d119db6e6e6be8e04c7eca9f4c04c811db27ca77106dd51.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/053dc4a217437220e5cc86553d633c043c76fc257552f691371572930af56638.jpg)  \nFigure 7: Analysis of series representations and multivariate correlations. Left: MSE and CKA similarity of representations comparison between Transformers and iTransformers. A higher CKA similarity indicates more favored representations for accurate predictions. Right: A case visualization of multivariate correlations of raw time series and the learned score maps by inverted self-attention.\n\nEfficient training strategy Due to the quadratic complexity of self-attention, it can be overwhelming for training on numerous variates, which is very common in real-world scenarios. In addition to efficient attention mechanisms, we propose a novel training strategy for high-dimensional multivariate series by taking advantage of previously demonstrated variate generation capability. Concretely, we randomly choose part of the variates in each batch and only train the model with selected variates. Since the number of variate channels is flexible because of our inverting, the model can predict all the variates for predictions. As shown in Figure 8, the performance of our proposed strategy is still comparable with full-variate training, while the memory footprint can be reduced significantly.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/ec7c330ac92042526a862d164a012f8e03134f90b2b10f3c3e24968ace278bf0.jpg)  \nFigure 8: Analysis of the efficient training strategy. While the performance (left) remains stable on partially trained variates of each batch with different sampled ratios, the memory footprint (right) can be cut off greatly. We provide the comprehensive model efficiency analysis in Appendix D.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/8fe85a08-441a-4ed2-b0b8-159936b9ccc5/c3733e9084b5d23e9f60fa9e2742977570ffddb1a22e72945390a0afd207be44.jpg)",
  "hyperparameter": "Lookback length T = 96 (default, also tested with T ∈ {48, 96, 192, 336, 720}); Prediction lengths S ∈ {96, 192, 336, 720} for most datasets and S ∈ {12, 24, 36, 48} for PEMS datasets; Number of Transformer blocks L (not explicitly specified but uses standard stacking); Embedding dimension D (converts from T to D via MLP); Projected attention dimension d_k; The model uses encoder-only architecture without position embeddings; Layer normalization is applied on individual variate representations; For efficient training on high-dimensional data, random variate sampling strategy is proposed where only a subset of variates are trained per batch while maintaining full prediction capability."
}