{
  "id": "Pyraformer_2021",
  "paper_title": "Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting",
  "alias": "Pyraformer",
  "year": 2021,
  "domain": "TimeSeries",
  "task": "anomaly_detection",
  "idea": "Pyraformer introduces a pyramidal attention mechanism for long-range time series forecasting that reduces computational complexity from O(L²) to O(L) while maintaining O(1) maximum path length. The core innovation is constructing a multi-resolution C-ary tree structure where nodes at coarser scales summarize information from C finer-scale nodes, enabling efficient capture of both short-range and long-range temporal dependencies through inter-scale and intra-scale connections in a pyramidal graph. A Coarser-Scale Construction Module (CSCM) initializes multi-resolution nodes via convolutions, and a customized CUDA kernel using TVM enables practical implementation of the sparse pyramidal attention pattern.",
  "introduction": "# 1 INTRODUCTION\n\nTime series forecasting is the cornerstone for downstream tasks such as decision making and risk management. As an example, reliable prediction of the online traffic for micro-services can yield early warnings of the potential risk in cloud systems. Furthermore, it also provides guidance for dynamic resource allocation, in order to minimize the cost without degrading the performance. In addition to online traffic, time series forecasting has also found vast applications in other fields, including disease propagation, energy management, and economics and finance.\n\nThe major challenge of time series forecasting lies in constructing a powerful but parsimonious model that can compactly capture temporal dependencies of different ranges. Time series often exhibit both short-term and long-term repeating patterns (Lai et al., 2018), and taking them into account is the key to accurate prediction. Of particular note is the more difficult task of handling long-range dependencies, which is characterized by the length of the longest signal traversing path (see Proposition 2 for the definition) between any two positions in the time series (Vaswani et al., 2017). The shorter the path, the better the dependencies are captured. Additionally, to allow the models to learn these long-term patterns, the historical input to the models should also be long. To this end, low time and space complexity is a priority.\n\nUnfortunately, the present state-of-the-art methods fail to accomplish these two objectives simultaneously. On one end, RNN (Salinas et al., 2020) and CNN (Munir et al., 2018) achieve a low time complexity that is linear in terms of the time series length  $L$ , yet their maximum length of the signal traversing path is  $\\mathcal{O}(L)$ , thus rendering them difficult to learn dependencies between distant positions. On the other extreme, Transformer dramatically shortens the maximum path to be  $\\mathcal{O}(1)$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/efb55c10-1224-4a1f-afb4-e00a00ccf72f/8798da7cf974da8e5c263b2cbbec02816c861f200796f92b155043f498587383.jpg)  \nFigure 1: Graphs of commonly used neural network models for sequence data.\n\nTable 1: Comparison of the complexity and the maximum signal traveling path for different models, where  $G$  is the number of global tokens in ETC. In practice, the  $G$  increases with  $L$ , and so the complexity of ETC is super-linear.  \n\n<table><tr><td>Method</td><td>Complexity per layer</td><td>Maximum path length</td></tr><tr><td>CNN (Munir et al., 2018)</td><td>O(L)</td><td>O(L)</td></tr><tr><td>RNN (Salinas et al., 2020)</td><td>O(L)</td><td>O(L)</td></tr><tr><td>Full-Attention (Vaswani et al., 2017)</td><td>O(L2)</td><td>O(1)</td></tr><tr><td>ETC (Ainslie et al., 2020)</td><td>O(GL)</td><td>O(1)</td></tr><tr><td>Longformer (Beltagy et al., 2020)</td><td>O(L)</td><td>O(L)</td></tr><tr><td>LogTrans (Li et al., 2019)</td><td>O(L log L)</td><td>O(log L)</td></tr><tr><td>Pyraformer</td><td>O(L)</td><td>O(1)</td></tr></table>\n\nat the sacrifice of increasing the time complexity to  $\\mathcal{O}(L^2)$ . As a consequence, it cannot tackle very long sequences. To find a compromise between the model capacity and complexity, variants of Transformer are proposed, such as Longformer (Beltagy et al., 2020), Reformer (Kitaev et al., 2019), and Informer (Zhou et al., 2021). However, few of them can achieve a maximum path length less than  $\\mathcal{O}(L)$  while greatly reducing the time and space complexity.\n\nIn this paper, we propose a novel pyramidal attention based Transformer (Pyraformer) to bridge the gap between capturing the long-range dependencies and achieving a low time and space complexity. Specifically, we develop the pyramidal attention mechanism by passing messages based on attention in the pyramidal graph as shown in Figure 1(d). The edges in this graph can be divided into two groups: the inter-scale and the intra-scale connections. The inter-scale connections build a multiresolution representation of the original sequence: nodes at the finest scale correspond to the time points in the original time series (e.g., hourly observations), while nodes in the coarser scales represent features with lower resolutions (e.g., daily, weekly, and monthly patterns). Such latent coarser-scale nodes are initially introduced via a coarser-scale construction module. On the other hand, the intra-scale edges capture the temporal dependencies at each resolution by connecting neighboring nodes together. As a result, this model provides a compact representation for long-range temporal dependencies among far-apart positions by capturing such behavior at coarser resolutions, leading to a smaller length of the signal traversing path. Moreover, modeling temporal dependencies of different ranges at different scales with sparse neighboring intra-scale connections significantly reduces the computational cost. In short, our key contributions comprise:\n\n- We propose Pyraformer to simultaneously capture temporal dependencies of different ranges in a compact multi-resolution fashion. To distinguish Pyraformer from the state-of-the-art methods, we summarize all models from the perspective of graphs in Figure 1.  \n- Theoretically, we prove that by choosing parameters appropriately, the maximum path length of  $\\mathcal{O}(1)$  and the time and space complexity of  $\\mathcal{O}(L)$  can be reached concurrently. To\n\nhighlight the appeal of the proposed model, we further compare different models in terms of the maximum path and the complexity in Table 1.\n\n- Experimentally, we show that the proposed Pyraformer yields more accurate predictions than the original Transformer and its variants on various real-world datasets under the scenario of both single-step and long-range multi-step forecasting, but with lower time and memory cost.\n",
  "method": "# 3 METHOD\n\nThe time series forecasting problem can be formulated as predicting the future  $M$  steps  $\\pmb{z}_{t + 1:t + M}$  given the previous  $L$  steps of observations  $\\pmb{z}_{t - L + 1:t}$  and the associated covariates  $\\pmb{x}_{t - L + 1:t + M}$  (e.g., hour-of-the-day). To move forward to this goal, we propose Pyraformer in this paper, whose overall architecture is summarized in Figure 2. As shown in the figure, we first embed the observed data, the covariates, and the positions separately and then add them together, in the same vein with Informer (Zhou et al., 2021). Next, we construct a multi-resolution  $C$ -ary tree using the coarser-scale construction module (CSCM), where nodes at a coarser scale summarize the information of  $C$  nodes at the corresponding finer scale. To further capture the temporal dependencies of different ranges, we introduce the pyramidal attention module (PAM) by passing messages using the attention mechanism in the pyramidal graph. Finally, depending on the downstream task, we employ different network structures to output the final predictions. In the sequel, we elaborate on each part of the proposed model. For ease of exposition, all notations in this paper are summarized in Table 4.\n\n# 3.1 PYRAMIDAL ATTENTION MODULE (PAM)\n\nWe begin with the introduction of the PAM, since it lies at the heart of Pyraformer. As demonstrated in Figure 1(d), we leverage a pyramidal graph to describe the temporal dependencies of the observed time series in a multiresolution fashion. Such a multiresolution structure has proved itself an effective and efficient tool for long-range interaction modeling in the field of computer vision (Sun et al., 2019; Wang et al., 2021) and statistical signal processing (Choi et al., 2008; Yu et al., 2019). We can decompose the pyramidal graph into two parts: the inter-scale and the intra-scale connections. The inter-scale connections form a  $C$ -ary tree, in which each parent has  $C$  children. For example, if we associate the finest scale of the pyramidal graph with hourly observations of the original time series, the nodes at coarser scales can be regarded as the daily, weekly, and even monthly features of the time series. As a consequence, the pyramidal graph offers a multi-resolution representation of the original time series. Furthermore, it is easier to capture long-range dependencies (e.g., monthly dependence) in the coarser scales by simply connecting the neighboring nodes via the intra-scale connections. In other words, the coarser scales are instrumental in describing long-range correlations in a manner that is graphically far more parsimonious than could be solely captured with a single, finest scale model. Indeed, the original single-scale Transformer (see Figure 1(a)) adopts a full graph that connects every two nodes at the finest scale so as to model the long-range dependencies, leading to a computationally burdensome model with  $\\mathcal{O}(L^2)$  time and space complexity (Vaswani\n\net al., 2017). In stark contrast, as illustrated below, the pyramidal graph in the proposed Pyraformer reduces the computational cost to  $\\mathcal{O}(L)$  without increasing the order of the maximum length of the signal traversing path.\n\nBefore delving into the PAM, we first introduce the original attention mechanism. Let  $\\mathbf{X}$  and  $\\mathbf{Y}$  denote the input and output of a single attention head respectively. Note that multiple heads can be introduced to describe the temporal pattern from different perspectives.  $\\mathbf{X}$  is first linearly transformed into three distinct matrices, namely, the query  $\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_{\\mathbf{Q}}$ , the key  $\\mathbf{K} = \\mathbf{X}\\mathbf{W}_{\\mathbf{K}}$ , and the value  $\\mathbf{V} = \\mathbf{X}\\mathbf{W}_{\\mathbf{V}}$ , where  $\\mathbf{W}_{\\mathbf{Q}}, \\mathbf{W}_{\\mathbf{K}}, \\mathbf{W}_{\\mathbf{V}} \\in \\mathbb{R}^{L\\times D_{\\mathbf{K}}}$ . For the  $i$ -th row  $\\mathbf{q}_i$  in  $\\mathbf{Q}$ , it can attend to any rows (i.e., keys) in  $\\mathbf{K}$ . In other words, the corresponding output  $\\mathbf{y}_i$  can be expressed as:\n\n$$\n\\boldsymbol {y} _ {i} = \\sum_ {\\ell = 1} ^ {L} \\frac {\\exp \\left(\\boldsymbol {q} _ {i} \\boldsymbol {k} _ {\\ell} ^ {T} / \\sqrt {D _ {K}}\\right) \\boldsymbol {v} _ {\\ell}}{\\sum_ {\\ell = 1} ^ {L} \\exp \\left(\\boldsymbol {q} _ {i} \\boldsymbol {k} _ {\\ell} ^ {T} / \\sqrt {D _ {K}}\\right)}, \\tag {1}\n$$\n\nwhere  $\\pmb{k}_{\\ell}^{T}$  denotes the transpose of row  $\\ell$  in  $\\pmb{K}$ . We emphasize that the number of query-key dot products (Q-K pairs) that need to be calculated and stored dictates the time and space complexity of the attention mechanism. Viewed another way, this number is proportional to the number of edges in the graph (see Figure 1(a)). Since all Q-K pairs are computed and stored in the full attention mechanism (1), the resulting time and space complexity is  $\\mathcal{O}(L^2)$ .\n\nAs opposed to the above full attention mechanism, every node only pays attention to a limited set of keys in the PAM, corresponding to the pyramidal graph in Figure 1d. Concretely, suppose that  $n_{\\ell}^{(s)}$  denotes the  $\\ell$ -th node at scale  $s$ , where  $s = 1, \\dots, S$  represents the bottom scale to the top scale sequentially. In general, each node in the graph can attend to a set of neighboring nodes  $\\mathbb{N}_{\\ell}^{(s)}$  at three scales: the adjacent  $A$  nodes at the same scale including the node itself (denoted as  $\\mathbb{A}_{\\ell}^{(s)}$ ), the  $C$  children it has in the  $C$ -ary tree (denoted as  $\\mathbb{C}_{\\ell}^{(s)}$ ), and the parent of it in the  $C$ -ary tree (denoted  $\\mathbb{P}_{\\ell}^{(s)}$ ), that is,\n\n$$\n\\left\\{ \\begin{array}{l l} \\mathbb {N} _ {\\ell} ^ {(s)} = & \\mathbb {A} _ {\\ell} ^ {(s)} \\cup \\mathbb {C} _ {\\ell} ^ {(s)} \\cup \\mathbb {P} _ {\\ell} ^ {(s)} \\\\ \\mathbb {A} _ {\\ell} ^ {(s)} = & \\left\\{n _ {j} ^ {(s)}: | j - \\ell | \\leq \\frac {A - 1}{2}, 1 \\leq j \\leq \\frac {L}{C ^ {s - 1}} \\right\\} \\\\ \\mathbb {C} _ {\\ell} ^ {(s)} = & \\left\\{n _ {j} ^ {(s - 1)}: (\\ell - 1) C <   j \\leq \\ell C \\right\\} \\quad \\text {i f} s \\geq 2 \\text {e l s e} \\emptyset \\\\ \\mathbb {P} _ {\\ell} ^ {(s)} = & \\left\\{n _ {j} ^ {(s + 1)}: j = \\lceil \\frac {\\ell}{C} \\rceil \\right\\} \\quad \\text {i f} s \\leq S - 1 \\text {e l s e} \\emptyset \\end{array} . \\right. \\tag {2}\n$$\n\nIt follows that the attention at node  $n_{\\ell}^{(s)}$  can be simplified as:\n\n$$\n\\boldsymbol {y} _ {i} = \\sum_ {\\ell \\in \\mathbb {N} _ {\\ell} ^ {(s)}} \\frac {\\exp \\left(\\boldsymbol {q} _ {i} \\boldsymbol {k} _ {\\ell} ^ {T} / \\sqrt {d _ {K}}\\right) \\boldsymbol {v} _ {\\ell}}{\\sum_ {\\ell \\in \\mathbb {N} _ {\\ell} ^ {(s)}} \\exp \\left(\\boldsymbol {q} _ {i} \\boldsymbol {k} _ {\\ell} ^ {T} / \\sqrt {d _ {K}}\\right)}, \\tag {3}\n$$\n\nWe further denote the number of attention layers as  $N$ . Without loss of generality, we assume that  $L$  is divisible by  $C^{S - 1}$ . We can then have the following lemma (cf. Appendix B for the proof and Table 4 for the meanings of the notations).\n\nLemma 1. Given  $A$ ,  $C$ ,  $L$ ,  $N$ , and  $S$  that satisfy Equation (4), after  $N$  stacked attention layers, nodes at the coarsest scale can obtain a global receptive field.\n\n$$\n\\frac {L}{C ^ {S - 1}} - 1 \\leq \\frac {(A - 1) N}{2}. \\tag {4}\n$$\n\nIn addition, when the number of scales  $S$  is fixed, the following two propositions summarize the time and space complexity and the order of the maximum path length for the proposed pyramidal attention mechanism. We refer the readers to Appendix C and D for proof.\n\nProposition 1. The time and space complexity for the pyramidal attention mechanism is  $\\mathcal{O}(AL)$  for given  $A$  and  $L$  and amounts to  $\\mathcal{O}(L)$  when  $A$  is a constant w.r.t.  $L$ .\n\nProposition 2. Let the signal traversing path between two nodes in a graph denote the shortest path connecting them. Then the maximum length of signal traversing path between two arbitrary nodes in the pyramidal graph is  $\\mathcal{O}(S + L / C^{S - 1} / A)$  for given  $A, C, L$ , and  $S$ . Suppose that  $A$  and  $S$  are fixed and  $C$  satisfies Equation (5), the maximum path length is  $\\mathcal{O}(1)$  for time series with length  $L$ .\n\n$$\n{ } ^ { S - 1 } \\sqrt { L } \\geq C \\geq \\sqrt [ s - 1 ] { \\frac { L } { ( A - 1 ) N / 2 + 1 } } . \\tag {5}\n$$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/efb55c10-1224-4a1f-afb4-e00a00ccf72f/81005cb6bb016a327791ec50a4b932346b255c462d7074eaad745ac1256c0c61.jpg)  \nFigure 3: Coarser-scale construction module:  $B$  is the batch size and  $D$  is the dimension of a node.\n\nIn our experiments, we fix  $S$  and  $N$ , and  $A$  can only take 3 or 5, regardless of the sequence length  $L$ . Therefore, the proposed PAM achieves the complexity of  $\\mathcal{O}(L)$  with the maximum path length of  $\\mathcal{O}(1)$ . Note that in the PAM, a node can attend to at most  $A + C + 1$  nodes. Unfortunately, such a sparse attention mechanism is not supported in the existing deep learning libraries, such as Pytorch and TensorFlow. A naive implementation of the PAM that can fully exploit the tensor operation framework is to first compute the product between all Q-K pairs, i.e.,  $q_{i}k_{\\ell}^{T}$  for  $\\ell = 1,\\dots ,L$ , and then mask out  $\\ell \\notin \\mathbb{N}_{\\ell}^{(s)}$ . However, the resulting time and space complexity of this implementation is still  $\\mathcal{O}(L^2)$ . Instead, we build a customized CUDA kernel specialized for the PAM using TVM (Chen et al., 2018), practically reducing the computational time and memory cost and making the proposed model amenable to long time series. Longer historical input is typically helpful for improving the prediction accuracy, as more information is provided, especially when long-range dependencies are considered.\n\n# 3.2 COARSEN-SCALE CONSTRUCTION MODULE (CSCM)\n\nCSCM targets at initializing the nodes at the coarser scales of the pyramidal graph, so as to facilitate the subsequent PAM to exchange information between these nodes. Specifically, the coarse-scale nodes are introduced scale by scale from bottom to top by performing convolutions on the corresponding children nodes  $\\mathbb{C}_{\\ell}^{(s)}$ . As demonstrated in Figure 3, several convolution layers with kernel size  $C$  and stride  $C$  are sequentially applied to the embedded sequence in the dimension of time, yielding a sequence with length  $L / C^s$  at scale  $s$ . The resulting sequences at different scales form a  $C$ -ary tree. We concatenate these fine-to-coarse sequences before inputting them to the PAM. In order to reduce the amount of parameters and calculations, we reduce the dimension of each node by a fully connected layer before inputting the sequence into the stacked convolution layers and restore it after all convolutions. Such a bottleneck structure significantly reduces the number of parameters in the module and can guard against over-fitting.\n\n# 3.3 PREDICTION MODULE\n\nFor single-step forecasting, we add an end token (by setting  $z_{t+1} = 0$ ) to the end of the historical sequence  $z_{t-L+1:t}$  before inputting it into the embedding layer. After the sequence is encoded by the PAM, we gather the features given by the last nodes at all scales in the pyramidal graph, concatenate and then input them into a fully connected layer for prediction.\n\nFor multi-step forecasting, we propose two prediction modules. The first one is the same with the single-step forecasting module, but maps the last nodes at all scales to all  $M$  future time steps in a batch. The second one, on the other hand, resorts to a decoder with two full attention layers. Specifically, similar to the original Transformer (Vaswani et al., 2017), we replace the observations at the future  $M$  time steps with 0, embed them in the same manner with the historical observations, and refer to the summation of the observation, covariate, and positional embedding as the \"prediction token\"  $\\pmb{F}_{p}$ . The first attention layer then takes the prediction tokens  $\\pmb{F}_{p}$  as the query and the output of the encoder  $\\pmb{F}_{e}$  (i.e., all nodes in the PAM) as the key and the value, and yields  $\\pmb{F}_{d1}$ . The second layer takes  $\\pmb{F}_{d1}$  as the query, but takes the concatenated  $\\pmb{F}_{d1}$  and  $\\pmb{F}_{e}$  as the key and the value. The historical information  $\\pmb{F}_{e}$  is fed directly into both attention layers, since such information is vital for accurate long-range forecasting. The final prediction is then obtained through a fully connected layer across the dimension of channels. Again, we output all future predictions together to avoid the problem of error accumulation in the autoregressive decoder of Transformer.\n\nTable 2: Single-step forecasting results on three datasets. \"Q-K pairs\" refer to the number of querykey dot products performed by all attention layers in the network, which encodes the time and space complexity. We write the number of attention layers by  $N$ , the number of attention heads by  $H$ , the number of scales by  $S$ , the dimension of a node by  $D$ , the dimension of a key by  $D_K$ , the maximum dimension of feed-forward layer by  $D_F$ , and the convolution stride by  $C$ .  \n\n<table><tr><td>Methods</td><td>Parameters</td><td>Datasets</td><td>NRMSE</td><td>ND</td><td>Q-K pairs</td></tr><tr><td rowspan=\"3\">Full-attention</td><td rowspan=\"3\">O(N(HDDK+DDF))</td><td>Electricity</td><td>0.328</td><td>0.041</td><td>456976</td></tr><tr><td>Wind</td><td>0.175</td><td>0.082</td><td>589824</td></tr><tr><td>App Flow</td><td>0.407</td><td>0.080</td><td>589824</td></tr><tr><td rowspan=\"3\">LogTrans</td><td rowspan=\"3\">O(N(HDDK+DDF))</td><td>Electricity</td><td>0.333</td><td>0.041</td><td>50138</td></tr><tr><td>Wind</td><td>0.173</td><td>0.081</td><td>58272</td></tr><tr><td>App Flow</td><td>0.387</td><td>0.073</td><td>58272</td></tr><tr><td rowspan=\"3\">Reformer</td><td rowspan=\"3\">O(N(HDDK+DDF))</td><td>Electricity</td><td>0.359</td><td>0.047</td><td>677376</td></tr><tr><td>Wind</td><td>0.183</td><td>0.086</td><td>884736</td></tr><tr><td>App Flow</td><td>0.463</td><td>0.095</td><td>884736</td></tr><tr><td rowspan=\"3\">ETC</td><td rowspan=\"3\">O(N(HDDK+DDF))</td><td>Electricity</td><td>0.324</td><td>0.041</td><td>79536</td></tr><tr><td>Wind</td><td>0.167</td><td>0.074</td><td>102144</td></tr><tr><td>App Flow</td><td>0.397</td><td>0.069</td><td>102144</td></tr><tr><td rowspan=\"3\">Longformer</td><td rowspan=\"3\">O(N(HDDK+DDF))</td><td>Electricity</td><td>0.330</td><td>0.041</td><td>41360</td></tr><tr><td>Wind</td><td>0.166</td><td>0.075</td><td>52608</td></tr><tr><td>App Flow</td><td>0.377</td><td>0.07</td><td>52608</td></tr><tr><td rowspan=\"3\">Pyraformer</td><td rowspan=\"3\">O(N(HDDK+DDF)+(S-1)CDK2)</td><td>Electricity</td><td>0.324</td><td>0.041</td><td>17648</td></tr><tr><td>Wind</td><td>0.161</td><td>0.072</td><td>20176</td></tr><tr><td>App Flow</td><td>0.366</td><td>0.067</td><td>20176</td></tr></table>\n",
  "experiments": "# 4 EXPERIMENTS\n\n# 4.1 DATASETS AND EXPERIMENT SETUP\n\nWe demonstrated the advantages of the proposed Pyraformer on the four real-world datasets, including Wind, App Flow, Electricity, and ETT. The first three datasets were used for single-step forecasting, while the last two for long-range multi-step forecasting. We refer the readers to Appendix E and F for more details regarding the data description and the experiment setup.\n\n# 4.2 RESULTS AND ANALYSIS\n\n# 4.2.1 SINGLE-STEP FORECASTING\n\nWe conducted single-step prediction experiments on three datasets: Electricity, Wind and App Flow. The historical length is 169, 192 and 192, respectively, including the end token. We benchmarked Pyraformer against 5 other attention mechanisms, including the original full-attention (Vaswani et al., 2017), the log-sparse attention (i.e., LogTrans) (Li et al., 2019), the LSH attention (i.e., Reformer) (Kitaev et al., 2019), the sliding window attention with global nodes (i.e., ETC) (Ainslie et al., 2020), and the dilated sliding window attention (i.e., Longformer) (Beltagy et al., 2020). In particular for ETC, some nodes with equal intervals at the finest scale were selected as the global nodes. A global node can attend to all nodes across the sequence and all nodes can attend to it in turn(see Figure 1(e)). The training and testing schemes were the same for all models. We further investigated the usefulness of the pretraining strategy (see Appendix G), the weighted sampler, and the hard sample mining on all methods, and the best results were presented. We adopted the NRMSE (Normalized RMSE) and the ND (Normalized Deviation) as the evaluation indicators (see Appendix H for the definitions). The results are summarized in Table 2. For a fair comparison, except for full-attention, the overall dot product number of all attention mechanisms was controlled to the same order of magnitude.\n\nOur experimental results show that Pyraformer outperforms Transformer and its variants in terms of NRMSE and ND, with the least number of query-key dot products (a.k.a. Q-K pairs). Con-\n\nTable 3: Long-range multi-step forecasting results.  \n\n<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Metrics</td><td colspan=\"3\">ETTh1</td><td colspan=\"3\">ETTm1</td><td colspan=\"3\">Electricity</td></tr><tr><td>168</td><td>336</td><td>720</td><td>96</td><td>288</td><td>672</td><td>168</td><td>336</td><td>720</td></tr><tr><td rowspan=\"3\">Informer</td><td>MSE</td><td>1.075</td><td>1.329</td><td>1.384</td><td>0.556</td><td>0.841</td><td>0.921</td><td>0.745</td><td>1.579</td><td>4.365</td></tr><tr><td>MAE</td><td>0.801</td><td>0.911</td><td>0.950</td><td>0.537</td><td>0.705</td><td>0.753</td><td>0.266</td><td>0.323</td><td>0.371</td></tr><tr><td>Q-K pairs</td><td>188040</td><td>188040</td><td>423360</td><td>276480</td><td>560640</td><td>560640</td><td>188040</td><td>188040</td><td>423360</td></tr><tr><td rowspan=\"3\">LogTrans</td><td>MSE</td><td>0.983</td><td>1.100</td><td>1.411</td><td>0.554</td><td>0.786</td><td>1.169</td><td>0.791</td><td>1.584</td><td>4.362</td></tr><tr><td>MAE</td><td>0.766</td><td>0.839</td><td>0.991</td><td>0.499</td><td>0.676</td><td>0.868</td><td>0.340</td><td>0.336</td><td>0.366</td></tr><tr><td>Q-K pairs</td><td>74664</td><td>74664</td><td>216744</td><td>254760</td><td>648768</td><td>648768</td><td>74664</td><td>74664</td><td>216744</td></tr><tr><td rowspan=\"3\">Longformer</td><td>MSE</td><td>0.860</td><td>0.975</td><td>1.091</td><td>0.526</td><td>0.767</td><td>1.021</td><td>0.766</td><td>1.591</td><td>4.361</td></tr><tr><td>MAE</td><td>0.710</td><td>0.769</td><td>0.832</td><td>0.507</td><td>0.663</td><td>0.788</td><td>0.311</td><td>0.343</td><td>0.368</td></tr><tr><td>Q-K pairs</td><td>63648</td><td>63648</td><td>249120</td><td>329760</td><td>1007136</td><td>1007136</td><td>63648</td><td>63648</td><td>249120</td></tr><tr><td rowspan=\"3\">Reformer</td><td>MSE</td><td>0.958</td><td>1.044</td><td>1.458</td><td>0.543</td><td>0.924</td><td>0.981</td><td>0.783</td><td>1.584</td><td>4.374</td></tr><tr><td>MAE</td><td>0.741</td><td>0.787</td><td>0.987</td><td>0.528</td><td>0.722</td><td>0.778</td><td>0.332</td><td>0.334</td><td>0.374</td></tr><tr><td>Q-K pairs</td><td>1016064</td><td>1016064</td><td>2709504</td><td>5308416</td><td>14450688</td><td>14450688</td><td>1016064</td><td>1016064</td><td>2709504</td></tr><tr><td rowspan=\"3\">ETC</td><td>MSE</td><td>1.025</td><td>1.084</td><td>1.137</td><td>0.762</td><td>1.227</td><td>1.272</td><td>0.777</td><td>1.586</td><td>4.361</td></tr><tr><td>MAE</td><td>0.771</td><td>0.811</td><td>0.866</td><td>0.653</td><td>0.880</td><td>0.908</td><td>0.326</td><td>0.340</td><td>0.368</td></tr><tr><td>Q-K pairs</td><td>125280</td><td>125280</td><td>288720</td><td>331344</td><td>836952</td><td>836952</td><td>125280</td><td>125280</td><td>288720</td></tr><tr><td rowspan=\"3\">Pyraformer</td><td>MSE</td><td>0.808</td><td>0.945</td><td>1.022</td><td>0.480</td><td>0.754</td><td>0.857</td><td>0.719</td><td>1.533</td><td>4.312</td></tr><tr><td>MAE</td><td>0.683</td><td>0.766</td><td>0.806</td><td>0.486</td><td>0.659</td><td>0.707</td><td>0.256</td><td>0.291</td><td>0.346</td></tr><tr><td>Q-K pairs</td><td>26472</td><td>26472</td><td>74280</td><td>57264</td><td>96384</td><td>96384</td><td>26472</td><td>26472</td><td>74280</td></tr></table>\n\ncretely, there are three major trends that can be gleaned from Table 2: (1) The proposed Pyraformer yields the most accurate prediction results, suggesting that the pyramidal graph can better explain the temporal interactions in the time series by considering dependencies of different ranges. Interestingly, for the Wind dataset, sparse attention mechanisms, namely, LogTrans, ETC, Longformer and Pyraformer, outperform the original full attention Transformer, probably because the data contains a large number of zeros and the promotion of adequate sparsity can help avoid over-fitting. (2) The number of Q-K pairs in Pyraformer is the smallest. Recall that this number characterizes the time and space complexity. Remarkably enough, it is  $65.4\\%$  fewer than that of LogTrans and  $96.6\\%$  than that of the full attention. It is worth emphasizing that this computational gain will continue to increase for longer time series. (3) The number of parameters for Pyraformer is slightly larger than that of the other models, resulting from the CSCM. However, this module is very lightweight, which incurs merely  $5\\%$  overhead in terms of model size compared to other models. Moreover, in practice, we can fix the hyper-parameters  $A$ ,  $S$  and  $N$ , and ensure that  $C$  satisfies  $C > \\sqrt[3]{L / ((A - 1)N / 2 + 1)}$ . Consequently, the extra number of parameters introduced by the CSCM is only  $\\mathcal{O}((S - 1)CD_K^2)\\approx \\mathcal{O}(\\sqrt[3]{L})$ .\n\n# 4.2.2 LONG-RANGE MULTI-STEP FORECASTING\n\nWe evaluated the performance of Pyraformer for long-range forecasting on three datasets, that is, Electricity, ETTh1, and ETTm1. In particular for ETTh1 and ETTm1, we predicted the future oil temperature and the 6 power load features at the same time, which is a multivariate time series forecasting problem. Both prediction modules introduced in Section 3.3 were tested for all models and the better results are listed in Table 3.\n\nIt is evident that Pyraformer still achieves the best performance with the least number of Q-K pairs for all datasets regardless of the prediction length. More precisely, in comparison with Informer (Zhou et al., 2021), the MSE given by Pyraformer for ETTh1 is decreased by  $24.8\\%$ ,  $28.9\\%$ ,  $26.2\\%$  respectively when the prediction length is 168, 336, and 720. Once again, this bolsters our belief that it is more beneficial to employ the pyramidal graph when describing the temporal dependencies. Interestingly, we notice that for Pyraformer, the results given by the first prediction module are better than those by the second one. One possible explanation is that the second prediction module based on the full attention layers cannot differentiate features with different resolutions, while the first module based on a single fully connected layer can take full advantages of such features in an automated fashion. To better elucidate the modeling capacity of Pyraformer for long-range forecasting, we refer the readers to Appendix I for a detailed example on synthetic data.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/efb55c10-1224-4a1f-afb4-e00a00ccf72f/52875e5af931a22551089147b1329d16f9338e388c7be3ac5a5d53cd9342f632.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-14/efb55c10-1224-4a1f-afb4-e00a00ccf72f/71369bdb58cd6ab215c77e96152e90ef4787edb32ef1210f823f4e3d7d68cf41.jpg)  \n(b)  \nFigure 4: Comparison of the time and memory consumption between the full, the prob-sparse, and the TVM implementation of the pyramidal attention: (a) computation time; (b) memory occupation.\n\n# 4.2.3 SPEED AND MEMORY CONSUMPTION\n\nTo check the efficiency of the customized CUDA kernel implemented based on TVM, we depicted the empirical computation time and memory cost as a function of the sequence length  $L$  in Figure 4. Here we only compared Pyraformer with the full attention and the prob-sparse attention in Informer (Zhou et al., 2021). All the computations were performed on a 12 GB Titan Xp GPU with Ubuntu 16.04, CUDA 11.0, and TVM 0.8.0. Figure 4 shows that the time and memory cost of the proposed Pyraformer based on TVM is approximately a linear function of  $L$ , as expected. Furthermore, the time and memory consumption of the TVM implementation can be several orders of magnitude smaller than that of the full attention and the prob-sparse attention, especially for relatively long time series. Indeed, for a 12GB Titan Xp GPU, when the sequence length reaches 5800, full attention encounters the out-of-memory (OOM) problem, yet the TVM implementation of Pyraformer only occupies 1GB of memory. When it comes to a sequence with 20000 time points, even Informer incurs the OOM problem, whereas the memory cost of Pyraformer is only 1.91GB and the computation time per batch is only 0.082s.\n\n# 4.3 ABLATION STUDY\n\nWe also performed ablation studies to measure the impact of  $A$  and  $C$ , the CSCM architecture, the history length, and the PAM on the prediction accuracy of Pyraformer. The results are displayed in Tables 7-10. Detailed Discussions on the results can be found in Appendix J. Here, we only provide an overview of the major findings: (1) it is better to increase  $C$  with  $L$  but fix  $A$  to a small constant for the sake of reducing the prediction error; (2) convolution with bottleneck strikes a balance between the prediction accuracy and the number of parameters, and hence, we use it as the CSCM; (3) more history helps increase the accuracy of forecasting; (4) the PAM is essential for accurate prediction.\n",
  "hyperparameter": "Key hyperparameters include: N (number of attention layers), H (number of attention heads), S (number of scales in the pyramid), D (dimension of a node), D_K (dimension of keys), D_F (maximum dimension of feed-forward layer), C (convolution stride/number of children in C-ary tree, recommended to satisfy C > ∛(L/((A-1)N/2+1))), A (number of adjacent nodes at same scale, fixed to 3 or 5 regardless of sequence length L), and L (historical sequence length: 169 for Electricity, 192 for Wind/App Flow in single-step; various lengths for multi-step forecasting). The relationship S, N, A, C must satisfy: L/C^(S-1) - 1 ≤ (A-1)N/2 for global receptive field. Bottleneck structure is used in CSCM to reduce parameters."
}