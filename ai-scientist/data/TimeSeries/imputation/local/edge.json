[
  {
    "source": "Informer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer-based encoder-decoder architecture for time series tasks. The source implementation provides reusable components: `DataEmbedding` for input representation, `EncoderLayer` and `DecoderLayer` structures, and the overall encoder-decoder framework. The target paper can directly reuse the embedding layers (temporal encoding, positional encoding) and the basic layer stacking logic.\n\n2. **Efficient Attention Mechanism Philosophy**: Both papers address the O(L²) complexity bottleneck of standard self-attention and propose O(L log L) solutions. Informer uses ProbSparse attention with query sparsity measurement, while Autoformer uses Auto-Correlation with top-k period selection. The implementation pattern of selecting sparse connections (top-k selection in both cases with k=c·log(L)) and the FFT-based efficient computation can be adapted. The source code's `ProbAttention` class structure (query/key/value projection, sparse selection, aggregation) provides a template for implementing Auto-Correlation.\n\n3. **Multi-Head Attention Framework**: Both papers employ multi-head mechanisms for diverse representation learning. The source implementation's `AttentionLayer` class with `query_projection`, `key_projection`, `value_projection`, and `out_projection` can be directly reused. The multi-head splitting logic (reshaping to [B, L, H, D/H]) and concatenation pattern in the source code applies identically to the target paper's Auto-Correlation mechanism.\n\n4. **Decoder Input Strategy with Start Tokens**: Both papers use a generative-style decoder that concatenates known historical information with placeholders for prediction. Informer uses `Concat(X_token, X_0)` and Autoformer uses `Concat(X_ens, X_0)` for seasonal part and `Concat(X_ent, X_Mean)` for trend part. The source implementation's decoder input preparation logic (Equation 6 in Informer) can be adapted by modifying the decomposition preprocessing.\n\n5. **Layer Normalization and Residual Connections**: Both architectures use LayerNorm and residual connections extensively. The source code's `DecoderLayer` implementation with `self.norm1`, `self.norm2`, `self.norm3` and residual addition patterns (x = x + dropout(attention_output)) can be directly reused in Autoformer's decoder layers.\n\n6. **Feed-Forward Network Design**: Both papers use identical position-wise feed-forward networks with two linear transformations and activation. The source implementation's `conv1`, `conv2` with activation and dropout in `DecoderLayer` can be reused without modification for Autoformer's feed-forward blocks.\n\n7. **Projection Layer for Final Output**: Both papers use a final linear projection to map hidden representations to target dimensions. The source code's `self.projection = nn.Linear(configs.d_model, configs.c_out)` can be directly reused, though Autoformer needs separate projections for seasonal (`W_S`) and trend components.",
    "differences": "1. **Core Innovation - Series Decomposition vs. ProbSparse Attention**: Informer's innovation is the ProbSparse attention mechanism with query sparsity measurement (Equation 2: M(q_i, K) using LSE and arithmetic mean). Autoformer's core innovation is the **built-in series decomposition architecture** that separates trend-cyclical and seasonal components at every layer (Equation 1: moving average pooling). **NEW IMPLEMENTATION REQUIRED**: A `SeriesDecomp` module that performs moving average pooling (`AvgPool` with padding) to extract trend, then subtracts to get seasonal component. This decomposition block must be inserted after every attention and feed-forward operation in both encoder and decoder.\n\n2. **Attention Mechanism - ProbSparse vs. Auto-Correlation**: Informer uses ProbSparse attention that selects top-u queries based on sparsity measurement and performs standard softmax attention. Autoformer uses **Auto-Correlation mechanism** that discovers period-based dependencies through autocorrelation and aggregates via time delay. **NEW IMPLEMENTATION REQUIRED**: (a) Autocorrelation computation using FFT (Equation 8: Wiener-Khinchin theorem with S_XX(f) = F(X)F*(X) and R_XX(τ) = F^(-1)(S_XX(f))), (b) Top-k period selection based on autocorrelation values (Equation 6: argTopk on R_Q,K(τ)), (c) Time delay aggregation using `Roll` operation to align sub-series at same phase positions, (d) Weighted aggregation by softmax-normalized autocorrelation confidences. The entire `ProbAttention` class needs to be replaced with an `AutoCorrelation` class.\n\n3. **Encoder Architecture - Distilling vs. Decomposition**: Informer's encoder uses self-attention distilling with `ConvLayer` (1D convolution + ELU + MaxPooling) to progressively reduce sequence length and extract dominant features (Equation 5). Autoformer's encoder focuses on **seasonal pattern modeling** by eliminating trend-cyclical components at each layer through decomposition blocks (Equation 3: decompose after Auto-Correlation and feed-forward). **NEW IMPLEMENTATION REQUIRED**: Remove the distilling `ConvLayer` components and replace with `SeriesDecomp` blocks that discard the extracted trend (\"_\" notation in equations) and only pass seasonal components to the next layer. The encoder output should contain only seasonal information for cross-attention.\n\n4. **Decoder Architecture - Standard vs. Progressive Trend Accumulation**: Informer uses a standard decoder with masked self-attention and encoder-decoder attention. Autoformer uses a **dual-path decoder** that separately processes seasonal and trend components (Equation 4). **NEW IMPLEMENTATION REQUIRED**: (a) Initialize decoder with decomposed seasonal (`X_des`) and trend (`X_det`) parts from encoder input (Equation 2), (b) Implement progressive trend accumulation structure: `T_de^l = T_de^(l-1) + W_l,1*T_de^(l,1) + W_l,2*T_de^(l,2) + W_l,3*T_de^(l,3)`, where three trend components are extracted from three decomposition blocks in each layer and projected by learnable weights `W_l,i`, (c) Maintain separate seasonal refinement path through Auto-Correlation blocks, (d) Final prediction combines refined seasonal (`W_S * X_de^M`) and accumulated trend (`T_de^M`).\n\n5. **Decoder Input Initialization - Single vs. Dual Component**: Informer initializes decoder with concatenated start token and zero placeholders (Equation 6: `Concat(X_token, X_0)`). Autoformer requires **separate initialization for seasonal and trend components** (Equation 2). **NEW IMPLEMENTATION REQUIRED**: (a) Decompose the latter half of encoder input (`X_en[I/2:I]`) into seasonal (`X_ens`) and trend (`X_ent`) parts, (b) Initialize seasonal decoder input: `X_des = Concat(X_ens, X_0)` with zero placeholders, (c) Initialize trend decoder input: `X_det = Concat(X_ent, X_Mean)` with mean value placeholders, (d) Manage two separate input streams throughout decoder processing.\n\n6. **Loss Function and Output - Single vs. Decomposed Reconstruction**: Informer uses single MSE loss on final predictions. Autoformer predicts decomposed components and combines them. **NEW IMPLEMENTATION REQUIRED**: (a) Separate projection for seasonal component (`W_S`), (b) Sum of seasonal and trend predictions as final output, (c) Potentially implement separate losses for seasonal and trend components (though paper uses combined MSE on final prediction), (d) Handle the dual-output structure in training and inference.\n\n7. **Computational Efficiency Strategy - Query Selection vs. Period Selection**: Informer achieves O(L log L) by selecting top-u queries based on max-mean measurement (Equation 4) and random sampling for approximation. Autoformer achieves O(L log L) through **FFT-based autocorrelation computation** and selecting top-k time delays. **NEW IMPLEMENTATION REQUIRED**: Implement FFT-based autocorrelation calculation for all lags simultaneously (Equation 8), which is fundamentally different from Informer's query-key dot-product sampling. The autocorrelation provides period-based confidence scores rather than point-wise attention scores.\n\n8. **Information Aggregation - Point-wise vs. Series-wise**: Informer aggregates information through point-wise weighted sum of values (standard attention aggregation). Autoformer uses **time delay aggregation** that rolls entire value series by detected periods and aggregates sub-series (Equation 6: `Σ Roll(V, τ_i) * R̂_Q,K(τ_i)`). **NEW IMPLEMENTATION REQUIRED**: Implement the `Roll` operation that shifts series by time delay τ (with wrap-around for elements shifted beyond boundaries) and aggregates multiple rolled versions weighted by autocorrelation confidences. This is a fundamentally different aggregation paradigm from attention's weighted sum of individual positions."
  },
  {
    "source": "Reformer_2020",
    "target": "Autoformer_2021",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the Transformer architecture with encoder-decoder structures. The source paper's implementation provides a complete Transformer framework (EncoderLayer, Encoder with normalization) that can be directly reused. The embedding layer (DataEmbedding) and basic projection mechanisms in Reformer's code are transferable to Autoformer.\n\n2. **Efficient Attention Mechanism Philosophy**: Both papers address the O(L²) complexity bottleneck of standard attention. Reformer uses LSH attention to achieve O(L log L) complexity, while Autoformer uses Auto-Correlation with FFT to achieve the same O(L log L) complexity. The implementation pattern of replacing standard attention with an efficient variant (ReformerLayer wrapping LSHSelfAttention) provides a template for implementing Autoformer's Auto-Correlation mechanism.\n\n3. **Multi-Head Mechanism**: Both maintain the multi-head attention paradigm. Reformer's multi-head LSH attention implementation (with concatenation and output projection) follows the same pattern that Autoformer's multi-head Auto-Correlation requires. The code structure for managing multiple heads, computing attention in parallel, and concatenating results can be reused.\n\n4. **Layer Stacking Pattern**: Both use stacked encoder/decoder layers with residual connections and normalization. Reformer's layer stacking approach (list comprehension creating multiple EncoderLayer instances with shared configuration) and the overall encoder structure with final layer normalization directly transfers to Autoformer's architecture.\n\n5. **Query-Key-Value Projection**: Both papers project input activations into Q, K, V representations. Reformer's shared-QK approach (where Q=K) demonstrates the projection mechanism, though Autoformer uses separate projections. The linear projection infrastructure and dimension management code can be adapted.\n\n6. **Sequence Length Handling**: Reformer's fit_length() method that pads sequences to satisfy bucket size requirements provides a pattern for Autoformer's need to handle variable sequence lengths in its decomposition and aggregation operations.",
    "differences": "1. **Core Mechanism - LSH vs. Auto-Correlation**: NEWLY IMPLEMENT the Auto-Correlation mechanism that replaces LSH attention. This requires: (a) Implementing autocorrelation computation using FFT (Equation 8: S_XX(f) = F(X_t)F*(X_t) and R_XX(τ) = F^(-1)(S_XX(f))), (b) Implementing argTopk to select top-k time delays based on autocorrelation values, (c) Implementing the Roll operation that shifts series by delay τ, (d) Implementing time delay aggregation that weights rolled series by softmax-normalized autocorrelations (Equation 6). This is fundamentally different from Reformer's hash-based bucketing and requires torch.fft module.\n\n2. **Series Decomposition Architecture**: NEWLY IMPLEMENT the series decomposition block that is central to Autoformer but absent in Reformer. This requires: (a) Implementing moving average pooling with padding to extract trend component (Equation 1: X_t = AvgPool(Padding(X))), (b) Computing seasonal component as residual (X_s = X - X_t), (c) Integrating decomposition blocks after each attention and feedforward layer in both encoder and decoder, (d) Managing separate seasonal and trend pathways throughout the network. Reformer has no equivalent decomposition mechanism.\n\n3. **Decoder Design - Accumulation vs. Standard**: NEWLY IMPLEMENT Autoformer's progressive trend accumulation structure. This requires: (a) Dual-component decoder inputs (seasonal X_des and trend X_det initialized per Equation 2), (b) Trend accumulation across layers (T_de^l = T_de^(l-1) + W_l,1*T_de^l,1 + W_l,2*T_de^l,2 + W_l,3*T_de^l,3 from Equation 4), (c) Three separate decomposition blocks per decoder layer extracting trends at different stages, (d) Learnable projection weights (W_l,i) for each extracted trend component. Reformer uses standard decoder without component-wise accumulation.\n\n4. **Input Initialization Strategy**: NEWLY IMPLEMENT Autoformer's specialized decoder input initialization. This requires: (a) Decomposing the latter half of encoder input (X_en[I/2:I]) to get X_ens and X_ent, (b) Concatenating with placeholders (X_0 for seasonal, X_Mean for trend) to form decoder inputs of length I/2+O, (c) Managing different placeholder strategies for seasonal (zeros) vs. trend (mean values). Reformer uses standard sequence inputs without this decomposition-based initialization.\n\n5. **Attention Pattern - Series-wise vs. Point-wise**: NEWLY IMPLEMENT series-wise connection logic. Auto-Correlation connects entire sub-series based on periodicity (aggregating O(log L) full-length series), while Reformer's LSH connects individual tokens within hash buckets. This requires: (a) Period-based dependency discovery selecting delays τ_1,...,τ_k, (b) Rolling entire value series by each delay, (c) Weighted aggregation of k rolled series (not k individual points). The aggregation granularity is fundamentally different.\n\n6. **Loss and Output Composition**: NEWLY IMPLEMENT the final prediction as sum of refined components (W_S * X_de^M + T_de^M). Reformer outputs a single projection, while Autoformer requires: (a) Separate projection for seasonal component (W_S), (b) Direct use of accumulated trend component, (c) Element-wise addition of projected seasonal and accumulated trend. This reflects Autoformer's decomposition philosophy throughout the entire forward pass.\n\n7. **Encoder-Decoder Cross-Attention Modification**: NEWLY IMPLEMENT Auto-Correlation for encoder-decoder attention where K,V from encoder are resized to length-O to match decoder's prediction horizon. This requires special handling of cross-series autocorrelation and dimension matching that doesn't exist in Reformer's standard cross-attention between same-length sequences.\n\n8. **Hyperparameter Philosophy**: NEWLY IMPLEMENT Autoformer-specific hyperparameters: (a) Moving average kernel size for decomposition, (b) Factor c for k = ⌊c × log L⌋ determining number of delays, (c) Separate learning rates or initialization for trend projection weights W_l,i. Reformer's hyperparameters (bucket_size, n_hashes) are specific to LSH and not applicable."
  },
  {
    "source": "DLinear_2022",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers adopt the DMS forecasting approach where the model directly predicts the entire future horizon in one shot, rather than autoregressive step-by-step prediction. DLinear explicitly validates this as superior to IMS (Iterative Multi-Step) methods. For PatchTST reproduction, the same forecasting paradigm can be reused - the encoder processes the input sequence and decoder/projection head directly outputs the full prediction horizon.\n\n2. **Series Decomposition Foundation**: DLinear uses series_decomp from Autoformer to separate trend and seasonal components. PatchTST likely benefits from similar preprocessing to handle non-stationary time series. The decomposition module (moving average kernel-based trend extraction) from DLinear's implementation can be directly reused or adapted as a preprocessing step in PatchTST, especially for handling distribution shifts across patches.\n\n3. **Channel Independence Assumption**: DLinear operates on each variate independently (individual=True mode) without modeling cross-variate correlations, using separate linear layers per channel. PatchTST similarly adopts channel-independent processing where each time series channel is handled separately. The channel iteration logic and ModuleList structure from DLinear can guide PatchTST's implementation of per-channel patch embedding and attention mechanisms.\n\n4. **Normalization Strategy**: DLinear's NLinear variant uses instance normalization (subtracting last value, then adding back) to handle distribution shifts. PatchTST requires similar normalization techniques for patch-based processing to ensure stable training across different data distributions. The normalization wrapper logic can be adapted from DLinear's implementation.\n\n5. **Simple Linear Projection Baseline**: Both papers challenge Transformer complexity - DLinear shows simple linear layers are competitive, while PatchTST aims to make Transformers efficient through patching. The basic temporal linear projection structure (nn.Linear along time axis) in DLinear serves as the baseline that PatchTST's patch-based Transformer must outperform. This provides a reference architecture for comparison.",
    "differences": "1. **Core Architecture - Patching Mechanism (NEW IMPLEMENTATION REQUIRED)**: DLinear uses simple linear layers on full sequences, while PatchTST's key innovation is dividing time series into non-overlapping patches (sub-sequences) and treating each patch as a token. NEW components needed: (a) Patch extraction module that segments input [B, L, D] into [B, N_patches, P, D] where P is patch length; (b) Patch embedding layer that projects each patch to embedding dimension; (c) Positional encoding for patch positions rather than point-wise positions. This is fundamentally different from DLinear's point-wise temporal processing.\n\n2. **Transformer Self-Attention vs Linear Layers (NEW IMPLEMENTATION REQUIRED)**: DLinear explicitly avoids self-attention, using only linear projections. PatchTST requires full Transformer encoder implementation with: (a) Multi-head self-attention mechanism operating on patch-level tokens; (b) Feed-forward networks; (c) Layer normalization and residual connections; (d) Multiple encoder layers stacking. The entire Transformer encoder block needs to be implemented from scratch, as DLinear contains no attention mechanisms.\n\n3. **Patch-Level vs Point-Level Temporal Modeling (NEW IMPLEMENTATION REQUIRED)**: DLinear operates on individual time points with weights [pred_len, seq_len]. PatchTST operates on patch-level representations requiring: (a) Patch-wise attention that captures dependencies between patches rather than points; (b) Patch stride and overlap strategies (if applicable); (c) Flattening/unflattening operations to convert between patch representation and time series; (d) Output projection that maps from patch embeddings back to point-wise predictions. The temporal granularity is fundamentally different.\n\n4. **Complexity and Efficiency Trade-offs (NEW IMPLEMENTATION REQUIRED)**: DLinear emphasizes O(1) complexity with simple matrix multiplication. PatchTST reduces Transformer complexity from O(L²) to O((L/P)²) through patching, but still requires: (a) Attention computation with optimized implementations (e.g., flash attention); (b) Memory-efficient training strategies for handling multiple patches; (c) Patch-level dropout and regularization; (d) Computational budget management for attention across patches. Performance optimization strategies differ completely.\n\n5. **Decoder and Output Projection Design (NEW IMPLEMENTATION REQUIRED)**: DLinear uses separate linear layers for seasonal and trend, then sums them (decomposition-based output). PatchTST requires: (a) Flatten operation to convert patch embeddings [B, N_patches, D_model] to sequence predictions [B, pred_len, D]; (b) Projection head that may use linear layers but operates on learned patch representations rather than decomposed components; (c) Potential use of decoder Transformer blocks if following encoder-decoder architecture; (d) Output denormalization and recomposition strategies specific to patch-based predictions. The output generation pipeline is architecturally distinct.\n\n6. **Training Objective and Loss Computation (IMPLEMENTATION DETAILS DIFFER)**: While both use MSE/MAE for forecasting, PatchTST's loss operates on: (a) Patch-level intermediate representations (if using auxiliary losses); (b) Reconstructed full sequences from patch predictions; (c) Potential masking strategies for self-supervised pre-training on patches (patch masking similar to masked language modeling). DLinear's straightforward point-wise loss needs extension to handle patch-based prediction errors and potential pre-training objectives."
  },
  {
    "source": "FEDformer_2022",
    "target": "PatchTST_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Pyraformer_2021",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Basic Encoder-Decoder Architecture for Time Series Processing**: Both papers employ encoder-based architectures for time series modeling. Pyraformer uses a pyramidal attention encoder, while PatchTST uses a Transformer encoder with patching. The basic embedding layers (DataEmbedding combining temporal and positional encodings) can be reused from Pyraformer's implementation. The `enc_embedding` module and the overall forward pass structure (input → embedding → encoding → projection) provide a solid foundation.\n\n2. **Positional and Temporal Encoding Framework**: Both models require positional information to capture temporal dependencies. Pyraformer's `DataEmbedding` class that combines observation embedding, covariate embedding, and positional embedding can be adapted for PatchTST. The temporal encoding mechanism (x_mark_enc for timestamps) is directly transferable, though PatchTST may need modifications to handle patch-level positioning.\n\n3. **Linear Projection for Output**: Both architectures use linear projection layers to map from the model's hidden dimension to the output space. Pyraformer's `self.projection = nn.Linear((len(window_size)+1)*self.d_model, configs.enc_in)` demonstrates this pattern, which can be adapted for PatchTST's reconstruction head. The projection module structure is reusable with dimension adjustments.\n\n4. **Layer Normalization and Feed-Forward Networks**: The `PositionwiseFeedForward` module with pre/post layer normalization, GELU activation, and residual connections is a standard Transformer component present in both architectures. This entire module can be reused directly in PatchTST's implementation.\n\n5. **Batch Processing and Device Handling**: The implementation patterns for handling batch dimensions, device placement (`.to(x_enc.device)`), and tensor operations are consistent across both models. The overall training loop structure and data flow patterns are transferable.",
    "differences": "1. **Core Architecture: Pyramidal Attention vs. Patch-based Attention**: Pyraformer implements a hierarchical multi-resolution pyramidal attention mechanism with O(L) complexity using custom CUDA kernels, while PatchTST adopts a patch-based approach where the input sequence is divided into non-overlapping patches. NEW IMPLEMENTATION NEEDED: (a) Patch creation module that segments time series into fixed-length patches, (b) Patch embedding layer that treats each patch as a token, (c) Standard self-attention mechanism operating on patch-level tokens instead of Pyraformer's pyramidal sparse attention, (d) Removal of the entire CSCM (Coarser-Scale Construction Module) and PAM (Pyramidal Attention Module) components.\n\n2. **Multi-Scale vs. Single-Scale Processing**: Pyraformer explicitly constructs a C-ary tree with multiple scales through `Bottleneck_Construct` and `ConvLayer` modules, creating hierarchical representations. PatchTST operates on a single scale with uniform patch size. NEW IMPLEMENTATION NEEDED: (a) Remove all multi-scale convolution layers, (b) Remove the `conv_layers`, `all_size`, and scale-related indexing logic, (c) Implement uniform patch segmentation without hierarchical downsampling, (d) Simplified attention mask for patch-level interactions instead of inter-scale and intra-scale masks.\n\n3. **Attention Mechanism Complexity**: Pyraformer uses custom sparse attention with `get_mask()` and `refer_points()` functions to implement pyramidal connectivity patterns, requiring specialized attention masking. PatchTST uses standard full self-attention or efficient variants at the patch level. NEW IMPLEMENTATION NEEDED: (a) Replace custom attention masking with standard Transformer attention (or optional linear attention for efficiency), (b) Remove the `RegularMask` class and custom mask generation, (c) Implement standard multi-head self-attention mechanism, (d) Remove the `indexes` gathering mechanism used for pyramid feature aggregation.\n\n4. **Sequence Length Handling and Tokenization**: Pyraformer processes sequences at multiple resolutions with varying lengths at each scale, while PatchTST divides the sequence into P patches of length L_p (where P = L/L_p). NEW IMPLEMENTATION NEEDED: (a) Patch creation function: `create_patches(x, patch_len, stride)` that reshapes [B, L, D] → [B, P, L_p*D], (b) Patch embedding layer to project patches to d_model dimension, (c) Modified positional encoding for patch positions rather than timestep positions, (d) Reconstruction logic that unfolds patches back to the original sequence length.\n\n5. **Imputation-Specific Design**: For the imputation task, PatchTST needs to handle masking at the patch level and reconstruct masked patches. NEW IMPLEMENTATION NEEDED: (a) Patch-level masking strategy where entire patches or partial patches are masked, (b) Mask token embedding for masked patches (similar to BERT's [MASK] token), (c) Reconstruction loss calculated at patch level or unpacked to timestep level, (d) Strategy for handling boundary effects when patches span masked and unmasked regions, (e) Potential pre-training and fine-tuning framework for self-supervised learning on masked patch prediction.\n\n6. **Channel Independence vs. Channel Mixing**: Pyraformer processes all channels jointly through its encoder, while PatchTST typically employs a channel-independent strategy where each channel is processed separately and then optionally mixed. NEW IMPLEMENTATION NEEDED: (a) Channel-independent processing loop or batch dimension manipulation, (b) Optional channel mixing layer after patch-level encoding, (c) Modified embedding layer to handle per-channel or joint-channel processing, (d) Adjusted projection head for channel-wise or joint reconstruction.\n\n7. **Computational Efficiency Approach**: Pyraformer achieves efficiency through sparse pyramidal attention with custom CUDA kernels, while PatchTST achieves efficiency through reduced sequence length via patching (processing P patches instead of L timesteps). NEW IMPLEMENTATION NEEDED: (a) No custom CUDA kernels required - use standard PyTorch operations, (b) Patch-based sequence length reduction logic, (c) Simpler attention mechanism without hierarchical complexity, (d) Different memory optimization strategy based on patch size tuning rather than attention sparsity patterns."
  },
  {
    "source": "Informer_2020",
    "target": "PatchTST_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers utilize Transformer architectures for time series processing, sharing fundamental building blocks like multi-head attention, feedforward networks, and layer normalization. The target paper can reuse the basic `AttentionLayer` structure, linear projections (query/key/value), and the feedforward network components from Informer's implementation.\n\n2. **Embedding Layer Design**: Both employ embedding mechanisms to project raw time series data into higher-dimensional representations. Informer's `DataEmbedding` module (combining value embedding, positional encoding, and temporal encoding) provides a solid foundation that can be adapted for PatchTST, though PatchTST will need modifications to handle patch-based inputs rather than point-wise inputs.\n\n3. **Encoder-based Processing Paradigm**: Both papers rely on encoder structures to extract temporal features. The `Encoder` and `EncoderLayer` classes from Informer can serve as templates, particularly the layer stacking mechanism, residual connections, and normalization strategies. The overall forward pass logic (embedding → encoder layers → projection) remains conceptually similar.\n\n4. **Batch Processing and Tensor Operations**: The implementation patterns for handling batched inputs (B, L, D format), attention mask operations, and GPU-accelerated tensor computations are directly transferable. The shape manipulation techniques in Informer's code provide practical guidance for implementing PatchTST's batch operations.\n\n5. **Training Infrastructure Compatibility**: Both models can share the same training loop structure, loss computation (MSE for imputation), optimizer configurations, and evaluation metrics, minimizing the need to redesign the training pipeline.",
    "differences": "1. **Core Innovation - Patch-based Processing vs. Point-wise Processing**: PatchTST's fundamental innovation is processing time series as patches (subsequences) rather than individual time points. This requires implementing:\n   - A new **patching module** that segments input sequences into fixed-length patches with stride\n   - Patch embedding layers that treat each patch as a token\n   - Modified positional encoding to reflect patch positions rather than point positions\n   - Informer processes sequences point-by-point (L time steps), while PatchTST processes N patches where N = (L - patch_len) / stride + 1\n\n2. **Attention Mechanism - Standard Self-Attention vs. ProbSparse Attention**: \n   - Informer uses **ProbSparse attention** with query sparsity measurement (Eq. 2-4), sampling mechanisms, and O(L log L) complexity optimization\n   - PatchTST uses **standard full self-attention** on patches, which is computationally feasible because the number of patches N << original sequence length L\n   - PatchTST needs to implement vanilla scaled dot-product attention without the complex sampling and sparsity mechanisms\n   - The `ProbAttention` class should be replaced with a simpler `FullAttention` implementation\n\n3. **Encoder Architecture - Distilling Encoder vs. Standard Stacked Encoder**:\n   - Informer employs **self-attention distilling** with ConvLayer downsampling (Eq. 5), progressive halving of sequence length, and pyramid-like multi-stack architecture\n   - PatchTST uses a **standard stacked Transformer encoder** without distilling or downsampling, maintaining constant patch count throughout layers\n   - Remove `ConvLayer` components and the distilling mechanism entirely\n   - Implement uniform layer stacking without the replica stacks at different resolutions\n\n4. **Decoder and Generative Inference - Absent in PatchTST**:\n   - Informer uses an encoder-decoder architecture with masked attention in decoder, start tokens, and generative inference (Eq. 6)\n   - PatchTST is **encoder-only** for imputation tasks, directly projecting encoder outputs to reconstructed values\n   - The entire `Decoder`, `DecoderLayer`, and cross-attention components can be removed\n   - Implement a simpler projection head that maps patch representations back to original time series resolution\n\n5. **Imputation Strategy - Masked Reconstruction Approach**:\n   - Informer's imputation (in the provided code) processes the full sequence and projects to output\n   - PatchTST requires implementing **masked patch modeling**: randomly masking certain patches during training and reconstructing only masked patches\n   - Need to implement:\n     - Random patch masking mechanism (similar to BERT's masking)\n     - Mask-aware loss computation (only computing loss on masked patches)\n     - Patch-to-point reconstruction that handles the mapping from patch-level predictions back to point-level imputation\n\n6. **Channel Independence vs. Multivariate Handling**:\n   - PatchTST typically processes each channel (variable) independently in its architecture design\n   - Informer handles multivariate series jointly through its attention mechanisms\n   - Implement channel-independent processing where each variable goes through separate Transformer encoders or implement explicit channel mixing strategies\n\n7. **Positional Encoding Adaptation**:\n   - Informer uses point-wise positional encoding for sequence length L\n   - PatchTST needs **patch-wise positional encoding** for N patches\n   - Implement learnable or sinusoidal positional embeddings at patch granularity\n   - Consider whether to add intra-patch positional information\n\n8. **Output Projection and Reconstruction**:\n   - Informer: Direct linear projection from d_model to c_out (number of channels)\n   - PatchTST: Needs a **patch unpacking mechanism** that:\n     - Projects each patch representation back to patch_len × c_out dimensions\n     - Handles overlapping patches if stride < patch_len\n     - Aggregates predictions from multiple patches for the same time point\n     - Implements proper boundary handling for sequence start/end"
  },
  {
    "source": "Pyraformer_2021",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers utilize Transformer-based architectures with encoder-decoder structures for time series tasks. The source code's `EncoderLayer` with self-attention and feed-forward networks can be partially reused. The basic attention computation framework (Q, K, V projections) and the `PositionwiseFeedForward` module from Pyraformer can serve as templates for implementing the base Transformer in Non-stationary Transformers.\n\n2. **Time Series Embedding Strategy**: Both employ similar embedding approaches combining data embedding with temporal covariates (x_mark_enc). Pyraformer's `DataEmbedding` module that handles observation embedding and positional encoding can be adapted for Non-stationary Transformer's input processing, though the latter requires additional handling for normalization statistics.\n\n3. **Multi-layer Encoder Processing**: Both use stacked encoder layers to progressively extract temporal features. The layer stacking logic in Pyraformer's `Encoder` class (iterating through `self.layers`) provides a direct template for implementing Non-stationary Transformer's encoder, with the main difference being the attention mechanism replacement.\n\n4. **Projection to Output Space**: Both use linear projection layers to map encoded features to the target output dimension. Pyraformer's `self.projection` layer demonstrates the basic pattern that can be adapted for Non-stationary Transformer's final prediction layer, though the latter needs to incorporate de-normalization.\n\n5. **Batch Processing and Tensor Operations**: Both implementations handle batched time series data with shape [B, L, D]. The tensor manipulation patterns, device handling (`.to(x_enc.device)`), and batch dimension management in Pyraformer's code provide practical guidance for implementing Non-stationary Transformer's data flow.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: Pyraformer uses Pyramidal Attention with multi-scale hierarchical structure (CSCM + PAM) achieving O(L) complexity through sparse connections across pyramid levels. Non-stationary Transformer requires implementing **De-stationary Attention** (Equation 6) which modifies standard attention with learned de-stationary factors τ (scaling) and Δ (shifting): `Softmax((τ * Q'K'^T + 1Δ^T) / sqrt(d_k)) * V'`. This requires: (a) MLP projectors to learn τ and Δ from input statistics (μ_x, σ_x), (b) Modified attention computation incorporating these factors, (c) Sharing τ and Δ across all layers.\n\n2. **Series Stationarization Wrapper - NEW IMPLEMENTATION REQUIRED**: Pyraformer has no explicit normalization/de-normalization wrapper. Non-stationary Transformer requires implementing: (a) **Normalization module** (Equation 1) computing μ_x and σ_x for each input series and transforming x → x' = (x - μ_x) / σ_x, (b) **De-normalization module** (Equation 2) restoring predictions ŷ = σ_x ⊙ y' + μ_x, (c) Preserving statistics (μ_x, σ_x) throughout forward pass for both de-normalization and de-stationary factor learning. This wrapper operates on the entire model input/output.\n\n3. **Multi-Resolution vs. Single-Resolution Processing**: Pyraformer constructs a C-ary tree with multiple scales through `Bottleneck_Construct` using strided convolutions (kernel_size=C, stride=C) and processes nodes at different temporal resolutions simultaneously. Non-stationary Transformer operates on a single temporal resolution with uniform sequence length, requiring removal of all multi-scale construction logic (CSCM, pyramid indexing with `refer_points`, scale-specific masking).\n\n4. **Attention Mask Strategy**: Pyraformer uses complex pyramidal masks (`get_mask`) defining inter-scale and intra-scale connections with O(L) Q-K pairs. Non-stationary Transformer likely uses standard causal or full attention masks without hierarchical constraints, requiring replacement of Pyraformer's `RegularMask` with simpler masking appropriate for stationary attention patterns.\n\n5. **Statistical Information Flow - NEW IMPLEMENTATION REQUIRED**: Pyraformer's architecture has no explicit handling of input statistics. Non-stationary Transformer requires: (a) Computing and storing μ_x, σ_x at the input stage, (b) Passing these statistics to MLP projectors in each attention layer, (c) Using learned de-stationary factors to modulate attention scores, (d) Maintaining normalized representations (x', Q', K', V') internally while leveraging original statistics. This requires significant architectural changes to propagate statistical information throughout the network.\n\n6. **Imputation Task Adaptation**: While both papers' provided methods focus on forecasting, for imputation tasks: Pyraformer would need modification to handle masked positions in its pyramid structure, whereas Non-stationary Transformer's stationarization approach naturally extends to imputation by normalizing partially observed sequences and applying de-stationary attention to reconstruct missing values, requiring implementation of mask-aware reconstruction logic that respects the normalization/de-normalization framework."
  },
  {
    "source": "Informer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Shared Encoder-Decoder Architecture**: Both papers utilize the standard Transformer encoder-decoder structure for time series tasks. The Informer's encoder with ProbSparse attention and the decoder with masked attention can serve as the base architecture. **Reusable Components**: The `Encoder`, `EncoderLayer`, `Decoder`, `DecoderLayer` classes from Informer can be directly reused as the base model backbone in Non-stationary Transformer, only requiring the attention mechanism to be replaced.\n\n2. **Embedding Layer Design**: Both use similar time series embedding strategies combining value embedding with temporal encoding (positional/time features). **Reusable Components**: The `DataEmbedding` class from Informer (which combines token embedding, positional embedding, and temporal embedding) can be directly reused without modification for the Non-stationary Transformer input processing.\n\n3. **Attention Mechanism Foundation**: Both papers build upon the standard scaled dot-product attention framework with queries, keys, and values. The Informer's `AttentionLayer` wrapper structure (with query/key/value projections and output projection) provides a clean abstraction. **Reusable Components**: The `AttentionLayer` class structure can be maintained, only replacing the `inner_attention` from `ProbAttention` to `De-stationary Attention`. The projection layers (query_projection, key_projection, value_projection, out_projection) remain identical.\n\n4. **Multi-Head Attention Implementation**: Both employ multi-head attention mechanisms with similar computational patterns. **Reusable Components**: The multi-head splitting and concatenation logic in Informer's `AttentionLayer.forward()` method can be directly reused.\n\n5. **Feed-Forward Network Design**: Both use standard position-wise feed-forward networks with Conv1d layers in encoder/decoder layers. **Reusable Components**: The Conv1d-based FFN implementation in `EncoderLayer` and `DecoderLayer` (conv1, conv2, activation, dropout) can be directly reused.\n\n6. **Layer Normalization and Residual Connections**: Both employ LayerNorm and residual connections in the same pattern. **Reusable Components**: The normalization layers (norm1, norm2, norm3) and residual connection patterns in encoder/decoder layers are identical and fully reusable.\n\n7. **Task-Agnostic Framework**: Both papers design their models to handle multiple time series tasks (forecasting, imputation). The Informer code shows a task-based routing pattern (e.g., `if self.task_name == 'imputation'`). **Reusable Components**: This task routing structure and the general forward pass logic can be adapted for Non-stationary Transformer's imputation task.",
    "differences": "1. **Core Innovation - Series Stationarization Wrapper**: Non-stationary Transformer introduces a normalization-denormalization wrapper around the entire model that is completely absent in Informer. **New Implementation Required**: (a) `Normalization` module: Compute μ_x and σ_x from input series x along temporal dimension, transform x to x' = (x - μ_x) / σ_x; (b) `De-normalization` module: Transform model output y' back to ŷ = σ_x ⊙ y' + μ_x; (c) These modules need to be integrated as pre-processing and post-processing steps wrapping the base Transformer.\n\n2. **Novel Attention Mechanism - De-stationary Attention**: While Informer uses ProbSparse attention for efficiency, Non-stationary Transformer requires De-stationary Attention to recover non-stationary information. **New Implementation Required**: (a) MLP projector to learn de-stationary factors: τ = exp(MLP(σ_x, x)) and Δ = MLP(μ_x, x); (b) Modified attention calculation: Softmax((τ·Q'K'^T + 1·Δ^T) / √d_k)·V'; (c) The de-stationary factors τ and Δ are shared across all layers, requiring a different parameter management strategy than Informer's layer-wise attention; (d) Replace Informer's `ProbAttention` class with new `DeStationaryAttention` class that takes additional τ, Δ parameters.\n\n3. **Statistical Information Flow**: Informer processes raw sequences directly, while Non-stationary Transformer requires explicit tracking and passing of statistical information (μ_x, σ_x) throughout the model. **New Implementation Required**: (a) Compute and store μ_x, σ_x at the input stage; (b) Pass these statistics to all De-stationary Attention layers; (c) Modify the forward pass signatures of encoder/decoder to accept and propagate τ, Δ parameters; (d) The implementation shows τ, delta parameters being passed through attention layers, which is not present in Informer.\n\n4. **Attention Computation Complexity**: Informer focuses on reducing O(L²) complexity to O(L log L) through sparse attention with query sampling and Top-u selection. Non-stationary Transformer maintains standard O(L²) attention but adds scaling and shifting operations. **New Implementation Required**: Remove Informer's ProbSparse mechanism including: (a) Query sparsity measurement M(q_i, K); (b) Top-u query selection; (c) Random sampling of key-query pairs; (d) Implement straightforward scaled dot-product attention with de-stationary factor adjustments instead.\n\n5. **Distilling Operation**: Informer uses self-attention distilling with Conv1d and MaxPooling to progressively reduce sequence length in the encoder. Non-stationary Transformer does not employ this distilling strategy. **New Implementation Required**: Remove or disable the `ConvLayer` distilling operations and the pyramid-style multi-stack encoder architecture from Informer when implementing Non-stationary Transformer.\n\n6. **Decoder Input Strategy**: Informer uses a generative inference approach with start tokens sampled from earlier sequence slices (e.g., 5-day history as token for 7-day prediction). Non-stationary Transformer follows standard Transformer decoder design without this specific token sampling strategy. **New Implementation Required**: Modify the decoder input preparation to use standard zero-initialized placeholders or other appropriate initialization strategies instead of Informer's start token sampling.\n\n7. **Model Assumptions and Linearity**: Non-stationary Transformer's theoretical foundation assumes approximate linearity of embedding and feed-forward layers to derive the de-stationary factor formulation (Equation 5). Informer makes no such assumption. **New Implementation Required**: While not requiring explicit code changes, the model design should consider this assumption when choosing activation functions and layer designs to maintain the theoretical validity of the de-stationary attention approximation.\n\n8. **Imputation-Specific Modifications**: For the imputation task, Non-stationary Transformer needs to handle masked inputs differently. **New Implementation Required**: (a) Adapt the normalization module to handle masked values (compute μ_x, σ_x only from observed values); (b) Ensure de-stationary factors are computed from available statistics; (c) Modify the projection layer to output imputed values only for masked positions; (d) The mask parameter shown in the forward signature needs proper integration with the stationarization wrapper."
  },
  {
    "source": "Reformer_2020",
    "target": "Nonstationary_Transformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the standard Transformer architecture with encoder-decoder structures. The Reformer implementation provides the basic building blocks (EncoderLayer, Encoder, DataEmbedding) that can be directly reused for Nonstationary Transformer. The existing `Encoder` and `EncoderLayer` classes in Reformer's implementation can serve as the base structure, requiring only attention mechanism replacement.\n\n2. **Attention Mechanism as Core Component**: Both papers focus on modifying the attention mechanism as their primary innovation. Reformer implements LSHSelfAttention while Nonstationary Transformer proposes De-stationary Attention. The attention layer interface in Reformer's implementation (taking queries, keys, values, attn_mask, tau, delta as inputs) is already compatible with Nonstationary Transformer's requirements, particularly the tau and delta parameters needed for de-stationary factors.\n\n3. **Multi-Head Attention Framework**: Both maintain the multi-head attention paradigm from vanilla Transformers. Reformer's implementation already handles the multi-head structure within its attention layers, and this framework can be preserved when implementing De-stationary Attention for Nonstationary Transformer.\n\n4. **Layer Normalization and Residual Connections**: Both papers utilize standard Transformer components like layer normalization and residual connections. Reformer's implementation includes these through the EncoderLayer structure, which can be directly inherited by Nonstationary Transformer without modification.\n\n5. **Embedding and Positional Encoding**: Both papers require temporal embedding for time series data. Reformer's `DataEmbedding` class (handling enc_in, d_model, embed type, frequency, dropout) provides a reusable component for Nonstationary Transformer's input processing.\n\n6. **Time Series Imputation Task Support**: Both implementations support the imputation task with similar forward pass structures. Reformer's `imputation()` method and the overall forward pass pattern (taking x_enc, x_mark_enc as inputs and producing reconstructed outputs) can be adapted for Nonstationary Transformer's imputation needs.",
    "differences": "1. **Core Attention Innovation - LSH vs De-stationary**: Reformer introduces Locality-Sensitive Hashing (LSH) attention to reduce complexity from O(L²) to O(L log L) through hash-bucketing and chunking mechanisms. This requires implementing hash functions, bucket sorting, and chunk-based attention computation. In contrast, Nonstationary Transformer proposes De-stationary Attention that rescales attention weights using learned de-stationary factors (τ and Δ). NEW IMPLEMENTATION NEEDED: A De-stationary Attention module that computes `Softmax((τ * Q'K'^T + 1Δ^T) / √d_k)V'` instead of LSH bucketing logic.\n\n2. **Series Stationarization Wrapper - Absent in Reformer**: Nonstationary Transformer introduces a critical two-stage normalization/de-normalization wrapper around the base model to handle non-stationary time series. This includes: (a) Normalization module: computing μ_x and σ_x from input series and normalizing x' = (x - μ_x) / σ_x; (b) De-normalization module: restoring output ŷ = σ_x ⊙ y' + μ_x. Reformer has no such preprocessing/postprocessing. NEW IMPLEMENTATION NEEDED: Normalization and De-normalization modules as wrapper functions that process inputs before encoder and outputs after decoder.\n\n3. **De-stationary Factor Learning - Novel Component**: Nonstationary Transformer learns de-stationary factors τ (scaling) and Δ (shifting) from the statistics of unstationarized series using MLPs: `log τ = MLP(σ_x, x)` and `Δ = MLP(μ_x, x)`. These factors are shared across all attention layers to re-inject non-stationary information. Reformer has no equivalent mechanism. NEW IMPLEMENTATION NEEDED: Two MLP projectors that take statistics (μ_x, σ_x) and original series x as input to generate de-stationary factors, and a mechanism to pass these factors to all attention layers.\n\n4. **Memory Efficiency Focus vs Statistical Properties**: Reformer's primary goal is memory and computational efficiency for long sequences (up to 64K tokens) through reversible layers, chunking, and LSH attention. It includes sophisticated memory management (reversible residual connections, chunked feed-forward layers, parameter swapping). Nonstationary Transformer focuses on handling non-stationarity in time series and does not emphasize extreme sequence length efficiency. NEW IMPLEMENTATION NEEDED: Remove or simplify Reformer's complex memory optimization components (reversible layers, chunking logic, hash-bucket size constraints) as they are not required for Nonstationary Transformer.\n\n5. **Attention Computation Complexity**: Reformer's LSH attention restricts attention to hash buckets with complexity O(L log L), requiring `fit_length()` method to ensure sequence length is divisible by `bucket_size * 2`. Nonstationary Transformer maintains standard O(L²) attention complexity but modifies the attention weights calculation. NEW IMPLEMENTATION NEEDED: Remove bucket size constraints and fit_length padding logic; implement standard full attention with de-stationary factor modifications.\n\n6. **Input Processing Pipeline**: Reformer directly processes embedded inputs through LSH attention layers. Nonstationary Transformer requires a specific pipeline: raw input → compute statistics (μ_x, σ_x) → normalize → embed → attention with de-stationary factors → de-normalize output. NEW IMPLEMENTATION NEEDED: A preprocessing stage to compute and store statistics before normalization, and integration of these statistics into the attention mechanism throughout the model.\n\n7. **Shared Q-K Requirement**: Reformer enforces Q=K (shared-QK) for LSH attention to work properly, using the same linear layer for both and normalizing key lengths. Nonstationary Transformer uses standard separate Q, K, V projections. NEW IMPLEMENTATION NEEDED: Restore separate Q and K projections in the attention mechanism, removing Reformer's shared-QK constraint.\n\n8. **Multi-Round Hashing vs Single-Pass Attention**: Reformer implements multi-round LSH with n_rounds parameter to reduce hash collision probability, performing attention n_rounds times in parallel. Nonstationary Transformer uses single-pass attention with de-stationary modifications. NEW IMPLEMENTATION NEEDED: Remove multi-round hashing logic and implement single-pass attention with τ and Δ scaling.\n\n9. **Causal Masking Implementation**: Reformer implements specialized causal masking for shared-QK attention, preventing tokens from attending to themselves except when necessary. Nonstationary Transformer uses standard causal masking for decoder. NEW IMPLEMENTATION NEEDED: Implement standard Transformer causal masking without Reformer's specialized shared-QK constraints.\n\n10. **Output Projection and Loss Calculation**: Both papers have projection layers, but Nonstationary Transformer requires the output to be de-normalized using stored statistics before computing loss. Reformer directly projects to output space. NEW IMPLEMENTATION NEEDED: Modify the output pipeline to apply de-normalization (ŷ = σ_x ⊙ y' + μ_x) before final projection and loss calculation for imputation tasks."
  },
  {
    "source": "Informer_2020",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Paradigm for Time Series Processing**: Both models employ an encoder-decoder architecture to process time series data, though with different underlying mechanisms. Informer uses Transformer-based ProbSparse attention while TiDE uses dense MLPs. For imputation tasks, the source paper's `enc_embedding` and basic encoder structure can be reused as a reference for handling temporal input representation. The embedding layer (DataEmbedding) that combines value embedding with temporal encoding can be adapted for TiDE's input processing.\n\n2. **Time Feature Encoding and Covariates Integration**: Both models incorporate temporal features/covariates alongside the main time series values. Informer's `DataEmbedding` class combines positional encoding with temporal marks (x_mark_enc, x_mark_dec), similar to TiDE's integration of dynamic covariates (x_t) and static attributes (a). The source implementation's time feature encoding mechanism in DataEmbedding can be directly reused or adapted for TiDE's feature projection step, particularly the temporal encoding components.\n\n3. **Residual Connections for Information Flow**: Both architectures employ residual connections to preserve information flow. Informer uses residual connections within its attention layers and feed-forward networks (visible in DecoderLayer and EncoderLayer), while TiDE explicitly uses ResidualBlocks throughout its architecture and adds a global linear residual from look-back to horizon. The source paper's layer normalization and dropout patterns in residual connections can guide TiDE's ResidualBlock implementation.\n\n4. **Multi-Layer Dense Transformation**: While Informer uses multi-head attention, both models employ stacked layers of dense transformations. Informer's feed-forward networks (conv1, conv2 in DecoderLayer with dimension expansion from d_model to d_ff) parallel TiDE's dense encoder/decoder structure. The source implementation's pattern of dimension expansion, activation (ELU/ReLU), dropout, and layer normalization can be reused for TiDE's residual blocks.\n\n5. **Look-back and Horizon Separation**: Both models explicitly separate historical context (look-back) from future prediction (horizon). Informer uses label_len and pred_len to define decoder input structure, similar to TiDE's L (look-back) and H (horizon) separation. The source paper's handling of x_enc (encoder input) and x_dec (decoder input with start token) provides a template for managing TiDE's temporal split, especially for imputation where both past and future context matter.",
    "differences": "1. **Core Architecture: Attention vs. Dense MLP** [NEW IMPLEMENTATION REQUIRED]: Informer's fundamental innovation is ProbSparse self-attention with O(L log L) complexity, using query sparsity measurement and top-u query selection. TiDE completely abandons attention mechanisms in favor of pure MLP-based processing with residual blocks. For TiDE reproduction, you must implement: (a) ResidualBlock class with single hidden layer, ReLU activation, skip connection, dropout, and layer norm; (b) Dense encoder/decoder stacks without any attention mechanism; (c) Remove all ProbAttention, AttentionLayer, and self-attention components from Informer's codebase.\n\n2. **Channel-Independent Processing vs. Multivariate Encoding** [NEW IMPLEMENTATION REQUIRED]: Informer processes all channels/variables jointly through its encoder-decoder with dimension c_out, treating multivariate forecasting holistically. TiDE explicitly adopts channel-independent processing where each time series y^(i) is processed separately with shared weights trained globally. For TiDE implementation: (a) Modify data loading to iterate over individual channels; (b) Reshape inputs to process single channel at a time: (y_{1:L}^(i), x_{1:L}^(i), a^(i)) → ŷ_{L+1:L+H}^(i); (c) Aggregate predictions across channels post-inference; (d) Adjust loss computation to handle per-channel processing during training.\n\n3. **Feature Projection and Dimensionality Reduction** [NEW IMPLEMENTATION REQUIRED]: Informer directly processes high-dimensional inputs through attention without explicit dimensionality reduction. TiDE introduces a novel feature projection step that maps dynamic covariates x_t^(i) at each time-step to lower dimensional representation x̃_t^(i) (temporalWidth ≪ r) using a residual block before encoder input. This prevents the flattened input from being prohibitively large ((L+H)r → (L+H)r̃). Implementation needs: (a) Feature projection residual block mapping r-dimensional covariates to r̃ dimensions; (b) Apply projection independently to each time-step in both look-back and horizon; (c) Flatten projected features before encoder: flatten([x̃_1:L+H, y_1:L, a]).\n\n4. **Self-Attention Distilling vs. Temporal Decoder Highway** [NEW IMPLEMENTATION REQUIRED]: Informer uses self-attention distilling with Conv1d and MaxPool to progressively reduce sequence length (halving at each layer) and build a pyramid structure for memory efficiency. TiDE employs a temporal decoder that creates a direct 'highway' from future covariates at time L+t to prediction at L+t: ŷ_{L+t}^(i) = TemporalDecoder(d_t^(i); x̃_{L+t}^(i)). This ensures strong direct effects (e.g., holidays) can influence predictions. Implementation requires: (a) Remove all ConvLayer, distilling operations, and pyramid stacking from Informer; (b) Implement temporal decoder as residual block with output size 1; (c) Concatenate decoded vector d_t with projected covariate x̃_{L+t} at each horizon time-step; (d) Process each time-step independently through temporal decoder.\n\n5. **Decoder Input Strategy: Start Token vs. Dense Reshape** [NEW IMPLEMENTATION REQUIRED]: Informer uses generative inference with start token (X_token from earlier slice) and placeholder X_0 for target sequence, feeding decoder with Concat(X_token, X_0). Predictions are made through masked multi-head attention preventing auto-regression. TiDE's decoder operates completely differently: (a) Dense decoder maps encoding e^(i) to vector g^(i) of size H×p; (b) Reshape g^(i) to matrix D^(i) ∈ R^{p×H}; (c) Each column d_t^(i) represents decoded vector for t-th horizon time-step; (d) No masking or auto-regressive structure. Implementation needs to replace Informer's token-based decoder with dense decoder outputting reshaped matrix.\n\n6. **Global Linear Residual Connection** [NEW IMPLEMENTATION REQUIRED]: While Informer has local residual connections within layers, TiDE adds a global residual that linearly maps the entire look-back y_{1:L}^(i) to a vector of horizon size H, which is added to final predictions. This ensures linear models (like DLinear from Zeng et al., 2023) are a subclass of TiDE. Implementation requires: (a) Add nn.Linear layer mapping L input time-steps to H output time-steps; (b) Apply this linear transformation to look-back y_{1:L}; (c) Add result element-wise to final decoder output ŷ_{L+1:L+H}; (d) This is completely absent in Informer and must be added as a new component.\n\n7. **Loss Function and Training Strategy for Imputation** [NEW IMPLEMENTATION REQUIRED]: Informer's provided implementation focuses on forecasting with MSE loss on future predictions. For imputation tasks, both models need modifications: (a) TiDE requires MSE loss on reconstructed values at missing positions only, using mask to select imputation targets; (b) Training should use overlapping (look-back, horizon) pairs from training period as per TiDE's description; (c) Rolling validation on test set for evaluation; (d) The imputation method in Informer's code (self.imputation) needs extension to handle TiDE's channel-independent processing and global residual connection; (e) Mask handling must be integrated into TiDE's temporal decoder to only compute loss on missing positions."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing Philosophy**: Both models process time series in a manner that can handle individual channels/series. Pyraformer's encoder processes embedded sequences independently before aggregation, while TiDE explicitly uses channel-independent processing where each time series is processed separately. **Reusable Component**: The embedding layer structure from Pyraformer (DataEmbedding) can be adapted for TiDE's feature projection step, particularly the temporal encoding mechanisms.\n\n2. **Residual Connection Architecture**: Both models heavily utilize residual connections for feature learning. Pyraformer's PositionwiseFeedForward module uses residual connections with layer normalization, while TiDE's core building block is the MLP residual block. **Reusable Component**: The residual block pattern from Pyraformer's PositionwiseFeedForward (with skip connections, dropout, and layer norm) can be directly adapted as TiDE's ResidualBlock implementation, requiring only minor modifications to match TiDE's specific dimensions.\n\n3. **Multi-Scale Feature Extraction**: Pyraformer uses pyramidal attention with coarser-scale construction (CSCM) to capture multi-resolution temporal patterns through convolution layers with different strides. TiDE implicitly performs multi-scale processing through its encoder-decoder structure with different hidden dimensions. **Reusable Component**: The bottleneck architecture pattern from Pyraformer's Bottleneck_Construct (dimension reduction before convolution, then restoration) can inform TiDE's feature projection design where covariates are reduced from dimension r to r̃.\n\n4. **Hierarchical Encoding-Decoding Structure**: Both models follow an encoder-decoder paradigm. Pyraformer encodes historical sequences through pyramidal attention layers and can decode for multi-step forecasting. TiDE uses dense encoder-decoder with feature projection and temporal decoder. **Reusable Component**: The overall encoder-decoder flow control logic and the pattern of processing embeddings through stacked layers can be adapted from Pyraformer's implementation.\n\n5. **Global Context Integration**: Pyraformer aggregates features from all pyramid scales using refer_points() for global receptive field. TiDE concatenates past values, projected covariates, and static attributes for global context. **Reusable Component**: The tensor gathering and concatenation operations from Pyraformer's refer_points() and index-based feature collection can be adapted for TiDE's feature stacking and flattening operations.\n\n6. **Normalization and Regularization**: Both employ layer normalization and dropout for regularization. Pyraformer uses LayerNorm in feed-forward layers and dropout in attention mechanisms. TiDE uses layer norm at residual block outputs and dropout on linear layers. **Reusable Component**: The normalization and dropout patterns from Pyraformer can be directly reused in TiDE's residual blocks.",
    "differences": "1. **Core Attention Mechanism vs. Pure MLP Architecture**: Pyraformer's innovation is the pyramidal attention mechanism (PAM) with O(L) complexity, using multi-scale C-ary tree structure for long-range dependency modeling through sparse attention patterns. TiDE completely abandons attention mechanisms in favor of pure MLP-based dense encoders/decoders. **New Implementation Required**: TiDE requires implementing pure feedforward MLP stacks without any attention computation, query-key-value projections, or attention masks. The dense encoder/decoder must be built from scratch using only linear layers and activations, without Pyraformer's attention infrastructure.\n\n2. **Temporal Modeling Philosophy**: Pyraformer models temporal dependencies through graph-based pyramidal structure with inter-scale and intra-scale connections, where coarser scales capture long-range patterns (daily→weekly→monthly). TiDE models temporal patterns through dense feature transformations and explicit temporal decoder that processes each horizon time-step with its corresponding projected covariates. **New Implementation Required**: TiDE's temporal decoder that concatenates decoded vectors d_t with projected covariates x̃_{L+t} for each time-step needs to be implemented from scratch. This per-timestep processing with covariate highway is fundamentally different from Pyraformer's pyramidal aggregation.\n\n3. **Covariate Handling Strategy**: Pyraformer treats covariates (x_mark) as additive embeddings combined with observation embeddings at the input stage, without explicit dimensionality reduction. TiDE introduces explicit feature projection step that maps high-dimensional covariates x_t (dimension r) to lower-dimensional projections x̃_t (dimension r̃) using residual blocks, then processes both look-back and horizon covariates separately. **New Implementation Required**: The feature projection module that reduces covariate dimensionality before encoder input, and the temporal decoder's covariate integration mechanism must be newly implemented. This includes handling future covariates (x_{L+1:L+H}) which Pyraformer doesn't explicitly model.\n\n4. **Multi-Resolution Construction vs. Flat Dense Processing**: Pyraformer uses CSCM (Coarser-Scale Construction Module) with bottleneck convolutions (stride C) to build C-ary tree structure, creating explicit multi-scale representations. TiDE uses flat dense processing where encoder directly maps flattened concatenation of (past values; projected covariates; static attributes) to embedding without hierarchical construction. **New Implementation Required**: TiDE's flat concatenation and dense transformation approach requires implementing a simple flattening and stacking operation followed by MLP layers, completely different from Pyraformer's convolutional pyramid construction.\n\n5. **Output Generation Mechanism**: Pyraformer offers two prediction modules: (1) batch prediction from last nodes at all scales, (2) decoder with two full attention layers using prediction tokens. Both aggregate multi-scale features. TiDE uses reshape operation to convert decoder output g (size H×p) into matrix D (p×H), then applies temporal decoder per time-step with covariate concatenation, plus global linear residual connection from look-back to horizon. **New Implementation Required**: TiDE's reshape-based decoding (converting flat vector to per-timestep representations), temporal decoder per-timestep processing, and global linear residual connection (look-back→horizon mapping) are entirely new components not present in Pyraformer.\n\n6. **Complexity and Scalability Design**: Pyraformer's core contribution is reducing attention complexity from O(L²) to O(L) through sparse pyramidal attention with custom CUDA kernels. TiDE's design prioritizes simplicity and efficiency through pure dense operations (matrix multiplications) that are hardware-optimized, arguing that simple MLPs can outperform complex attention mechanisms. **New Implementation Required**: TiDE requires no custom CUDA kernels or sparse attention implementations. Instead, need to implement straightforward dense MLP stacks with standard PyTorch operations, focusing on proper hyperparameter tuning (hiddenSize, numEncoderLayers, numDecoderLayers, temporalDecoderHidden, decoderOutputDim) rather than attention pattern optimization.\n\n7. **Static Attribute Integration**: Pyraformer doesn't explicitly model static attributes in its architecture. TiDE explicitly incorporates static attributes a^(i) by concatenating them with past values and projected covariates in the encoder input. **New Implementation Required**: Need to implement static attribute handling in TiDE's encoder, including proper concatenation with temporal features and ensuring the model can leverage time-invariant information for better predictions.\n\n8. **Training Objective and Evaluation**: While both use MSE loss, Pyraformer focuses on single-step and multi-step forecasting with different decoder architectures. TiDE emphasizes rolling validation/evaluation where all possible (look-back, horizon) pairs from test set are evaluated, with explicit channel-independent training where each time series is processed separately but weights are trained globally. **New Implementation Required**: Need to implement TiDE's specific training loop that handles channel-independent batching, proper construction of overlapping (look-back, horizon) pairs during training, and rolling evaluation strategy for test set."
  },
  {
    "source": "FEDformer_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Time Series Forecasting Foundation**: Both papers address long-term time series forecasting problems with encoder-decoder architectures. FEDformer's DataEmbedding class (handling temporal embeddings with positional encoding) can be partially reused for TiDE's covariate processing, particularly the temporal feature extraction logic.\n\n2. **Residual Connection Philosophy**: FEDformer extensively uses residual connections in its MOEDecomp blocks and layer structures (e.g., `S_en^{l,1} = MOEDecomp(FEB(X_en^{l-1}) + X_en^{l-1})`). TiDE's core component is the Residual Block with skip connections. The implementation pattern from FEDformer's residual structure (add-then-norm) can guide TiDE's ResidualBlock implementation, though TiDE uses simpler MLP-based residuals.\n\n3. **Decomposition for Feature Processing**: FEDformer's series_decomp module (seasonal-trend decomposition with moving average) demonstrates feature extraction from raw time series. While TiDE doesn't explicitly decompose, the feature projection step (mapping x_t to lower dimensional \\tilde{x}_t) serves a similar dimensionality reduction purpose. FEDformer's decomposition logic can inform how to preprocess time series before feeding into TiDE's feature projection layer.\n\n4. **Multi-Layer Dense Architecture**: FEDformer's encoder/decoder stack multiple layers (n_e and n_d layers) with consistent hidden dimensions. TiDE similarly uses stacked residual blocks in its Dense Encoder (n_e layers) and Dense Decoder (n_d layers). The layer stacking pattern and initialization strategies from FEDformer's EncoderLayer/DecoderLayer can be adapted for TiDE's encoder/decoder blocks.\n\n5. **Global Linear Residual**: FEDformer includes linear projections (e.g., W_S projection in final prediction). TiDE explicitly adds a global linear residual from look-back to horizon. The implementation of FEDformer's projection layers (nn.Linear with proper initialization) can be directly reused for TiDE's global residual connection.\n\n6. **Batch Processing and Training Loop**: FEDformer's training handles batched time series data with shape [B, L, D]. TiDE processes data in mini-batches with overlapping time-points during training. The data loading, batching logic, and MSE loss computation from FEDformer's training pipeline can be adapted for TiDE's training procedure.",
    "differences": "1. **Core Architecture Paradigm - NEWLY IMPLEMENT**: FEDformer uses frequency-domain operations (Fourier/Wavelet transforms) with attention mechanisms for capturing temporal dependencies. TiDE is purely MLP-based with NO attention or frequency transforms. Need to implement: (a) Simple MLP Residual Blocks (one hidden layer + ReLU + dropout + layer norm), (b) Flatten-and-stack operations for time series, (c) Reshape operations for decoder output (H×p → p×H matrix).\n\n2. **Covariate Handling - NEWLY IMPLEMENT**: FEDformer does not explicitly model external covariates; it focuses on the time series itself with time embeddings. TiDE heavily relies on dynamic covariates (x_t^(i)) and static attributes (a^(i)). Need to implement: (a) Feature Projection layer (ResidualBlock mapping r-dim covariates to \\tilde{r}-dim), (b) Concatenation logic for [y_past; \\tilde{x}_past; \\tilde{x}_future; a_static], (c) Temporal Decoder that combines decoded vectors with projected future covariates.\n\n3. **Channel-Independent Processing - NEWLY IMPLEMENT**: FEDformer processes all channels (features) jointly in shape [B, L, H, E] with multi-head attention across features. TiDE uses channel-independent approach: processes each time series (y^(i), x^(i), a^(i)) separately with shared weights. Need to implement: (a) Loop or batch processing over individual time series, (b) Weight sharing mechanism across all time series in dataset, (c) Data preprocessing to separate channels before feeding to model.\n\n4. **Encoder-Decoder Information Flow - NEWLY IMPLEMENT**: FEDformer's decoder receives encoder output via cross-attention (FEA module) at each layer, maintaining temporal alignment. TiDE's decoder receives a single encoded vector e^(i) (no temporal dimension), then expands it to H time-steps via Dense Decoder + Reshape. Need to implement: (a) Encoding to fixed-size vector (flattening temporal dimension), (b) Decoder output reshaping from (p·H) to (p×H), (c) Per-timestep temporal decoder combining d_t with \\tilde{x}_{L+t}.\n\n5. **Decomposition Strategy - NEWLY IMPLEMENT**: FEDformer uses MOEDecomp (Mixture of Experts Decomposition) with multiple average pooling filters for seasonal-trend separation, applied recursively throughout the network. TiDE has NO explicit decomposition; it relies on the MLP's capacity to learn non-linear mappings. The global linear residual in TiDE serves as a simple baseline, but doesn't decompose. Need to implement: Simple linear layer for global residual (already straightforward), no complex decomposition logic needed.\n\n6. **Frequency/Wavelet Operations - NOT NEEDED**: FEDformer's core innovation (FEB-f, FEB-w, FEA-f, FEA-w) involves FFT/IFFT, mode selection, complex multiplication, and wavelet transforms. TiDE completely avoids these operations. Do NOT implement: (a) Fourier/Wavelet transform modules, (b) Frequency mode selection, (c) Complex number operations, (d) Multi-resolution decomposition. TiDE's simplicity is its strength.\n\n7. **Loss Function and Evaluation - MINOR MODIFICATION**: Both use MSE loss, but FEDformer evaluates on forecasting metrics (MAE, MSE on future horizon). TiDE uses the same but emphasizes rolling validation with overlapping time-points. The evaluation logic from FEDformer can be reused with minor modifications to handle TiDE's channel-independent predictions and ensure proper alignment of predictions with ground truth during rolling evaluation.\n\n8. **Hyperparameter Structure - NEWLY DEFINE**: FEDformer's hyperparameters focus on: modes (frequency modes), L (wavelet decomposition levels), moving_avg (decomposition window), n_heads (attention heads). TiDE's hyperparameters are: temporalWidth (\\tilde{r}), hiddenSize (encoder/decoder hidden dim), decoderOutputDim (p), numEncoderLayers (n_e), numDecoderLayers (n_d), temporalDecoderHidden. Need to define: New config class/dict structure matching TiDE's architecture, with appropriate default values and tuning ranges."
  },
  {
    "source": "DLinear_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing Architecture**: Both models process each time series variate independently without modeling cross-variate correlations. DLinear explicitly states 'shares weights across different variates and does not model any spatial correlations', while TiDE uses 'channel independent manner' where input is processed one time-series at a time. The DLinear implementation's loop structure `for i in range(self.channels)` can be directly reused as a template for TiDE's per-channel processing.\n\n2. **Linear Residual Connection Philosophy**: Both architectures employ global linear skip connections from input to output. DLinear uses direct linear layers mapping lookback to horizon (`self.Linear_Seasonal` and `self.Linear_Trend`), while TiDE adds 'a global residual connection that linearly maps the look-back to a vector the size of the horizon'. The DLinear implementation's weight initialization strategy `(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])` provides a strong baseline for implementing TiDE's linear residual path.\n\n3. **Decomposition-Based Feature Processing**: DLinear uses seasonal-trend decomposition (`self.decompsition = series_decomp(configs.moving_avg)`) to separate components before linear projection. TiDE's feature projection step (`ResidualBlock` for dimensionality reduction) serves a similar purpose of preprocessing raw inputs into more manageable representations. The decomposition module from DLinear's implementation can be adapted as a preprocessing option in TiDE.\n\n4. **Direct Mapping Strategy (DMS)**: Both employ Direct Multi-Step forecasting rather than autoregressive approaches. DLinear directly maps L historical points to T future points via `nn.Linear(self.seq_len, self.pred_len)`. TiDE similarly maps the entire lookback window to the full horizon in one forward pass. The DLinear encoder structure provides a blueprint for TiDE's end-to-end mapping architecture.\n\n5. **Permutation of Temporal Dimensions**: Both implementations use dimension permutation for temporal processing. DLinear applies `.permute(0, 2, 1)` to reshape [B, L, C] to [B, C, L] for channel-wise linear operations, then permutes back. TiDE's architecture requires similar reshaping operations (flatten, reshape) for its dense encoder/decoder, making DLinear's permutation handling directly reusable.\n\n6. **Individual vs. Shared Weight Options**: DLinear implements both individual channel models (`self.individual=True` with `nn.ModuleList`) and shared weight models. TiDE, while trained globally, processes channels independently, suggesting a similar architectural pattern. The DLinear implementation's conditional logic for individual/shared weights provides a template for TiDE's weight sharing strategy.",
    "differences": "1. **Core Architecture Paradigm - Linear vs. MLP-Based**: DLinear uses pure linear transformations (single-layer `nn.Linear` modules) without non-linearities except in decomposition. TiDE introduces deep MLP architectures with multiple residual blocks containing ReLU activations and layer normalization. **NEW IMPLEMENTATION REQUIRED**: Multi-layer ResidualBlock class with hidden layers, ReLU, dropout, skip connections, and LayerNorm - completely absent in DLinear's linear-only design.\n\n2. **Covariate Integration Strategy**: DLinear operates solely on historical target values without using time covariates (paper explicitly states 'do not use time-covariates as they hurt performance'). TiDE's core innovation is sophisticated covariate handling: (a) Feature projection for dynamic covariates at each timestep, (b) Concatenation of projected covariates with static attributes, (c) Temporal decoder highway from future covariates to predictions. **NEW IMPLEMENTATION REQUIRED**: Entire covariate processing pipeline including separate feature projection networks, static attribute handling, and temporal decoder with covariate concatenation.\n\n3. **Encoder-Decoder Architecture Complexity**: DLinear has a flat architecture with direct linear mapping (2 linear layers for seasonal/trend). TiDE employs a hierarchical encoder-decoder with: (a) Feature Projection (ResidualBlock), (b) Dense Encoder (n_e stacked residual blocks), (c) Dense Decoder (n_d stacked residual blocks), (d) Temporal Decoder (per-timestep ResidualBlock). **NEW IMPLEMENTATION REQUIRED**: Multi-stage encoding/decoding pipeline with configurable depth (numEncoderLayers, numDecoderLayers), intermediate reshape operations, and temporal decoder for per-timestep refinement.\n\n4. **Dimensionality Reduction and Representation Learning**: DLinear maintains original dimensionality throughout (only temporal dimension changes L→T). TiDE implements aggressive dimensionality reduction: covariates r→r̃ via feature projection, then encoding to fixed-size embedding e^(i), then decoding to H×p matrix, finally per-timestep decoding. **NEW IMPLEMENTATION REQUIRED**: Multi-stage dimensionality transformation logic with flatten/reshape operations, configurable hidden dimensions (hiddenSize, decoderOutputDim, temporalDecoderHidden, temporalWidth).\n\n5. **Decomposition Strategy Differences**: DLinear uses Autoformer's moving average decomposition as core architecture (applied at input, separate linear layers for each component). TiDE has no explicit decomposition - instead uses deep MLPs to learn non-linear representations implicitly. **NEW IMPLEMENTATION REQUIRED**: Replace decomposition-based dual-path architecture with unified MLP encoding, requiring different initialization strategies and loss formulations.\n\n6. **Temporal Processing Granularity**: DLinear processes entire lookback window as single unit (one linear transformation per component). TiDE operates at multiple granularities: (a) Per-timestep feature projection for all L+H points, (b) Global encoding of flattened sequence, (c) Per-timestep temporal decoding with individual highways. **NEW IMPLEMENTATION REQUIRED**: Hybrid processing with both global (encoder/decoder) and local (per-timestep temporal decoder) operations, requiring careful dimension tracking and concatenation logic.\n\n7. **Regularization and Training Components**: DLinear uses simple weight initialization and no explicit regularization in architecture. TiDE incorporates: dropout in residual blocks, layer normalization after each block, specific hidden size configurations. **NEW IMPLEMENTATION REQUIRED**: Comprehensive regularization infrastructure including dropout placement in residual connections, layer norm integration, and hyperparameter management for multiple hidden dimensions.\n\n8. **Output Generation Mechanism**: DLinear generates final output by summing seasonal and trend predictions (`x = seasonal_output + trend_output`). TiDE uses temporal decoder that concatenates decoded vector d_t with projected covariates x̃_{L+t} for each timestep, plus global linear residual. **NEW IMPLEMENTATION REQUIRED**: Per-timestep concatenation and decoding logic with covariate highways, fundamentally different from DLinear's component summation approach."
  },
  {
    "source": "PatchTST_2022",
    "target": "TiDE_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing Philosophy**: Both models adopt a channel-independent approach where each time series is processed separately with globally shared weights. PatchTST processes each variate independently, and TiDE explicitly states it applies the model to one time-series at a time (y_{1:L}^{(i)}, x_{1:L}^{(i)}, a^{(i)}) → ŷ_{L+1:L+H}^{(i)}. The source implementation's channel-independent data loading and batching logic can be directly reused for TiDE's training pipeline.\n\n2. **Residual Connection Design**: Both architectures employ residual connections to preserve linear relationships. PatchTST uses skip connections in its architecture, while TiDE explicitly adds a global linear residual connection from look-back to horizon to ensure linear models remain a subclass. The source paper's residual connection implementation patterns (skip connections, layer normalization) can guide TiDE's ResidualBlock implementation.\n\n3. **Mini-Batch Training with Overlapping Windows**: Both use mini-batch gradient descent where epochs consist of all (look-back, horizon) pairs that can be constructed from the training period, allowing overlapping time-points between batches. The source implementation's sliding window data generation logic, batch construction, and training loop structure can be directly adapted for TiDE.\n\n4. **MSE Loss for Forecasting**: Both models use Mean Squared Error (MSE) as the primary training loss function for time series forecasting tasks. The loss computation and backpropagation framework from PatchTST can be reused.\n\n5. **Rolling Validation/Evaluation Strategy**: Both employ rolling evaluation on test sets using all possible (look-back, horizon) pairs. The source paper's evaluation pipeline, metrics computation (MSE, MAE), and validation set handling can be adapted for TiDE's evaluation needs.",
    "differences": "1. **Core Architecture Paradigm - NEW IMPLEMENTATION REQUIRED**: PatchTST is a Transformer-based model using self-attention over patches, while TiDE is purely MLP-based with dense encoder-decoder architecture. TiDE requires implementing: (a) ResidualBlock with single hidden layer, ReLU activation, dropout, and layer normalization; (b) Dense Encoder with n_e stacked residual blocks; (c) Dense Decoder with n_d stacked residual blocks; (d) Temporal Decoder as a specialized residual block. None of PatchTST's attention mechanisms, patch embedding, or positional encoding can be reused.\n\n2. **Covariate Handling and Feature Projection - NEW IMPLEMENTATION REQUIRED**: TiDE explicitly incorporates dynamic covariates (x_t) and static attributes (a) through a novel feature projection step that maps high-dimensional covariates to lower-dimensional space (r → r̃) using a residual block before flattening. This dimensionality reduction prevents input explosion from (L+H)r to (L+H)r̃. PatchTST does not have this covariate handling mechanism. Need to implement: (a) Feature projection residual block for temporal covariates; (b) Concatenation logic for static attributes; (c) Separate handling of look-back and horizon covariates.\n\n3. **Temporal Decoding with Highway Connections - NEW IMPLEMENTATION REQUIRED**: TiDE's temporal decoder creates a 'highway' from future covariates at time-step L+t directly to predictions at L+t by concatenating decoded vectors d_t^{(i)} with projected covariates x̃_{L+t}^{(i)}. This allows strong direct effects (e.g., holidays) to influence specific time-steps. PatchTST has no equivalent time-step-specific covariate integration. Need to implement per-time-step decoding with covariate concatenation.\n\n4. **Input Representation and Flattening Strategy - NEW IMPLEMENTATION REQUIRED**: TiDE flattens and concatenates: (past values y_{1:L}) + (all projected covariates x̃_{1:L+H}) + (static attributes a) into a single vector for the encoder. Then decoder reshapes output g^{(i)} ∈ R^{p·H} to matrix D^{(i)} ∈ R^{p×H}. PatchTST uses patch-based tokenization without flattening entire sequences. Need to implement custom flatten/reshape operations and concatenation logic.\n\n5. **Hyperparameter Structure - NEW CONFIGURATION REQUIRED**: TiDE introduces distinct hyperparameters: temporalWidth (r̃), hiddenSize (encoder/decoder), numEncoderLayers (n_e), numDecoderLayers (n_d), decoderOutputDim (p), temporalDecoderHidden. These control the encoder-decoder depth and width separately, unlike PatchTST's patch_len, stride, d_model, n_heads. Need to implement separate configuration for feature projection, dense encoding/decoding, and temporal decoding dimensions.\n\n6. **No Patching or Attention Mechanism**: TiDE processes the entire look-back window as a flattened vector without patching or self-attention, relying purely on dense layers for temporal dependency modeling. All of PatchTST's patching logic, attention masks, and multi-head attention implementations are irrelevant and should not be ported."
  },
  {
    "source": "PatchTST_2022",
    "target": "MultiPatchFormer_2025",
    "type": "in-domain",
    "similarities": "1. **Patch-based Tokenization Architecture**: Both papers adopt patch-based tokenization as the fundamental input representation strategy. PatchTST divides time series into non-overlapping patches and treats each patch as a token, which can be directly reused as the base tokenization module. The patch embedding layer, including the linear projection and positional encoding components from PatchTST's implementation, provides a ready-to-use foundation for MultiPatchFormer.\n\n2. **Transformer Backbone with Channel Independence**: Both architectures employ Transformer encoders and maintain channel-independent processing philosophy. PatchTST's channel-independent (CI) mode processes each variable separately, which aligns with MultiPatchFormer's design. The self-attention mechanism, feed-forward networks, and layer normalization components from PatchTST can be directly reused. The multi-head attention implementation with proper masking support is transferable.\n\n3. **Supervised Learning Framework for Imputation**: Both papers frame imputation as a supervised reconstruction task where the model learns to predict missing values from observed data. PatchTST's training loop structure, including the forward pass through patches, loss computation on masked regions, and backpropagation logic, can serve as the training framework template. The mask-based loss calculation (computing reconstruction error only on missing positions) is a shared component.\n\n4. **Positional Encoding Strategy**: Both utilize positional encodings to preserve temporal order information in patch sequences. PatchTST's learnable or sinusoidal positional encoding implementation can be adapted for MultiPatchFormer's temporal position tracking across different patch scales.\n\n5. **Normalization Techniques**: Both papers employ instance normalization or similar strategies to handle distribution shifts across different time series. PatchTST's RevIN (Reversible Instance Normalization) module can be reused for input preprocessing and output denormalization.",
    "differences": "1. **Multi-Scale Patch Hierarchy (Core Innovation)**: MultiPatchFormer introduces multiple patch sizes simultaneously to capture temporal patterns at different granularities, which is the primary innovation. Unlike PatchTST's single fixed patch length, MultiPatchFormer requires implementing: (a) parallel patch extraction modules with different patch sizes (e.g., 16, 32, 64), (b) separate embedding layers for each scale, (c) a fusion mechanism to aggregate multi-scale representations. This multi-scale patch encoder with hierarchical feature extraction is entirely new and needs custom implementation.\n\n2. **Cross-Scale Attention Mechanism**: MultiPatchFormer needs a novel cross-scale attention module to enable interaction between different patch granularities, which doesn't exist in PatchTST. This requires implementing: (a) attention mechanisms that operate across patches of different sizes, (b) alignment strategies to handle different sequence lengths from various patch scales, (c) learnable fusion weights or gating mechanisms to combine multi-scale features. This cross-scale interaction layer is a critical new component.\n\n3. **Hierarchical Feature Fusion Strategy**: While PatchTST processes patches at a single scale, MultiPatchFormer requires designing and implementing a hierarchical fusion architecture that combines: (a) fine-grained features from small patches (capturing local patterns), (b) coarse-grained features from large patches (capturing global trends). This may involve concatenation, weighted summation, or learned gating mechanisms across scales, requiring new fusion modules.\n\n4. **Scale-Adaptive Masking Strategy**: MultiPatchFormer needs a sophisticated masking strategy that works coherently across multiple patch scales. Unlike PatchTST's single-scale random masking, this requires: (a) consistent mask propagation across different patch sizes (ensuring the same time points are masked at all scales), (b) scale-specific mask handling logic, (c) potentially different masking ratios or patterns optimized for each scale. This demands new mask generation and alignment logic.\n\n5. **Multi-Scale Reconstruction Head**: The output reconstruction mechanism differs significantly. MultiPatchFormer needs to implement: (a) scale-specific decoder heads that project from each patch scale back to the original temporal resolution, (b) a fusion strategy to combine predictions from multiple scales, (c) potentially weighted loss functions that balance reconstruction quality across scales. PatchTST's single-scale linear projection head is insufficient.\n\n6. **Computational Efficiency Optimization**: Processing multiple patch scales simultaneously increases computational cost. MultiPatchFormer requires implementing efficiency optimizations such as: (a) shared Transformer layers across scales with scale-specific adapters, (b) progressive fusion strategies that reduce redundant computation, (c) efficient attention mechanisms for handling variable-length sequences from different scales. These optimization strategies are not present in PatchTST.\n\n7. **Loss Function Design**: MultiPatchFormer likely employs a multi-scale reconstruction loss that combines errors from different patch granularities, potentially with scale-specific weighting. This requires implementing: (a) separate loss computation for each scale's reconstruction, (b) learnable or heuristic weights to balance multi-scale objectives, (c) possibly auxiliary losses for cross-scale consistency. PatchTST's simple MSE loss on the single-scale reconstruction is insufficient."
  },
  {
    "source": "DLinear_2022",
    "target": "MultiPatchFormer_2025",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Crossformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "MultiPatchFormer_2025",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "PatchTST_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Segmentation-based Processing Philosophy**: Both papers adopt a segmentation approach to time series data. PatchTST divides time series into patches (non-overlapping segments), while SegRNN uses segments with configurable overlap. The source paper's patching mechanism (patch_len, stride parameters) can be directly reused by setting stride=patch_len for non-overlapping segments in SegRNN. The data preparation pipeline for creating segments from continuous time series is highly transferable.\n\n2. **Channel-Independent Processing**: Both architectures process each channel (variate) independently rather than modeling cross-channel dependencies. PatchTST uses channel-independent Transformer layers, while SegRNN processes each channel separately through RNN cells. The channel iteration logic, data reshaping operations (B, C, L -> processing each C independently), and the overall training loop structure can be reused from PatchTST's implementation.\n\n3. **Instance Normalization Strategy**: Both papers apply normalization at the instance level to handle distribution shifts. PatchTST uses RevIN (Reversible Instance Normalization), and SegRNN employs similar instance-level normalization. The normalization module (computing mean/std per instance, normalizing, then denormalizing) from PatchTST can be adapted with minimal changes for SegRNN.\n\n4. **Forecasting-to-Imputation Adaptation**: Both papers are primarily designed for forecasting but can be adapted for imputation tasks. The mask-based training strategy (masking certain positions during training, computing loss only on masked positions) and the evaluation protocol for imputation can be shared. PatchTST's implementation of mask generation, loss computation on masked regions, and metric calculation (MSE, MAE on imputed values) provides a solid foundation.\n\n5. **Embedding and Projection Layers**: Both use linear projections to map input segments/patches to hidden representations and final outputs back to the original dimension. PatchTST's linear embedding layer for patches and output projection can be reused, adjusting only the input/output dimensions to match SegRNN's segment configuration.",
    "differences": "1. **Core Architecture - Transformer vs RNN**: PatchTST uses multi-head self-attention Transformer encoder blocks, while SegRNN replaces this with simple RNN cells (GRU/LSTM). **NEW IMPLEMENTATION REQUIRED**: Replace the entire Transformer encoder stack with RNN processing. Implement segment-level RNN that processes each segment as a single time step (not token-by-token within segment). The RNN hidden state propagates across segments, requiring careful state management and initialization strategies.\n\n2. **Temporal Modeling Granularity**: PatchTST performs token-level attention within and across patches using positional encoding, while SegRNN treats each segment holistically as a single representation fed to RNN. **NEW IMPLEMENTATION REQUIRED**: Implement segment averaging/pooling mechanism to compress each segment into a single vector representation before RNN processing. Remove all positional encoding and attention mechanisms. Implement the segment-to-representation compression (e.g., mean pooling over segment length) and the reverse expansion mechanism.\n\n3. **Sequence Length Handling**: PatchTST reduces sequence length through patching (L/patch_len tokens), while SegRNN's effective sequence length depends on segment configuration and overlap strategy. **NEW IMPLEMENTATION REQUIRED**: Implement overlapping segment generation logic if overlap is used (e.g., sliding window with configurable stride). Handle boundary conditions for overlapping segments during reconstruction (averaging overlapped predictions or using specific merge strategies).\n\n4. **Computational Complexity Focus**: PatchTST aims to reduce Transformer's quadratic complexity through patching, while SegRNN emphasizes extreme simplicity and linear complexity of RNNs. **NEW IMPLEMENTATION REQUIRED**: Implement efficient RNN unrolling for long sequences. Optimize for RNN's sequential nature (cannot parallelize across time like Transformers). May need gradient clipping and careful initialization to prevent vanishing/exploding gradients in long sequences.\n\n5. **Imputation-Specific Reconstruction**: For imputation, PatchTST can use attention to aggregate information from observed patches, while SegRNN relies on RNN's hidden state propagation. **NEW IMPLEMENTATION REQUIRED**: Implement bidirectional RNN processing for imputation (forward and backward passes to capture context from both directions). Design segment-level masking strategy compatible with RNN's sequential processing. Implement reconstruction logic that combines forward/backward RNN outputs for masked segments. Handle variable-length gaps and different missing patterns with RNN state management.\n\n6. **Model Expressiveness vs Simplicity Trade-off**: PatchTST uses sophisticated multi-head attention with learned query-key-value projections, while SegRNN intentionally uses minimal parameterization (simple RNN cells, basic linear layers). **NEW IMPLEMENTATION REQUIRED**: Remove all multi-head attention components, layer normalization in attention blocks, and feed-forward networks. Implement lightweight RNN cells with minimal parameters. Focus on segment representation quality rather than complex interactions."
  },
  {
    "source": "TimesNet_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Normalization Strategy for Missing Data**: Both papers employ instance normalization techniques to handle distribution shifts in time series with missing values. TimesNet uses Non-stationary Transformer normalization (mean-std normalization with masking) in its imputation method, computing statistics only from observed values (mask==1). SegRNN likely adopts similar normalization approaches. **Code Reuse**: The normalization logic in TimesNet's `imputation()` method (lines computing means/stdev with mask consideration, and subsequent normalization/denormalization) can be directly adapted for SegRNN's preprocessing pipeline.\n\n2. **Embedding Layer Architecture**: Both approaches utilize temporal embedding mechanisms to encode raw time series into higher-dimensional feature spaces before processing. TimesNet uses `DataEmbedding` with positional and temporal encodings. **Code Reuse**: The embedding infrastructure (`DataEmbedding` class) can serve as a template, though SegRNN may require simpler embeddings given its RNN-based architecture versus TimesNet's transformer-style processing.\n\n3. **Task-Agnostic Framework Design**: Both models are designed as general time series analysis frameworks adaptable to multiple tasks including imputation. They share the pattern of: input embedding → feature extraction → projection to output space. **Code Reuse**: The overall model structure template (embedding → backbone → projection) and the `forward()` method signature with `x_enc, x_mark_enc, x_dec, x_mark_dec, mask` parameters can be maintained for consistency.\n\n4. **Mask-Based Training and Inference**: Both handle missing values through explicit mask tensors that distinguish observed from missing values during training and evaluation. **Code Reuse**: The mask handling logic in TimesNet's imputation method (masking operations, mask-aware statistics computation) provides a foundation for SegRNN's mask processing.",
    "differences": "1. **Core Architecture Philosophy - NEW IMPLEMENTATION REQUIRED**: TimesNet transforms 1D time series into 2D temporal variations using FFT-based period detection and processes them with Inception blocks (2D CNNs). SegRNN uses a fundamentally different approach based on segmentation and RNN processing. **New Components**: (a) Segment-based decomposition module to split time series into fixed-length segments, (b) RNN backbone (likely LSTM/GRU) to process segments sequentially, (c) Segment aggregation mechanism to reconstruct full-length outputs. The entire `TimesBlock` with FFT period detection, 2D reshaping, and Inception convolutions needs to be replaced.\n\n2. **Temporal Pattern Extraction Method - NEW IMPLEMENTATION REQUIRED**: TimesNet explicitly discovers multi-periodicity through FFT analysis (`FFT_for_Period` function) and adaptively aggregates representations from different periods using learned weights. SegRNN relies on RNN's inherent sequential processing and segment-level patterns. **New Components**: (a) Remove FFT-based period detection entirely, (b) Implement segment-wise RNN processing where each segment is treated as a sequence element, (c) Design segment-level feature extraction that captures local temporal patterns within segments rather than global periodic patterns.\n\n3. **Feature Processing Paradigm - NEW IMPLEMENTATION REQUIRED**: TimesNet uses parameter-shared 2D convolutional blocks (Inception) across multiple period-based views with adaptive fusion. SegRNN processes segments through RNN cells with hidden state propagation. **New Components**: (a) Replace `nn.ModuleList([TimesBlock...])` with RNN layers (e.g., `nn.LSTM` or `nn.GRU`), (b) Implement segment encoding/decoding mechanisms, (c) Design hidden state initialization and propagation strategies across segments, (d) Remove the adaptive aggregation mechanism (softmax-weighted fusion) used in TimesNet.\n\n4. **Imputation-Specific Design - NEW IMPLEMENTATION REQUIRED**: TimesNet processes the entire sequence through stacked TimesBlocks with residual connections, treating imputation as sequence-to-sequence reconstruction. SegRNN likely employs segment-wise imputation strategies. **New Components**: (a) Segment-level mask handling where missing values within segments need special treatment, (b) Potential bidirectional RNN processing for better context utilization in imputation, (c) Segment boundary handling mechanisms to ensure smooth transitions, (d) Modified projection layer that operates on segment-level or point-level outputs from RNN.\n\n5. **Model Complexity and Efficiency Trade-offs - NEW IMPLEMENTATION REQUIRED**: TimesNet's multi-period processing with k parallel branches and 2D convolutions is computationally intensive but captures complex patterns. SegRNN emphasizes simplicity and efficiency through lightweight RNN processing. **New Components**: (a) Simplified layer structure with fewer parameters, (b) Efficient segment processing without parallel branches, (c) Streamlined projection mechanisms, (d) Potentially different layer normalization strategies optimized for RNN outputs rather than convolutional features."
  },
  {
    "source": "DLinear_2022",
    "target": "SegRNN_2023",
    "type": "in-domain",
    "similarities": "1. **Direct Mapping Strategy (DMS) for Time Series Processing**: Both papers adopt a direct multi-step forecasting/imputation approach rather than autoregressive methods. DLinear's encoder directly maps input sequences to output sequences through linear transformations, which can be reused as a baseline comparison framework. The `encoder()` method structure in DLinear provides a template for processing entire sequences at once.\n\n2. **Temporal Linear Transformation Foundation**: Both utilize linear transformations along the temporal axis as core operations. DLinear's `nn.Linear(self.seq_len, self.pred_len)` demonstrates weight-sharing across variates, which aligns with efficient temporal modeling. The initialization strategy `(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])` can serve as a baseline for temporal weight initialization in SegRNN.\n\n3. **Channel-wise Processing with Optional Individual Treatment**: DLinear's `individual` parameter controlling whether to use separate models per variate (via `nn.ModuleList()`) provides a reusable pattern. This channel-wise processing logic with configurable sharing can be adapted for SegRNN's potential multi-variate handling, reusing the loop structure `for i in range(self.channels)`.\n\n4. **Series Decomposition as Preprocessing**: DLinear employs `series_decomp()` to separate trend and seasonal components before processing. This decomposition framework can be directly reused in SegRNN for handling complex temporal patterns, as the moving average kernel approach is domain-agnostic.\n\n5. **Unified Task Interface**: Both papers implement a unified forward interface `forward(x_enc, x_mark_enc, x_dec, x_mark_dec, mask)` supporting multiple tasks. DLinear's task routing through `self.task_name` and dedicated methods like `imputation()` provides a reusable architecture pattern for implementing SegRNN's imputation task.",
    "differences": "1. **Core Architecture Innovation - RNN vs Linear Layers**: DLinear uses simple linear layers (`nn.Linear`) for temporal mapping, while SegRNN (based on naming convention) likely employs recurrent neural network architectures with segment-based processing. NEW IMPLEMENTATION NEEDED: RNN cells (LSTM/GRU), hidden state management, and segment-wise sequential processing logic that captures temporal dependencies beyond linear transformations.\n\n2. **Segment-based Temporal Modeling**: DLinear processes the entire sequence uniformly, whereas SegRNN likely divides sequences into segments for hierarchical temporal modeling. NEW IMPLEMENTATION NEEDED: Segment partitioning logic, segment-level encoding mechanisms, inter-segment dependency modeling, and potentially segment embedding layers to capture local-global temporal hierarchies.\n\n3. **Missing Value Mask Integration**: DLinear's implementation doesn't explicitly handle masks in the encoder (mask parameter exists but unused in provided code). SegRNN for imputation requires explicit mask handling. NEW IMPLEMENTATION NEEDED: Mask-aware forward propagation in RNN cells, masked loss computation focusing only on missing positions, and potentially mask-conditioned attention mechanisms or gating structures.\n\n4. **Temporal Context Preservation Mechanism**: DLinear relies on decomposition and linear weighting for temporal patterns. SegRNN likely uses RNN's inherent sequential memory and segment-level context aggregation. NEW IMPLEMENTATION NEEDED: Bidirectional RNN processing for capturing both past and future context, segment boundary handling mechanisms, and potentially hierarchical attention across segments.\n\n5. **Loss Function Design for Imputation**: DLinear's code doesn't show imputation-specific loss (only returns reconstructed sequence). SegRNN requires specialized imputation loss. NEW IMPLEMENTATION NEEDED: Masked MSE/MAE loss that computes reconstruction error only on originally missing positions, potentially with auxiliary losses for temporal consistency across segments, and validation metrics specifically for imputation quality (separate metrics for observed vs. imputed values).\n\n6. **Complexity and Parameter Efficiency Trade-off**: DLinear emphasizes extreme simplicity with O(L) complexity through single linear layers. SegRNN with RNN architecture introduces O(L×H²) complexity (H=hidden size) and sequential dependencies. NEW IMPLEMENTATION NEEDED: Efficient RNN implementations (potentially using cuDNN-optimized cells), gradient clipping for training stability, and careful initialization strategies for RNN weights to prevent vanishing/exploding gradients in long sequences.\n\n7. **Segment-level Feature Extraction**: DLinear treats all timesteps uniformly after decomposition. SegRNN likely extracts segment-level representations. NEW IMPLEMENTATION NEEDED: Segment pooling operations (max/mean pooling over segments), learnable segment embeddings, segment-wise normalization schemes, and mechanisms to aggregate segment representations for final imputation output."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers leverage series decomposition to separate trend and seasonal components. Autoformer uses `series_decomp` with moving average kernel (Equation 1), and DLinear explicitly adopts this same decomposition scheme. The `series_decomp` and `moving_avg` classes from Autoformer can be directly reused for DLinear's decomposition preprocessing.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both models employ DMS forecasting rather than autoregressive approaches. Autoformer's decoder accumulates trend components progressively while outputting full sequences, and DLinear directly predicts the entire future horizon. The projection layers and output strategies in Autoformer's implementation provide guidance for DLinear's output design.\n\n3. **Embedding Infrastructure**: Both use similar data embedding strategies including temporal embeddings and value embeddings. DLinear can leverage Autoformer's `DataEmbedding_wo_pos`, `TokenEmbedding`, `TemporalEmbedding`, and `TimeFeatureEmbedding` classes for input preprocessing, though DLinear ultimately uses simpler linear projections.\n\n4. **Training Pipeline and Data Handling**: Both handle multivariate time series with similar input formats (x_enc, x_mark_enc for temporal features). Autoformer's data loading and preprocessing pipeline can be adapted for DLinear with minimal modifications, particularly the normalization and batching strategies.\n\n5. **Trend-Seasonal Aggregation**: Both aggregate decomposed components for final predictions. Autoformer sums seasonal (from decoder) and trend (accumulated) components, while DLinear sums outputs from two separate linear layers applied to each component. The summation strategy is identical.",
    "differences": "1. **Core Architecture - Transformer vs Linear**: Autoformer uses a full Transformer architecture with encoder-decoder structure, Auto-Correlation mechanism (Equations 6-7), multi-head attention layers, and deep neural blocks. DLinear replaces ALL of this with simple one-layer linear transformations (W ∈ ℝ^(T×L)). NEW IMPLEMENTATION: Single linear layer per component without any attention mechanism, no encoder-decoder structure, no Auto-Correlation computation.\n\n2. **Temporal Dependency Modeling**: Autoformer discovers period-based dependencies through Auto-Correlation using FFT (Equation 8) and time delay aggregation (Figure 2), with O(L log L) complexity. DLinear uses direct weighted sum across the temporal axis with shared weights across variates, assuming linear temporal relationships. NEW IMPLEMENTATION: Simple matrix multiplication along temporal dimension without any frequency domain analysis or correlation computation.\n\n3. **Input Processing Complexity**: Autoformer uses complex embedding with positional encoding, temporal convolutions, and multi-scale feature extraction through stacked encoder layers (Equation 3). DLinear either: (a) directly applies decomposition then linear layers (DLinear variant), or (b) uses simple last-value normalization (NLinear variant). NEW IMPLEMENTATION: Minimal preprocessing - just decomposition or normalization, no deep embedding layers.\n\n4. **Model Depth and Capacity**: Autoformer has N encoder layers and M decoder layers with series decomposition blocks at each layer, feedforward networks (d_ff = 4×d_model), and residual connections throughout. DLinear is strictly one-layer per component with no intermediate transformations. NEW IMPLEMENTATION: Single-layer architecture with parameters only in final projection matrices, no stacked layers or residual connections.\n\n5. **Decoder Design**: Autoformer's decoder progressively refines predictions through inner Auto-Correlation, encoder-decoder Auto-Correlation, and accumulates trend components across layers (Equation 4). DLinear has no decoder - it directly outputs predictions from linear transformations. NEW IMPLEMENTATION: Remove entire decoder structure; replace with direct linear projection from input length L to output length T.\n\n6. **Normalization Strategy**: Autoformer uses custom `my_Layernorm` for seasonal components and standard layer normalization. DLinear introduces NLinear variant with input normalization by subtracting last value and adding it back post-prediction - a simple distribution shift handling mechanism. NEW IMPLEMENTATION: Last-value subtraction/addition normalization scheme not present in Autoformer.\n\n7. **Computational Complexity**: Autoformer has O(L log L) complexity due to FFT-based Auto-Correlation and multiple layer processing. DLinear has O(L×T) complexity from single matrix multiplication, making it orders of magnitude faster. NEW IMPLEMENTATION: Drastically simplified forward pass with single linear operation.\n\n8. **Feature Interaction**: Autoformer models cross-variate dependencies through multi-head attention and shared embedding spaces. DLinear explicitly does NOT model spatial correlations - it processes each variate independently with shared temporal weights. NEW IMPLEMENTATION: Independent per-variate processing with no cross-channel attention or interaction.\n\n9. **Philosophical Approach**: Autoformer assumes semantic correlations between time points and uses permutation-invariant attention with positional encoding. DLinear challenges this assumption, arguing that temporal order (not point-wise semantics) is crucial, making simple linear transformations more appropriate for time series. NEW IMPLEMENTATION: Weight-sharing linear model that directly encodes temporal ordering through matrix structure rather than attention mechanisms."
  },
  {
    "source": "Informer_2020",
    "target": "DLinear_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Pyraformer_2021",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Forecasting Task Framework**: Both papers address long-term time series forecasting (LTSF) problems with the same input-output formulation: predicting future M steps given previous L steps of observations. The source paper's basic data loading pipeline, evaluation metrics (MSE, MAE), and dataset preprocessing can be directly reused. The `DataEmbedding` class and data normalization utilities from Pyraformer's implementation provide a solid foundation for DLinear's data preprocessing.\n\n2. **Decomposition-Based Architecture Component**: Both papers utilize time series decomposition strategies. Pyraformer implicitly uses multi-resolution decomposition through its pyramidal structure (coarse-to-fine scales), while DLinear explicitly employs seasonal-trend decomposition with moving average kernels. The `Bottleneck_Construct` module in Pyraformer that performs hierarchical convolutions can be adapted - specifically, the moving average operation used in DLinear's decomposition can leverage similar convolution kernel concepts from Pyraformer's `ConvLayer` class. The decomposition logic (separating trend and seasonal components) can be extracted and simplified from Pyraformer's multi-scale construction.\n\n3. **Direct Multi-Step (DMS) Forecasting Strategy**: Both papers employ DMS forecasting to avoid error accumulation from autoregressive approaches. Pyraformer's prediction module that 'maps the last nodes at all scales to all M future time steps in a batch' demonstrates the same philosophy as DLinear's direct prediction. The implementation pattern from Pyraformer's `projection` layer (a simple linear transformation that outputs all future steps simultaneously) can be directly adapted for DLinear's linear layers.\n\n4. **Embedding and Normalization Infrastructure**: Pyraformer's `DataEmbedding` class handles temporal embeddings and normalization, which provides reusable components for DLinear. While DLinear uses simpler normalization (NLinear's last-value subtraction), the normalization framework and batch processing logic from Pyraformer can be adapted. The `nn.LayerNorm` and batch normalization patterns used throughout Pyraformer's code provide tested normalization utilities.\n\n5. **Training Loop and Evaluation Framework**: Both papers use similar training paradigms with encoder-decoder structures (though DLinear's is dramatically simplified). Pyraformer's training infrastructure including loss computation, optimizer setup, learning rate scheduling, and evaluation loops can be directly reused. The forward pass pattern `forward(x_enc, x_mark_enc, x_dec, x_mark_dec, mask)` from Pyraformer provides the exact signature needed for DLinear, requiring only simplified internal logic.",
    "differences": "1. **Core Architecture Philosophy - Complexity vs Simplicity**: Pyraformer uses a complex pyramidal attention mechanism with O(L) complexity through multi-scale C-ary trees, custom CUDA kernels, and sophisticated inter-scale/intra-scale connections. DLinear completely abandons this complexity for a single temporal linear layer with weights W ∈ R^(T×L). IMPLEMENTATION REQUIREMENT: Remove all attention mechanisms (EncoderLayer, AttentionLayer, FullAttention), pyramidal graph construction (get_mask, refer_points), and CSCM modules. Implement a simple `nn.Linear(seq_len, pred_len)` layer as the core forecasting module.\n\n2. **Decomposition Strategy - Implicit Multi-Scale vs Explicit Trend-Seasonal**: Pyraformer performs implicit decomposition through hierarchical convolutions with multiple scales (window_size=[4,4]), creating a pyramidal representation. DLinear uses explicit moving average decomposition to separate trend and seasonal components, then applies separate linear layers to each. IMPLEMENTATION REQUIREMENT: Replace Pyraformer's `Bottleneck_Construct` with a simple moving average kernel (e.g., `nn.AvgPool1d`) for trend extraction, compute seasonal as residual (seasonal = input - trend), and implement two parallel linear layers for trend and seasonal components that are summed for final prediction.\n\n3. **Temporal Modeling Mechanism - Attention-Based vs Weight-Based**: Pyraformer relies on query-key-value attention mechanisms with learnable attention weights computed through softmax over dot products. DLinear uses fixed linear weights learned directly on the temporal dimension without any attention computation. IMPLEMENTATION REQUIREMENT: Eliminate all Q-K-V transformations, attention score calculations, and softmax operations. Replace with direct temporal linear transformation: `self.Linear = nn.Linear(lookback_window, forecast_horizon)` applied along the time dimension.\n\n4. **Positional and Temporal Encoding - Rich Embeddings vs None**: Pyraformer uses sophisticated temporal embeddings including positional encodings, covariate embeddings (hour-of-day, day-of-week), and channel projections through the `DataEmbedding` class. DLinear deliberately avoids any explicit positional encoding, relying solely on the implicit temporal ordering captured by the linear layer weights. IMPLEMENTATION REQUIREMENT: Remove all embedding layers (`enc_embedding`, positional encodings, temporal embeddings). For NLinear variant, implement simple last-value normalization: `normalized = input - input[:, -1:, :]` before the linear layer and add it back after: `output = linear(normalized) + input[:, -1:, :]`.\n\n5. **Model Capacity and Parameter Efficiency**: Pyraformer has substantial parameters from multi-head attention (O(N(HDD_K+DD_F))), convolution layers, and multi-scale construction. DLinear has minimal parameters - only the weights of 1-2 linear layers (O(L×T) for vanilla, O(2L×T) for DLinear with decomposition). IMPLEMENTATION REQUIREMENT: Implement three variants: (a) Vanilla Linear: single `nn.Linear(L, T)`, (b) DLinear: moving average decomposition + two `nn.Linear(L, T)` layers + summation, (c) NLinear: last-value normalization + `nn.Linear(L, T)` + denormalization. Remove all feed-forward networks (`PositionwiseFeedForward`), multi-head mechanisms, and hierarchical structures.\n\n6. **Handling Distribution Shifts - Pyramidal Scaling vs Normalization Tricks**: Pyraformer handles varying data distributions through multi-scale representations and layer normalization at each scale. DLinear's NLinear variant specifically addresses distribution shifts through a simple subtraction-addition normalization scheme that removes the last value's influence. IMPLEMENTATION REQUIREMENT: For NLinear, implement the normalization as: `last_value = x_enc[:, -1:, :].detach()`, `x_normalized = x_enc - last_value`, apply linear layer, then `output = linear_output + last_value`. This requires NO layer normalization or batch normalization from Pyraformer.\n\n7. **Inference Complexity and Speed - Graph Traversal vs Matrix Multiplication**: Pyraformer requires complex graph traversal through pyramidal structures, custom CUDA kernels for sparse attention, and multiple attention layers with O(AL) complexity. DLinear performs a single matrix multiplication with O(L×T) complexity. IMPLEMENTATION REQUIREMENT: Remove all custom CUDA operations, mask generation (`get_mask`), index gathering (`refer_points`), and multi-layer processing. Implement straightforward forward pass: `output = self.linear(input.permute(0,2,1)).permute(0,2,1)` to apply temporal linear transformation across the time dimension."
  },
  {
    "source": "FEDformer_2022",
    "target": "DLinear_2022",
    "type": "in-domain",
    "similarities": "",
    "differences": ""
  },
  {
    "source": "DLinear_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition Framework**: Both papers utilize series decomposition as a core preprocessing strategy. DLinear employs moving average kernels to extract trend-cyclical and seasonal components (from Autoformer), while Crossformer's hierarchical encoder also benefits from multi-scale decomposition. The `series_decomp` module from DLinear's implementation can be directly reused as a preprocessing layer in Crossformer's encoder input pipeline.\n\n2. **Direct Multi-Step (DMS) Forecasting Strategy**: Both adopt non-autoregressive DMS forecasting to avoid error accumulation. DLinear predicts all future steps simultaneously through linear projections, and Crossformer's decoder generates predictions at multiple scales simultaneously. The implementation pattern of outputting complete sequences (shape `[B, pred_len, D]`) without iterative decoding can be shared.\n\n3. **Channel-Independent Processing Philosophy**: DLinear processes each dimension independently (when `individual=True`) through separate linear layers, avoiding cross-dimension complexity. Crossformer's Cross-Time Stage also applies MSA independently to each dimension (`for d in 1...D`). The modular design pattern of dimension-wise processing loops from DLinear can guide Crossformer's Cross-Time Stage implementation.\n\n4. **Linear Projection for Final Output**: Both use learnable linear transformations to project latent representations to prediction space. DLinear uses `nn.Linear(seq_len, pred_len)` for temporal mapping, while Crossformer uses `W^l ∈ R^(L_seg × d_model)` to project segment vectors. The simple projection layer architecture and initialization strategy can be adapted.\n\n5. **Normalization and Residual Connections**: Both leverage LayerNorm for training stability. While DLinear applies it implicitly through decomposition, Crossformer explicitly uses it in TSA layers. The normalization placement pattern (pre-norm or post-norm) from DLinear's stable training can inform Crossformer's implementation.\n\n6. **Batch-First Tensor Format**: Both implementations use `[Batch, Length, Dimension]` tensor ordering. DLinear's permutation operations (`x.permute(0,2,1)`) for switching between temporal and channel dimensions can be reused in Crossformer's dimension-segment embedding and TSA layer operations.",
    "differences": "1. **Core Architecture Paradigm**: DLinear is a pure linear model without attention mechanisms, using only `nn.Linear` layers for temporal mapping. Crossformer requires implementing complex Transformer components including Multi-Head Self-Attention (MSA), Two-Stage Attention (TSA) layers with separate cross-time and cross-dimension stages, and a router mechanism. NEW: Complete attention infrastructure including query-key-value projections, scaled dot-product attention, and multi-head concatenation must be built from scratch.\n\n2. **Embedding Strategy - Dimension-Segment-Wise (DSW)**: DLinear operates on full sequences without segmentation, directly mapping `seq_len → pred_len`. Crossformer introduces DSW embedding that divides each dimension into segments of length `L_seg`, creating a 2D array `[T/L_seg, D]` of segment vectors. NEW: Implement segment partitioning logic, learnable projection matrix `E ∈ R^(d_model × L_seg)`, and 2D positional embeddings `E^(pos)_(i,d)` for each segment position.\n\n3. **Two-Stage Attention (TSA) Layer**: DLinear has no attention mechanism. Crossformer's TSA requires: (a) Cross-Time Stage with dimension-wise MSA sharing weights across D dimensions, (b) Cross-Dimension Stage with router mechanism using `c` learnable router vectors to aggregate and distribute information with O(DL) complexity instead of O(D²L). NEW: Implement the complete TSA pipeline with two sequential MSA operations, router vectors `R ∈ R^(L×c×d_model)`, and the aggregation-distribution pattern (`MSA₁^dim` and `MSA₂^dim`).\n\n4. **Hierarchical Encoder-Decoder Architecture**: DLinear uses a flat single-scale architecture with direct input-output mapping. Crossformer employs a hierarchical structure with N encoder layers performing segment merging (concatenating and projecting adjacent segments via `M ∈ R^(d_model × 2d_model)`), and N+1 decoder layers operating at different temporal scales. NEW: Implement segment merging operations, multi-scale feature extraction, cross-attention between encoder-decoder at each scale, and hierarchical prediction aggregation (summing predictions from all scales).\n\n5. **Cross-Dimension Dependency Modeling**: DLinear explicitly avoids modeling spatial/cross-dimension correlations (processes channels independently). Crossformer's core innovation is capturing cross-dimension dependency through the router mechanism in Cross-Dimension Stage, building all-to-all dimension connections. NEW: Implement the router-based dimension communication pattern where routers first query all dimensions (`MSA₁^dim(R, Z^time, Z^time)`), then dimensions query routers (`MSA₂^dim(Z^time, B, B)`).\n\n6. **Multi-Scale Prediction Fusion**: DLinear produces a single prediction from one temporal scale. Crossformer generates predictions at N+1 scales (from each decoder layer) and sums them: `x^pred = Σ(l=0 to N) x^(pred,l)`. NEW: Implement per-layer prediction heads with linear projections `W^l`, segment rearrangement logic to convert 2D arrays back to sequences, and the multi-scale fusion mechanism.\n\n7. **Complexity and Computational Pattern**: DLinear has O(1) complexity per channel with simple matrix multiplication. Crossformer has O(DL² + DL) per TSA layer and O(D·T²/L_seg²) per encoder layer. NEW: Implement efficient attention computation with proper masking, batch matrix multiplication for parallel dimension processing, and memory-efficient segment merging to handle the quadratic temporal complexity.\n\n8. **Positional Information Encoding**: DLinear relies solely on the implicit ordering in linear weights. Crossformer uses explicit learnable 2D positional embeddings for both encoder (`E^(pos)_(i,d)` for each segment position) and decoder (`E^(dec)` for prediction positions). NEW: Initialize and manage learnable positional embedding matrices that capture both temporal and dimensional position information."
  },
  {
    "source": "FEDformer_2022",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Hierarchical Encoder-Decoder Architecture**: Both papers employ multi-layer encoder-decoder frameworks with progressive information aggregation. FEDformer's encoder structure (Eq. 1-2) with seasonal-trend decomposition at each layer shares conceptual similarity with Crossformer's hierarchical encoder (Eq. 6) that captures multi-scale dependencies. **Code Reuse**: The basic encoder-decoder loop structure, layer normalization, skip connections, and MLP blocks from FEDformer's `Encoder` and `Decoder` classes can be adapted for Crossformer's hierarchical structure.\n\n2. **Embedding Strategy with Position Information**: Both use learnable embeddings with positional encoding. FEDformer's `DataEmbedding` combines value embedding with temporal features, while Crossformer's DSW embedding (Eq. 2) uses linear projection plus learnable position embeddings. **Code Reuse**: The embedding initialization pattern, linear projection layers (`nn.Linear`), and position embedding registration (`register_buffer` or `nn.Parameter`) from FEDformer can be directly adapted.\n\n3. **Attention Mechanism Foundation**: Both build upon multi-head attention with query-key-value paradigm. FEDformer's FEA (Eq. 5-7) and Crossformer's TSA both use `MSA(Q,K,V)` formulation with LayerNorm and residual connections. **Code Reuse**: The basic attention computation flow, including `torch.einsum` operations for attention scores, softmax activation, LayerNorm application, and residual connection patterns from FEDformer's `FourierCrossAttention` can serve as templates.\n\n4. **Frequency Domain Processing Capability**: FEDformer extensively uses FFT/DFT operations (Eq. 3-4) for frequency enhancement. While Crossformer focuses on cross-dimension dependency, understanding FEDformer's frequency selection mechanism (`get_frequency_modes`, mode selection) provides insights for potential frequency-aware extensions. **Code Reuse**: The FFT computation patterns (`torch.fft.rfft`, `torch.fft.irfft`) and complex number handling utilities from FEDformer could be useful if Crossformer needs frequency analysis.\n\n5. **Multi-Scale Information Processing**: FEDformer's wavelet decomposition (Eq. 8-9) recursively processes signals at different scales, similar to Crossformer's segment merging across encoder layers. Both aggregate coarse-to-fine information. **Code Reuse**: The recursive decomposition logic structure and scale-wise processing loops from FEDformer's `MWT_CZ1d` can inform Crossformer's segment merging implementation.\n\n6. **Training Infrastructure**: Both require similar training utilities including MSE/MAE loss computation, optimizer setup, learning rate scheduling, and batch processing. **Code Reuse**: FEDformer's training loop, loss calculation, metric computation, data loading pipeline, and configuration management can be directly reused with minimal modifications.",
    "differences": "1. **Core Innovation - Cross-Dimension Dependency vs Frequency Enhancement**: FEDformer focuses on frequency domain processing using Fourier/Wavelet transforms to capture temporal patterns efficiently (O(L) complexity). Crossformer introduces Dimension-Segment-Wise (DSW) embedding (Eq. 1-2) to explicitly model cross-dimension dependencies that existing methods ignore. **New Implementation**: Need to implement DSW embedding that segments each dimension separately, creating a 2D array structure [L×D×d_model] instead of FEDformer's 1D sequence. This requires new segment partitioning logic and dimension-aware position embeddings.\n\n2. **Attention Architecture - Two-Stage vs Frequency-Enhanced**: FEDformer replaces standard attention with frequency-enhanced blocks (FEB-f/FEB-w, FEA-f/FEA-w) operating in Fourier/Wavelet space. Crossformer proposes Two-Stage Attention (TSA) with sequential Cross-Time Stage (Eq. 3) and Cross-Dimension Stage (Eq. 4). **New Implementation**: Must implement the TSA layer with two separate attention stages: (1) Cross-Time: dimension-wise MSA applied to Z[:,d] for all d; (2) Cross-Dimension: router mechanism with learnable vectors R to aggregate/distribute information across dimensions, reducing complexity from O(D²L) to O(DL).\n\n3. **Router Mechanism for Scalability**: FEDformer uses mode selection (random/low) to reduce frequency components. Crossformer introduces a novel router mechanism (Eq. 4) with fixed number c<<D of learnable vectors to enable all-to-all dimension connections efficiently. **New Implementation**: Need to implement: (a) Learnable router vectors R ∈ R^(L×c×d_model); (b) Two-step MSA: MSA₁(R, Z, Z) for aggregation into B, then MSA₂(Z, B, B) for distribution; (c) Router sharing across time steps while maintaining separate parameters.\n\n4. **Segment Merging vs Decomposition**: FEDformer uses seasonal-trend decomposition (MOEDecomp with mixture of experts, Eq. 10) to extract trend components. Crossformer uses segment merging (Eq. 6) where adjacent segments are concatenated and projected: Ẑ_{i,d}^{enc,l} = M[Z_{2i-1,d} · Z_{2i,d}]. **New Implementation**: Need to implement concatenation-based merging with learnable matrix M ∈ R^(d_model×2d_model), handling padding for odd-length sequences, and maintaining 2D array structure throughout hierarchy.\n\n5. **Decoder Architecture - Multi-Scale Prediction vs Single Output**: FEDformer's decoder outputs single prediction by accumulating trend components (Eq. 2: T_de^l = T_de^(l-1) + weighted trends). Crossformer's decoder (Eq. 7-8) produces predictions at each hierarchical level l and sums them: x_pred = Σ_l x^(pred,l). **New Implementation**: Must implement: (a) Learnable decoder position embeddings E^(dec); (b) Layer-specific linear projections W^l ∈ R^(L_seg×d_model); (c) Multi-scale prediction aggregation where each decoder layer contributes to final output; (d) Encoder-decoder cross-attention at each scale level.\n\n6. **Input Representation - Segment-Based vs Point-Based**: FEDformer processes point-wise embeddings with length I×D. Crossformer fundamentally changes input representation to segment-wise: each vector represents a segment x_{i,d}^(s) ∈ R^L_seg rather than single time step. **New Implementation**: Need to implement segment extraction logic that divides T time steps into T/L_seg segments per dimension, ensuring T and τ are divisible by L_seg, and handling segment-to-sequence conversion in final projection.\n\n7. **Complexity Optimization Strategy**: FEDformer achieves O(L) complexity through frequency mode selection (M=64 modes). Crossformer achieves O(DL²) through router mechanism reducing cross-dimension complexity from O(D²L) to O(DL). **New Implementation**: The router mechanism requires careful implementation of: (a) Fixed router count c as hyperparameter; (b) Dimension-wise attention sharing across time; (c) Efficient batching for two-stage attention to maintain computational benefits."
  },
  {
    "source": "Pyraformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Hierarchical Multi-Scale Architecture**: Both papers employ hierarchical structures to capture temporal patterns at different scales. Pyraformer uses a pyramidal C-ary tree with coarser-scale construction module (CSCM) to build multi-resolution representations, while Crossformer uses segment merging in its hierarchical encoder-decoder. **Code Reuse**: The CSCM's ConvLayer and Bottleneck_Construct modules can be adapted for Crossformer's segment merging operation - both downsample temporal sequences through convolution with stride. The bottleneck structure (down projection → conv → up projection) in Pyraformer's CSCM is directly applicable to Crossformer's merging mechanism.\n\n2. **Embedding Strategy with Position Encoding**: Both models use learnable embeddings that combine data projection with positional information. Pyraformer's DataEmbedding adds observation, covariate, and positional embeddings, while Crossformer's DSW embedding uses linear projection plus position embedding (Eq. 2). **Code Reuse**: The embedding layer structure from Pyraformer (Linear projection + position embedding + normalization) can serve as a template for implementing Crossformer's DSW embedding, only requiring modification to handle 2D segment arrays instead of 1D sequences.\n\n3. **Attention Mechanism Foundation**: Both use multi-head self-attention as the core building block, with modifications to standard Transformer attention. Pyraformer implements pyramidal attention (PAM) and Crossformer implements two-stage attention (TSA). **Code Reuse**: The basic AttentionLayer and FullAttention modules from Pyraformer (including Q-K-V computation, softmax normalization, and multi-head mechanism) can be directly reused as the foundation for Crossformer's MSA operations in both cross-time and cross-dimension stages.\n\n4. **Layer Normalization and Residual Connections**: Both architectures extensively use LayerNorm with residual connections in their encoder layers. Pyraformer's EncoderLayer applies attention followed by position-wise FFN with residuals, matching Crossformer's pattern in Eq. 3-4. **Code Reuse**: The PositionwiseFeedForward module with GELU activation, dropout, and layer normalization can be directly transplanted to implement Crossformer's MLP blocks.\n\n5. **Forecasting Task Framework**: Both papers are primarily designed for time series forecasting but can be adapted for imputation. They use encoder-decoder architectures with prediction modules. **Code Reuse**: The overall model structure (embedding → encoder → decoder → projection) and the training loop infrastructure can be adapted, though Crossformer requires modifications for its specific 2D array processing and cross-dimension dependency modeling.",
    "differences": "1. **Core Innovation - Dimension-Segment-Wise (DSW) Embedding vs Pyramidal Attention**: Pyraformer focuses on reducing computational complexity through pyramidal graph structure with O(L) complexity, treating time series as 1D sequences. Crossformer introduces DSW embedding (Eq. 1-2) that explicitly segments each dimension separately and arranges them as 2D arrays (L_seg × D), fundamentally changing how multivariate dependencies are modeled. **New Implementation Required**: (a) 2D vector array data structure to hold segments from all dimensions; (b) Segment-wise embedding logic that partitions each dimension independently; (c) 2D position encoding for (time_segment_index, dimension_index) pairs; (d) Reshaping operations to convert between 1D time series and 2D segment arrays.\n\n2. **Attention Pattern - Pyramidal Graph vs Two-Stage Attention (TSA)**: Pyraformer uses sparse attention on a pyramidal graph with inter-scale (parent-child) and intra-scale (neighboring nodes) connections, requiring custom CUDA kernels. Crossformer proposes TSA with distinct cross-time stage (Eq. 3) and cross-dimension stage (Eq. 4) that explicitly separate temporal and dimensional dependency modeling. **New Implementation Required**: (a) Cross-time stage: dimension-wise self-attention where each dimension attends to its own temporal segments independently; (b) Cross-dimension stage with router mechanism: learnable router vectors (R) that aggregate information from all dimensions via MSA_1 and distribute via MSA_2, reducing complexity from O(D²) to O(D); (c) Sequential execution of two stages rather than Pyraformer's single pyramidal attention pass; (d) Router parameter initialization and management.\n\n3. **Hierarchical Structure Construction**: Pyraformer builds a C-ary tree bottom-up using fixed window_size convolutions (e.g., [4,4]), creating parent nodes that summarize C children. Crossformer uses segment merging (Eq. 6) where every two adjacent segments in time are concatenated and linearly projected through matrix M, maintaining the dimension axis intact. **New Implementation Required**: (a) Pairwise segment concatenation logic along time axis only; (b) Learnable merging matrix M ∈ R^(d_model × 2d_model) instead of convolution kernels; (c) Padding mechanism when sequence length is not divisible by 2; (d) Preservation of dimension structure throughout hierarchy (Pyraformer flattens to 1D).\n\n4. **Decoder Architecture and Cross-Attention**: Pyraformer's decoder uses full attention layers where prediction tokens attend to all encoder outputs, or direct projection from last nodes. Crossformer's decoder (Eq. 7) has a more sophisticated design: (a) TSA layer on decoder embeddings first; (b) dimension-wise cross-attention where each dimension's decoder query attends only to the same dimension's encoder output (Z^dec_:,d queries Z^enc_:,d); (c) layer-wise predictions from all N+1 decoder layers summed together (Eq. 8). **New Implementation Required**: (a) Dimension-wise cross-attention implementation that processes each dimension separately; (b) Multi-scale prediction aggregation mechanism that collects predictions from all decoder layers; (c) Learnable decoder position embeddings E^(dec) for future time steps; (d) Linear projection matrices W^l for each decoder layer to convert vectors back to time series segments.\n\n5. **Complexity and Scalability Focus**: Pyraformer emphasizes O(L) time/space complexity for long sequences through sparse pyramidal connections, with custom CUDA kernels for efficiency. Crossformer addresses high-dimensional MTS (large D) through the router mechanism, achieving O(DL²) complexity where the router constant c << D prevents quadratic growth in dimensions. **New Implementation Required**: (a) Router mechanism with fixed small number of learnable vectors per time step; (b) Two-stage aggregation-distribution pattern in cross-dimension attention; (c) Handling of potentially very high-dimensional inputs (e.g., D=321 in Electricity dataset); (d) Efficient 2D array operations without custom CUDA kernels (using standard PyTorch operations).\n\n6. **Imputation-Specific Adaptations**: For imputation tasks, Pyraformer's code shows a simple projection layer approach. Crossformer would require: **New Implementation Required**: (a) Mask-aware DSW embedding that handles missing segments; (b) Modified TSA layers that can process partially observed 2D arrays; (c) Dimension-wise imputation strategy leveraging cross-dimension dependency explicitly; (d) Reconstruction loss calculated per segment rather than per time point; (e) Handling of irregular missing patterns across both time and dimension axes in the 2D structure."
  },
  {
    "source": "Autoformer_2021",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon Transformer architecture for time series tasks, using encoder-decoder structures with multi-head attention mechanisms. For imputation tasks, Crossformer can reuse Autoformer's basic embedding layers (TokenEmbedding, PositionalEmbedding, TemporalEmbedding) and LayerNorm components from the implementation.\n\n2. **Series Decomposition Philosophy**: While Autoformer explicitly uses series decomposition blocks to separate trend and seasonal components, Crossformer implicitly handles multi-scale patterns through hierarchical segment merging. For imputation, both approaches recognize that time series contain patterns at different scales. Crossformer can adapt Autoformer's moving_avg and series_decomp modules as preprocessing steps to stabilize the imputation process.\n\n3. **Feedforward Network Structure**: Both use similar MLP/feedforward networks after attention layers with residual connections and layer normalization. The conv1/conv2 structure in Autoformer's EncoderLayer can be directly reused in Crossformer's TSA layers with minor modifications to handle 2D array inputs.\n\n4. **Training Infrastructure**: Both require similar data loading, masking strategies, and loss computation for imputation tasks. Autoformer's imputation method (projecting encoder output directly) provides a baseline approach that Crossformer can extend with its hierarchical predictions.\n\n5. **Embedding Strategy**: Both use learned embeddings combined with positional information. Crossformer's DSW embedding can leverage Autoformer's TokenEmbedding (Conv1d-based) by applying it segment-wise rather than point-wise, requiring only dimensional reshaping rather than complete reimplementation.",
    "differences": "1. **Core Attention Mechanism - CRITICAL NEW IMPLEMENTATION**: Autoformer uses Auto-Correlation with FFT-based period discovery and time-delay aggregation (O(L log L) complexity), while Crossformer introduces Two-Stage Attention (TSA) with separate Cross-Time and Cross-Dimension stages. For imputation, Crossformer needs:\n   - NEW: Cross-Time Stage with dimension-wise MSA (Eq. 3)\n   - NEW: Cross-Dimension Stage with router mechanism (Eq. 4) to handle large D\n   - NEW: Router learnable vectors (R) and two-stage aggregation (MSA₁ᵈⁱᵐ, MSA₂ᵈⁱᵐ)\n   - Cannot reuse: Autoformer's AutoCorrelation, time_delay_agg functions, FFT-based correlation computation\n\n2. **Input Representation - FUNDAMENTAL DIFFERENCE**: Autoformer embeds point-wise (all dimensions at time t → vector hₜ), while Crossformer uses Dimension-Segment-Wise (DSW) embedding where each vector represents a segment of single dimension (Eq. 1-2). For imputation:\n   - NEW: Segment-based embedding creating 2D array H[L×D×d_model] instead of 1D sequence\n   - NEW: Handling of L_seg parameter for segment length\n   - NEW: 2D positional embedding E^(pos)_{i,d} for (time, dimension) positions\n   - Implication: Mask handling must operate on segments rather than individual points\n\n3. **Hierarchical Multi-Scale Architecture - NEW DECODER DESIGN**: Autoformer uses progressive decomposition with trend accumulation in decoder, while Crossformer employs hierarchical encoder-decoder with segment merging and multi-scale prediction fusion. For imputation:\n   - NEW: Segment merging operation (Eq. 6) with learnable matrix M to merge adjacent segments\n   - NEW: Multi-scale decoder layers (Eq. 7) where each layer predicts at different scales\n   - NEW: Layer-wise predictions (Eq. 8) with summation: x^pred = Σ x^(pred,l)\n   - NEW: Encoder-decoder cross-attention at each hierarchical level\n   - Cannot reuse: Autoformer's trend accumulation structure (T_de^l = T_de^(l-1) + ...)\n\n4. **Missing Value Reconstruction Strategy**: For imputation tasks:\n   - Autoformer: Direct projection from encoder output (single-scale reconstruction)\n   - Crossformer needs: Multi-scale reconstruction where each hierarchical level contributes to final imputation, requiring:\n     - NEW: Scale-specific projection matrices W^l for each decoder layer\n     - NEW: Aggregation of predictions across scales with proper upsampling\n     - NEW: Handling of segment-wise masks during hierarchical processing\n\n5. **Computational Complexity and Scalability**: \n   - Autoformer: O(L log L) per layer due to FFT-based Auto-Correlation\n   - Crossformer: O(DL²) for TSA layer with router mechanism reducing cross-dimension from O(D²L) to O(DL)\n   - For imputation with large D (many variables):\n     - NEW: Router mechanism implementation critical for scalability\n     - NEW: Careful handling of dimension-wise computations to avoid O(D²) explosion\n     - Autoformer's efficiency tricks (time_delay_agg_training vs inference) not applicable\n\n6. **Mask Application and Propagation**:\n   - Autoformer: Masks applied at point level, propagated through Auto-Correlation\n   - Crossformer requires:\n     - NEW: Segment-level mask representation (if any point in segment is masked)\n     - NEW: Mask handling in Cross-Time Stage (within dimensions)\n     - NEW: Mask handling in Cross-Dimension Stage (across dimensions via routers)\n     - NEW: Hierarchical mask propagation through segment merging operations"
  },
  {
    "source": "Informer_2020",
    "target": "Crossformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the encoder-decoder Transformer architecture for time series tasks. Crossformer can reuse Informer's basic encoder-decoder structure (Encoder/Decoder classes), LayerNorm, residual connections, and MLP feedforward networks. The DataEmbedding component from Informer (position encoding + value embedding) can serve as a baseline for Crossformer's embedding design.\n\n2. **Multi-Head Attention Mechanism**: Both utilize multi-head self-attention as the core building block. Crossformer can leverage Informer's AttentionLayer implementation for query/key/value projections and multi-head splitting logic. The basic attention computation pattern (scaled dot-product) and output projection mechanisms are directly transferable.\n\n3. **Hierarchical Feature Processing**: Both employ hierarchical structures to capture multi-scale temporal patterns. Informer uses ConvLayer with MaxPooling for distilling (Eq. 5), while Crossformer uses segment merging (Eq. 6). The concept of progressively reducing temporal resolution and the implementation pattern of stacking layers with decreasing sequence length can be adapted from Informer's distilling mechanism.\n\n4. **Training Infrastructure**: Both papers use MSE loss for training and share similar training paradigms. Crossformer can directly reuse Informer's loss computation, optimizer setup, data loading pipeline, and evaluation metrics (MSE/MAE). The imputation task implementation in Informer (mask-based reconstruction) provides a foundation for handling missing values.\n\n5. **Embedding and Normalization**: Both apply learnable embeddings and layer normalization. Informer's DataEmbedding (combining temporal encoding and value projection) and LayerNorm usage patterns can be adapted. The position encoding strategy and dropout mechanisms are transferable components.",
    "differences": "1. **Core Attention Innovation - Two-Stage Attention (TSA) Layer**: Crossformer introduces a fundamentally different attention mechanism that separates cross-time and cross-dimension dependencies (Eq. 3-4). **NEW IMPLEMENTATION REQUIRED**: (a) Cross-Time Stage: Apply MSA independently to each dimension (D separate attention operations), (b) Cross-Dimension Stage with Router Mechanism: Implement learnable routers (R ∈ R^(L×c×d_model)) that aggregate information from all dimensions via two sequential MSA operations (MSA₁^dim and MSA₂^dim), reducing complexity from O(D²L) to O(DL). This is completely different from Informer's ProbSparse attention which focuses on query sparsity.\n\n2. **Dimension-Segment-Wise (DSW) Embedding**: Crossformer embeds segments of single dimensions (Eq. 1-2) rather than Informer's time-step-wise embedding. **NEW IMPLEMENTATION REQUIRED**: (a) Segment the input into blocks of length L_seg for each dimension separately, creating a 2D array H ∈ R^(T/L_seg × D × d_model), (b) Apply separate linear projections (E ∈ R^(d_model × L_seg)) to each segment, (c) Add 2D position embeddings E^(pos)_(i,d) indexed by both time and dimension. This fundamentally changes how input data is organized compared to Informer's 1D sequence embedding.\n\n3. **Segment Merging vs. Distilling**: Crossformer uses explicit segment merging (Eq. 6) with learnable matrices M ∈ R^(d_model × 2d_model) to concatenate and project adjacent time segments. **NEW IMPLEMENTATION REQUIRED**: Implement the merging operation that concatenates Z^(enc,l)_(2i-1,d) and Z^(enc,l)_(2i,d) followed by linear projection, maintaining the 2D structure. This differs from Informer's Conv1d-based distilling which applies 1D convolution + ELU + MaxPooling to reduce sequence length.\n\n4. **2D Array Processing Throughout**: Crossformer maintains a 2D vector array structure (time × dimension) throughout the entire network, requiring all operations to handle this structure. **NEW IMPLEMENTATION REQUIRED**: (a) Modify all layer operations to process 2D arrays instead of 1D sequences, (b) Implement dimension-wise operations where each dimension is processed separately in cross-time stage, (c) Handle time-wise operations in cross-dimension stage. Informer processes 1D sequences, so extensive architectural changes are needed.\n\n5. **Hierarchical Decoder with Multi-Scale Fusion**: Crossformer's decoder uses N+1 layers that make predictions at different scales and sum them (Eq. 7-8). **NEW IMPLEMENTATION REQUIRED**: (a) Implement learnable position embeddings E^(dec) for decoder initialization, (b) Create layer-specific projection matrices W^l ∈ R^(L_seg × d_model) for each decoder layer, (c) Implement the multi-scale prediction fusion mechanism that sums predictions from all decoder layers (∑_(l=0)^N x^(pred,l)). Informer's decoder uses a single-scale generative inference with start tokens, which is conceptually different.\n\n6. **Router Mechanism for Cross-Dimension Attention**: The router mechanism (Eq. 4) is a unique component with fixed number of learnable routers (c << D). **NEW IMPLEMENTATION REQUIRED**: (a) Initialize learnable router array R ∈ R^(L×c×d_model), (b) Implement two-stage MSA: first using routers as queries to aggregate from all dimensions (MSA₁^dim), then using dimensions as queries to receive from routers (MSA₂^dim), (c) Ensure all time steps share the same MSA parameters. This aggregation-distribution pattern has no equivalent in Informer.\n\n7. **Complexity Optimization Strategy**: Crossformer optimizes for large D (many dimensions) with O(DL²) complexity, while Informer optimizes for long L (sequence length) with O(L log L) ProbSparse attention. **NEW IMPLEMENTATION REQUIRED**: The entire complexity reduction strategy differs - Crossformer needs router-based dimension reduction while Informer uses query sparsity measurement (Eq. 2) and Top-u query selection. The implementation philosophies are orthogonal."
  },
  {
    "source": "Autoformer_2021",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Philosophy**: Both papers adopt explicit decomposition architectures to handle time series components, though with different decomposition strategies. Autoformer uses moving average-based series decomposition to extract trend-cyclical and seasonal parts (series_decomp module), while ETSformer decomposes into level, growth, and seasonal components inspired by exponential smoothing. For imputation reproduction, the series_decomp block from Autoformer can serve as a foundational template for implementing ETSformer's component extraction logic, particularly the residual learning pattern where components are progressively extracted and subtracted.\n\n2. **Encoder-Decoder Architecture with Progressive Refinement**: Both models employ multi-layer encoder-decoder architectures that progressively refine representations. Autoformer's encoder focuses on seasonal modeling through Auto-Correlation layers with series decomposition blocks, while the decoder accumulates trend components. ETSformer similarly uses cascaded encoder layers to extract growth/seasonal representations from residuals. The overall structural pattern in Autoformer's implementation (Encoder/Decoder classes with layer stacking, residual connections, and layer normalization) can be directly adapted for ETSformer's encoder-decoder framework, requiring mainly modifications to the attention mechanisms and component extraction logic.\n\n3. **Frequency Domain Operations**: Both papers leverage FFT for efficient computation. Autoformer uses FFT in the Auto-Correlation mechanism (Equation 8) to compute series autocorrelation via the Wiener-Khinchin theorem, achieving O(L log L) complexity. ETSformer's Frequency Attention also employs DFT to identify dominant seasonal patterns by selecting top-K frequency components. The FFT infrastructure in Autoformer's AutoCorrelation class (torch.fft.rfft, torch.fft.irfft) can be reused for implementing ETSformer's Frequency Attention module, though the post-FFT processing differs (autocorrelation vs. amplitude-based selection).\n\n4. **Embedding Strategy**: Both models use similar input embedding approaches. Autoformer employs DataEmbedding_wo_pos with TokenEmbedding (Conv1d-based), PositionalEmbedding, and optional TemporalEmbedding. ETSformer uses a simpler Conv1d-based embedding without positional encodings or temporal features. The TokenEmbedding implementation from Autoformer (kernel_size=3, Conv1d with circular padding) can be directly reused for ETSformer's input embedding module, simplifying the reproduction effort.\n\n5. **Multi-Head Attention Framework**: While the attention mechanisms differ significantly in design, both implement multi-head versions. Autoformer's AutoCorrelationLayer structure (with query/key/value projections, multi-head splitting, and output projection) provides a reusable template for implementing ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA). The projection layers and head management logic can be adapted, replacing only the core attention computation logic.\n\n6. **Layer Normalization and Feedforward Networks**: Both architectures use layer normalization after attention blocks and employ position-wise feedforward networks. Autoformer's EncoderLayer/DecoderLayer implementations with LayerNorm and Conv1d-based feedforward (activation + dropout pattern) can serve as direct templates for ETSformer's encoder layers, requiring only modifications to the attention module calls and component extraction logic.",
    "differences": "1. **Core Attention Mechanism Innovation**: **NEW IMPLEMENTATION REQUIRED** - The fundamental attention mechanisms are entirely different. Autoformer introduces Auto-Correlation that discovers period-based dependencies through autocorrelation and performs time delay aggregation by rolling series based on detected periods (Equation 6). ETSformer proposes two novel mechanisms: (a) Exponential Smoothing Attention (ESA) that assigns weights based on relative time lag following exponential decay (not input content), implemented via efficient cross-correlation with O(L log L) complexity using Algorithm 1; (b) Frequency Attention (FA) that selects top-K frequency components by amplitude and reconstructs seasonal patterns (Equation 4). The entire attention computation logic needs to be implemented from scratch, including the efficient ESA algorithm using FFT-based cross-correlation and the FA module's DFT-based frequency selection and inverse transform.\n\n2. **Component Extraction and Accumulation Strategy**: **NEW IMPLEMENTATION REQUIRED** - Autoformer extracts trend-cyclical components through series decomposition blocks (moving average-based) and accumulates them in the decoder via weighted projections (Equation 4: T_de^l = T_de^(l-1) + W_{l,1}*T_de^{l,1} + ...). ETSformer implements a fundamentally different approach: (a) Level Module uses learnable exponential smoothing with parameter α to compute adjusted level as weighted average of de-seasonalized current level and previous level-growth forecast; (b) Growth extraction via MH-ESA operates on successive differences of residuals; (c) Growth Damping (TD) in decoder uses learnable damping parameter γ to forecast multi-step ahead trend with exponential decay. These components require new implementations including the recurrent exponential smoothing equation for level extraction and the multi-head damping mechanism.\n\n3. **Decoder Design and Forecast Composition**: **NEW IMPLEMENTATION REQUIRED** - Autoformer's decoder performs cross-attention with encoder outputs and progressively refines seasonal components while accumulating trend, with final prediction as sum of projected seasonal and accumulated trend. ETSformer's decoder has a distinct architecture: (a) N Growth+Seasonal (G+S) Stacks that use Growth Damping and Frequency Attention to generate B_{t:t+H}^(n) and S_{t:t+H}^(n); (b) Level Stack that simply repeats the last level E_t^(N) across forecast horizon; (c) Final forecast composition (Equation 3) as E_{t:t+H} + Linear(Σ(B_{t:t+H}^(n) + S_{t:t+H}^(n))). The entire decoder structure needs redesign, particularly the G+S stacks and the composition mechanism.\n\n4. **Temporal Context and Inductive Bias**: **CONCEPTUAL DIFFERENCE** - Autoformer's Auto-Correlation explicitly discovers periodic patterns through autocorrelation peaks and aggregates similar sub-series from underlying periods, creating series-wise connections rather than point-wise. ETSformer's ESA mechanism embeds an exponential decay inductive bias directly into attention weights (non-adaptive, based purely on time lag), while FA extracts seasonality through frequency domain analysis. For imputation tasks, this means Autoformer may better handle periodic missing patterns by leveraging detected periods, while ETSformer's ESA naturally emphasizes recent observations for local imputation. Implementation-wise, ETSformer requires careful handling of the non-adaptive nature of ESA (weights computed without input content) and the frequency-based seasonal extraction.\n\n5. **Initialization and Placeholder Strategy**: **IMPLEMENTATION DETAIL DIFFERENCE** - Autoformer initializes decoder inputs with decomposed components from the latter half of encoder input plus placeholders (X_0 for seasonal, X_Mean for trend, Equation 2). ETSformer does not use explicit placeholders; instead, it initializes with Conv-based embedding and relies on the Level Stack to repeat the last level, Growth Damping to extrapolate from last growth, and Frequency Attention to extrapolate seasonal patterns beyond lookback window. For imputation, this affects how missing segments are initialized: Autoformer's approach may be more suitable for masked imputation with explicit placeholders, while ETSformer's extrapolation-based approach needs adaptation for scattered missing values.\n\n6. **Mask Handling for Imputation Task**: **CRITICAL NEW IMPLEMENTATION** - Neither paper's original method section explicitly addresses imputation with missing value masks. Autoformer's implementation shows a generic imputation method that applies enc_embedding and encoder, then projects to output. For ETSformer imputation reproduction, several new components are needed: (a) Mask-aware component extraction - how to handle masked values in level/growth/seasonal extraction; (b) Modified ESA mechanism to skip or interpolate over masked positions while maintaining exponential decay property; (c) Frequency Attention adaptation for incomplete signals (potentially using interpolation before DFT or masking frequency components); (d) Loss function design to compute reconstruction error only on originally masked positions. These mask-handling strategies require careful design as they are not present in the forecasting-focused original paper.\n\n7. **Learnable Parameters and Optimization**: **NEW PARAMETER DESIGN** - Autoformer's learnable parameters include projection weights in Auto-Correlation, decomposition block kernels, and layer-specific trend projections W_{l,i}. ETSformer introduces distinct learnable parameters: (a) Exponential smoothing parameter α ∈ R^m for level module; (b) Initial state v_0 for ESA mechanism; (c) Damping parameter γ (multi-head version) for growth damping; (d) Standard projection weights for Linear layers. For imputation reproduction, these parameters need careful initialization and may require different learning rates or regularization strategies, especially the α and γ parameters which control smoothing/damping behavior critical for handling missing data patterns."
  },
  {
    "source": "Informer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the encoder-decoder paradigm for time series forecasting, which can be adapted for imputation tasks. The basic structure of stacking multiple layers with residual connections is shared. For imputation, ETSformer can reuse Informer's encoder structure pattern (multiple layers with normalization and feedforward networks) as a starting point, though the attention mechanisms will differ.\n\n2. **Efficient O(L log L) Complexity**: Both models achieve O(L log L) computational complexity through different mechanisms - Informer via ProbSparse attention with sampling, ETSformer via FFT-based operations in Frequency Attention and efficient exponential smoothing. For imputation implementation, this shared complexity goal means both can handle long sequences efficiently. The FFT operations in ETSformer's FA module can potentially leverage similar optimization strategies as Informer's efficient attention computation.\n\n3. **Embedding and Input Representation**: Both use embedding layers to map raw inputs to latent space (Informer uses DataEmbedding with temporal encodings, ETSformer uses Conv1d). For imputation, the input embedding module from Informer (DataEmbedding class) can be partially reused, though ETSformer's simpler Conv-based embedding may need to be adapted to handle masked inputs by incorporating mask information in the embedding stage.\n\n4. **Layer Normalization and Feedforward Networks**: Both architectures employ LayerNorm and position-wise feedforward networks (FF) as standard components. For imputation reproduction, these components from Informer's implementation can be directly reused in ETSformer, as the FF module structure (Linear-Activation-Linear) is identical.\n\n5. **Multi-Head Mechanism**: Both leverage multi-head designs (Informer's multi-head ProbAttention, ETSformer's MH-ESA with multiple smoothing parameters). The multi-head projection pattern (query/key/value projections in Informer, multiple heads with different parameters in ETSformer) follows similar design principles. The multi-head wrapper structure from Informer can guide ETSformer's implementation.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: \n   - **Informer**: Uses ProbSparse self-attention with query sparsity measurement M(q,K) and Top-u query selection based on KL divergence approximation. Adaptive, content-based attention weights computed via softmax(QK^T/√d).\n   - **ETSformer**: Introduces two novel non-adaptive attention mechanisms: (a) Exponential Smoothing Attention (ESA) with fixed exponential decay weights α(1-α)^j based on time lag only, not input content; (b) Frequency Attention (FA) using DFT to select top-K frequency components. \n   - **Implementation Impact**: The entire attention computation logic needs to be rewritten. Cannot reuse Informer's ProbAttention class. Must implement: (1) Algorithm 1 for efficient O(L log L) ESA via FFT-based cross-correlation; (2) DFT-based frequency selection in FA (Equation 4); (3) Learnable smoothing parameter α and initial state v₀ for ESA.\n\n2. **Component Decomposition Philosophy - NEW ARCHITECTURE REQUIRED**:\n   - **Informer**: Does not explicitly decompose time series into interpretable components. Treats forecasting as a sequence-to-sequence mapping problem with attention on the entire latent representation.\n   - **ETSformer**: Explicitly decomposes into Level (E), Growth/Trend (B), and Seasonal (S) components inspired by exponential smoothing theory. Each encoder layer extracts B and S from residual Z, with level computed via exponential smoothing equation E_t = α*(E_t - S_t) + (1-α)*(E_{t-1} + B_{t-1}).\n   - **Implementation Impact**: For imputation, must implement: (1) Separate extraction modules for growth (MH-ESA on successive differences) and seasonality (FA on residuals); (2) Level module with exponential smoothing recurrence; (3) Residual update logic (Z := Z - S, Z := Z - B) between component extractions; (4) Final reconstruction as composition E + Linear(Σ(B+S)). This requires a fundamentally different forward pass structure than Informer.\n\n3. **Decoder Design for Missing Value Reconstruction - NEW LOGIC NEEDED**:\n   - **Informer**: Uses masked multi-head attention with start tokens and placeholder zeros for autoregressive generation. Decoder attends to encoder output via cross-attention.\n   - **ETSformer**: Decoder has Growth Damping (TD) with learnable damping parameter γ for trend extrapolation (Σγ^i*B_t), and FA for seasonal pattern extrapolation. No cross-attention between encoder-decoder.\n   - **Implementation Impact**: For imputation, ETSformer needs: (1) Growth Damping module with multi-head damping parameters to project growth representations to missing positions; (2) FA module to extrapolate seasonal patterns from observed to missing timestamps; (3) Level repetition strategy (Repeat_H) for missing positions; (4) Mask-aware composition logic to reconstruct only missing values while preserving observed ones. Cannot reuse Informer's DecoderLayer with cross-attention.\n\n4. **Temporal Modeling Strategy - DIFFERENT INDUCTIVE BIAS**:\n   - **Informer**: Uses self-attention distilling with Conv1d (kernel=3) and MaxPooling for hierarchical feature extraction. Attention weights are data-dependent and adaptive.\n   - **ETSformer**: Uses fixed exponential decay for recent bias (non-adaptive ESA), frequency domain analysis for periodicity (FA with Top-K selection), and explicit damping for trend extrapolation. No distilling or pooling operations.\n   - **Implementation Impact**: For imputation: (1) Remove Informer's distilling layers (ConvLayer with downsampling); (2) Implement frequency domain operations (FFT, amplitude/phase extraction, Top-K selection, inverse FFT); (3) Add trend damping with cumulative sum Σγ^i; (4) Design mask propagation through frequency domain (how to handle missing values in DFT).\n\n5. **Loss Function and Training Objective - ADAPTATION NEEDED**:\n   - **Informer**: MSE loss on forecast vs. target sequence. Single-step gradient computation.\n   - **ETSformer**: MSE loss on composed forecast (E + Linear(Σ(B+S))). For imputation, needs mask-weighted MSE only on missing positions.\n   - **Implementation Impact**: Must implement: (1) Mask-aware loss computation (loss only on missing values); (2) Separate loss terms for level, growth, and seasonal components if using auxiliary losses; (3) Gradient flow through component composition and frequency domain operations; (4) Potential regularization on smoothing parameters (α, γ) to ensure stability (0 < α, γ < 1).\n\n6. **No Positional/Temporal Encodings in ETSformer - SIMPLIFICATION**:\n   - **Informer**: Uses explicit temporal encodings (time features like month, day-of-week) in DataEmbedding.\n   - **ETSformer**: Does not use manually designed temporal covariates; FA automatically discovers seasonal patterns.\n   - **Implementation Impact**: For imputation, can remove Informer's time feature engineering (x_mark_enc, x_mark_dec). But need to ensure FA can handle irregularly spaced missing values - may need to adapt frequency selection to account for non-uniform sampling when many values are missing."
  },
  {
    "source": "Reformer_2020",
    "target": "ETSformer_2022",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanisms for Long Sequences**: Both papers address the computational bottleneck of standard Transformer attention for long sequences. Reformer introduces LSH attention with O(L log L) complexity, while ETSformer's Exponential Smoothing Attention (ESA) and Frequency Attention (FA) both achieve O(L log L) complexity through FFT-based implementations. The source paper's efficient attention computation framework (memory-efficient attention, chunking strategies) can be directly adapted as the computational backbone.\n\n2. **Multi-Head Attention Architecture**: Both employ multi-head attention mechanisms. Reformer's multi-head LSH attention structure (with separate Q, K, V projections and head concatenation) provides a reusable template for ETSformer's Multi-Head Exponential Smoothing Attention (MH-ESA). The source implementation's head splitting, parallel computation, and output projection logic can be directly leveraged.\n\n3. **Encoder-Based Feature Extraction**: Both use stacked encoder layers for hierarchical feature learning. Reformer's ReformerLayer with attention + feedforward blocks parallels ETSformer's encoder structure that extracts growth/seasonal components layer-by-layer. The source paper's layer stacking, residual connections, and layer normalization infrastructure can be reused for ETSformer's cascaded decomposition architecture.\n\n4. **FFT-Based Efficient Computation**: ETSformer's Frequency Attention relies on DFT/inverse DFT operations, which aligns with Reformer's use of FFT for efficient LSH bucketing and cross-correlation in the efficient A_ES algorithm. The source implementation's FFT utilities and batched frequency domain operations can be adapted for ETSformer's seasonal pattern extraction.\n\n5. **Embedding and Projection Layers**: Both papers use linear projections to map between different representation spaces. Reformer's DataEmbedding and output projection layers provide a foundation for ETSformer's input Conv embedding and multiple Linear projection layers (for level, growth, seasonal components).",
    "differences": "1. **Core Attention Mechanism - Content-Based vs. Time-Based**: **NEW IMPLEMENTATION REQUIRED**: Reformer's LSH attention is content-adaptive, computing attention weights through query-key similarity via locality-sensitive hashing. ETSformer's ESA is fundamentally different - it's a non-adaptive, time-lag-based attention that assigns weights purely based on temporal distance (α(1-α)^j pattern), independent of input content. This requires implementing: (a) exponential decay weight computation with learnable α parameter, (b) efficient recurrent formulation via cross-correlation/FFT (Algorithm 1 in ETSformer), (c) initial state v_0 handling. The LSH bucketing, sorting, and hash-based neighbor selection logic from Reformer is NOT applicable.\n\n2. **Frequency Domain Seasonal Extraction**: **NEW IMPLEMENTATION REQUIRED**: ETSformer's Frequency Attention is entirely novel - it performs DFT on input sequences, selects top-K frequency components by amplitude, and reconstructs seasonal patterns via inverse DFT (Equation 4). This requires: (a) batched DFT/IDFT operations along temporal dimension, (b) amplitude/phase extraction, (c) Top-K frequency selection per dimension, (d) seasonal pattern extrapolation to forecast horizon. Reformer has no equivalent frequency domain analysis component.\n\n3. **Explicit Time-Series Decomposition Architecture**: **NEW IMPLEMENTATION REQUIRED**: ETSformer's encoder-decoder explicitly decomposes time series into level (E), growth (B), and seasonal (S) components at each layer, with component-specific modules (Level Module, Growth Damping, FA). Each encoder layer performs: residual → seasonal extraction → growth extraction → residual update. This requires: (a) Level Module implementing exponential smoothing equation with learnable α, (b) Growth Damping (TD) with learnable damping parameter γ, (c) component-wise residual learning, (d) final additive composition (Equation 3). Reformer's architecture has no such decomposition - it's a standard encoder with uniform attention layers.\n\n4. **Forecasting-Specific Decoder Design**: **NEW IMPLEMENTATION REQUIRED**: ETSformer's decoder generates H-step forecasts through: (a) Level Stack: repeating last level E_t^(N) across forecast horizon, (b) G+S Stacks: Growth Damping extrapolating growth with exponential decay (∑γ^i B_t), Frequency Attention extrapolating seasonal patterns, (c) Linear projection combining all components. Reformer has no decoder (encoder-only for representation learning) and no forecasting mechanism - imputation uses direct encoder output projection.\n\n5. **Reversible Layers and Memory Optimization**: Reformer implements reversible residual connections (RevNet) to eliminate activation storage across layers, achieving memory complexity independent of layer count. It also uses chunking for feed-forward layers. ETSformer does NOT use reversible layers - it follows standard residual connections with layer normalization. If implementing ETSformer for very long sequences, Reformer's memory optimization techniques would need to be carefully adapted to preserve ETSformer's component decomposition semantics.\n\n6. **Input Representation and Covariates**: Reformer's implementation uses DataEmbedding with positional encodings and temporal features. ETSformer explicitly avoids manual time-dependent covariates (no month-of-year, day-of-week), relying solely on raw signals with a simple Conv embedding (kernel size 3). This design philosophy difference means ETSformer's embedding layer is simpler but the FA module must automatically discover seasonality.\n\n7. **Loss Function and Training Objective**: **NEW IMPLEMENTATION REQUIRED**: For imputation tasks, Reformer uses standard reconstruction loss on masked positions. ETSformer's forecasting loss is MSE on the H-step ahead predictions composed from level, growth, and seasonal forecasts. For adapting ETSformer to imputation, a new training strategy is needed: either (a) treating missing positions as a forecasting problem from observed context, or (b) modifying the decoder to reconstruct missing values within the lookback window using the decomposed components."
  },
  {
    "source": "Autoformer_2021",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Decomposition Architecture Foundation**: Both papers adopt a deep decomposition architecture that separates time series into seasonal and trend components. FEDformer directly builds upon Autoformer's decomposition framework. **Code Reuse**: The basic encoder-decoder structure from Autoformer (Encoder/Decoder classes with decomposition blocks) can be directly reused. The series_decomp module and the progressive trend accumulation mechanism in the decoder are fundamental components that FEDformer inherits.\n\n2. **Series Decomposition Block**: Both use moving average-based decomposition to extract trend and seasonal components. **Code Reuse**: The series_decomp class with moving_avg pooling operation can be directly reused or minimally modified. The decomposition logic X_trend = AvgPool(Padding(X)) and X_seasonal = X - X_trend is identical in both papers.\n\n3. **Encoder-Decoder Framework with Decomposition**: Both employ a multi-layer encoder-decoder architecture where: (a) Encoder focuses on seasonal pattern modeling, (b) Decoder accumulates trend progressively while refining seasonal components, (c) Each layer performs decomposition after attention/correlation mechanisms. **Code Reuse**: The overall encoder-decoder loop structure, layer-wise decomposition after each sub-block (Equations 3-4 in Autoformer, Equations 1-2 in FEDformer), and the trend accumulation formula in decoder can be directly adapted.\n\n4. **Input Initialization Strategy**: Both use the same decoder input initialization: combining the latter half of encoder input (I/2) with placeholders (length O). **Code Reuse**: The input preparation logic in Equation 2 of Autoformer can be directly reused for FEDformer.\n\n5. **Embedding Layer**: Both use DataEmbedding_wo_pos for input embedding without positional encoding, relying on temporal embeddings. **Code Reuse**: The entire embedding module (TokenEmbedding, TemporalEmbedding, TimeFeatureEmbedding) can be directly reused.\n\n6. **Feed-Forward Network and Residual Connections**: Both use standard FFN with residual connections after attention mechanisms. **Code Reuse**: The Conv1d-based FFN implementation in EncoderLayer/DecoderLayer can be reused.\n\n7. **Normalization Strategy**: Both apply layer normalization (my_Layernorm for seasonal parts). **Code Reuse**: The my_Layernorm class can be directly reused.\n\n8. **Final Prediction Strategy**: Both combine refined seasonal and trend components: W_S * X_de^M + T_de^M. **Code Reuse**: The final projection and combination logic can be directly adapted.",
    "differences": "1. **Core Attention Mechanism Replacement**: Autoformer uses Auto-Correlation mechanism based on FFT autocorrelation and time-delay aggregation (Equations 5-7), while FEDformer replaces this with Frequency Enhanced Blocks (FEB) and Frequency Enhanced Attention (FEA). **New Implementation Required**: (a) FEB-f: Fourier-based block with mode selection, parameterized kernel R in frequency domain (Equations 3-4), (b) FEB-w: Wavelet-based block with recursive decomposition using Legendre wavelets (Equation 8-9), (c) FEA-f: Frequency domain cross-attention with mode selection (Equations 6-7), (d) FEA-w: Wavelet-based cross-attention with recursive structure. The entire AutoCorrelation and AutoCorrelationLayer classes need to be replaced.\n\n2. **Frequency Domain Processing**: Autoformer operates primarily in time domain with FFT only for autocorrelation computation, while FEDformer performs attention operations directly in frequency domain. **New Implementation Required**: (a) Mode selection mechanism (Select operator in Equation 3), (b) Frequency domain multiplication with learnable kernel (Q ⊙ R in Equation 4), (c) Padding and inverse transform back to time domain, (d) For wavelet version: multi-scale decomposition/reconstruction with fixed filter matrices (H^(0), H^(1), G^(0), G^(1)).\n\n3. **Decomposition Block Enhancement**: Autoformer uses fixed kernel size moving average (series_decomp), while FEDformer introduces Mixture of Experts Decomposition (MOEDecomp) with multiple average filters and data-dependent weights. **New Implementation Required**: (a) Multiple average pooling filters with different kernel sizes, (b) Learnable weight prediction network L(x), (c) Softmax-based mixing of multiple trend extractions (Equation 10). Replace all series_decomp calls with MOEDecomp.\n\n4. **Complexity and Mode Selection**: Autoformer has O(L log L) complexity with Top-k autocorrelation selection, while FEDformer achieves O(L) complexity with fixed random mode selection. **New Implementation Required**: (a) Random mode selection strategy (pre-selected M=64 modes), (b) Efficient frequency domain operations avoiding full FFT, (c) For wavelet: fixed recursive depth L=3 instead of adaptive selection.\n\n5. **Multi-Scale Processing**: Autoformer processes at single scale, while FEDformer-w uses recursive multi-scale wavelet decomposition. **New Implementation Required**: (a) Recursive decomposition ladder (L cycles with 1/2 decimation), (b) Separate processing modules for high-frequency (Ud), low-frequency (Us), and remaining signals, (c) Reconstruction stage combining multi-scale outputs, (d) Shared FEB-f modules across decomposition cycles.\n\n6. **Cross-Attention Design**: Autoformer uses Auto-Correlation for encoder-decoder attention, while FEDformer uses FEA (frequency domain cross-attention). **New Implementation Required**: (a) Separate Fourier/Wavelet transforms for queries (from decoder), keys and values (from encoder), (b) Frequency domain attention computation: σ(Q̃·K̃^T)·Ṽ in Equation 7, (c) Activation function selection (softmax or tanh), (d) For wavelet version: recursive cross-attention with separate q, k, v decomposition.\n\n7. **Parameterization Strategy**: Autoformer learns time-delay weights through autocorrelation, while FEDformer learns frequency domain kernels. **New Implementation Required**: (a) Complex-valued learnable kernel R ∈ C^(D×D×M) for FEB-f, (b) Multiple FEB-f modules (A_n, B_n, C_n) for wavelet processing, (c) Perceptron F̄ for coarsest scale processing in FEDformer-w.\n\n8. **Activation Functions in Attention**: Autoformer uses Softmax for autocorrelation normalization, while FEDformer uses configurable activation (softmax or tanh) in FEA. **New Implementation Required**: Add activation function selection logic in FEA implementation."
  },
  {
    "source": "Informer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both models adopt the Transformer encoder-decoder paradigm for time series processing. The Informer's implementation provides the basic encoder-decoder structure (Encoder/Decoder classes with multi-layer stacking) that can be directly reused. FEDformer maintains this architecture but replaces attention mechanisms with frequency-enhanced blocks. **Code Reuse**: The overall model skeleton, layer stacking logic, and forward pass structure from Informer can serve as the template.\n\n2. **Embedding Layer and Input Representation**: Both use DataEmbedding for converting raw time series into high-dimensional representations with positional encoding. Informer's embedding implementation (enc_embedding, dec_embedding) handles temporal features and value embeddings identically to FEDformer's requirements. **Code Reuse**: The entire DataEmbedding module from Informer can be directly imported and used without modification.\n\n3. **Decoder Input Strategy with Start Token**: Both employ a generative decoder approach using start tokens. Informer concatenates historical context (X_token) with zero-padded target placeholders (X_0), while FEDformer uses a similar (I/2 + O) input structure. This avoids autoregressive step-by-step generation. **Code Reuse**: The decoder input preparation logic and masking strategy can be adapted from Informer's implementation.\n\n4. **Multi-Layer Processing with Residual Connections**: Both use residual connections after each sub-block (attention/FEB + feedforward) followed by normalization. Informer's layer structure with Add&Norm operations matches FEDformer's design pattern. **Code Reuse**: The residual connection and layer normalization logic from EncoderLayer/DecoderLayer can be preserved.\n\n5. **Feed-Forward Networks**: Both employ position-wise feed-forward networks (Conv1d-based in Informer, standard FFN in FEDformer) after attention/frequency blocks. The FFN structure with activation and dropout is identical. **Code Reuse**: The FeedForward implementation from Informer can be directly used or minimally adapted.\n\n6. **Projection Layer for Final Output**: Both use linear projection layers to map decoder outputs to target dimensions. Informer's self.projection matches FEDformer's W_S projection. **Code Reuse**: The final projection layer implementation is directly transferable.\n\n7. **Training Objective**: Both use MSE loss for forecasting tasks, optimizing the difference between predictions and ground truth. The loss computation logic is identical. **Code Reuse**: Training loop and loss calculation from Informer can be directly applied.",
    "differences": "1. **Core Attention Mechanism → Frequency Enhanced Blocks (FEB)**: Informer uses ProbSparse self-attention with query sparsity measurement and Top-u selection in the time domain. FEDformer completely replaces this with Frequency Enhanced Blocks that operate in frequency domain using DFT/DWT. **New Implementation Needed**: \n   - FEB-f: Implement Fourier transform-based blocks with random mode selection (Select operator), parameterized kernel R for frequency domain multiplication, and padding/inverse transform.\n   - FEB-w: Implement wavelet decomposition with recursive processing, including fixed Legendre basis matrices (H, G, Σ), multi-scale coefficient processing (s_l^n, d_l^n), and separate FEB-f modules for high/low frequency components.\n   - Replace all ProbAttention and AttentionLayer instances with FEB modules.\n\n2. **Cross-Attention → Frequency Enhanced Attention (FEA)**: Informer uses standard cross-attention between encoder and decoder outputs. FEDformer introduces FEA-f/FEA-w that performs attention in frequency domain. **New Implementation Needed**:\n   - FEA-f: Transform Q, K, V to frequency domain, select M modes, compute attention as σ(Q̃·K̃ᵀ)·Ṽ in frequency space, then inverse transform.\n   - FEA-w: Extend FEA-f with wavelet decomposition, processing Q, K, V through recursive decomposition/reconstruction with separate FEA-f modules.\n   - Implement activation function selection (softmax vs tanh) for different datasets.\n\n3. **Encoder Output Processing**: Informer uses self-attention distilling with Conv1d and MaxPooling for progressive dimension reduction across layers. FEDformer maintains full sequence length through all encoder layers without distilling. **New Implementation Needed**: Remove ConvLayer and distilling operations; ensure encoder outputs maintain input length for FEA cross-attention.\n\n4. **Seasonal-Trend Decomposition Architecture**: Informer has no explicit decomposition mechanism. FEDformer introduces deep decomposition with MOEDecomp blocks after each sub-block. **New Implementation Needed**:\n   - Implement MOEDecomp: Multiple average pooling filters with different kernel sizes, learnable mixing weights via Softmax(L(x)), separating seasonal and trend components.\n   - Integrate MOEDecomp after FEB, FEA, and FeedForward blocks in both encoder (extracting seasonal only) and decoder (extracting both seasonal and trend).\n   - Implement trend accumulation in decoder: T_de^l = T_de^(l-1) + W_{l,1}·T_de^{l,1} + W_{l,2}·T_de^{l,2} + W_{l,3}·T_de^{l,3}.\n\n5. **Decoder Output Composition**: Informer directly projects decoder output. FEDformer combines refined seasonal and accumulated trend components: W_S·X_de^M + T_de^M. **New Implementation Needed**: Implement dual-path processing in decoder, maintaining separate seasonal and trend tensors, with final summation for prediction.\n\n6. **Frequency Domain Mode Selection Strategy**: Informer's sparsity is query-based (Top-u queries via KL divergence measurement). FEDformer uses random mode selection in frequency domain (fixed M=64 modes). **New Implementation Needed**: Implement random mode selection before DFT/DWT operations, ensuring O(L) complexity instead of O(L log L).\n\n7. **Wavelet Transform Infrastructure**: Informer has no wavelet processing. FEDformer-w requires complete wavelet transform pipeline. **New Implementation Needed**:\n   - Implement fixed Legendre multiwavelet basis decomposition/reconstruction matrices.\n   - Build recursive decomposition with L cycles (default L=3), handling high-frequency (d_l^n), low-frequency (s_l^n), and remaining signals separately.\n   - Implement ladder-down decomposition and ladder-up reconstruction with signal length decimation/upsampling by factor 2.\n\n8. **Model Complexity and Efficiency**: Informer achieves O(L log L) complexity via ProbSparse attention. FEDformer achieves O(L) complexity through fixed mode selection. **New Implementation Needed**: Ensure mode selection M and wavelet decomposition steps L are fixed hyperparameters, not dependent on sequence length, to maintain linear complexity.\n\n9. **Multi-Expert Architecture**: Informer uses single-path processing. FEDformer employs mixture of experts for decomposition with learnable weights. **New Implementation Needed**: Implement parallel average pooling with different window sizes and learnable combination weights for adaptive trend extraction.\n\n10. **Imputation Task Adaptation**: While both papers focus on forecasting, the provided Informer code includes an imputation method. For FEDformer imputation, **New Implementation Needed**: Adapt FEB/FEA blocks to handle masked inputs, potentially masking frequency modes corresponding to missing time steps, and ensure MOEDecomp handles incomplete seasonal-trend separation."
  },
  {
    "source": "Reformer_2020",
    "target": "FEDformer_2022",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the Transformer architecture with encoder-decoder structures. The Reformer implementation provides the basic Transformer components (EncoderLayer, Encoder, DataEmbedding) that can be directly reused in FEDformer. The `layers.Transformer_EncDec` module with its multi-layer encoder structure is directly applicable to FEDformer's encoder framework.\n\n2. **Sequence Processing for Time Series**: Both models process sequential time series data with similar input/output formats [batch_size, length, d_model]. The embedding mechanism in Reformer (`DataEmbedding` with positional encoding and temporal features) can be reused for FEDformer's input processing. The projection layers for final output generation follow the same pattern.\n\n3. **Multi-Head Mechanism Preservation**: While implementing different attention mechanisms, both maintain the multi-head structure for capturing diverse patterns. Reformer's multi-head LSH attention framework provides a template for implementing FEDformer's multi-head frequency attention, particularly the dimension splitting and concatenation logic.\n\n4. **Layer Normalization and Residual Connections**: Both architectures employ layer normalization and residual connections within their encoder/decoder blocks. The feed-forward network structure with dropout and activation functions in Reformer's EncoderLayer can be directly adapted for FEDformer's blocks.\n\n5. **Efficient Computation Focus**: Both papers emphasize computational efficiency for long sequences. Reformer's memory-efficient implementation patterns (chunking strategies, gradient checkpointing considerations) provide valuable insights for implementing FEDformer's efficient frequency domain operations.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: \n   - **Reformer**: Uses Locality-Sensitive Hashing (LSH) attention with hash bucketing, sorting, and chunking to achieve O(L log L) complexity in time domain. Implements `LSHSelfAttention` with hash functions, bucket assignments, and nearest-neighbor lookups.\n   - **FEDformer**: Operates in frequency domain using Fourier/Wavelet transforms. Requires implementing **FEB (Frequency Enhanced Block)** with DFT/DWT operations, mode selection, and learnable frequency kernels. The attention computation happens on selected frequency modes rather than bucketed time-domain tokens.\n   - **Implementation Gap**: Need to build entirely new frequency domain transformation modules (FFT/DWT), frequency mode selection logic, and complex-valued kernel operations that don't exist in Reformer.\n\n2. **Signal Decomposition Architecture - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Uses reversible layers for memory efficiency but maintains a single representation stream without explicit decomposition.\n   - **FEDformer**: Implements **MOEDecomp (Mixture of Experts Decomposition)** that explicitly separates seasonal and trend components at each layer. Requires implementing multiple average pooling filters with different window sizes and learnable mixture weights.\n   - **Implementation Gap**: Need to create the seasonal-trend decomposition module with multiple parallel pooling operations and soft attention-based mixing mechanism. The decoder must maintain separate seasonal (S) and trend (T) pathways throughout all layers.\n\n3. **Cross-Attention Design - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Focuses on self-attention optimization; cross-attention follows standard Transformer patterns.\n   - **FEDformer**: Implements **FEA (Frequency Enhanced Attention)** for encoder-decoder interaction in frequency domain. Queries, keys, and values are all transformed to frequency domain before attention computation.\n   - **Implementation Gap**: Need to implement frequency-domain cross-attention with separate DFT transforms for Q, K, V, mode selection for each, and frequency-domain attention computation with activation functions (softmax/tanh) before inverse transform.\n\n4. **Dual Transform Support - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Single attention mechanism (LSH) applied uniformly across all layers.\n   - **FEDformer**: Provides two complete implementations: **FEB-f/FEA-f** (Fourier-based) and **FEB-w/FEA-w** (Wavelet-based). Wavelet version requires recursive multi-scale decomposition with separate processing for high-frequency, low-frequency, and remaining components.\n   - **Implementation Gap**: Need to implement discrete wavelet transform with Legendre basis, recursive decomposition/reconstruction logic (up to L levels), and separate FEB-f modules for each frequency band. The wavelet path involves significantly more complex multi-scale processing.\n\n5. **Complexity and Memory Management - DIFFERENT STRATEGIES**:\n   - **Reformer**: Achieves O(L log L) through hash-based bucketing and reversible layers. Uses `fit_length` to pad sequences to multiples of bucket_size * 2.\n   - **FEDformer**: Achieves O(L) complexity through fixed mode selection (M=64) in frequency domain. No sequence padding required; operates on original sequence length.\n   - **Implementation Gap**: FEDformer's linear complexity comes from limiting frequency modes rather than bucketing, requiring a completely different approach to sequence processing and memory optimization.\n\n6. **Output Aggregation Strategy - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Single output stream from encoder, direct projection to target dimension.\n   - **FEDformer**: Combines refined seasonal component (W_S · X_de^M) and accumulated trend component (T_de^M) from all decoder layers. Each decoder layer contributes to trend accumulation through learnable projectors (W_{l,i}).\n   - **Implementation Gap**: Need to implement trend accumulation mechanism across decoder layers with separate projection matrices for each decomposition block's trend output, and final seasonal-trend fusion for prediction.\n\n7. **Parameter Initialization and Learning - DIFFERENT FOCUS**:\n   - **Reformer**: Random projection matrices for LSH (fixed during training), learnable Q/K/V projections.\n   - **FEDformer**: Learnable complex-valued frequency kernels (R ∈ C^{D×D×M}) for mode-wise transformations, learnable mixture weights for MOEDecomp, and separate parameters for seasonal vs. trend processing.\n   - **Implementation Gap**: Need to handle complex-valued parameter initialization and gradients, implement mode-specific learnable kernels, and create the expert mixture learning mechanism for decomposition."
  },
  {
    "source": "Informer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "FiLM_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Reformer_2020",
    "target": "Informer_2020",
    "type": "in-domain",
    "similarities": "1. **Transformer-based Architecture Foundation**: Both papers build upon the standard Transformer encoder-decoder architecture with multi-head attention mechanisms. The Reformer implementation's basic attention computation structure (Q, K, V projection, multi-head mechanism) can be directly reused as the foundation for Informer. The DataEmbedding, EncoderLayer, and basic feed-forward network components from Reformer's code can serve as starting templates.\n\n2. **Efficiency-Oriented Attention Design**: Both papers aim to reduce the O(L²) complexity of standard attention mechanisms for long sequences. Reformer uses LSH (Locality-Sensitive Hashing) attention while Informer uses ProbSparse attention, but both share the core principle of selective attention - only computing attention for a subset of query-key pairs. The Reformer implementation's attention masking logic and the general pattern of restricting attention computation can inform Informer's Top-u query selection mechanism.\n\n3. **Memory-Efficient Implementation Strategy**: Both papers address memory bottlenecks in processing long sequences. Reformer's reversible layers and chunking strategies demonstrate memory-efficient computation patterns. While Informer doesn't use reversible layers, the Reformer code's approach to batching, sequence padding (fit_length function), and efficient tensor operations provides valuable implementation patterns for handling variable-length sequences in Informer.\n\n4. **Encoder-Decoder Structure with Task Adaptation**: Both implementations use encoder-decoder architectures adapted for time-series tasks. The Reformer code's task-specific forward methods (imputation, forecasting) and the pattern of separating encoder embedding from decoder processing can be adapted for Informer's generative inference decoder design.\n\n5. **Bucket/Chunk-based Processing**: Reformer's bucket_size parameter for LSH hashing shares conceptual similarity with Informer's need to process queries in chunks. The Reformer implementation's handling of sequence chunking and the logic for ensuring proper sequence lengths divisible by bucket sizes provides implementation guidance for Informer's self-attention distilling operation.",
    "differences": "1. **Core Attention Mechanism - ProbSparse vs LSH**: NEWLY IMPLEMENT: Informer requires a completely new ProbSparse self-attention mechanism based on query sparsity measurement M(q_i, K). This involves: (a) Computing the max-mean measurement (Equation 4) to identify Top-u dominant queries, (b) Implementing random sampling of U = L_K * ln(L_Q) dot-products for efficient sparsity measurement, (c) Creating sparse query matrix Q̄ containing only Top-u queries, (d) Setting sampling factor c to control u = c·ln(L_Q). Reformer's LSH attention uses hash buckets and random projections, which is fundamentally different and cannot be reused.\n\n2. **Self-Attention Distilling with Encoder Pyramid**: NEWLY IMPLEMENT: Informer's encoder uses a unique self-attention distilling operation (Equation 5) that progressively reduces sequence length through: (a) 1D convolutional layers (kernel width=3) with ELU activation on the time dimension, (b) Max-pooling with stride 2 after each attention block to halve the sequence length, (c) Multi-stack pyramid architecture where replicas process halving inputs with progressively fewer layers, (d) Concatenation of all stack outputs for final representation. This achieves O((2-ε)L·logL) memory complexity. Reformer uses no such distilling mechanism - it maintains sequence length through layers.\n\n3. **Generative Decoder with Start Token Strategy**: NEWLY IMPLEMENT: Informer's decoder uses a one-forward-pass generative inference mechanism: (a) Concatenating start token X_token (sampled from earlier sequence, e.g., 5 days) with placeholder X_0 for target sequence, (b) Feeding the combined sequence to decoder in one forward pass instead of auto-regressive decoding, (c) Using only time stamps (not values) in X_0 for the target period. Reformer's implementation uses standard causal masking without this generative token strategy.\n\n4. **Forecasting-Specific Architecture vs General Efficiency**: NEWLY IMPLEMENT: Informer is specifically designed for long sequence time-series forecasting (LSTF) with: (a) Explicit handling of prediction horizons (pred_len) ranging from 24 to 960 time steps, (b) Task-specific loss functions for forecasting (MSE on prediction vs target sequences), (c) Encoder output feeding into decoder for multi-step ahead prediction. Reformer's implementation is more general-purpose (shown for imputation task) and doesn't include forecasting-specific components like prediction horizon handling or the start token mechanism.\n\n5. **Complexity Analysis and Theoretical Foundation**: NEWLY IMPLEMENT: Informer requires implementing the theoretical measurement framework: (a) Query sparsity measurement M(q_i, K) based on KL divergence between attention probability and uniform distribution, (b) Lemma 1 bounds and max-mean approximation M̄(q_i, K), (c) Proposition 1 for boundary relaxation in Top-u selection. This theoretical foundation guides the implementation of ProbSparse attention and is absent in Reformer's LSH approach.\n\n6. **Multi-Scale Temporal Encoding**: NEWLY IMPLEMENT: Informer's experiments show handling of multiple datasets (ETTh1, ETTh2, ETTm1, Weather, ECL) with different temporal granularities (hourly, 15-minute intervals). The implementation needs dataset-specific embedding strategies and temporal encoding that captures different time scales, which is more sophisticated than Reformer's general sequence processing."
  },
  {
    "source": "PatchTST_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Only Transformer Architecture**: Both papers adopt encoder-only Transformer architectures rather than encoder-decoder structures. iTransformer explicitly states it uses 'encoder-only architecture of Transformer' (Section 3.1), which aligns with PatchTST's design philosophy. The source implementation's basic Transformer blocks (layer normalization, feed-forward networks, self-attention) can be directly reused as the foundational building blocks for iTransformer.\n\n2. **Token-Based Representation Learning**: Both approaches focus on representation learning through tokenization, though applied to different dimensions. PatchTST uses patching to create tokens from temporal segments, while iTransformer treats entire variates as tokens. The source code's token embedding modules (MLP-based Embedding: ℝ^T → ℝ^D) and projection layers (Projection: ℝ^D → ℝ^S) can be adapted for iTransformer's variate-level embedding, requiring only dimensional adjustments.\n\n3. **Layer Normalization for Stability**: Both architectures employ layer normalization for training stability and convergence. iTransformer's Equation 2 shows LayerNorm applied to individual variate representations, which is conceptually similar to PatchTST's normalization approach. The source implementation's LayerNorm modules can be reused with modified application scope (per-variate instead of per-timestamp).\n\n4. **Feed-Forward Network as Core Representation Extractor**: Both papers leverage identical feed-forward networks (FFN) applied to each token for representation extraction. iTransformer explicitly mentions FFN's role in extracting 'complicated representations to describe a time series' (Section 3.2), similar to PatchTST's patch representation learning. The source code's FFN implementation (typically 2-layer MLP with activation) can be directly reused.\n\n5. **Linear Forecasting Philosophy**: Both papers acknowledge and build upon the effectiveness of linear models for time series forecasting. iTransformer states that 'the task to generate the predicted series is essentially delivered to linear layers' (Section 3.1), which aligns with PatchTST's efficient design. The source implementation's linear projection heads can serve as the basis for iTransformer's output projection.\n\n6. **No Positional Embedding Requirement**: iTransformer explicitly states 'the position embedding in the vanilla Transformer is no longer needed' (Section 3.1) because order information is implicitly stored in the feed-forward network's neuron permutation. If PatchTST similarly omits or minimizes positional encoding, this design choice can be directly transferred.",
    "differences": "1. **CORE INNOVATION - Inverted Attention Dimension**: The fundamental difference is the attention application dimension. PatchTST applies self-attention across temporal patches (time dimension), while iTransformer inverts this to apply attention across variates (channel dimension). NEW IMPLEMENTATION NEEDED: Transpose input data from [Batch, Time, Channels] to [Batch, Channels, Features] before feeding into attention modules, where each channel becomes a token with temporal features embedded in its representation.\n\n2. **Token Definition and Embedding Strategy**: PatchTST creates tokens from temporal patches (subsequences of time steps), while iTransformer treats each entire variate time series as a single token. NEW IMPLEMENTATION NEEDED: Redesign the embedding layer to process full time series (X[:,n] for each variate n) rather than temporal patches. The embedding function must map ℝ^T → ℝ^D where T is the full lookback window length, not a patch length.\n\n3. **Attention Semantic Interpretation**: In PatchTST, attention captures temporal dependencies between patches. In iTransformer, attention reveals multivariate correlations, with the attention score map A ∈ ℝ^(N×N) exhibiting 'multivariate correlations between paired variate tokens' (Section 3.2). NEW IMPLEMENTATION NEEDED: Modify attention visualization and interpretation tools to analyze variate-variate correlations instead of time-time dependencies. The pre-Softmax scores A_{i,j} ∝ q_i^T k_j now represent correlation between variates i and j.\n\n4. **Layer Normalization Application Scope**: PatchTST normalizes across the feature dimension of temporal tokens, while iTransformer normalizes 'the series representation of individual variate' (Equation 2), applying normalization to reduce 'discrepancies caused by inconsistent measurements' across different variates. NEW IMPLEMENTATION NEEDED: Modify LayerNorm to operate on the feature dimension of each variate token independently (normalize h_n for each n=1,...,N) rather than across temporal positions.\n\n5. **Handling Multivariate Heterogeneity**: iTransformer explicitly addresses that 'elements of X_{t,:} can be distinct from each other in physical measurements and statistical distributions' (Section 3), while PatchTST focuses on temporal patterns. NEW IMPLEMENTATION NEEDED: Implement variate-specific normalization strategies and ensure the model architecture respects the independence of variates (Channel Independence principle). Each variate should be processed through FFN independently before attention-based correlation.\n\n6. **Flexible Variate Number at Inference**: iTransformer states 'the token number can vary from training to inference, and the model is allowed to be trained on arbitrary numbers of variates' (Section 3.1). NEW IMPLEMENTATION NEEDED: Design the architecture to support variable numbers of input variates between training and testing, which requires attention mechanisms that can handle dynamic token counts and potentially zero-shot generalization to unseen variates.\n\n7. **Task Formulation for Imputation**: While both papers focus on forecasting in their primary descriptions, adapting iTransformer to imputation requires NEW IMPLEMENTATION: Design a masking strategy that operates on the variate-token level rather than temporal patches. Missing values across different time steps within a variate token need to be handled through the embedded representation, potentially requiring masked attention within the feature dimension or specialized reconstruction heads that can output partial series with missing segments.\n\n8. **Interpretability Focus**: iTransformer emphasizes 'enhanced interpretability revealing multivariate correlations' (Figure 4b) through attention maps. NEW IMPLEMENTATION NEEDED: Develop visualization tools specifically for variate correlation matrices derived from attention scores, and potentially add regularization or constraints to encourage meaningful correlation learning (e.g., sparsity constraints if domain knowledge suggests limited variate interactions)."
  },
  {
    "source": "Crossformer_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Only Architecture Foundation**: Both models utilize encoder-based Transformer architectures for multivariate time series processing. Crossformer's hierarchical encoder structure with TSA layers can inform iTransformer's stacked encoder blocks. The source implementation's `Encoder` class with multiple attention layers provides a reusable template for building iTransformer's L-layer encoder stack.\n\n2. **Layer Normalization for Distribution Alignment**: Both papers apply layer normalization to handle distribution discrepancies, though on different dimensions. Crossformer's `LayerNorm` in TSA layers (Eq. 3-4) and iTransformer's normalization on series representations (Eq. 2) share the same stabilization goal. The source code's `nn.LayerNorm` modules can be directly reused, only adjusting the normalization dimension from (batch, time, dim) to (batch, variate, feature).\n\n3. **Feed-Forward Network as Representation Learner**: Both architectures employ MLP/FFN for feature transformation. Crossformer's `MLP1` and `MLP2` in TSA layers (with GELU activation) closely match iTransformer's FFN design for series representation extraction. The source implementation's two-layer FFN structure `nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model))` can be directly adapted for iTransformer's token-wise FFN.\n\n4. **Multi-Head Self-Attention Mechanism**: Both use multi-head self-attention for capturing dependencies, though on inverted dimensions. Crossformer's `AttentionLayer` and `FullAttention` classes with Q/K/V projections provide a complete attention implementation. For iTransformer, the same `AttentionLayer` code can be reused by simply transposing input dimensions from (batch, time, dim) to (batch, variate, feature).\n\n5. **Embedding and Projection Layers**: Both require input embedding and output projection. Crossformer's `PatchEmbedding` with linear projection and iTransformer's Embedding MLP share similar purposes. The source's `nn.Linear` projection matrices in `DecoderLayer.linear_pred` demonstrate how to map representations to predictions, which can guide iTransformer's Projection: R^D → R^S implementation.\n\n6. **Dropout Regularization**: Both models apply dropout for regularization. Crossformer's dropout in attention layers (`attention_dropout`) and after projections can be directly reused in iTransformer's attention and FFN modules with the same dropout rate configuration.\n\n7. **Residual Connections**: Both architectures use skip connections around attention and FFN modules. Crossformer's implementation pattern `x = norm(x + dropout(attention(x)))` in TSA layers exactly matches iTransformer's residual design, enabling direct code reuse for the block structure.",
    "differences": "1. **CORE INNOVATION - Inverted Attention Dimension**: iTransformer's fundamental contribution is applying attention on the variate dimension rather than time dimension. NEW IMPLEMENTATION REQUIRED: Transpose input from (B, T, N) to (B, N, T) before feeding to Transformer blocks, where each variate's full time series becomes a token. This requires creating a new input preprocessing module that reshapes data to treat N variates as N tokens, fundamentally different from Crossformer's (B, D, L, d_model) 2D array structure.\n\n2. **Token Definition and Embedding Strategy**: Crossformer uses Dimension-Segment-Wise (DSW) embedding, dividing each dimension into segments of length L_seg and embedding them as 2D arrays (Eq. 1-2). iTransformer embeds the ENTIRE time series of each variate as one token (Eq. 1). NEW IMPLEMENTATION: Replace `PatchEmbedding` with a simple MLP: `Embedding: R^T → R^D` that takes the full lookback window X_{:,n} ∈ R^T and projects it to D-dimensional token h_n^0. This eliminates segment-wise processing entirely.\n\n3. **Elimination of Position Embedding**: Crossformer uses learnable position embeddings `E_{i,d}^{(pos)}` for segment positions (Eq. 2) and decoder position embeddings `E^{(dec)}`. iTransformer explicitly states \"position embedding in the vanilla Transformer is no longer needed\" because temporal order is implicitly stored in FFN neuron permutations. NEW IMPLEMENTATION: Remove all `nn.Parameter` position embedding modules from the source code.\n\n4. **Two-Stage vs Single-Stage Attention**: Crossformer's TSA layer has two stages - Cross-Time Stage (Eq. 3) and Cross-Dimension Stage with router mechanism (Eq. 4), processing time and dimension separately with O(DL²) complexity. iTransformer uses standard single-stage self-attention on variate tokens with O(N²D) complexity. NEW IMPLEMENTATION: Replace the entire `TwoStageAttentionLayer` class with vanilla multi-head self-attention operating on (B, N, D) shaped variate tokens, removing router mechanism entirely.\n\n5. **Hierarchical Encoder-Decoder vs Flat Encoder-Only**: Crossformer uses hierarchical structure with segment merging (Eq. 6), multiple encoder scales, and a decoder with cross-attention (Eq. 7). iTransformer is encoder-only with flat L identical blocks. NEW IMPLEMENTATION: (1) Remove `SegMerging` module and all hierarchical scaling logic. (2) Remove entire `Decoder` class and cross-attention mechanisms. (3) Implement simple stacked identical blocks: for l in range(L): H^{l+1} = TrmBlock(H^l).\n\n6. **Output Projection Strategy**: Crossformer aggregates predictions from multiple decoder layers at different scales (Eq. 8): final_predict = Σ_{l=0}^N x^{pred,l}. iTransformer uses independent linear projections for each variate token: Ŷ_{:,n} = Projection(h_n^L). NEW IMPLEMENTATION: Replace multi-scale aggregation with N independent MLPs (can be implemented as one shared MLP applied token-wise): `self.projection = nn.Linear(D, S)` or `nn.Sequential(nn.Linear(D, hidden), nn.ReLU(), nn.Linear(hidden, S))`.\n\n7. **Normalization Dimension**: Crossformer normalizes across d_model dimension in 2D arrays (batch, ts_d, seg_num, d_model). iTransformer normalizes each variate token's feature dimension: LayerNorm(H) = {(h_n - Mean(h_n))/√Var(h_n)} (Eq. 2). NEW IMPLEMENTATION: Ensure `nn.LayerNorm(D)` operates on the last dimension of (B, N, D) tensors, normalizing each of N variate tokens independently across D features.\n\n8. **Interpretability of Attention Maps**: Crossformer's attention captures cross-time and cross-dimension dependencies in segmented form. iTransformer's attention scores A_{i,j} ∝ q_i^T k_j reveal variate-wise correlations in the N×N score map, providing direct multivariate correlation interpretation. NEW IMPLEMENTATION: Add attention score extraction and visualization modules to output the N×N correlation matrix for interpretability analysis, which is central to iTransformer's contribution.\n\n9. **Missing Value Handling for Imputation**: For imputation tasks, Crossformer processes masked segments through its hierarchical structure. iTransformer would need NEW IMPLEMENTATION: (1) Mask-aware embedding that handles missing values in the full series X_{:,n} before tokenization. (2) Reconstruction loss computed only on masked positions after projecting back to time domain. (3) Potential mask token injection strategy where missing portions of series are replaced with learnable mask embeddings before feeding to the encoder.\n\n10. **Scalability and Complexity Trade-offs**: Crossformer's router mechanism (factor c << D) reduces cross-dimension complexity from O(D²L) to O(DL). iTransformer has O(N²D) attention complexity on variates, requiring efficient attention variants when N is large. NEW IMPLEMENTATION: For large N, integrate efficient attention mechanisms (Flash Attention, Linear Attention) as plugins, which is mentioned but not implemented in the source code. This requires replacing `FullAttention` with efficient attention variants that maintain the variate-dimension attention semantics."
  },
  {
    "source": "TiDE_2023",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Channel-Independent Processing Philosophy**: Both models process each variate independently during the core computation. TiDE explicitly applies the model \"in a channel independent manner\" where input is processed per time-series (y^(i), x^(i), a^(i)) → ŷ^(i), while iTransformer treats each variate's entire time series X[:,n] as an independent token. The TiDE implementation's loop `[self.imputation(x_enc[:, :, feature]...) for feature in range(x_enc.shape[-1])]` can be directly adapted for iTransformer's per-variate token processing.\n\n2. **Normalization Strategy for Non-Stationarity**: Both employ instance-wise normalization to handle non-stationary time series. TiDE normalizes each channel: `means = x_enc.mean(1, keepdim=True); x_enc = x_enc - means; stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5); x_enc /= stdev`, then denormalizes predictions. iTransformer's LayerNorm applies to series representation of individual variates: `(h_n - Mean(h_n))/sqrt(Var(h_n))`. The TiDE normalization code can be reused as a preprocessing step before embedding in iTransformer.\n\n3. **MLP-Based Feature Extraction**: Both leverage MLPs for series representation learning. TiDE uses ResBlock (MLP with residual connections) for feature encoding and temporal decoding. iTransformer uses MLP for Embedding (R^T → R^D) and Projection (R^D → R^S), with FFN in Transformer blocks for series representation. The ResBlock implementation with `fc1→ReLU→fc2→dropout + skip_connection→LayerNorm` can serve as the foundation for iTransformer's embedding/projection layers.\n\n4. **Residual Connection Design**: Both incorporate global residual connections. TiDE has `residual_proj = nn.Linear(self.seq_len, self.seq_len)` that linearly maps lookback to horizon, added to final predictions. iTransformer can adopt similar residual connections from input series to output predictions, reusing TiDE's linear projection pattern.\n\n5. **Temporal Feature Encoding**: Both utilize temporal covariates (time features). TiDE's feature encoder: `feature_encoder = ResBlock(self.feature_dim, self.res_hidden, self.feature_encode_dim)` processes x_mark_enc. iTransformer, while not explicitly using external features in the core formulation, can incorporate them similarly. The TiDE feature encoding module can be adapted for iTransformer's optional temporal feature integration.\n\n6. **Task Flexibility (Forecasting/Imputation)**: Both papers address forecasting but can be adapted for imputation. TiDE's implementation includes an `imputation` method that reconstructs the lookback window, which demonstrates the architecture's flexibility. iTransformer's encoder-only structure similarly supports both tasks by adjusting the projection layer's output dimension (S for forecasting, T for imputation).",
    "differences": "1. **Core Architectural Paradigm - NEEDS NEW IMPLEMENTATION**: TiDE uses sequential MLP encoding-decoding (Feature Projection → Dense Encoder → Dense Decoder → Temporal Decoder), while iTransformer inverts the Transformer architecture to treat variates as tokens. NEW COMPONENTS REQUIRED: (a) Self-attention mechanism operating on variate tokens (N×D) instead of temporal tokens, (b) Multi-head attention with Q, K, V projections where attention scores A_{i,j} ∝ q_i^T k_j reveal multivariate correlations, (c) Stacked Transformer blocks with inverted attention, requiring implementation of `TrmBlock` with attention on variate dimension.\n\n2. **Token Representation - NEEDS NEW IMPLEMENTATION**: TiDE flattens temporal sequences: `torch.cat([x_enc, feature.reshape(feature.shape[0], -1)], dim=-1)` creates a flat vector of size (L + (L+H)×feature_encode_dim). iTransformer treats each complete time series X[:,n] ∈ R^T as a token h_n ∈ R^D. NEW COMPONENTS REQUIRED: (a) Embedding layer that maps entire series (R^T → R^D) per variate, not flattening across time, (b) Token-wise processing where each h_n represents a variate's full temporal context, (c) Modified data flow where batch shape is [B, N, D] instead of [B, flattened_features].\n\n3. **Multivariate Correlation Modeling - NEEDS NEW IMPLEMENTATION**: TiDE processes channels independently with shared weights but no explicit inter-channel interaction during forward pass (only through shared training). iTransformer explicitly models multivariate correlations via self-attention on variate tokens. NEW COMPONENTS REQUIRED: (a) Attention score map A ∈ R^{N×N} that captures variate-variate correlations, (b) Softmax(QK^T/√d_k)V operation where values are aggregated based on variate similarity, (c) Interpretable attention weights that can be visualized to show which variates influence each other, (d) Mechanism to handle varying numbers of variates between training and inference.\n\n4. **Position Encoding Strategy - NEEDS NEW IMPLEMENTATION**: TiDE uses no explicit position encoding; temporal order is implicit in the flattened sequence and processed by MLPs. iTransformer states \"position embedding in the vanilla Transformer is no longer needed\" because \"order of sequence is implicitly stored in the neuron permutation of the feed-forward network.\" However, the temporal structure is encoded differently. NEW COMPONENTS REQUIRED: (a) Ensure FFN architecture implicitly captures temporal patterns within each variate token, (b) Potentially implement optional learnable variate embeddings if variate identity matters, (c) Design embedding layer that preserves temporal ordering within each series representation.\n\n5. **Layer Normalization Application - NEEDS MODIFICATION**: TiDE applies LayerNorm to the output of each ResBlock across the feature dimension. iTransformer applies LayerNorm to each variate token's representation: `LayerNorm(H) = {(h_n - Mean(h_n))/sqrt(Var(h_n)) | n=1,...,N}`, normalizing across the D-dimensional representation. MODIFICATION REQUIRED: Change LayerNorm from normalizing across output features to normalizing each token's representation independently, implementing the formula in Equation 2 of iTransformer paper.\n\n6. **Decoder Architecture - NEEDS NEW IMPLEMENTATION**: TiDE uses Dense Decoder (multiple ResBlocks) + Temporal Decoder (per-timestep processing): `decoded = decoders(hidden).reshape(..., self.seq_len, self.decode_dim); dec_out = temporalDecoder(torch.cat([feature, decoded], dim=-1))`. iTransformer uses a simpler Projection layer: `Ŷ[:,n] = Projection(h_n^L)` where R^D → R^S is a direct MLP mapping. NEW COMPONENTS REQUIRED: (a) Simplified projection that directly maps final token representation to output series, (b) Remove the reshape and per-timestep decoding logic from TiDE, (c) Implement straightforward MLP: `nn.Linear(D, S)` or multi-layer MLP for projection.\n\n7. **Feature Integration Mechanism - NEEDS MODIFICATION**: TiDE concatenates projected features at multiple stages: encoder input includes `[y_{1:L}, x̃_{1:L+H}, a]`, and temporal decoder uses `[d_t, x̃_{L+t}]`. iTransformer's core formulation (Equation 1) shows `h_n^0 = Embedding(X[:,n])` without explicit external features, focusing on endogenous series. MODIFICATION REQUIRED: If adapting for covariates, need to design how temporal features are integrated with variate tokens, potentially through: (a) Concatenation before embedding, (b) Cross-attention between variate tokens and feature tokens, (c) Additive feature embeddings to token representations.\n\n8. **Training Objective for Imputation - NEEDS ADAPTATION**: TiDE's imputation method reconstructs the lookback window: `dec_out` has shape [B, L, D] matching input. For iTransformer imputation, need to: (a) Modify projection to output T timesteps instead of S (forecasting horizon), (b) Implement mask-aware loss that only computes reconstruction error on missing values, (c) Design masking strategy compatible with variate-token architecture where entire series may have partial masks, (d) Potentially implement attention masking to prevent information leakage from future to past during imputation."
  },
  {
    "source": "TimesNet_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Embedding and Normalization Infrastructure**: Both papers use DataEmbedding for initial feature extraction and LayerNorm for stabilization. TimesNet's `self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)` and `self.layer_norm = nn.LayerNorm(configs.d_model)` can be directly reused. The normalization-denormalization pattern in TimesNet's imputation method (computing means/stdev, normalizing input, denormalizing output) is applicable to iTransformer's variate-wise normalization approach.\n\n2. **Projection Layer for Task-Specific Output**: Both architectures use linear projection layers to map learned representations to task outputs. TimesNet's `self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)` demonstrates the projection pattern. For iTransformer imputation, similar projection layers can map from D-dimensional variate tokens back to S-dimensional reconstructed series: `Projection: R^D → R^S`.\n\n3. **Residual Connection Philosophy**: TimesNet employs residual connections in TimesBlock (`res = res + x`), which aligns with Transformer's residual structure. This pattern can be adapted for iTransformer's stacked blocks, though applied on the inverted dimension (variate tokens rather than temporal tokens).\n\n4. **Missing Value Masking Strategy**: TimesNet's imputation method uses explicit masking (`x_enc = x_enc.masked_fill(mask == 0, 0)`) and mask-aware statistics computation (`torch.sum(x_enc, dim=1) / torch.sum(mask == 1, dim=1)`). This mask-based approach for handling missing values during normalization and loss computation can be adapted to iTransformer's variate-centric processing.\n\n5. **Multi-Layer Stacking Pattern**: Both use sequential layer stacking with layer normalization. TimesNet's loop `for i in range(self.layer): enc_out = self.layer_norm(self.model[i](enc_out))` demonstrates the pattern that can guide iTransformer's implementation of Equation 1: `H^(l+1) = TrmBlock(H^l)` for l=0 to L-1.",
    "differences": "1. **Core Token Representation - CRITICAL NEW IMPLEMENTATION**: TimesNet treats time points as tokens (shape [B, T, C] → process along T dimension), while iTransformer inverts this to treat variates as tokens (shape [B, T, N] → transpose to [B, N, T] → embed each N). NEW REQUIRED: Implement variate-wise embedding where each complete time series X[:,n] ∈ R^T is embedded as a single token h_n ∈ R^D. This requires creating `Embedding: R^T → R^D` using MLP that processes the entire lookback window, fundamentally different from TimesNet's temporal embedding.\n\n2. **Attention Mechanism Inversion - NEW ATTENTION PATTERN**: TimesNet applies 2D convolutions on reshaped temporal structures (period × frequency), not using attention for multivariate correlation. iTransformer requires NEW IMPLEMENTATION of self-attention on variate tokens where attention scores A_{i,j} ∝ q_i^T k_j reveal multivariate correlations. The attention input shape changes from [B, T, D] in typical Transformers to [B, N, D] where N is variate count. Need to implement: `Q, K, V = Linear(H)` where H ∈ R^{N×D}, computing attention over N variates instead of T timesteps.\n\n3. **Feed-Forward Network Role Reversal - NEW FFN APPLICATION**: TimesNet's FFN (Inception blocks) processes 2D spatial-temporal patterns. iTransformer's FFN must be redesigned to extract series representations from individual variate tokens. NEW REQUIRED: FFN operates on series representation h_n ∈ R^D for each variate independently, learning to encode temporal patterns within a single series. The FFN serves as both encoder (extracting features from lookback series) and decoder (projecting to future predictions), acting as the \"universal approximator\" for time series mentioned in Section 3.2.\n\n4. **Dimension-Specific Layer Normalization - NEW NORMALIZATION AXIS**: TimesNet normalizes across the feature dimension of temporal representations. iTransformer requires NEW IMPLEMENTATION of variate-wise normalization as Equation 2: `LayerNorm(H) = {(h_n - Mean(h_n))/sqrt(Var(h_n)) | n=1,...,N}`, normalizing each variate token's representation independently. This addresses non-stationarity at the variate level rather than temporal level.\n\n5. **Periodicity Detection vs Multivariate Correlation - COMPLETELY DIFFERENT FEATURE EXTRACTION**: TimesNet's core innovation is FFT-based period detection (`FFT_for_Period`) and 2D reshaping based on discovered periods, which is ABSENT in iTransformer. Instead, iTransformer relies on attention-based multivariate correlation discovery. NEW REQUIRED: Remove all period-detection and 2D-reshaping logic. Implement attention score visualization capabilities to interpret learned multivariate correlations (Section 4.3 mentions score map A ∈ R^{N×N} visualization).\n\n6. **Imputation Loss Computation - NEW MASK-AWARE LOSS ON INVERTED DIMENSION**: TimesNet computes imputation loss on masked temporal positions across all variates simultaneously. iTransformer requires NEW IMPLEMENTATION where loss is computed per-variate after reconstruction. Since each variate is processed as an independent token, the mask and loss must be applied to reconstructed series Ŷ[:,n] for each variate n, requiring careful handling of the inverted data flow: embed [B,T,N] → transpose to [B,N,T] → process → project back → transpose to [B,T,N] → compute masked loss.\n\n7. **Variable Variate Handling - NEW GENERALIZATION CAPABILITY**: TimesNet operates on fixed C channels throughout. iTransformer's design allows NEW FEATURE: training on arbitrary numbers of variates and inference on unseen variates (mentioned in Section 3.1). This requires implementing flexible token handling where N can vary between training and inference, necessitating careful design of embedding/projection layers that can handle variable-length token sequences."
  },
  {
    "source": "DLinear_2022",
    "target": "iTransformer_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers utilize seasonal-trend decomposition for preprocessing time series data. DLinear explicitly decomposes input into trend and seasonal components using moving average kernels, which can be directly reused in iTransformer's preprocessing pipeline. The `series_decomp` module from DLinear's implementation (using `layers.Autoformer_EncDec`) can serve as a foundation for iTransformer's data normalization and preprocessing stages.\n\n2. **Channel-wise Processing Philosophy**: Both models process each variate independently to some extent. DLinear offers an 'individual' mode where separate linear layers are applied per channel, which aligns with iTransformer's core principle of treating each variate's entire time series as an independent token. The `individual` parameter and `nn.ModuleList` implementation pattern in DLinear can guide the implementation of variate-wise token embedding in iTransformer.\n\n3. **Direct Mapping Strategy**: Both avoid autoregressive decoding and adopt direct multi-step (DMS) forecasting. DLinear directly maps input sequence length to prediction length through linear layers, while iTransformer uses MLP-based Embedding and Projection layers for the same purpose. The weight initialization strategy in DLinear (`(1/self.seq_len) * torch.ones([self.pred_len, self.seq_len])`) provides insights for initializing iTransformer's projection layers.\n\n4. **Task-Agnostic Architecture**: Both models support multiple tasks including forecasting and imputation through the same core architecture. DLinear's `imputation` method demonstrates how the encoder can be reused for imputation tasks, which directly translates to iTransformer's unified framework where the same inverted Transformer blocks handle both forecasting and imputation.\n\n5. **Normalization for Distribution Shift**: Both papers address non-stationary issues through normalization. DLinear's NLinear variant uses subtraction of the last value for normalization, while iTransformer applies Layer Normalization on each variate token. The normalization logic in DLinear's preprocessing can inform iTransformer's Layer Normalization implementation along the variate dimension.",
    "differences": "1. **Core Architecture Paradigm**: DLinear uses simple linear layers for temporal modeling, requiring only `nn.Linear` modules. iTransformer requires implementing a complete inverted Transformer architecture including: (a) Multi-head self-attention mechanism operating on variate tokens instead of temporal tokens, (b) Feed-forward networks (FFN) applied to series representations, (c) Stacked Transformer blocks with residual connections. NEW IMPLEMENTATION NEEDED: Full Transformer encoder blocks with attention mechanism, query/key/value projections, and multi-layer FFN.\n\n2. **Token Representation Inversion**: DLinear operates on the temporal dimension directly with shape [batch, channels, seq_len], applying linear transformations along seq_len. iTransformer inverts this by treating each variate's entire time series as a token with shape [batch, num_variates, embedding_dim]. NEW IMPLEMENTATION NEEDED: (a) Embedding layer that converts time series of length T to embedding dimension D (MLP: R^T → R^D), (b) Projection layer that converts embeddings back to predicted series (MLP: R^D → R^S), (c) Dimension permutation logic to handle inverted token structure.\n\n3. **Multivariate Correlation Modeling**: DLinear explicitly avoids modeling spatial correlations between variates (as stated: 'does not model any spatial correlations'). iTransformer's core innovation is learning multivariate correlations through self-attention on variate tokens. NEW IMPLEMENTATION NEEDED: (a) Self-attention mechanism where attention scores A ∈ R^(N×N) represent variate-wise correlations, (b) Query/Key/Value projections operating on variate dimension, (c) Attention score visualization tools for interpretability analysis.\n\n4. **Layer Normalization Strategy**: DLinear uses series decomposition for handling non-stationarity but doesn't employ Layer Normalization within the model. iTransformer applies Layer Normalization along the series representation dimension of each variate token (Equation 2: normalizing h_n for each variate n). NEW IMPLEMENTATION NEEDED: Custom Layer Normalization that operates on the feature dimension of each variate token independently, computing mean and variance along the embedding dimension rather than across variates.\n\n5. **Positional Information Handling**: DLinear implicitly encodes temporal order through the weight matrix positions in linear layers. iTransformer explicitly states that 'position embedding in the vanilla Transformer is no longer needed' because temporal order is implicitly stored in neuron permutations of the FFN. NEW IMPLEMENTATION NEEDED: (a) Removal of traditional positional encoding mechanisms, (b) Verification that FFN can capture temporal patterns without explicit position embeddings, (c) Potentially learnable variate embeddings if needed for variate identification.\n\n6. **Complexity and Scalability**: DLinear has O(T) complexity with minimal parameters (two linear layers per channel in individual mode). iTransformer has O(N²) complexity for self-attention on N variates, requiring significantly more parameters. NEW IMPLEMENTATION NEEDED: (a) Efficient attention implementations (Flash Attention, Linear Attention) as mentioned for handling large variate numbers, (b) Memory-efficient training strategies for deep Transformer stacks, (c) Flexible token number handling for training/inference on different variate counts.\n\n7. **Representation Learning Depth**: DLinear uses shallow 1-2 layer linear transformations. iTransformer employs deep stacked Transformer blocks (L layers) for hierarchical representation learning. NEW IMPLEMENTATION NEEDED: (a) Multi-layer stacking logic with residual connections, (b) Gradient flow management for deep networks, (c) Layer-wise learning rate scheduling if needed, (d) Dropout and regularization strategies appropriate for deep Transformers.\n\n8. **Imputation-Specific Adaptations**: For imputation tasks, DLinear directly applies its encoder without modification. iTransformer would need specific handling: (a) Mask-aware attention mechanisms to prevent information leakage from masked positions, (b) Modified loss computation focusing only on imputed positions, (c) Potentially different normalization strategies when dealing with partial observations. NEW IMPLEMENTATION NEEDED: Mask integration into self-attention computation and loss calculation specific to imputation scenarios."
  },
  {
    "source": "TimesNet_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "KANAD_2024",
    "type": "unknown",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "KANAD_2024",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "DLinear_2022",
    "target": "KANAD_2024",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Informer_2020",
    "target": "KANAD_2024",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "Autoformer_2021",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Forecasting Foundation**: Both papers address multivariate time series forecasting problems with the same input-output formulation (input-I-predict-O). The basic data embedding components from Autoformer (TokenEmbedding, PositionalEmbedding, TemporalEmbedding) can be directly reused for LightTS, as both require similar temporal feature encoding mechanisms.\n\n2. **Decomposition Philosophy**: While implemented differently, both models recognize the importance of separating temporal patterns. Autoformer explicitly uses series_decomp to extract trend-cyclical and seasonal components, while LightTS implicitly achieves pattern separation through continuous (short-term local) and interval (long-term global) sampling. The moving_avg operation from Autoformer's series_decomp block could inform preprocessing strategies for LightTS.\n\n3. **Multi-scale Temporal Pattern Capture**: Both architectures capture patterns at different temporal scales. Autoformer uses Auto-Correlation to discover period-based dependencies across multiple lags, while LightTS uses continuous sampling (C consecutive tokens) for local patterns and interval sampling (tokens with fixed intervals) for global patterns. The core insight of processing time series at multiple resolutions is shared.\n\n4. **Layer-wise Feature Extraction**: Both employ stacked layers for progressive feature refinement. Autoformer's EncoderLayer/DecoderLayer structure with residual connections parallels LightTS's stacked IEBlocks. The residual connection pattern (x + dropout(layer(x))) from Autoformer can guide the implementation of skip connections in LightTS's IEBlocks.\n\n5. **Normalization Strategies**: Autoformer's my_Layernorm (which removes bias for seasonal components) provides insights for normalizing features in LightTS, particularly when handling the down-sampled sub-sequences that may have different statistical properties.",
    "differences": "1. **Core Architecture Paradigm - NEW IMPLEMENTATION REQUIRED**: \n   - Autoformer: Transformer-based with Auto-Correlation mechanism replacing self-attention, using Q-K-V projections and FFT-based autocorrelation computation\n   - LightTS: Pure MLP-based architecture with no attention mechanism whatsoever\n   - **Implementation Need**: Complete IEBlock structure with temporal/channel/output projections (MLP layers with weight sharing), bottleneck design (F' << F), and 2D matrix operations along different dimensions\n\n2. **Temporal Dependency Discovery - NEW IMPLEMENTATION REQUIRED**:\n   - Autoformer: Uses FFT-based autocorrelation (Equation 8) to compute time-delay similarities R(τ), then aggregates similar sub-series via Roll operations based on top-k period lengths\n   - LightTS: Uses sampling-oriented approach with continuous sampling (Equation 1: consecutive tokens) and interval sampling (Equation 2: tokens with fixed intervals)\n   - **Implementation Need**: Custom sampling functions to transform input X_t ∈ R^T into X_t^con ∈ R^(C×T/C) and X_t^int ∈ R^(C×T/C), no FFT or autocorrelation computation needed\n\n3. **Information Aggregation Strategy - NEW IMPLEMENTATION REQUIRED**:\n   - Autoformer: Series-wise connections through time_delay_agg (training vs inference versions), aggregating rolled sequences weighted by softmax-normalized autocorrelations\n   - LightTS: Token-wise and channel-wise projections through IEBlock's three-stage process (temporal projection: R^H→R^F', channel projection: R^W→R^W, output projection: R^F'→R^F)\n   - **Implementation Need**: Weight-shared MLPs applied column-wise and row-wise on 2D feature matrices, with explicit bottleneck architecture where F' is a small fraction of F\n\n4. **Model Structure and Data Flow - NEW IMPLEMENTATION REQUIRED**:\n   - Autoformer: Encoder-decoder architecture with separate seasonal/trend-cyclical paths, progressive trend accumulation in decoder (Equation 4: T_de^l = T_de^(l-1) + weighted trends)\n   - LightTS: Two-part sequential architecture: Part I (independent per-variable feature extraction with IEBlock-A/B on sampled sequences) → Part II (cross-variable interaction with IEBlock-C on concatenated features R^(2F×N))\n   - **Implementation Need**: Part I processes each time series independently through two parallel IEBlock branches (continuous and interval sampling), then concatenates results; Part II takes R^(2F×N) input and outputs R^(L×N) predictions\n\n5. **Sequence Length Handling - NEW IMPLEMENTATION REQUIRED**:\n   - Autoformer: Handles full-length sequences with O(L log L) complexity through FFT, with encoder processing length-I input and decoder generating length-O output\n   - LightTS: Down-samples to C×(T/C) matrices, drastically reducing computational burden for long sequences (only processes sub-sequences of length C instead of full length T)\n   - **Implementation Need**: Efficient down-sampling operations that preserve all tokens but reorganize them into non-overlapping sub-sequences, followed by linear down-projection from R^(T/C)→R^1 to aggregate sub-sequence features\n\n6. **Cross-Variable Interaction Mechanism - NEW IMPLEMENTATION REQUIRED**:\n   - Autoformer: No explicit cross-variable modeling in encoder; decoder uses encoder-decoder Auto-Correlation for cross-information but primarily focuses on temporal dependencies\n   - LightTS: Explicit cross-variable interaction in Part II through channel projection (MLP applied on each row of R^(2F×N) matrix), treating different time series variables as channels\n   - **Implementation Need**: Channel-wise MLP (R^N→R^N) applied F' times after temporal projection, enabling explicit modeling of interdependencies among N time series variables\n\n7. **Feature Dimension Management - NEW IMPLEMENTATION REQUIRED**:\n   - Autoformer: Uses fixed d_model throughout encoder/decoder, with d_ff=4*d_model for feedforward expansion\n   - LightTS: Uses bottleneck design with three distinct dimensions (H for input temporal, F' for bottleneck << F, F for output features), enabling efficient computation while maintaining expressiveness\n   - **Implementation Need**: Three-stage projection architecture with carefully tuned F' hyperparameter (typically F'=16 or 32 while F=256-512), requiring proper initialization and regularization for narrow bottleneck layers"
  },
  {
    "source": "Informer_2020",
    "target": "LightTS_2022",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers employ an encoder-decoder paradigm for time series processing, though with different internal mechanisms. The Informer implementation provides a solid foundation for understanding how to structure encoder-decoder flows, including input embedding (DataEmbedding), normalization layers, and projection heads. The target paper can reuse the overall architectural pattern and data flow logic.\n\n2. **Input Embedding and Positional Encoding**: Both models require temporal feature extraction from raw time series. Informer's DataEmbedding module (combining value embedding, temporal encoding, and positional encoding) provides reusable components for LightTS. The embedding strategy for handling temporal features (x_mark_enc, x_mark_dec) can be adapted, though LightTS may simplify this given its MLP-based nature.\n\n3. **Multi-Step Forecasting Framework**: Both papers address the long sequence time series forecasting (LSTF) problem with similar problem formulations - given historical sequence X_t of length T, predict future L steps. The training loop structure, loss computation (MSE), and evaluation metrics from Informer's implementation can be directly reused. The decoder's generative inference concept (using start tokens) in Informer provides insights for LightTS's prediction mechanism.\n\n4. **Batch Processing and Data Pipeline**: Informer's implementation demonstrates efficient batch processing for multivariate time series with shapes [B, L, N]. This batching strategy, including handling of temporal marks and mask tensors, can be directly adopted for LightTS. The data normalization and preprocessing pipelines are transferable.\n\n5. **Layer Normalization and Residual Connections**: Both architectures benefit from normalization strategies. Informer's use of LayerNorm in encoder/decoder layers provides a template for stabilizing training in deep architectures. LightTS's IEBlocks can incorporate similar normalization patterns from Informer's implementation.\n\n6. **Projection Layers for Output Generation**: Both models use linear projection layers to map from hidden representations to output predictions. Informer's final projection layer (nn.Linear(d_model, c_out)) demonstrates how to handle multivariate output generation, which is directly applicable to LightTS's output projection needs.",
    "differences": "1. **Core Attention Mechanism vs. Pure MLP Architecture**: Informer's ProbSparse attention mechanism (with query sparsity measurement, Top-u selection, and O(L log L) complexity) is fundamentally different from LightTS's pure MLP-based Information Exchange Blocks. NEW IMPLEMENTATION NEEDED: (a) Continuous and interval sampling methods (Equations 1-2) that transform sequences into 2D matrices without using attention, (b) IEBlock architecture with temporal projection, channel projection, and output projection using only MLPs with weight sharing, (c) Bottleneck design where F' << F to reduce computational cost in channel projections.\n\n2. **Sampling-Oriented Feature Extraction vs. Self-Attention Distilling**: Informer uses self-attention distilling with Conv1d and MaxPooling (Equation 5) to progressively reduce sequence length in the encoder, creating a pyramid structure. LightTS uses a completely different approach. NEW IMPLEMENTATION NEEDED: (a) Two parallel sampling paths (continuous sampling for local patterns, interval sampling for global patterns) that operate independently on each time series, (b) Feature extraction through IEBlock-A and IEBlock-B that process C × T/C matrices, (c) Down-projection from R^(F × T/C) to R^F using simple linear mapping, eliminating the need for convolutional distilling.\n\n3. **Variable Independence vs. Cross-Attention**: Informer's decoder uses cross-attention between encoder and decoder representations to capture dependencies. LightTS explicitly separates temporal pattern learning (Part I, treating variables independently) from interdependency learning (Part II, concatenating all features). NEW IMPLEMENTATION NEEDED: (a) Part I: Process each of N time series independently through sampling and IEBlocks, producing N feature vectors of dimension F, (b) Part II: Concatenate features into R^(2F × N) matrix (combining continuous and interval features), (c) IEBlock-C that performs both temporal fusion (2F dimension) and cross-variable communication (N dimension) simultaneously through channel/temporal projections.\n\n4. **Query-Key-Value Mechanism vs. Column-Row MLP Operations**: Informer's attention computes scaled dot-products between queries, keys, and values with complexity considerations. LightTS uses a fundamentally different information exchange pattern. NEW IMPLEMENTATION NEEDED: (a) Temporal projection: MLP of R^H → R^F' applied column-wise with weight sharing across all W columns, (b) Channel projection: MLP of R^W → R^W applied row-wise with weight sharing across all F' rows, (c) Output projection: MLP of R^F' → R^F applied column-wise, (d) All projections use simple feedforward networks without attention scores or softmax operations.\n\n5. **Generative Decoding with Start Tokens vs. Direct Prediction**: Informer uses a start token mechanism (X_token from earlier slice) and masked attention for autoregressive-style generation. LightTS directly predicts all L future steps through IEBlock-C output. NEW IMPLEMENTATION NEEDED: (a) Remove decoder structure entirely - LightTS has no separate decoder component, (b) IEBlock-C directly outputs R^(L × N) predictions without iterative generation, (c) No masking mechanism needed since predictions are made in one forward pass, (d) Training uses direct supervision on all L output steps simultaneously.\n\n6. **Distilling Strategy vs. Bottleneck Design**: Informer's encoder uses replicated stacks with progressive halving and concatenation of feature maps. LightTS uses a bottleneck strategy within IEBlocks. NEW IMPLEMENTATION NEEDED: (a) Bottleneck IEBlock design where temporal projection reduces H to F' (F' << H and F' << F), (b) Channel projection operates on reduced F' dimension instead of original H, improving efficiency, (c) Output projection expands F' to final F dimension, (d) No multi-stack replication or feature map concatenation from different scales, (e) Single-path processing for each sampling method rather than pyramid structure.\n\n7. **Complexity Focus**: Informer emphasizes O(L log L) complexity through sparse attention and efficient query selection (Lemma 1, max-mean measurement). LightTS achieves efficiency through sampling-based dimension reduction and lightweight MLP operations. NEW IMPLEMENTATION NEEDED: (a) Implement C-length sub-sequence creation (C << T) to reduce computational burden, (b) Weight-shared MLPs across columns/rows instead of attention matrices, (c) No need for Top-u query selection, LSE operations, or sparsity measurements, (d) Complexity reduction comes from processing C × T/C matrices instead of full T-length sequences with attention."
  },
  {
    "source": "Reformer_2020",
    "target": "LightTS_2022",
    "type": "cross-domain",
    "relation": null
  },
  {
    "source": "FEDformer_2022",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Framework**: Both papers adopt seasonal-trend decomposition as a fundamental design principle. FEDformer uses MOEDecomp (Mixture of Experts Decomposition) with multiple average pooling filters of different sizes, while MICN uses MHDecomp (Multi-scale Hybrid Decomposition) with similar multi-scale averaging. The core decomposition logic from FEDformer's `series_decomp` class can be directly reused as a baseline, requiring only modification to support MICN's mean-based aggregation instead of learned weights.\n\n2. **Multi-scale Pattern Extraction**: Both models recognize that time series contain patterns at different temporal scales. FEDformer's frequency domain approach (selecting different Fourier modes) and MICN's multi-scale convolution branches share the conceptual goal of capturing multi-resolution features. The infrastructure for managing multiple parallel processing paths in FEDformer's encoder/decoder layers can inform MICN's multi-branch MIC layer implementation.\n\n3. **Embedding Strategy**: Both use comprehensive embedding combining temporal features, positional encoding, and value embedding. FEDformer's `DataEmbedding` class (combining TimeFeatureEmbedding, PositionalEmbedding, and TokenEmbedding) can be directly reused for MICN with minor adaptations to handle MICN's concatenation of input series with zero placeholders.\n\n4. **Normalization and Residual Connections**: Both architectures extensively use layer normalization and residual connections throughout their blocks. FEDformer's `my_Layernorm` and residual connection patterns in EncoderLayer/DecoderLayer can serve as templates for MICN's MIC layer normalization structure.\n\n5. **Forecasting Task Formulation**: Both papers frame long-term forecasting as predicting future sequence of length O from past sequence of length I, using similar notation and problem setup. The overall training loop, loss computation (MSE), and evaluation metrics can be largely reused from FEDformer's implementation.",
    "differences": "1. **Core Architecture Paradigm**: FEDformer operates in frequency domain using Fourier/Wavelet transforms with attention mechanisms (FEB, FEA blocks), while MICN uses spatial domain convolutions (isometric convolution, Conv1d/Conv2d). MICN requires implementing: (a) Isometric convolution with specific padding strategy (pad S-1 zeros for sequence length S with kernel=S), (b) Local-global module combining downsampling Conv1d and upsampling Conv1dTranspose, (c) Conv2d-based merging of multi-scale branches. None of these convolution-based components exist in FEDformer's frequency-domain implementation.\n\n2. **Encoder-Decoder vs. Direct Prediction**: FEDformer uses traditional encoder-decoder architecture with cross-attention between encoder output and decoder, requiring separate processing paths. MICN eliminates the encoder-decoder structure, using a simpler approach: concatenate input series X_s with zero placeholders X_zero of length O, then directly process through stacked MIC layers. This requires implementing a new forward pass logic without cross-attention, replacing FEDformer's complex Encoder/Decoder classes with a sequential MIC layer stack.\n\n3. **Trend-Cyclical Prediction Strategy**: FEDformer accumulates trend components extracted from decomposition blocks throughout the decoder layers (equation 2 in FEDformer). MICN proposes two explicit strategies: (a) MICN-regre: simple linear regression on X_t, (b) MICN-mean: mean value of X_t. This requires implementing new `regression()` function for trend prediction, which is absent in FEDformer's implicit trend accumulation approach.\n\n4. **Attention Mechanism Replacement**: FEDformer's core innovation is frequency-enhanced attention (FourierBlock, FourierCrossAttention with FFT operations, mode selection, complex multiplication). MICN completely removes attention mechanisms, replacing them with isometric convolution for global correlation modeling. The entire attention infrastructure (Q/K/V projections, attention score computation, FourierBlock/MultiWaveletTransform classes) is not needed for MICN. Instead, implement isometric convolution as a global correlation alternative.\n\n5. **Multi-scale Branch Merging**: FEDformer uses Softmax-weighted mixing in MOEDecomp (learned data-dependent weights via L(x) in equation 10). MICN uses: (a) simple mean for decomposition (equation 2), (b) Conv2d for merging seasonal predictions from different scale branches (equation 9). This requires implementing Conv2d-based weighted merging where weights are learned through convolution kernels rather than attention-like softmax weights.\n\n6. **Sequence Length Handling**: FEDformer maintains sequence length through padding in Fourier/Wavelet transforms, with decoder input being I/2 + O. MICN uses I + O throughout processing via zero placeholder concatenation, then truncates to O at the end. This fundamental difference requires reimplementing the data flow: MICN needs Concat(X_s, X_zero) → MIC layers → Projection → Truncate, versus FEDformer's encoder(I) → decoder(I/2 + O) flow.\n\n7. **Local Feature Extraction**: FEDformer extracts features through frequency domain operations (selecting specific Fourier modes, wavelet decomposition). MICN uses spatial downsampling via Conv1d with stride=kernel=i for different scales i ∈ {I/4, I/8, ...}, followed by isometric convolution on compressed features, then Conv1dTranspose upsampling. This entire local-global module with scale-specific downsampling/upsampling is new and requires implementation from scratch."
  },
  {
    "source": "Autoformer_2021",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Series Decomposition Philosophy**: Both papers adopt decomposition-based architectures that separate time series into seasonal and trend-cyclical components. Autoformer uses `series_decomp` with moving average (AvgPool), while MICN extends this with Multi-scale Hybrid Decomposition (MHDecomp). **Code Reuse**: The `series_decomp` class and `moving_avg` class from Autoformer can be directly adapted as the foundation for MICN's MHDecomp block - simply extend it to support multiple kernel sizes and mean aggregation across scales.\n\n2. **Embedding Strategy**: Both use similar embedding approaches combining value embedding, positional encoding, and temporal features. Autoformer's `DataEmbedding_wo_pos` with `TokenEmbedding`, `PositionalEmbedding`, and `TemporalEmbedding`/`TimeFeatureEmbedding` can be reused. **Code Reuse**: MICN's embedding module (Equation 6) can directly leverage Autoformer's embedding classes with minor modifications to handle the concatenated input `Concat(X_s, X_zero)`.\n\n3. **Trend-Cyclical Prediction Approach**: Both papers handle trend prediction separately from seasonal modeling. Autoformer accumulates trend through decoder layers with projection layers (W_l,i), while MICN uses simpler linear regression or mean strategies. **Code Reuse**: The linear projection mechanism from Autoformer's decoder can be adapted for MICN's trend prediction block, particularly for the regression-based approach (MICN-regre).\n\n4. **Normalization and Residual Connections**: Both employ layer normalization and residual connections throughout their architectures. Autoformer's `my_Layernorm` (special seasonal layernorm) and standard residual patterns can be reused. **Code Reuse**: The normalization patterns in Autoformer's encoder/decoder layers can be directly applied to MICN's MIC layers.\n\n5. **Task Formulation**: Both address the input-I-predict-O long-term forecasting problem with similar data flow patterns. For imputation tasks, both would use encoder-only architectures focusing on reconstructing the input sequence. **Code Reuse**: The overall model structure (embedding → processing layers → projection) follows similar patterns, allowing architectural scaffolding reuse.",
    "differences": "1. **Core Innovation - Multi-scale Isometric Convolution vs. Auto-Correlation**: Autoformer's key contribution is the Auto-Correlation mechanism using FFT to discover period-based dependencies and time delay aggregation (Equations 5-8). MICN completely replaces this with Multi-scale Isometric Convolution (MIC) layers that use Conv1d for local feature extraction and IsometricConv for global correlations. **New Implementation Needed**: The entire MIC layer architecture including: (a) Multi-scale branching with different kernel sizes {I/4, I/8, ...}, (b) Local module with downsampling Conv1d (stride=kernel), (c) IsometricConv operation (padding S-1 zeros with kernel=S), (d) Conv1dTranspose for upsampling, (e) Conv2d-based merge operation for weighted pattern fusion.\n\n2. **Decomposition Strategy - Single vs. Multi-scale**: Autoformer uses single-kernel moving average decomposition, while MICN introduces Multi-scale Hybrid Decomposition (MHDecomp) that applies multiple kernels and averages results (Equation 2). **New Implementation Needed**: Extend the decomposition block to: (a) Support multiple kernel sizes simultaneously, (b) Compute decomposition for each kernel, (c) Average the trend components across kernels, (d) Derive seasonal as residual from averaged trend.\n\n3. **Global Correlation Modeling - Self-Attention vs. Isometric Convolution**: Autoformer uses AutoCorrelationLayer (multi-head with Q/K/V projections) for capturing dependencies, while MICN proposes IsometricConv as an alternative to self-attention for shorter sequences. **New Implementation Needed**: Implement IsometricConv module that: (a) Pads input sequence of length S with S-1 zeros, (b) Applies Conv1d with kernel_size=S, (c) Provides translation equivariance for global temporal inductive bias, (d) Enables sequential inference without encoder-decoder structure.\n\n4. **Architecture Pattern - Encoder-Decoder vs. Single-Path with Complementary Strategy**: Autoformer uses traditional encoder-decoder architecture with cross-attention between encoder output and decoder, while MICN simplifies to a single-path architecture with zero-padding complementary strategy (Concat(X_s, X_zero)). **New Implementation Needed**: (a) Remove encoder-decoder separation, (b) Implement direct zero-padding of prediction length O to input, (c) Stack MIC layers sequentially without cross-attention, (d) Use truncate operation to extract final O-length prediction.\n\n5. **Trend Prediction Mechanism - Accumulation vs. Regression/Mean**: Autoformer progressively accumulates trend through decoder layers with learned projections (T_de^l = T_de^{l-1} + W_{l,1}*T_de^{l,1} + ...), while MICN uses either simple linear regression or mean operation (Equations 3-4). **New Implementation Needed**: (a) Implement linear regression module for trend prediction, (b) Implement mean-based trend prediction as alternative, (c) Remove progressive accumulation structure, (d) Apply trend prediction once after decomposition.\n\n6. **Pattern Merging Strategy - No Explicit Merging vs. Conv2d Weighted Fusion**: Autoformer doesn't explicitly merge multiple patterns (single decomposition path), while MICN uses Conv2d to learn weighted fusion of multi-scale patterns (Equation 9). **New Implementation Needed**: Implement Conv2d-based merge module that: (a) Takes multiple branch outputs as input channels, (b) Learns optimal weighting for different temporal patterns, (c) Outputs merged representation for subsequent processing.\n\n7. **Imputation-Specific Adaptations**: For imputation tasks, Autoformer would use encoder-only architecture with masked attention, while MICN's design needs adaptation since it's primarily for forecasting. **New Implementation Needed**: (a) Modify MICN to handle masks in input, (b) Adapt zero-padding strategy for imputation (mask positions instead of future positions), (c) Ensure IsometricConv respects mask patterns, (d) Modify loss computation to focus only on masked positions."
  },
  {
    "source": "Informer_2020",
    "target": "MICN_2023",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both models adopt encoder-decoder paradigms for time series processing. MICN can reuse Informer's DataEmbedding class (value embedding + positional encoding + time features encoding) with minor modifications. The embedding infrastructure including temporal feature extraction (MinuteOfHour, HourOfDay, DayOfWeek, etc.) can be directly leveraged.\n\n2. **Input Padding Strategy**: Both use placeholder padding for future sequences. Informer uses X_token (start token) + X_0 (zero placeholders) in decoder input (Eq. 6), while MICN uses Concat(X_s, X_zero) for seasonal prediction (Eq. 5). The padding logic and concatenation operations from Informer's decoder can be adapted for MICN's embedding layer.\n\n3. **Normalization and Residual Connections**: Both employ LayerNorm and residual connections extensively. Informer's DecoderLayer structure with norm1, norm2, norm3 and residual additions can inform MICN's normalization strategy in MIC layers (Eq. 8-9). The dropout and normalization patterns are similar.\n\n4. **Multi-layer Stacking**: Both use multiple layers for feature extraction. Informer's encoder stacking with distilling (ConvLayer between layers) provides a template for MICN's N-layer MIC stacking. The layer-wise processing loop structure can be reused.\n\n5. **Projection to Output**: Both use final linear projection to map hidden states to output dimension. Informer's self.projection (nn.Linear(d_model, c_out)) directly maps to MICN's Projection operation (Eq. 10). This component is identical in purpose and implementation.",
    "differences": "1. **Core Architecture: Attention vs. Convolution**: Informer uses ProbSparse self-attention with O(L log L) complexity for global dependency modeling, including query sparsity measurement (Eq. 2), Top-u query selection, and sparse attention computation (Eq. 3). MICN completely replaces this with **isometric convolution** for global correlation modeling (Eq. 8), requiring NEW implementation of: (a) Isometric convolution with kernel=S and padding=S-1 (Figure 4), (b) Conv1d for local downsampling with stride=kernel, (c) Conv1dTranspose for upsampling, (d) Conv2d for multi-scale merging (Eq. 9). None of Informer's attention mechanism (ProbAttention, AttentionLayer, masking) can be reused.\n\n2. **Time Series Decomposition**: Informer has NO explicit decomposition mechanism. MICN introduces **Multi-scale Hybrid Decomposition (MHDecomp)** as a core innovation, requiring NEW implementation of: (a) Multi-kernel AvgPool with different kernel sizes (kernel_1, ..., kernel_n) in Eq. 2, (b) Mean aggregation across multiple decomposed patterns, (c) Separate trend-cyclical (X_t) and seasonal (X_s) pathways. This is completely absent in Informer.\n\n3. **Trend-Cyclical Prediction Block**: Informer has no trend modeling. MICN adds NEW **linear regression strategy** (Eq. 3) or mean strategy (Eq. 4) for trend prediction (Y_t^regre or Y_t^mean). This requires implementing a separate regression module that operates on X_t and produces O-length predictions, then adding Y_t + Y_s for final output.\n\n4. **Multi-scale Processing**: Informer uses single-scale processing with optional distilling (MaxPool stride=2 after Conv1d in Eq. 5). MICN implements **multi-scale isometric convolution (MIC)** with multiple branches (i ∈ {I/4, I/8, ...}) processing different temporal patterns simultaneously. NEW implementation needed: (a) Parallel branch processing with different scale sizes, (b) Scale-specific AvgPool in Eq. 7, (c) Local-global module for each scale (Figure 3), (d) Conv2d-based weighted merging across scales (Eq. 9) instead of simple concatenation.\n\n5. **Masking and Missing Value Handling**: Informer uses TriangularCausalMask and ProbMask for attention masking in decoder (preventing future information leakage). MICN's convolution-based approach does NOT use attention masks. For imputation tasks, MICN would need NEW mask-aware convolution implementation where: (a) Masked positions are handled in convolution operations, (b) Loss computation focuses on masked positions only, (c) No ProbMask or attention-based masking logic is applicable.\n\n6. **Computational Complexity Focus**: Informer optimizes for O(L log L) complexity through sparse attention (ProbSparse mechanism with U=L_K*ln(L_Q) sampling). MICN achieves efficiency through: (a) Local downsampling reducing sequence length to (I+O)/i, (b) Isometric convolution on compressed sequences, (c) No complexity analysis provided but relies on convolution efficiency. The optimization strategies are fundamentally different.\n\n7. **Decoder Design**: Informer uses standard Transformer decoder with masked self-attention + cross-attention (DecoderLayer with two attention sublayers). MICN has NO separate decoder - the Seasonal Prediction Block acts as both encoder and decoder, directly producing O-length output through truncation (Eq. 10). This architectural difference means MICN needs a completely NEW single-path prediction structure.\n\n8. **Feature Extraction Strategy**: Informer's encoder uses self-attention distilling with Conv1d (kernel=3) + ELU + MaxPool for feature map refinement (Eq. 5). MICN uses **local-global modules** with scale-specific Conv1d (kernel=i, stride=i) for local compression + isometric convolution for global modeling (Eq. 7-8). The convolution kernel sizes, padding strategies, and activation functions (Tanh in MICN vs ELU in Informer) are all different, requiring NEW implementation."
  },
  {
    "source": "Reformer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Efficient Attention Mechanism Philosophy**: Both papers address the O(L²) complexity problem of standard Transformer attention for long sequences. Reformer uses LSH (Locality-Sensitive Hashing) attention to reduce complexity to O(L log L), while Pyraformer uses pyramidal attention to achieve O(L). The source implementation's `LSHSelfAttention` class demonstrates hash-bucketing and chunking strategies that inform how to implement sparse attention patterns. The `fit_length` function in ReformerLayer shows how to handle sequence length constraints, which is conceptually similar to Pyraformer's need to ensure L is divisible by C^(S-1).\n\n2. **Multi-Head Attention Framework**: Both maintain the multi-head attention paradigm with separate Q, K, V projections. The source code's `ReformerLayer` wraps the LSH attention mechanism while preserving the standard attention interface (queries, keys, values, attention_mask). This modular design can be directly reused for Pyraformer - the same `EncoderLayer` structure with different attention mechanisms can be employed. The embedding layer (`DataEmbedding`) combining positional, temporal, and value embeddings is identical in both approaches.\n\n3. **Encoder-Only Architecture for Time Series**: Both use encoder-only architectures for their primary tasks (Reformer for language modeling, Pyraformer for forecasting/imputation). The source implementation's `Encoder` class with stacked `EncoderLayer` modules can be directly adapted. The overall model structure: embedding → encoder layers → projection is preserved. For imputation tasks, both would process the input sequence through multiple attention layers and project to output space.\n\n4. **Chunking and Memory Efficiency**: Reformer's chunking strategy (dividing sequences into chunks for feed-forward computation) parallels Pyraformer's coarser-scale construction. The source code demonstrates how to handle variable-length sequences and batch processing efficiently. The memory-efficient attention implementation principle (avoiding full materialization of attention matrices) is fundamental to both approaches.\n\n5. **Layer Normalization and Residual Connections**: Both employ standard Transformer components like LayerNorm and residual connections. The source implementation's integration of these components in `EncoderLayer` (with dropout, activation functions) provides a ready-to-use template that only requires swapping the attention mechanism.",
    "differences": "1. **Core Attention Mechanism - NEW IMPLEMENTATION REQUIRED**: \n   - **Reformer**: Uses LSH hashing to group similar queries/keys into buckets, processes attention within buckets. Requires hash function implementation (random projections), bucket sorting, and chunk-based attention.\n   - **Pyraformer**: Uses pyramidal graph structure with inter-scale (parent-child) and intra-scale (neighboring nodes) connections. Requires implementing: (a) Coarser-Scale Construction Module (CSCM) with cascaded convolutions (kernel=C, stride=C) to build the C-ary tree; (b) Custom sparse attention kernel that computes attention only for neighbors N_ℓ^(s) = A_ℓ^(s) ∪ C_ℓ^(s) ∪ P_ℓ^(s); (c) Multi-scale node concatenation before attention; (d) Custom CUDA kernel (using TVM) for efficient sparse attention computation.\n   - **Implementation Gap**: The entire pyramidal graph construction and multi-scale attention routing must be built from scratch. Cannot reuse LSH bucketing logic.\n\n2. **Sequence Organization and Hierarchy - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Operates on flat sequences with hash-based reorganization. Sequence is sorted by hash buckets dynamically during attention computation.\n   - **Pyraformer**: Requires explicit multi-resolution hierarchy with S scales. Must implement: (a) CSCM module with bottleneck structure (dimension reduction → convolutions → dimension restoration); (b) Scale-wise sequence generation (length L/C^s at scale s); (c) Node indexing system to track parent-child relationships; (d) Concatenation of all scales into unified representation for PAM input.\n   - **Implementation Gap**: Need to build the entire tree construction pipeline with convolution layers and manage multi-scale representations.\n\n3. **Attention Pattern and Connectivity - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Attention within hash buckets (typically 32-64 items per bucket) plus one chunk back. Symmetric attention patterns determined by LSH.\n   - **Pyraformer**: Asymmetric, structured attention based on graph topology: each node attends to A adjacent nodes (same scale), C children (finer scale), and 1 parent (coarser scale). Must implement: (a) Neighbor set computation N_ℓ^(s) for each node; (b) Custom attention masking for pyramidal structure; (c) Scale-aware attention routing; (d) Handling boundary nodes (no parent at top scale, no children at bottom scale).\n   - **Implementation Gap**: Cannot reuse Reformer's bucketing-based attention pattern. Need custom sparse attention kernel.\n\n4. **Prediction/Output Module - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Simple linear projection from final hidden states to output vocabulary (for language modeling) or output dimensions (for imputation).\n   - **Pyraformer**: Task-specific prediction modules: (a) Single-step: gather last nodes from all S scales → concatenate → FC layer; (b) Multi-step batch: same as single-step but map to M future steps; (c) Multi-step decoder: two full attention layers with prediction tokens, combining F_p (future embeddings) and F_e (encoder output), outputting all M steps simultaneously.\n   - **Implementation Gap**: Need to implement scale-wise feature gathering, concatenation logic, and optional decoder with full attention layers.\n\n5. **Hyperparameter Configuration and Constraints - NEW IMPLEMENTATION REQUIRED**:\n   - **Reformer**: Key parameters: bucket_size (typically 4-8), n_hashes (1-8 for multi-round hashing). Sequence length must be divisible by bucket_size × 2.\n   - **Pyraformer**: Complex constraint system: (a) L must be divisible by C^(S-1); (b) Must satisfy Equation 4 for global receptive field: L/C^(S-1) - 1 ≤ (A-1)N/2; (c) C must satisfy Equation 5 for O(1) path length; (d) Parameters: A (adjacent nodes, 3 or 5), C (tree branching factor), S (number of scales), N (attention layers).\n   - **Implementation Gap**: Need validation logic to ensure hyperparameter constraints are met, and automatic scale calculation based on sequence length.\n\n6. **Reversibility and Memory Optimization - NOT NEEDED FOR PYRAFORMER**:\n   - **Reformer**: Implements reversible layers (RevNet) to eliminate activation storage across layers, reducing memory from O(n_l × L × d_model) to O(L × d_model). Uses chunked feed-forward computation.\n   - **Pyraformer**: Does not use reversible architecture. Relies on O(L) attention complexity and sparse connectivity for memory efficiency. Standard residual connections are sufficient.\n   - **Implementation Gap**: Reformer's reversibility components should NOT be ported to Pyraformer.\n\n7. **Temporal Dependency Modeling - NEW CONCEPTUAL APPROACH**:\n   - **Reformer**: Implicit long-range dependencies through LSH grouping of similar patterns, regardless of temporal distance. Hash collisions bring distant similar tokens together.\n   - **Pyraformer**: Explicit hierarchical temporal modeling: fine-scale captures local patterns (hourly), coarse-scale captures long-range patterns (daily, weekly, monthly). Multi-resolution structure directly encodes temporal hierarchy.\n   - **Implementation Gap**: Need to design CSCM convolutions to meaningfully aggregate temporal information across scales (e.g., ensuring C aligns with natural time hierarchies like 24 hours/day)."
  },
  {
    "source": "Informer_2020",
    "target": "Pyraformer_2021",
    "type": "in-domain",
    "similarities": "1. **Encoder-Decoder Architecture Foundation**: Both papers adopt the Transformer-based encoder-decoder paradigm for time series processing. The source paper's implementation provides reusable components including:\n   - `DataEmbedding` module for input representation (combining positional, temporal, and value embeddings)\n   - Basic `EncoderLayer` and `DecoderLayer` structures with self-attention and feed-forward networks\n   - Layer normalization and dropout mechanisms\n   - The target paper can directly reuse these embedding and basic layer structures, only replacing the attention mechanism\n\n2. **Multi-Head Attention Framework**: Both utilize multi-head attention to capture temporal dependencies from different perspectives. The source implementation's `AttentionLayer` class provides:\n   - Query, Key, Value projection layers (linear transformations)\n   - Multi-head splitting and concatenation logic\n   - Output projection layer\n   - The target paper can inherit this framework structure, only replacing the inner attention computation (full attention → pyramidal attention)\n\n3. **Hierarchical Feature Extraction**: Both papers employ hierarchical/multi-scale processing:\n   - Source: `ConvLayer` with MaxPooling for self-attention distilling, progressively reducing sequence length\n   - Target: Coarser-Scale Construction Module (CSCM) using convolutions to build multi-resolution pyramidal structure\n   - The source's convolutional downsampling implementation (Conv1d + BatchNorm + ELU + MaxPool) can be adapted for the target's CSCM, changing stride and kernel size to match the C-ary tree construction\n\n4. **Positional and Temporal Encoding**: Both papers emphasize temporal context through embeddings:\n   - Source: `DataEmbedding` combines value, positional, and temporal embeddings with configurable frequency encoding\n   - Target: Uses similar embedding strategy for historical observations and covariates\n   - The source's embedding implementation can be directly reused, as both papers require the same input representation\n\n5. **Batch Processing and Masking Infrastructure**: Both handle variable-length sequences and attention masking:\n   - Source: Implements `TriangularCausalMask` and `ProbMask` for decoder self-attention\n   - Target: Requires masking for pyramidal attention neighbors\n   - The masking infrastructure and batch processing logic from the source can be adapted for the target's pyramidal graph structure",
    "differences": "1. **Core Attention Mechanism - REQUIRES NEW IMPLEMENTATION**:\n   - Source: ProbSparse attention with O(L log L) complexity through query sparsity measurement (top-u query selection based on KL divergence approximation)\n   - Target: Pyramidal attention with O(L) complexity through graph-structured sparse attention (nodes attend only to neighbors in pyramidal graph: adjacent nodes, children, and parent)\n   - **NEW IMPLEMENTATION NEEDED**: \n     a) Pyramidal graph construction logic defining neighbor sets N_ℓ^(s) = A_ℓ^(s) ∪ C_ℓ^(s) ∪ P_ℓ^(s)\n     b) Custom CUDA kernel for efficient pyramidal attention (source uses random sampling for ProbSparse, target needs structured neighbor attention)\n     c) Multi-scale attention computation across S scales instead of single-scale with query selection\n\n2. **Multi-Resolution Architecture - REQUIRES NEW IMPLEMENTATION**:\n   - Source: Self-attention distilling with sequential halving (each layer reduces sequence by half using MaxPool)\n   - Target: C-ary tree construction with S scales where each parent summarizes C children\n   - **NEW IMPLEMENTATION NEEDED**:\n     a) CSCM module with configurable C-ary tree structure (C=4 or 8) instead of fixed 2x reduction\n     b) Bottleneck structure (dimension reduction before convolution, restoration after) for parameter efficiency\n     c) Scale-wise concatenation of features from all pyramid levels (source concatenates only final encoder outputs)\n\n3. **Complexity and Path Length Guarantees - DIFFERENT THEORETICAL FOUNDATION**:\n   - Source: O(L log L) complexity with random query sampling, no explicit path length constraint\n   - Target: O(L) complexity with O(1) maximum path length when C satisfies Equation (5): ^(S-1)√L ≥ C ≥ ^(S-1)√(L/((A-1)N/2+1))\n   - **NEW IMPLEMENTATION NEEDED**:\n     a) Hyperparameter validation logic ensuring Lemma 1 constraint: L/C^(S-1) - 1 ≤ (A-1)N/2\n     b) Dynamic C selection based on sequence length L to maintain O(1) path length\n     c) Global receptive field verification across pyramid scales\n\n4. **Decoder Strategy - REQUIRES NEW IMPLEMENTATION**:\n   - Source: Generative inference with start token sampling (takes L_token length historical slice as context)\n   - Target: Two prediction modules:\n     a) Direct mapping: concatenate last nodes from all scales → FC layer → all M predictions\n     b) Decoder with full attention: uses prediction tokens F_p (zero-filled future) with two full attention layers attending to encoder output F_e\n   - **NEW IMPLEMENTATION NEEDED**:\n     a) Multi-scale feature gathering from pyramid (last node at each scale)\n     b) Dual prediction module architecture (direct vs. decoder-based)\n     c) Full attention decoder layers (different from source's ProbSparse decoder attention)\n     d) Non-autoregressive prediction outputting all M steps simultaneously\n\n5. **Neighbor Attention Implementation - REQUIRES CUSTOM CUDA KERNEL**:\n   - Source: Uses standard PyTorch operations for ProbSparse attention (random sampling + top-k selection)\n   - Target: Requires custom sparse attention pattern not supported by standard libraries\n   - **NEW IMPLEMENTATION NEEDED**:\n     a) TVM-based CUDA kernel for pyramidal attention (mentioned in paper but not in standard implementation)\n     b) Efficient neighbor lookup for A+C+1 connections per node\n     c) Avoid naive O(L²) masking approach by directly computing only required Q-K pairs\n     d) Memory-efficient storage of pyramidal graph structure\n\n6. **Scale-Specific Processing - REQUIRES NEW IMPLEMENTATION**:\n   - Source: Uniform processing across all time steps with progressive downsampling\n   - Target: Different processing for nodes at different scales (s=1 to S)\n   - **NEW IMPLEMENTATION NEEDED**:\n     a) Scale-aware attention computation (different neighbor sets per scale)\n     b) Inter-scale and intra-scale connection handling\n     c) Scale-specific positional encoding (nodes at scale s represent C^(s-1) time steps)\n     d) Feature aggregation across scales for final prediction"
  },
  {
    "source": "DLinear_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Time Series Decomposition Framework**: Both papers utilize series decomposition as a core preprocessing technique. DLinear explicitly decomposes input into trend and seasonal components using a moving average kernel (from Autoformer's `series_decomp`), while TimesNet implicitly leverages periodicity analysis through FFT-based decomposition. The `series_decomp` implementation from DLinear can be directly reused as a baseline decomposition module for TimesNet's preprocessing pipeline.\n\n2. **Direct Mapping Strategy (DMS)**: Both architectures employ direct multi-step forecasting rather than autoregressive approaches, avoiding error accumulation. DLinear's encoder directly maps input length to prediction length through linear layers (`self.Linear_Seasonal` and `self.Linear_Trend`), while TimesNet's TimesBlock also performs direct transformation from input to output through its 2D convolution pathway. The overall pipeline structure (embedding → processing → output projection) can be adapted from DLinear's code structure.\n\n3. **Channel-wise Processing**: Both models process each variate independently or with shared parameters across channels. DLinear offers both individual and shared linear layers controlled by the `individual` flag, while TimesNet processes channels through the channel dimension in its 2D tensors (`C` dimension in `X_2D`). DLinear's channel handling logic (ModuleList for individual vs single module for shared) provides a template for implementing TimesNet's channel processing strategy.\n\n4. **Residual Connection Architecture**: Both employ residual connections for stable training. DLinear uses implicit residual structure through decomposition (seasonal + trend), while TimesNet explicitly uses residual connections in Equation 4: `X_1D^l = TimesBlock(X_1D^(l-1)) + X_1D^(l-1)`. DLinear's residual implementation pattern can guide TimesNet's stacked block structure.\n\n5. **Task-Agnostic Design**: Both papers design their models to handle multiple time series tasks (forecasting, imputation, classification, anomaly detection). DLinear's `task_name` parameter and task-specific forward methods (`imputation`, `encoder`) provide a structural template for implementing TimesNet's multi-task capabilities with minimal architectural changes.",
    "differences": "1. **Core Innovation - 1D-to-2D Transformation**: TimesNet's fundamental contribution is transforming 1D time series into 2D tensors based on multi-periodicity analysis (Equations 1-3), which is completely absent in DLinear. NEW IMPLEMENTATION REQUIRED: (a) FFT-based period detection module using `torch.fft.fft()` to compute amplitude spectrum, (b) Top-k frequency selection mechanism with `torch.topk()`, (c) Reshape operation `Reshape_{p_i, f_i}()` to convert 1D sequences into multiple 2D tensors with different period lengths, (d) Padding and truncation operations to handle length compatibility. This is the most critical new component distinguishing TimesNet from linear models.\n\n2. **Spatial-Temporal Feature Extraction**: DLinear uses simple temporal linear layers operating on the time axis only, while TimesNet employs 2D Inception blocks (Equation 5) to capture both intraperiod-variation (columns) and interperiod-variation (rows) simultaneously. NEW IMPLEMENTATION REQUIRED: (a) Parameter-efficient Inception block with multi-scale 2D convolutional kernels (typically 1×1, 3×3, 5×5 convolutions), (b) 2D convolution operations replacing DLinear's 1D linear transformations, (c) Parallel processing of multiple 2D tensors with shared Inception parameters for efficiency. This requires importing and adapting vision backbone architectures.\n\n3. **Adaptive Multi-Period Aggregation**: DLinear processes a single temporal view with fixed decomposition (one trend + one seasonal), while TimesNet aggregates representations from k different periods using amplitude-based weighting (Equation 6). NEW IMPLEMENTATION REQUIRED: (a) Softmax-based amplitude normalization across selected frequencies, (b) Weighted summation mechanism combining k different 1D representations, (c) Dynamic period selection logic that varies per layer (computing `Period()` at each TimesBlock), (d) Handling variable numbers of periods across different samples or layers.\n\n4. **Hierarchical Multi-Scale Architecture**: DLinear has a flat two-component structure (seasonal + trend with single-scale moving average), while TimesNet employs stacked TimesBlocks (Equation 4) with multi-scale processing at each layer. NEW IMPLEMENTATION REQUIRED: (a) Stacked block architecture with l layers, each performing period detection → 2D transformation → Inception processing → aggregation, (b) Deep feature embedding layer converting raw inputs to d_model dimensions, (c) Layer-wise period adaptation where each block can discover different periodicities in learned representations, (d) More complex residual pathway management across multiple blocks.\n\n5. **Frequency Domain Analysis**: DLinear operates entirely in the time domain with moving average decomposition, while TimesNet heavily relies on frequency domain analysis for period discovery. NEW IMPLEMENTATION REQUIRED: (a) FFT computation module with amplitude extraction using `torch.fft.fft()` and `torch.abs()`, (b) Frequency spectrum averaging across channels using `Avg()` operation, (c) Frequency-to-period conversion logic (⌈T/f_i⌉), (d) Handling of conjugate symmetry by limiting frequency selection to [1, T/2], (e) Noise filtering through top-k selection to avoid high-frequency artifacts. This frequency analysis is fundamental to TimesNet's multi-periodicity discovery mechanism.\n\n6. **Parameter Efficiency Strategy**: DLinear uses separate linear layers for trend and seasonal (2× parameters) or shared weights across channels, while TimesNet uses a shared Inception block across k different 2D tensors to maintain parameter count invariant to k. NEW IMPLEMENTATION REQUIRED: (a) Single shared Inception module processing all k transformed 2D tensors sequentially or in batch, (b) Dynamic batch processing logic to handle variable k values, (c) Memory-efficient implementation to process multiple large 2D tensors without k× memory overhead, (d) Careful weight initialization for the shared Inception block to handle diverse period structures.\n\n7. **Loss Design for Imputation**: While both handle imputation tasks, DLinear applies loss uniformly across the sequence, whereas TimesNet's 2D structure requires careful handling of padding regions and period boundaries. NEW IMPLEMENTATION REQUIRED: (a) Mask-aware loss computation that accounts for padded zeros in 2D tensors, (b) Period-boundary handling to avoid artifacts at reshape boundaries, (c) Potentially different loss weighting for different period representations before aggregation, (d) Truncation-aware loss that only evaluates on the original sequence length T, not the padded length (p_i × f_i)."
  },
  {
    "source": "FEDformer_2022",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Embedding and Decomposition Foundation**: Both papers use DataEmbedding for input representation and series_decomp for seasonal-trend decomposition. TimesNet can directly reuse FEDformer's DataEmbedding layer (time, positional, value embeddings) for initial feature extraction. The decomposition concept is similar - FEDformer uses MOEDecomp with multiple average pooling kernels, while TimesNet implicitly decomposes through frequency domain analysis.\n\n2. **Frequency Domain Analysis**: Both leverage frequency domain transformations. FEDformer uses FFT in FourierBlock and FourierCrossAttention for mode selection and frequency-based attention. TimesNet uses FFT in Period() function to discover dominant periods via amplitude analysis. The FFT infrastructure from FEDformer (torch.fft.rfft, amplitude calculation) can be adapted for TimesNet's period detection in Equation 1-2.\n\n3. **Multi-Scale Representation Learning**: FEDformer processes multiple frequency modes (randomly selected top-k modes in frequency domain), while TimesNet processes multiple periods (top-k periods from amplitude). Both use parameter-efficient designs - FEDformer shares FEB-f blocks across modes, TimesNet shares inception blocks across different 2D tensors. The concept of selecting top-k important components and processing them with shared parameters is transferable.\n\n4. **Encoder-Decoder Architecture for Imputation**: Both can be adapted for imputation tasks using encoder-only or encoder-decoder structures. FEDformer's imputation() method shows: encode input → project to output dimension. TimesNet would follow similar pattern: embed → stack TimesBlocks → project. The residual connection pattern (Equation 4 in TimesNet, Equations 1-2 in FEDformer) is shared.\n\n5. **Masking Strategy**: For imputation, both need to handle masked inputs. FEDformer's implementation shows mask parameter in forward() and attention mechanisms. TimesNet would need similar mask handling in its 2D convolution operations, where masked positions should not contribute to feature learning.",
    "differences": "1. **Core Representation Space (CRITICAL NEW COMPONENT)**: FEDformer operates entirely in 1D temporal + frequency domain using Fourier/Wavelet transforms. TimesNet's KEY INNOVATION is transforming 1D time series into 2D tensors (Equation 3: Reshape_{p_i,f_i}) based on discovered periods, where rows=interperiod-variation, columns=intraperiod-variation. This requires implementing: (a) Period discovery via FFT amplitude analysis (Equation 1-2), (b) Padding and Reshape operations to create p_i × f_i × C tensors, (c) Reverse reshape and truncation after 2D processing. This 1D→2D→1D transformation pipeline is completely absent in FEDformer.\n\n2. **Backbone Architecture (NEW IMPLEMENTATION NEEDED)**: FEDformer uses frequency-enhanced attention blocks (FEB-f/FEB-w, FEA-f/FEA-w) with complex multiplication in frequency domain. TimesNet uses **2D Inception blocks** (Equation 5) with multi-scale 2D convolutions to capture spatial patterns in transformed 2D tensors. Need to implement: (a) Inception module with parallel 2D conv branches (different kernel sizes for multi-scale), (b) Parameter sharing across k different 2D tensors, (c) 2D convolution operations that respect the temporal 2D structure (not available in FEDformer's 1D frequency operations).\n\n3. **Aggregation Mechanism**: FEDformer aggregates frequency modes through learned complex weights (self.weights1, self.weights2 in FourierBlock) via complex multiplication. TimesNet uses **amplitude-based adaptive aggregation** (Equation 6): Softmax-normalized amplitudes from FFT as weights to combine k different 1D representations. This requires: (a) Storing and normalizing amplitude values {A_{f_1},...,A_{f_k}}, (b) Weighted sum aggregation instead of attention-based fusion, (c) No learnable aggregation parameters (data-driven via amplitudes).\n\n4. **Temporal Locality Modeling**: FEDformer captures dependencies through frequency-domain attention (global receptive field via FFT). TimesNet explicitly models **two types of localities**: intraperiod (adjacent time points in columns) and interperiod (adjacent periods in rows) through 2D convolutions. For imputation: (a) FEDformer reconstructs via frequency mode interpolation, (b) TimesNet needs to reconstruct by learning from 2D spatial patterns where masked positions lose both intraperiod and interperiod context - requires careful mask propagation through 2D reshape operations.\n\n5. **Mask Handling in Different Spaces**: FEDformer applies masks in 1D temporal space before frequency transform, affecting selected frequency modes uniformly. TimesNet must handle masks in **both 1D and 2D spaces**: (a) Mask propagation during Reshape: masked time points scatter across 2D tensor, (b) 2D convolution should avoid using masked positions in receptive field, (c) After inverse reshape, need to ensure only originally masked positions are reconstructed. This requires implementing mask transformation logic: mask_1D → mask_2D (via same Reshape) → mask-aware 2D conv → mask_2D → mask_1D (via Trunc).\n\n6. **Multi-Periodicity vs. Multi-Frequency**: FEDformer selects frequencies for spectral filtering (low-pass or random mode selection). TimesNet discovers **data-adaptive periods** (p_i = ⌈T/f_i⌉) and creates multiple 2D views with different temporal structures. For imputation: (a) FEDformer: missing values affect frequency spectrum uniformly, (b) TimesNet: missing values create irregular patterns in each 2D tensor depending on period alignment, requiring robust 2D feature extraction that handles structural irregularities across different period-based views."
  },
  {
    "source": "Pyraformer_2021",
    "target": "TimesNet_2022",
    "type": "in-domain",
    "similarities": "1. **Embedding Layer Architecture**: Both papers use similar embedding strategies combining observation embedding, covariate embedding, and positional embedding. The source paper's `DataEmbedding` module can be directly reused or adapted for TimesNet's initial feature projection layer (`Embed(·)` in Equation 4). The embedding dimension transformation logic (d_model) is consistent across both architectures.\n\n2. **Multi-Scale Hierarchical Processing**: Both models employ multi-scale temporal representations. Pyraformer uses pyramidal attention with C-ary tree structure across scales, while TimesNet uses multi-period 2D transformations. The **modular stacking pattern** in Pyraformer's encoder layers (multiple attention layers with residual connections) directly parallels TimesNet's stacked TimesBlocks with residual connections (Equation 4). The source code's layer stacking logic and residual connection implementation can be reused.\n\n3. **Normalization and Feed-Forward Components**: Both architectures use LayerNorm and position-wise feed-forward networks. Pyraformer's `PositionwiseFeedForward` class with GELU activation, dropout, and layer normalization can be directly adapted for TimesNet's feature transformation needs. The normalize-before/after strategies are applicable to both models.\n\n4. **Projection for Task-Specific Output**: Both models use linear projection layers to map learned representations to target dimensions. Pyraformer's `self.projection` layer for imputation can serve as a template for TimesNet's final output projection, requiring only dimension adjustments based on the specific task requirements.\n\n5. **Batch Processing and Tensor Operations**: The source implementation's efficient batch processing patterns (handling B×L×D tensors) and CUDA optimization considerations provide valuable guidance for implementing TimesNet's 2D tensor operations, particularly for the reshape operations and batch-wise FFT computations.",
    "differences": "1. **Core Temporal Modeling Mechanism**: Pyraformer uses **pyramidal attention with sparse Q-K pairs** (O(L) complexity) operating on 1D sequences across multiple scales, while TimesNet requires **1D-to-2D transformation with FFT-based period detection** followed by 2D convolution. NEW IMPLEMENTATION NEEDED: (a) FFT-based period discovery module (`Period(·)` in Equation 2) with amplitude calculation and Top-k frequency selection, (b) Dynamic reshape operations (`Reshape_{p_i,f_i}(·)`) to convert 1D sequences into multiple 2D tensors based on detected periods, (c) Adaptive padding/truncating logic to handle incompatible sequence lengths.\n\n2. **Feature Extraction Architecture**: Pyraformer uses **customized CUDA kernel for pyramidal attention** with inter-scale and intra-scale connections (Equations 2-3), while TimesNet requires **Inception block with multi-scale 2D convolutions**. NEW IMPLEMENTATION NEEDED: (a) 2D Inception module with parallel convolutional branches (different kernel sizes like 3×3, 5×5) operating on (p_i × f_i × d_model) tensors, (b) Shared Inception block mechanism across k different periods to maintain parameter efficiency, (c) 2D-to-1D back-transformation logic with truncation to restore original sequence length.\n\n3. **Multi-Resolution Aggregation Strategy**: Pyraformer uses **fixed C-ary tree structure** with CSCM (Coarser-Scale Construction Module) using strided convolutions to build hierarchical features bottom-up, gathering features from all pyramid levels via fixed indexing. TimesNet requires **adaptive amplitude-based weighted aggregation** (Equation 6). NEW IMPLEMENTATION NEEDED: (a) Softmax-normalized amplitude weighting mechanism that dynamically adjusts contribution of each period-based representation, (b) Weighted sum aggregation across k transformed representations, (c) Dynamic period selection logic that may vary across layers (A^{l-1} calculation for each TimesBlock).\n\n4. **Attention Mechanism vs. Convolution**: Pyraformer's core is **sparse attention with query-key-value mechanism** and attention masks for pyramidal graph structure, while TimesNet uses **parameter-free transformation followed by 2D convolution**. NEW IMPLEMENTATION NEEDED: (a) Replace all attention-related components (AttentionLayer, FullAttention, RegularMask) with 2D convolutional operations, (b) Implement locality-preserving 2D kernels that capture intraperiod-variation (column-wise) and interperiod-variation (row-wise) simultaneously, (c) Remove Q-K pair computation entirely—no attention masks or attention dropout needed.\n\n5. **Temporal Context Modeling for Imputation**: Pyraformer captures context through **multi-scale attention paths** with maximum path length O(1) via pyramidal structure, suitable for direct 1D imputation. TimesNet's imputation requires **period-aware 2D context modeling**. NEW IMPLEMENTATION NEEDED: (a) Missing value handling must account for 2D tensor structure after reshape—masks need to be transformed from 1D to 2D space, (b) Imputation loss calculation should consider both intraperiod and interperiod consistency in 2D space before back-transformation, (c) Potential modification to handle cases where missing values disrupt period detection in FFT (may need masked FFT or interpolation preprocessing).\n\n6. **Computational Complexity and Bottleneck Design**: Pyraformer uses **bottleneck structure in CSCM** (d_model → d_model/4 → d_model) to reduce parameters in convolution layers. TimesNet's parameter efficiency comes from **shared Inception block across k periods**. NEW IMPLEMENTATION NEEDED: (a) Implement parameter sharing mechanism where the same Inception block processes all k different 2D tensors {X_{2D}^{l,1}, ..., X_{2D}^{l,k}}, (b) Design efficient batching strategy to process k different tensor shapes through shared convolution layers, (c) Memory-efficient implementation for storing and processing multiple 2D representations simultaneously (potentially requiring custom CUDA kernels for reshape operations similar to Pyraformer's attention kernel)."
  }
]
