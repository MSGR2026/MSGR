{
  "id": "STAMP_2018",
  "paper_title": "STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation",
  "alias": "STAMP",
  "year": 2018,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "STAMP proposes a Short-Term Attention/Memory Priority model for session-based recommendation that explicitly captures both general interests (long-term) and current interests (short-term) of users. The core innovation is using a trilinear composition of three components: (1) attention-weighted session representation for general interests, (2) last-click embedding for current interests, and (3) candidate item embedding. The attention mechanism considers the last-click, session context, and target item simultaneously to compute attention weights, giving priority to short-term interests while capturing important items from the entire session. This differs from RNN-based approaches by avoiding complex recurrent calculations and directly modeling interest drift through explicit short-term memory priority.",
  "introduction": "# 1 INTRODUCTION\n\nSession-based Recommender systems (SRS) are an important component of modern commercial online systems, usually used for improving user experiences by making suggestions based on user behavior encoded in browser sessions, and the recommender's task is to predict users' next actions (click on an item) based on the sequence of the actions in the current session[5, 21]. Recent studies have highlighted the importance of using recurrent neural networks (RNNs) in a wide variety of recommender systems, among which the application of RNNs in session-based recommendation tasks has led to significant progress in the past few years [6, 17]. Although RNN models have been proven useful in capturing users' general interests from a sequence of actions[20], learning to predict from sessions is still a challenging problem to tackle largely due to the uncertainty inherent in user behavior and the limited information provided by browser sessions[18].\n\nBased on existing literature, almost all the RNN-based SRS models only consider modeling the session as a sequence of items, without explicitly taking into account that users' interests drift with time[6], which could be problematic in practice. For example, if a specific digital camera link has just been clicked by a user and recorded in a session, it is highly likely that the user's next intended action is in response to the current action. (1) If the current action is to browse the product description before making a purchase decision, then the user is very likely to visit another digital camera brand catalog in the next move. (2) If the current action is to add a camera into the shopping cart, then the user's browsing interest is likely be changed to other peripherals such as memory cards. In this case, to recommend another digital camera to that user would not be a good idea, albeit that the initial intention of this session is to buy a digital camera (as was reflected in the previous actions).\n\nIn typical SRS tasks, the session consists of a sequence of named items, and the user interests is hidden in these implicit feedbacks(e.g., clicks). In order to further improve the predictive accuracy of the RNN models, it is important to have the ability to learn both\n\nlong-term interests and short-term interests of such implicit feedbacks. As Jannach et al. [7] noted that both the users' short-term and long-term interests are of great importance for recommendation, but traditional RNN architectures are not designed to distinguish and exploit these two types of interests simultaneously [11].\n\nIn this study, we consider to solve this problem by introducing a recent action priority mechanism into the SRS model, called Short-Term Attention/Memory Priority (STAMP) model, which can take into account the user's interests in general and his/her current interests simultaneously. In STAMP, the users' interests in general are captured by an external memory built from all the historical clicks in a session prefix (including the last-click), and this is where the term \"Memory\" enters. The term \"last-click\" denotes the last action (item) of a session prefix, the objective of SRS is to predict the \"next click\" with regard to this \"last-click\". In this study, the embedding of the last-click is used to represent the user's current interests, and the proposed attention mechanism is built on top of it. Since the last-click is a component of the external memory it can be regarded as short-term memory of the users' interests. Similarly, the users' attention built on top of the last-click can be seen as a short-term attention. To our knowledge, this is the first effort to simultaneously take the long/short term memory into account when constructing a neural attention model for session-based recommendations. The major contributions of this study are as follows:\n\n- We introduce a short-term attention/memory priority model that learns: (a) a uniform embedding space with items across sessions and (b) a novel neural attention model for next-click prediction in session-based recommender systems.\n\n- A novel attention mechanism is proposed for implementation of the STAMP model, in which the attention weights are calculated from the session context and being enhanced with the current interests of the users. The output attention vector is read as a compositional representation of the user's temporal interests, and is more sensitive to user's interests drift with time than other neural attention based solutions. Therefore, it is capable of simultaneously capturing both the users' long-term interests in general (in response to the initial purpose) and their short-term attention (current interests). The validity and efficacy of the proposed attention mechanism is verified through comparison studies.\n\n- The proposed model is evaluated on two real world datasets, the Yoochoose dataset from RecSys 2015, and the Diginetical dataset from CIKM Cup 2016, respectively. Experimental results show that STAMP achieves state-of-the-art, and the proposed attention mechanism plays an important role.",
  "method": "# 3 METHODS\n\n# 3.1 Symbolic Description\n\nA typical session-based recommender system is built upon historical sessions, and makes prediction based upon current user sessions. Each session, denoted by  $S = [s_1, s_2, \\dots, s_N]$ , consists of a sequence of actions (items clicked by the user), where  $s_i$  represents an item (ID) clicked at time-step  $i$ .  $S_t = \\{s_1, s_2, \\dots, s_t\\}$ ,  $1 \\leq t \\leq N$  denotes a prefix of the action sequence truncated at time  $t$  with regard to session  $S$ . Let  $V = \\{v_1, v_2, \\dots, v_{|V|}\\}$  denotes a set of unique items in the SRS system, called item dictionary.\n\nLet  $X = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_{|V|}\\}$  denote the embedding vectors with respect to the item dictionary  $V$ . The proposed STAMP model learns a  $d$ -dimensional real-valued embedding  $\\mathbf{x}_i \\in \\mathbb{R}^d$  for each of the item  $i$  in  $V$ . Specifically, symbol  $\\mathbf{x}_t \\in \\mathbb{R}^d$  represents the embedding of the last click  $s_t$  of the current session prefix  $S_t$ . The goal of our models is to predict the next possible click (i.e.  $s_{t+1}$ ) based on given session prefix  $S_t$ . To be exact, our models are constructed\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/812b941d4f3ef8e668514ed7a1e36e953f0727ee3abd6a0e4c6ef0de6b968de9.jpg)  \nFigure 1: Schematic illustration of the STMP model.\n\nand trained as a classifier that learns to generate a score for each of the candidates in item dictionary  $V$ , let  $\\hat{\\mathbf{y}} = \\{\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_{|V|}\\}$  denote the output score vector, where  $\\hat{y}_i$  corresponds to the score of item  $v_i$ . After getting this prediction result, the elements in  $\\hat{\\mathbf{y}}$  are ranked in descending order, and the items corresponding to the top-k scores are used for recommendation. For notational convenience, we define the trilinear product of three vectors as:\n\n$$\n<   a, b, c > = \\sum_ {i = 1} ^ {d} a _ {i} b _ {i} c _ {i} = \\mathbf {a} ^ {T} (\\mathbf {b} \\odot \\mathbf {c}) \\tag {1}\n$$\n\nwhere  $a, b, c \\in \\mathbb{R}^d$ , and  $\\odot$  denotes the Hadamard product, i.e. the element-wise product between two vectors  $\\mathbf{b}$  and  $\\mathbf{c}$ .\n\n# 3.2 The Short-Term Memory Priority Model\n\nThe proposed STAMP model is built upon a so-called Short-Term Memory Priority model (STMP), as illustrated in Figure 1.\n\nFrom Figure 1 one can see that the STMP model takes two embeddings  $(\\mathbf{m}_s$  and  $\\mathbf{m}_t)$  as inputs, where  $\\mathbf{m}_s$  denotes the user's interests in general with respect to the current session, which is defined as the average of the external memory of the session:\n\n$$\n\\mathbf {m} _ {s} = \\frac {1}{t} \\sum_ {i = 1} ^ {t} \\mathbf {x} _ {i} \\tag {2}\n$$\n\nwhere the term external memory means the item embedding sequence of the current session prefix  $S_{t}$ . The symbol  $\\mathbf{m}_t$  denotes the current interests of the user in that session, in this study, the last-click  $\\mathbf{x}_t$  is used to represent the user's current interests:  $\\mathbf{m}_t = \\mathbf{x}_t$ . Since  $\\mathbf{x}_t$  is taken from the external memory of the session, we call it the short-term memory of the user's interests. The general interests  $\\mathbf{m}_s$  and current interests  $\\mathbf{m}_t$  are then processed with two MLP networks for the purpose of feature abstraction. The network structure of the MLP cells illustrated in Figure 1 are identical to each other, except that they have independent parameter settings. A simple MLP without hidden layer is used for feature abstraction, the operation on  $\\mathbf{m}_s$  is defined as:\n\n$$\n\\mathbf {h} _ {s} = f \\left(\\mathbf {W} _ {s} \\mathbf {m} _ {s} + \\mathbf {b} _ {s}\\right) \\tag {3}\n$$\n\nwhere  $\\mathbf{h}_s\\in \\mathbb{R}^d$  denotes the output state,  $\\mathbf{W}_s\\in \\mathbb{R}^{d\\times d}$  is a weighting matrix, and  $\\mathbf{b}_s\\in \\mathbb{R}^d$  is the bias vector.  $f(\\cdot)$  is a non-linear activation function (we use tanh in this study). The state vector  $\\mathbf{h}_t$  with regard\n\nto  $\\mathbf{m}_t$  can be calculated similar to  $\\mathbf{h}_s$ . And then, for a given candidate item  $\\mathbf{x}_i \\in V$ , the score function is defined as:\n\n$$\n\\hat {\\mathbf {z}} _ {i} = \\sigma \\left(<   \\mathbf {h} _ {s}, \\mathbf {h} _ {t}, \\mathbf {x} _ {i} >\\right) \\tag {4}\n$$\n\nwhere  $\\sigma (\\cdot)$  denotes the sigmoid function. Let  $\\hat{\\mathbf{z}}\\in \\mathbb{R}^{|V|}$  denote the vector that consists of the trilinear products  $\\hat{\\mathbf{z}}_i$  , in which each  $\\hat{\\mathbf{z}}_i$ $(i\\in [1,\\dots ,|V|])$  represents the unnormalized cosine similarity between the representation of the weighted user interests with regard to the current session prefix  $S_{t}$  and the candidate item  $\\mathbf{x}_i$  Then it is processed by a softmax function to obtain the output  $\\hat{\\mathbf{y}}$  ..\n\n$$\n\\hat {\\mathbf {y}} = \\text {s o f t m a x} (\\hat {\\mathbf {z}}) \\tag {5}\n$$\n\nwhere  $\\hat{\\mathbf{y}}\\in \\mathbb{R}^{|V|}$  denotes the output vector of the model, which represents a probability distribution over the items  $v_{i}\\in V$ , each element  $\\hat{\\mathbf{y}}_i\\in \\hat{\\mathbf{y}}$  denotes the probability of the event that item  $v_{i}$  is going to appear as the next-click in this session.\n\nFor any given session prefix  $S_{t} \\in S$  ( $t \\in [1, \\dots, N]$ ), the loss function is defined as the cross-entropy of the prediction results  $\\hat{y}$ :\n\n$$\n\\mathcal {L} (\\hat {\\mathbf {y}}) = - \\sum_ {i = 1} ^ {| V |} \\mathbf {y} _ {i} \\log \\left(\\hat {\\mathbf {y}} _ {i}\\right) + (1 - \\mathbf {y} _ {i}) \\log \\left(1 - \\hat {\\mathbf {y}} _ {i}\\right) \\tag {6}\n$$\n\nwhere  $\\mathbf{y}$  denotes a one-hot vector exclusively activated by  $s_{t + 1} \\in S$  (the ground truth). For example, if  $s_{t + 1}$  denotes the  $i$ -th element  $v_i$  in item dictionary  $V$ , then  $\\mathbf{y}_k = 1$ , if  $i == k$ , and  $\\mathbf{y}_k = 0$  if  $i \\neq k$ . An iterative stochastic gradient descent (SGD) optimizer is then performed to optimize the cross-entropy loss.\n\nFrom the definition of the STMP model (Equation 4) one can see that it makes predictions on the next-click based on the inner product of the candidate item and the weighted user interests, where the weighted user interests are represented through bilinear composition of the long-term memory (averaged historical clicks) and the short-term memory (the last-click). The validity of this trilinear composition model is verified in Section 4.5, the experimental results demonstrate that the proposed short-term memory priority mechanism can be very effective in capturing users' temporal interests that benefit the next-click prediction, and it achieves state-of-the-art performance on all the benchmark data sets.\n\nHowever, as can be seen from Equation 2, when modeling the user's interests in general  $\\mathbf{m}_s$  from the external memory of the current session, the STMP model treats each item in the session prefix as equally important, which we consider would be problematic in capturing the user's interests drift (probably caused by unintended clicks), especially in case of long sessions. Therefore, we propose an attention model to tackle this problem - which has been demonstrated effective in capturing the attention drift in long sequences. The proposed attention model is designed based on the STMP model, and it follows the same idea as STMP in that it also gives priority to short-term attention, hence we call it the Short-Term Attention/Memory Priority Model (STAMP).\n\n# 3.3 The STAMP Model\n\nThe architecture of the STAMP model is illustrated in Figure 2. As can be seen from Figure 2, the only difference between these two models is that in the STMP model the abstract feature vector of user's interests in general (the state vector  $\\mathbf{h}_s$ ) is calculated from the average of the external memory  $\\mathbf{m}_s$ , while in STAMP model the  $\\mathbf{h}_s$  is calculated from an attention based user's interests in general\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/690444b79cca445f1d76c1f8026da350416cc069499825ae36a95bf3f525034a.jpg)  \nFigure 2: Schematic illustration of the STAMP model.\n\n(a real-valued vector  $\\mathbf{m}_a$ ) as depicted in 2, which is produced by the proposed attention mechanism, called attention net.\n\nThe proposed attention net consists of two components: (1) a simple feed-forward neural network (FNN) that is responsible for generating attention weights for each of the items within the current session prefix  $S_{t}$ , and (2) an attention composite function that is responsible for calculating the attention based user's interests in general  $\\mathbf{m}_a$ . The FNN used for attention computation is defined as:\n\n$$\n\\alpha_ {i} = \\mathbf {W} _ {0} \\sigma \\left(\\mathbf {W} _ {1} \\mathbf {x} _ {i} + \\mathbf {W} _ {2} \\mathbf {x} _ {t} + \\mathbf {W} _ {3} \\mathbf {m} _ {s} + \\mathbf {b} _ {a}\\right) \\tag {7}\n$$\n\nwhere  $\\mathbf{x}_i\\in \\mathbb{R}^d$  denotes the  $i$ -th item  $s_i\\in S_t$ ,  $\\mathbf{x}_t\\in \\mathbb{R}^d$  denotes the last-click,  $\\mathbf{W}_0\\in \\mathbb{R}^{1\\times d}$  is a weighting vector,  $\\mathbf{W}_1,\\mathbf{W}_2,\\mathbf{W}_3\\in \\mathbb{R}^{d\\times d}$  are weighting matrices,  $\\mathbf{b}_a\\in \\mathbb{R}^d$  is a bias vector, and  $\\sigma (\\cdot)$  denotes the sigmoid function.  $\\alpha_{i}$  represents the attention coefficient of item  $\\mathbf{x}_i$  within the current session prefix. From Equation 7 one can see that the attention coefficients of the items in a session prefix are calculated based on the embedding of the target item  $\\mathbf{x}_i$ , the last-click  $\\mathbf{x}_t$  and session representation  $\\mathbf{m}_s$ , therefore, it is capable of capturing the correlations between the target item and the long/short term memory of the user's interests. Note that in Equation 7, the short-term memory is explicitly considered, which is distinctly different from the related works, and this is why the proposed attention model is called the short-term attention priority model.\n\nAfter obtaining the attention coefficients vector  $\\alpha = (\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{t})$  with respect to the current session prefix  $S_{t}$ , the attention based user's interests in general  $\\mathbf{m}_a$  with regard to the current session prefix  $S_{t}$  can be calculated as follows, and then add the  $\\mathbf{m}_s$  in it:\n\n$$\n\\mathbf {m} _ {a} = \\sum_ {i = 1} ^ {t} \\alpha_ {i} \\mathbf {x} _ {i} \\tag {8}\n$$\n\n# 3.4 The Short-Term Memory Only Model\n\nTo evaluate the validity of the basic idea of this study, that is, assigning a priority to the short-term attention/memory of the users' behavior when making decisions according to the session (sequence of actions), in this section, we propose a Short-Term Memory Only (STMO) model, which makes predictions of the next-click  $s_{t+1}$  only based on the last-click  $s_t$  of the current session prefix  $S_t$ .\n\nSimilar to the STMP model, a simple MLP without a hidden layer is used for feature abstraction in the STMO model. The MLP takes\n\nthe last-click  $s_t$  as input, and outputs a vector  $\\mathbf{h}_t \\in \\mathbb{R}^d$  just as the \"MLP CELL B\" in STMP (see Figure 1), defined as:\n\n$$\n\\mathbf {h} _ {t} = f \\left(\\mathbf {W} _ {t} \\mathbf {x} _ {t} + \\mathbf {b} _ {t}\\right) \\tag {9}\n$$\n\nwhere  $\\mathbf{h}_t$  denotes the output state,  $\\mathbf{W}_t \\in \\mathbb{R}^{d \\times d}$  is a weighting matrix, and  $\\mathbf{b}_t \\in \\mathbb{R}^d$  is the bias vector.  $f(\\cdot)$  denotes the activation function  $\\tanh$ . Then for a given candidate item  $\\mathbf{x}_i \\in V$ , the score function is defined as the inner product between  $\\mathbf{x}_i$  and  $\\mathbf{h}_t$ :\n\n$$\n\\hat {\\mathbf {z}} _ {i} = \\mathbf {h} _ {t} ^ {T} \\mathbf {x} _ {i} \\tag {10}\n$$\n\nAfter obtaining the score vector  $\\hat{\\mathbf{z}}\\in \\mathbb{R}^{|V|}$ , one can make predictions based on the ranking list calculated with Equation 5, or optimize the parameters of the model based on Equation 6, just like the situation in STMP model.",
  "experiments": "# 4 EXPERIMENTS\n\n# 4.1 Datasets and Data Preparation\n\nWe evaluate the proposed models on two datasets, the first one is called Yoochoose from the RecSys'15 Challenge<sup>1</sup>, which consists of six months of click-streams gathered from an e-commerce web site, where the training set only contains session events. Another one is the Digimetica dataset coming from the CIKM Cup 2016<sup>2</sup>, for which only the transaction data is used in this study.\n\nFollowing [5] and [10], we filter out sessions of length 1 and items that appear less than 5 times in both of the datasets. The test set of Yoochoose consists of the sessions of subsequent days with respect to the training set, and we filter out clicks (items) that did not appear in the training set. And for Diginetica, the only difference is that we use the sessions of subsequent week for testing. After the pre-processing phase, there remains 7,966,257 sessions of 31,637,239 clicks on 37,483 items in Yoochoose dataset, and 202,633 sessions of 982,961 clicks on 43,097 items in Diginetica dataset.\n\nSame as [17], we use a sequence splitting preprocess that for an input session  $S = \\{s_1, s_2, \\ldots, s_n\\}$ , we generate the sequences and corresponding labels([s1], s2), ([s1, s2], s3)... ([s1, s2, ..., sn-1], sN) for training and testing on both datasets, which proves to be effective. Because the Yoochoose training set is quite large and training on the recent fractions yields better results than training on the entire fractions as per the experiments of [17], we use the recent fractions 1/64 and 1/4 of training sequences. The statistics of the three datasets are shown in Table 1.\n\n# 4.2Baselines\n\nThe following models, including the state-of-art and closely related work, are used as baselines to evaluate the performance of the proposed STAMP model:\n\n- POP: A naive SRS model that always recommends items based on occurrence frequency in the training set.  \n- Item-KNN[14]: An item-to-item model which recommends items similar to the existing items based on cosine similarity between the candidate item and the existing items within the session. A constraint is included to avoid coincidental high similarities between rarely visited items as in [4, 20].\n\nTable 1: Statistics of the experiment datasets.  \n\n<table><tr><td>Dataset</td><td>Yoochoose 1/64</td><td>Yoochoose 1/4</td><td>Diginetica</td></tr><tr><td># train</td><td>375,073</td><td>5,969,416</td><td>719,470</td></tr><tr><td># test</td><td>55,898</td><td>55,898</td><td>60,858</td></tr><tr><td># clicks</td><td>565,552</td><td>7,980,529</td><td>982,961</td></tr><tr><td># items</td><td>17,694</td><td>30,660</td><td>43,097</td></tr><tr><td>avg. len.</td><td>6.16</td><td>5.71</td><td>5.12</td></tr></table>\n\n- FPMC[13]: A state-of-the-art hybrid model for next-basket recommendation. In order to make it work on session-based recommendation, we do not consider the user latent representations when computing recommendation scores.  \n- GRU4Rec[5]: An RNN based deep learning model for session based recommendation, which consists of GRU units, it utilizes session-parallel mini-batch training process and also employs ranking-based loss functions during the training.  \n- GRU4Rec+[17]: A improved model based on GRU4Rec which adopts two techniques to improve the performance of GRU4Rec, including a data augmentation process and a method to account for shifts in the input data distribution.  \n- NARM[10]: An RNN based state-of-the-art model which employs attention mechanism to capture main purpose from the hidden states and combines it with the sequential behavior as final representation to generate recommendations.\n\n# 4.3 Evaluation\n\nWe use the following metrics for evaluation of the performance of the SRS models, which are also widely used in other related works.\n\nP@20: The P@K score is widely used as a measure of predictive accuracy in SRS area. P @ K represents the proportion of test cases which has the correctly recommended items in a top k position in a ranking list. In this paper, P@20 is used for all the tests, defined as:\n\n$$\nP @ K = \\frac {n _ {\\text {h i t}}}{N} \\tag {11}\n$$\n\nwhere  $N$  denotes the number of test data in the SRS system G,  $n_{hit}$  denotes the number of cases which have the desired items in top K ranking lists, a hit occurs when  $t$  appears in the top  $K$  position of the ranking list of  $G$ .\n\nMRR@20: The average of reciprocal ranks of the desired item  $t$ . The reciprocal rank is set to zero if the rank is above 20.\n\n$$\nM R R @ K = \\frac {1}{N} \\sum_ {t \\in G} \\frac {1}{\\operatorname {R a n k} (t)} \\tag {12}\n$$\n\nThe MRR is a normalized score of range [0, 1], an increase in its value reflects that the majority \"hits\" will appear higher in the ranking order of the recommendation list, which indicates a better performance of the corresponding recommender system.\n\n# 4.4 Parameters\n\nThe hyper-parameters are optimized via extensive grid search on all the data sets, and the best models are selected by early stopping based on the  $\\mathrm{P@20}$  score on the validation set. Hyper-parameter ranges for the grid search are the following: embedding dimension  $d$  in  $\\{50, 100, 200, 300\\}$ , learning rate  $\\eta$  in  $\\{0.001, 0.005, 0.01, 0.1, 1\\}$ ,\n\nTable 2: Next-click prediction on 3 benchmark data sets.  \n\n<table><tr><td>Datasets</td><td colspan=\"2\">Yoochoose 1/64</td><td colspan=\"2\">Yoochoose 1/4</td><td colspan=\"2\">Diginetica</td></tr><tr><td>Measures</td><td>P@20</td><td>MRR@20</td><td>P@20</td><td>MRR@20</td><td>P@20</td><td>MRR@20</td></tr><tr><td>POP</td><td>6.71</td><td>1.65</td><td>1.33</td><td>0.30</td><td>0.91</td><td>0.23</td></tr><tr><td>Item-KNN</td><td>51.60</td><td>21.81</td><td>52.31</td><td>21.70</td><td>28.35</td><td>9.45</td></tr><tr><td>FPMC</td><td>45.62</td><td>15.01</td><td>-</td><td>-</td><td>31.55</td><td>8.92</td></tr><tr><td>GRU4Rec</td><td>60.64</td><td>22.89</td><td>59.53</td><td>22.60</td><td>43.82</td><td>15.46</td></tr><tr><td>GRU4Rec+</td><td>67.84</td><td>29.00</td><td>69.11</td><td>29.22</td><td>57.95</td><td>24.93</td></tr><tr><td>NARM</td><td>68.32</td><td>28.76</td><td>69.73</td><td>29.23</td><td>62.58</td><td>27.35</td></tr><tr><td>STMO</td><td>64.22</td><td>25.81</td><td>66.22</td><td>26.69</td><td>58.62</td><td>25.90</td></tr><tr><td>STMP</td><td>67.79</td><td>28.63</td><td>69.19</td><td>28.94</td><td>60.91</td><td>25.34</td></tr><tr><td>STAMP</td><td>68.74</td><td>29.67</td><td>70.44</td><td>30.00</td><td>62.03</td><td>27.38</td></tr></table>\n\nlearning rate decay  $\\lambda$  in  $\\{0.75, 0.8, 0.85, 0.9, 0.95, 1.0\\}$ . According to the averaged performance, in this study we use the following hyper-parameters for all the tests on two datasets:  $\\{d:100,\\eta:0.005,\\lambda:1.0\\}$ . The mini-batch settings are: batch size: 512, epoch: 30. All weighting matrices are initialized by sampling from a normal distribution  $N(0,0.05^2)$ , and all biases are set to zeros. All the items embeddings are initialized randomly with a normal distribution  $N(0,0.002^2)$ , which are then jointly trained with other parameters.\n\n# 4.5 The Next-Click Prediction\n\nTo demonstrate the overall performance of the proposed model, we compare it with the state-of-the-art item recommendation approaches, and the numerical results on all of the benchmark data sets are illustrated in Table 2, in which the best result of each column is highlighted in boldface. As one can see from Table 2, STAMP achieves state-of-the-art performances in terms of P@20 and MRR@20 on both of the Yoochoose data sets and the Diginetica dataset, which verifies the efficacy and validity of the proposed model. The following observations can be made from table 2:\n\nThe performance of traditional methods such as Item-KNN and FPMC are not competitive, as they only outperform the naive POP model. These results help verify the importance of taking the user's behavior (interactions) into consideration in session-based recommendation tasks as the results show that making recommendations solely based on co-occurrence popularity of the items (POP), or simply taking transitions over successive items could be very problematic in making accurate recommendations. In addition, such global solutions can be time and memory consuming, making them not scalable to for large-scale datasets.\n\nAll of the neural network baselines significantly outperform conventional models, thus proving the effectiveness of deep learning technology in this field. GRU4Rec+ improves the performances of GRU4Rec by using the data augmentation techniques that split a single session into several sub-sessions for training. While GRU4Rec+ does not modify the model structure of GRU4Rec, they both only take the sequential behavior into account which may encounter difficulties with users' interest drift. NARM achieves the best performances among the baselines, because it not only models the sequential behavior using RNN with GRU units but also uses attention mechanism to capture main purpose, which indicates the importance of main purpose information in recommendations. This\n\nTable 3: The results of  $\\mathbf{P}@\\mathbf{K}$ , MRR@K when  $\\mathrm{K} = 5,10$ .  \n\n<table><tr><td>Model</td><td>Metrics</td><td>Yoochoose 1/64</td><td>Yoochoose 1/4</td><td>Diginetica</td></tr><tr><td rowspan=\"2\">NARM</td><td>P@5</td><td>44.34</td><td>44.34</td><td>40.67</td></tr><tr><td>MRR@5</td><td>26.21</td><td>26.08</td><td>25.02</td></tr><tr><td rowspan=\"2\">STAMP</td><td>P@5</td><td>45.69</td><td>46.39</td><td>41.04</td></tr><tr><td>MRR@5</td><td>27.26</td><td>27.47</td><td>25.21</td></tr><tr><td rowspan=\"2\">NARM</td><td>P@10</td><td>57.50</td><td>57.83</td><td>51.91</td></tr><tr><td>MRR@10</td><td>27.97</td><td>28.10</td><td>26.53</td></tr><tr><td rowspan=\"2\">STAMP</td><td>P@10</td><td>58.07</td><td>59.62</td><td>52.07</td></tr><tr><td>MRR@10</td><td>28.92</td><td>29.24</td><td>26.69</td></tr></table>\n\nis reasonable as part of items in the current session may reflect the user's main purpose and relate to the next item.\n\nAmong our proposed models, the STAMP model obtains the highest  $\\mathrm{P@20}$  and MRR@20 on Yoochoose dataset in 2 experiments and achieves comparable results on the Diginetica dataset. The STMO model cannot capture general interest information from previous clicks in the current session, so it generates the same recommendation whenever it encounters the same last-click, although given different sessions. Unsurprisingly the model has the worst performance in our proposed models, since it cannot take advantage of the general interest information. But compared with traditional machine learning methods such as Item-KNN and FPMC, STMO achieves significantly better performances which demonstrates the ability of our proposed model framework to learn effective uniform item embedding representation. The STMP as an extension to STMO simply uses an average pooling function to generate session representation as the long-term interest and applies last-click information to capture short-term interest. It outperforms STMO in all three experiments and performs comparably with GRU4Rec+ but a little inferior to NARM. As expected, considering both session context information and last click information is suitable for this task as STMP is able to better make the session-based recommendations for a given session. Compared with STMP, STAMP applies item-level attention mechanism and achieves  $0.95\\%$ ,  $1.25\\%$ ,  $1.12\\%$  improvements on  $\\mathrm{P@20}$  and  $1.04\\%$ ,  $1.06\\%$ ,  $2.04\\%$  on MRR@20 in three experiments respectively. The results show that the session representation generated in this way is more effective than average pooling function, which confirms that not all items in the current session are equally important in generating the next recommendation, and part of the important items can be captured by the proposed attention mechanism to model useful features of interest; the state-of-the-art results prove the validity of STAMP.\n\n# 4.6 Compare STAMP with NARM\n\nSession-based recommender systems have become an indispensable part of many e-commerce systems, helping users to sort out items of interest from large inventories. In fact, there are always more than  $10^{5}$  items in an e-commerce website and most users are only interested in viewing recommendations on the first page of real-world recommender systems [6]. In order to verify the performance of our proposed STAMP model and the recent state-of-the-art NARM model in real production environment, where recommendation systems can only suggest a few items at once, the relevant item should be amongst the first few items in the recommendation list[12]. We\n\nTable 4: Runtime of each training epoch.  \n\n<table><tr><td>Method</td><td>Dataset</td><td>Time (seconds)</td></tr><tr><td rowspan=\"3\">NARM</td><td>Yoochoose 1/64</td><td>155.3</td></tr><tr><td>Yoochoose 1/4</td><td>961.4</td></tr><tr><td>Diginetica</td><td>99.6</td></tr><tr><td rowspan=\"3\">STAMP</td><td>Yoochoose 1/64</td><td>33.3</td></tr><tr><td>Yoochoose 1/4</td><td>356.1</td></tr><tr><td>Diginetica</td><td>52.0</td></tr></table>\n\ntherefore evaluate the recommendation quality in terms of  $\\mathrm{P@5}$ , MRR@5,  $\\mathrm{P@10}$  and MRR@10 in trying to simulate the practical situation. The results are summarized in Table 3, and argue that the experimental results may to some extent reflect their performance in the real production environment. We can observe that STAMP performs well on this mission and much more competitively than NARM when evaluated under stricter rules in an simulated production environment. Our model performs consistently better than NARM and shows obvious advantages in three experiments which demonstrates the effectiveness of considering both general interests and short-term interests, and the validity of the learned item embeddings. The results prove that the proposed STAMP tends to make more accurate recommendations as seen in the above experimental results and the main results in subsection 4.5.\n\nWe also record the runtime of the recurrent neural model NARM and the proposed STAMP approach. We implement both models with the same 100-dimensional embedding vectors, and test them on the same GPU server. The training time of each epoch on three datasets is given in Table 4, which illustrates that STAMP is more efficient than NARM. We argue that this is because the NARM model contains a lot of complex operations in each GRU unit, and our proposed model is simpler and faster as it introduces a simplified neural model to save the cost of recurrent calculations in dealing with sequential inputs. All above results imply that STAMP may be more suitable for practical application since computational efficiency is crucial in real-world session-based recommender systems, which always comprise of large amounts of sessions and items.\n\n# 4.7 Effects of the last click\n\nIn this section, we design a series of contrast models to verify the validity of applying the last click information on the basis of session context to make session-based recommendations:\n\n- STMP-: On the basis of STMP, not using last click item embedding in the trilinear layer.  \n- STMP: The STMP model proposed in the paper.  \n- STAMP-: On the basis of STAMP, not using last click item embedding in the trilinear layer.  \n- STAMP: The STAMP model proposed in the paper.\n\nThe numerical results in Table 5 show that all the models in which the last click is combined with the session context vector have better performance than those without. The results prove that employing the last click positively contributes to recommendations of a given session. Our models are based on simultaneously capturing long-term and short-term interest and enhancing the last click information, which we believe is advantageous in handling\n\nTable 5: Impacts of the last-click.  \n\n<table><tr><td>Datasets</td><td colspan=\"2\">Yoochoose 1/64</td><td colspan=\"2\">Yoochoose 1/4</td><td colspan=\"2\">Diginetica</td></tr><tr><td>Measures</td><td>P@20</td><td>MRR@20</td><td>P@20</td><td>MRR@20</td><td>P@20</td><td>MRR@20</td></tr><tr><td>STMP-</td><td>60.59</td><td>21.70</td><td>62.92</td><td>24.52</td><td>57.20</td><td>21.55</td></tr><tr><td>STMP</td><td>67.79</td><td>28.63</td><td>69.19</td><td>28.94</td><td>60.91</td><td>25.34</td></tr><tr><td>STAMP-</td><td>65.19</td><td>24.95</td><td>67.96</td><td>26.67</td><td>60.15</td><td>24.47</td></tr><tr><td>STAMP</td><td>68.74</td><td>29.67</td><td>70.44</td><td>30.00</td><td>62.03</td><td>27.38</td></tr></table>\n\nlong sessions as users' interest may change during a long browsing period and the user's next action may be more related to last click that reflects a short-term interest. In order to verify the effects of last click, we investigate the P@20 with different session lengths and the results on Yoochoose 1/64 dataset are shown in Figure 3.\n\nWe first present experimental results varying the length of sessions on STMP, STAMP and NARM as shown by Figure 3(a). We can observe that when the length of sessions is above 20 the performance of NARM quickly decreases in contrast with STMP and STAMP. This suggests that short-term interests priority based models may be more powerful in handling long sessions than NARM. On the other hand, in Figure 3(b) we find that the P@20 results of STMP and STAMP when the lengths are between 1 to 30 are significantly higher than each corresponding model without feeding last click into the trilinear layer, respectively. The reason is that with current interests captured in last click or session representation, STMP and STAMP may better model the user interest for the next click recommendation. For longer sessions lengths, the performance margins between STMP- and STMP and between STAMP- and STAMP become larger. This proves that although it is important to capture general interests from the session context, explicitly taking advantage of temporal interests can enhance the quality of recommendations. Moreover, STAMP- outperforms STMP- which results from the hybrid interests captured by the attention mechanism in STAMP- while STMP- only considers the general interests; this demonstrates the importance of the last click information in the session-based recommendation task.\n\n# 4.8 Comparison among Proposed Models\n\nTo further verify the efficacy and validity of different proposed models being those that capture user interests from only last click, those that combine last click with session context, and lastly those that apply attention mechanism; we compare the models by making comparison studies on different session lengths to show their performances and the advantages in different situations. To achieve this purpose, we partition sessions into two groups: 'Short' indicates that the length of sessions is 5 events or less while 'Long' represents sessions having more than 5 events, where 5 is almost an average length of total sessions in all original data sets. The statistics on the percentage of sessions belonging to Short group are  $70.10\\%$  and  $76.40\\%$ , and to Long are  $29.90\\%$  and  $23.60\\%$  for both test datasets of Yoochoose and Diginetica. For each approach, we compute the results of  $\\mathrm{P@20}$  and MRR@20 for each length group on each data set. Experimental results are illustrated in Figure 4 (a) and (b) for Yoochoose and Diginetica respectively.\n\nFigure 4 (a) shows the results on Yoochoose. We can see that all methods obtain lower  $\\mathrm{P@20}$  and MRR@20 results in Long group in\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/3d24812e2c0b62fc899bcf7d58c458c44279e8bb88c3d1fa78c93b0d907c5e22.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/5cb033bd5c1ec132a699c4ddb6f9f71cb30ac801f7f07963c5f1c8e71286d4f8.jpg)  \n(b)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/6b68dc9c250a3f65100ee9824017dbd68d5e993883542eaa2de83a67fb2299e6.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/a6c640fd01ad4332c656b6d111a86e560682c86ff7917fd77c03c79fcdd51526.jpg)  \nFigure 3: The P@20 evaluated on different lengths of sessions' test cases in Yoochoose 1/64.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/b7434f60b24feabf836aa43af59b112dcb5bdf84868c5b080a0e18fcb3ee2e52.jpg)  \n(b)  \nFigure 4: P@20 and MRR@20 of different session lengths. (a) on Yoochoose, (b) on Diginetica.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/23b3d436bf3d39074308545288e848aca1844ee1e922d35d34a370358c86cf3f.jpg)\n\ncomparison to Short group, highlighting the challenge of making session-based recommendations for long sessions on this dataset. We suspect it may be because of difficulty in capturing users' interest drift as the session grows in length. In addition both STMP and STAMP outperform STMO in two groups and the margin becomes wider as the session length increases, meaning that a model considering both general and current interests may be more powerful in handling long sessions, in comparison to only applying the last click information for recommendations. This confirms our intuition that session context and last click information can simultaneously and effectively be used to learn user interests and predict the next selected item in session-based recommendations.\n\nFigure 4 (b)shows the results on Diginetica. STMO has better MRR@20 results than STMP, and the gap grows from  $0.38\\%$  to\n\nTable 6: Statistics of sessions have repeated items.  \n\n<table><tr><td>Dataset</td><td>2</td><td>3</td><td>4</td><td>5</td><td>&gt;5</td></tr><tr><td>Diginetica-train</td><td>0.1839</td><td>0.3272</td><td>0.4374</td><td>0.5229</td><td>0.7016</td></tr><tr><td>Diginetica-test</td><td>0.1880</td><td>0.3304</td><td>0.4420</td><td>0.5351</td><td>0.7149</td></tr><tr><td>YooChoose-train</td><td>0.1796</td><td>0.3298</td><td>0.4272</td><td>0.5091</td><td>0.7181</td></tr><tr><td>YooChoose-test</td><td>0.1770</td><td>0.3166</td><td>0.4139</td><td>0.5015</td><td>0.7563</td></tr></table>\n\n$1.11\\%$  with increasing session length. This performance probably indicates that average aggregation in STMP has its disadvantages which influence the rank of correct items in recommendations, also the results of STMO may imply the validity of the short-term interests for making accurate recommendations. Overall, STAMP is still the best performing model which also highlights the need for effective session representation to obtain hybrid interests, this proves the advantages of the proposed attention mechanism.\n\nFurthermore, Figure 4 shows that the trend between Short and Long group on the Yoochoose dataset is much different from that on the Diginetica dataset. To explain this phenomenon, we analyze the two datasets and show the ratio of sessions which have repeated clicks(i.e. the click appears at least twice within a session) in the two datasets with respect to the session length. From Table 6 we can see that the ratio of sessions which have repeated clicks in Yoochoose is smaller on Short group and larger on Long group than those in Diginetica dataset. From these results, we find that repeated clicks in the session have an impact on the recommendations, which have an inversely proportional ratio to model performance. It may be because repeated clicks may emphasize invalid information from unimportant items and make it difficult to capture user interests associated with the next action. In STAMP, we model the user interests using short-term attention priority, whereby the attention mechanism selects important items from the given session to model user interests. Both of these can effectively mitigate the impact of repeated clicks in a session. Conversely, only last click or average click information is used in other approaches, these models usually lose important information and are unable to overcome problems associated with repeated clicks. This proves the validity of short-term attention priority and the proposed attention mechanism.\n\n# 4.9 Further Investigation\n\nIn this section, we repeatedly selected random multiple sets of examples from the Yoochoose test sets for analysis, and they consistently showed the same patterns. Figure 5 illustrates the attention results of the proposed item-level attention mechanism and its advantage.\n\nIn Figure 5, the depth of the color indicates the importance of an item, the darker the color the more important an item is. Because it's hard to directly evaluate the association between each context item and the target item in the absence of item specific information, the validity of the attention mechanism can be partially explained based on the category of an item. For example, in session 11255991 we can observe that the items which have the same category with the target item have larger attention weights than other items. The category of item can reflect the interest of the user to a certain extent, and the higher weight of the item with the same category as the target item can partially prove that the attention mechanism can capture user interests for the next action.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/30b3b6ea-db59-4b8b-b4fd-89aa88639638/a6f85a4a999fd46465e122789e9ad6516b62ce58b1d0971f2a591e9ce15cf886.jpg)  \nFigure 5: Attention visualization. Attention weights is used for color-coding, the depth of the color indicates the importance of an item. The numbers above the bar are session IDs, and the category ID of each item is given below the item.\n\nOur method is capable of highlighting a number of factors in determining the next action as shown in Figure 5. Firstly, not all items are important in determining the next action and our method is able to pick important items and ignore unintended clicks. Secondly, although some important items are not near the current action in a session they can be flagged as important by our method, we believe that this demonstrates that our model is capable of capturing the users' interests in general in response to the initial or main purpose. Thirdly, items whose position is close to the end of the session often have larger weights, especially the last click item in a session with a long length. This proves our intuition that the user's intended action may be more in response to the current action. It shows that the proposed attention mechanism is sensitive to interests drift in a given session and correctly captures the current interests which is one of the reasons why STAMP can outperform other models which mainly focus on long-term interest. Moreover, the results illustrate that important items can be captured regardless of their position (i.e. beginning or end of session) in a given session (e.g. session 11255788, 11255819). This proves our conjecture that the proposed item-level attention mechanism can capture pivotal items from a global perspective to construct hybrid features of general interests and current interests. Therefore based on the visualization results, we argue that the proposed item-level attention mechanism captures important parts for predicting next action in a session by computing attention weights, enabling the model to consider both long-term interest and short-term interest and make more accurate and effective recommendations.",
  "hyperparameter": "Embedding dimension d: 100 (searched in {50, 100, 200, 300}); Learning rate η: 0.005 (searched in {0.001, 0.005, 0.01, 0.1, 1}); Learning rate decay λ: 1.0 (searched in {0.75, 0.8, 0.85, 0.9, 0.95, 1.0}); Batch size: 512; Number of epochs: 30; Weight matrix initialization: Normal distribution N(0, 0.05²); Item embedding initialization: Normal distribution N(0, 0.002²); Bias initialization: zeros; Activation function: tanh for MLP layers, sigmoid for attention computation and score function"
}