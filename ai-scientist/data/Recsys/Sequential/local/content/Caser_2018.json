{
  "id": "Caser_2018",
  "paper_title": "Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding",
  "alias": "Caser",
  "year": 2018,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "Caser (Convolutional Sequence Embedding Recommendation) applies CNN to sequential recommendation by treating the embedding matrix of L previous items as an 'image' in latent space. It uses horizontal convolutional filters of varying heights to capture union-level sequential patterns (e.g., successive item combinations) and vertical filters to capture point-level patterns through weighted aggregation of item embeddings. The model combines these convolutional layers with user embeddings to capture both short-term sequential patterns and long-term user preferences, while supporting skip behaviors by predicting T target items simultaneously.",
  "introduction": "# 1 INTRODUCTION\n\nRecommender systems have become a core technology in many applications. Most systems, e.g., top-N recommendation [9][19], recommend the items based on the user's general preferences without paying attention to the recency of items.\n\nFor example, some user always prefer Apple's products to Samsung's products. General preferences represent user's long term\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nWSDM 2018, February 5-9, 2018, Marina Del Rey, CA, USA\n\n© 2018 ACM. 978-1-4503-5581-0/18/02...$15.00\n\nDOI: 10.1145/3159652.3159656\n\nand static behaviors. Another type of user behaviors is sequential patterns where the next item or action more likely depends on the items or actions the user engaged recently. Sequential patterns represent the user's short term and dynamic behaviors and come from a certain relationship between the items within a close proximity of time. For example, a user likely buys phone accessories soon after buying an iPhone, though in general the user does not buy phone accessories. In this case, the systems that consider only general preferences will miss the opportunity of recommending phone accessories after selling an iPhone since buying phone accessories is not a long term user behavior.\n\n## 1.1 Top-  \\(N\\)  Sequential Recommendation\n\nTo model user's sequential patterns, the work in [17, 21] considers top- \\(N\\)  sequential recommendation that recommends  \\(N\\)  items that a user likely interacts with in a near future. This problem assumes a set of users  \\(\\mathcal{U} = \\{u_1, u_2, \\dots, u_{|\\mathcal{U}|}\\}\\)  and a universe of items  \\(\\bar{I} = \\{i_1, i_2, \\dots, i_{|\\bar{I}|}\\}\\) . Each user  \\(u\\)  is associated with a sequence of some items from  \\(\\bar{I}\\) ,  \\(S^u = (S_1^u, \\dots, S_{|\\mathcal{S}^u|}^u)\\) , where  \\(S_i^u \\in \\bar{I}\\) . The index  \\(t\\)  for  \\(S_t^u\\)  denotes the order in which an action occurs in the sequence  \\(S^u\\) , not the absolute timestamp as in temporal recommendation like [14, 31, 34]. Given all users' sequences  \\(S^u\\) , the goal is to recommend each user a list of items that maximize her/his future needs, by considering both general preferences and sequential patterns. Unlike conventional top- \\(N\\)  recommendation, top- \\(N\\)  sequential recommendation models the user behavior as a sequence of items, instead of a set of items.\n\n## 1.2 Limitations of Previous Work\n\nThe Markov chain based model [2, 6, 21, 30] is an early approach to top- \\(N\\)  sequential recommendation, where an  \\(L\\) -order Markov chain makes recommendations based on  \\(L\\)  previous actions. The first-order Markov chain is an item-to-item transition matrix learnt using maximum likelihood estimation. Factorized personalized Markov chains (FPMC) [21] proposed by Rendle et al. and its variant [2] improved this method by factorizing this transition matrix into two latent and low-rank sub-matrices. Factorized Sequential Prediction with Item Similarity ModeLs (Fossil) [6] proposed by He et al. generalizes this method to high-order Markov chains using a weighted sum aggregation over previous items' latent representations. However, existing approaches suffered from two major limitations:\n\nFail to model union-Level sequential patterns. As shown in Figure 1a, the Markov chain models only point-level sequential patterns where each of the previous actions (blue) influences the target action (yellow) individually, instead of collectively. FPMC\n\n![](images/4a93521316dd1a246ae5d7149aeaba6ee62d24ba1981dcda96c98ee06d33a8a5.jpg)  \n(a) point-level\n\n![](images/212f1b2bacedca3b9027cc2f35e378c940336f4395cccba68ea3d1530a1b3944.jpg)  \n(b) union-level, no skip\n\n![](images/8ac391577fa9383ef07f3603a776a6c28a2c9a5cd1d0299b1fa24981d6b64b28.jpg)  \n(c) union-level, skip once  \nFigure 1: An example of point and union level dynamic pattern influences, the order of Markov chain  \\(L = 3\\)\n\nand Fossil fall into this taxonomy. Although Fossil [6] considers a high-order Markov chain, the overall influence is a weighted sum of previous items' latent representations factorized from first-order Markov transition matrices. Such aggregation of point-level influences is not sufficient to model the union-level influences shown in Figure 1b where several previous actions, in that order, jointly influence the target action. For example, buying both milk and butter together leads to a higher probability of buying flour than buying milk or butter individually; buying both RAM and Hard Drive is a better indication of buying Operating System next than buying only one of the components.\n\nFail to allow skip behaviors. Existing models don't consider skip behaviors of sequential patterns as shown in Figure 1c, where the impact from past behaviors may skip a few steps and still have strength. For example, a tourist has check-ins sequentially at airport, hotel, restaurant, bar, and attraction. While the check-ins at the airport and hotel do not immediately precede the check-in of the attraction, they are strongly associated with the latter. On the other hand, the check-in at the restaurant or bar has little influence on the check-in of the attraction (because they do not necessarily occur). A  \\(L\\) -order Markov chain does not explicitly model such skip behaviors because it assumes that the  \\(L\\)  previous steps have an influence on the immediate next step.\n\nTo provide evidences of union-level influences and skip behaviors, we mine sequential association rules [1, 4] of the following form from two real life data sets, MovieLens and Gowalla (see the details of these data sets in Section 4)\n\n\\[\n\\left(S _ {t - L} ^ {u}, \\dots , S _ {t - 2} ^ {u}, S _ {t - 1} ^ {u}\\right)\\rightarrow S _ {t} ^ {u}. \\tag {1}\n\\]\n\nFor a rule  \\(X \\to Y\\)  of the above form, the support count  \\(\\sup(XY)\\)  is the number of sequences in which  \\(X\\)  and  \\(Y\\)  occur in order as in the rule, and the confidence,  \\(\\frac{\\sup(XY)}{\\sup(X)}\\) , is the percentage of the sequences in which  \\(Y\\)  follows  \\(X\\)  among those in which  \\(X\\)  occurs. This rule represents the joint influence of all the items in  \\(X\\)  on  \\(Y\\) . By changing the right hand side to  \\(S_{t+1}^u\\)  or  \\(S_{t+2}^u\\) , the rule also captures the influences with one or two step skips. Figure 2 summarizes the number of rules found versus the Markov order  \\(L\\)  and skip steps with the minimum support count  \\(= 5\\)  and the minimum confidence  \\(= 50\\%\\)  (we also tried the minimum confidence of  \\(10\\%, 20\\%,\\)  and  \\(30\\%\\) , these trends are similar). Most rules have the orders  \\(L = 2\\)  and  \\(L = 3\\)  and the confidence of rules gets higher for larger  \\(L\\) . The figure also tells that a sizable number of rules have skip steps 1 or 2. These findings support the existence of union-level influences and skip behaviors.\n\n![](images/78bf04454c8a60e7ea0fc3a357d6dd70f1d813ee92088c68801718196d79d7d8.jpg)  \n(a) MovieLens  \nFigure 2: The number of association rules vs  \\(L\\)  and skip steps. The minimum support count = 5 and the minimum confidence = 50%.\n\n![](images/f402f56a3f349396d268079984e488b0009e9f58fb3f1cc761549c6994a54fa1.jpg)  \n(b) Gowalla\n\n## 1.3 Contributions\n\nTo address these above limitations of existing works, we propose a ConvolutionAl Sequence Embedding Recommendation Model, or Caser for short, as a solution to top- \\(N\\)  sequential recommendation. This model leverages the recent success of convolution filters of Convolutional Neural Network (CNN) to capture local features for image recognition [11, 16] and natural language processing [12]. The novelty of Caser is to represent the previous  \\(L\\)  items as an  \\(L \\times d\\)  matrix  \\(E\\) , where  \\(d\\)  is the number of latent dimensions and the rows preserve the order of the items. Similar to [12], we regard this embedding matrix as the \"image\" of the  \\(L\\)  items in the latent space and search for sequential patterns as local features of this \"image\" using various convolutional filters. Unlike image recognition, however, this \"image\" is not given in the input and must be learnt simultaneously with all filters.\n\nCompared to existing methods, Caser offers several distinct advantages. (1) Caser uses horizontal and vertical convolutional filters to capture sequential patterns at point-level, union-level, and of skip behaviors. (2) Caser models both users' general preferences and sequential patterns, and generalizes several existing state-of-the-art methods in a single unified framework. (3) Caser outperforms state-of-the-art methods for top- \\(N\\)  sequential recommendation on real life data sets. In the rest of the paper, we discuss further related work in Section 2, the Caser method in Section 3, and experimental studies in Section 4.\n",
  "method": "# 3 PROPOSED METHODOLOGY\n\nThe proposed model, ConvolutionAl Sequence Embedding Recommendation (Caser), incorporates the Convolutional Neural Network (CNN) to learn sequential features, and Latent Factor Model (LFM) to learn user specific features. The goal of Caser's network design is multi-fold: capture both user's general preferences and sequential patterns, at both union-level and point-level, and capture skip behaviors, all in unobserved spaces. Shown in Figure 3 Caser consists of three components: Embedding Look-up, Convolutional Layers, and Fully-connected Layers. To train the CNN, for each user  \\(u\\) , we extract every  \\(L\\)  successive items as input and their next  \\(T\\)  items as the targets from the user's sequence  \\(S^u\\) , shown on the left side of Figure 3. This is done by sliding a window of size  \\(L + T\\)  over the user's sequence, and each window generates a training instance for  \\(u\\) , denoted by a triplet  \\((u, \\text{previous } L \\text{ items}, \\text{next } T \\text{ items})\\) .\n\n## 3.1 Embedding Look-up\n\nCaser captures sequence features in the latent space by feeding the embeddings of previous  \\(L\\)  items into the neural network. The embedding  \\(Q_{i}\\in \\mathbb{R}^{d}\\)  for item  \\(i\\)  is a similar concept to its latent factors. Here  \\(d\\)  is the number of latent dimensions. The embedding look-up operation retrieves the previous  \\(L\\)  items' embeddings and stacks them together, resulting in a matrix  \\(E^{(u,t)}\\in \\mathbb{R}^{L\\times d}\\)  for user\n\n![](images/486f023bca333fdf3e51780e0de7056e137463a594e5ca9591c07127f363ef69.jpg)  \nFigure 4: Darker colors mean larger values. The first filter captures \" (Airport, Hotel)  \\(\\rightarrow\\)  Great Wall\" by interacting with the embedding of airport and hotel and skipping that of fast food and restaurant. The second filter captures \" (Fast Food, Restaurant)  \\(\\rightarrow\\)  Bar\".\n\n\\(u\\)  at time step  \\(t\\) :\n\n\\[\n\\boldsymbol {E} ^ {(u, t)} = \\left[ \\begin{array}{c} Q _ {\\mathcal {S} _ {t - L} ^ {u}} \\\\ \\vdots \\\\ Q _ {\\mathcal {S} _ {t - 2} ^ {u}} \\\\ Q _ {\\mathcal {S} _ {t - 1} ^ {u}} \\end{array} \\right]. \\tag {2}\n\\]\n\nAlong with the item embeddings, we also have an embedding  \\(P_{u} \\in \\mathbb{R}^{d}\\)  for a user  \\(u\\) , representing user features in latent space. These embeddings are represented by blue and purple circles in the box of Embedding Look-up in Figure 3.\n\n### 3.2 Convolutional Layers\n\nOur approach leverages the recent success of convolution filters of CNN in capturing local features for image recognition [11, 16] and natural language processing [12]. Borrows the idea of using CNN in text classification [12], our approach regards the  \\(L \\times d\\)  matrix  \\(E\\)  as the \"image\" of the previous  \\(L\\)  items in the latent space and regard sequential patterns as local features of this \"image\". This approach enables the use of convolution filters to search for sequential patterns. Figure 4 shows two \"horizontal filters\" that capture two union-level sequential patterns. These filters, represented as  \\(h \\times d\\)  matrices, have the height  \\(h = 2\\)  and the full width equal to  \\(d\\) . They pick up signals for sequential patterns by sliding over the rows of  \\(E\\) . For example, the first filter picks up the sequential pattern \"(Airport, Hotel) → Great Wall\" by having larger values in the latent dimensions where Airport and Hotel have larger values. Similarly, a \"vertical filter\" is a  \\(L \\times 1\\)  matrix and will slide over the columns of  \\(E\\) . More details are explained below. Unlike image recognition, the \"image\"  \\(E\\)  is not given because the embedding  \\(Q_{i}\\)  for all items  \\(i\\)  must be learnt simultaneously with all filters.\n\nHorizontal Convolutional Layer. This layer, shown in the upper part of the second component in Figure 3, has  \\(n\\)  horizontal filters  \\(F^k \\in \\mathbb{R}^{h \\times d}\\) ,  \\(1 \\leq k \\leq n\\) .  \\(h \\in \\{1, \\dots, L\\}\\)  is the height of a filter. For example, if  \\(L = 4\\) , one may choose to have  \\(n = 8\\)  filters, two for each  \\(h\\)  in  \\(\\{1, 2, 3, 4\\}\\) .  \\(F^k\\)  will slide from top to bottom on  \\(E\\)  and interact with all horizontal dimensions of  \\(E\\)  of the items  \\(i\\) ,  \\(1 \\leq i \\leq L - h + 1\\) . The result of the interaction is the  \\(i\\) -th convolution value given by\n\n\\[\n\\boldsymbol {c} _ {i} ^ {k} = \\phi_ {c} \\left(\\boldsymbol {E} _ {i: i + h - 1} \\odot \\boldsymbol {F} ^ {k}\\right). \\tag {3}\n\\]\n\nwhere the symbol  \\(\\odot\\)  denotes the inner product operator and  \\(\\phi_c(\\cdot)\\)  is the activation function for convolutional layers. This value is the\n\ninner product between  \\(F^k\\)  and the sub-matrix formed by the row  \\(i\\)  to row  \\(i - h + 1\\)  of  \\(E\\) , denoted by  \\(E_{i:i + h - 1}\\) . The final convolution result of  \\(F^k\\)  is the vector\n\n\\[\n\\boldsymbol {c} ^ {k} = \\left[ \\begin{array}{l l l} \\boldsymbol {c} _ {1} ^ {k} & \\boldsymbol {c} _ {2} ^ {k} & \\dots \\end{array} \\boldsymbol {c} _ {L - h + 1} ^ {k} \\right]. \\tag {4}\n\\]\n\nWe then apply a max pooling operation to  \\(c^k\\)  to extract the maximum value from all values produced by this particular filter. The maximum value captures the most significant feature extracted by the filter. Therefore, for the  \\(n\\)  filters in this layer, the output value  \\(o \\in \\mathbb{R}^n\\)  is\n\n\\[\n\\boldsymbol {o} = \\left\\{\\max  \\left(c ^ {1}\\right), \\max  \\left(c ^ {2}\\right), \\dots , \\max  \\left(c ^ {n}\\right) \\right\\}. \\tag {5}\n\\]\n\nHorizontal filters interact with every successive  \\(h\\)  items through their embeddings  \\(E\\) . Both the embeddings and the filters are learnt to minimize an objective function that encodes the prediction error of target items (more in Section 3.4). By sliding filters of various heights, a significant signal will be picked up regardless of location. Therefore, horizontal filters can be trained to capture union-level patterns with multiple union sizes.\n\nVertical Convolutional Layer. This layer is shown in the lower part of the second component in Figure 3. We use tilde  \\((\\sim)\\)  for the symbols of this layer. Suppose that there are  \\(\\tilde{n}\\)  vertical filters  \\(\\tilde{F}^k \\in \\mathbb{R}^{L \\times 1}\\) ,  \\(1 \\leq k \\leq \\tilde{n}\\) . Each filter  \\(\\tilde{F}^k\\)  interacts with the columns of  \\(E\\)  by sliding  \\(d\\)  times from left to right on  \\(E\\) , yielding the vertical convolution result  \\(\\tilde{c}^k\\) :\n\n\\[\n\\tilde {\\boldsymbol {c}} ^ {k} = \\left[ \\tilde {\\boldsymbol {c}} _ {1} ^ {k} \\tilde {\\boldsymbol {c}} _ {2} ^ {k} \\dots \\tilde {\\boldsymbol {c}} _ {d} ^ {k} \\right]. \\tag {6}\n\\]\n\nFor the inner product interaction, it is easy to verify that this result is equal to the weighted sum over the  \\(L\\)  rows of  \\(E\\)  with  \\(\\tilde{F}^k\\)  as the weights:\n\n\\[\n\\tilde {\\boldsymbol {c}} ^ {k} = \\sum_ {l = 1} ^ {L} \\tilde {\\boldsymbol {F}} _ {l} ^ {k} \\cdot \\boldsymbol {E} _ {l}, \\tag {7}\n\\]\n\nwhere  \\(E_{l}\\)  is the  \\(l\\) -th row of  \\(E\\) . Therefore, with vertical filters we can learn to aggregate the embeddings of the  \\(L\\)  previous items, similar to Fossil's [6] weighted sum to aggregate the  \\(L\\)  previous items' latent representations. The difference is that each filter  \\(\\tilde{F}^k\\)  is acting like a different aggregator. Thus, similar to Fossil, these vertical filters are capturing point-level sequential patterns through weighted sums over previous items' latent representations. While Fossil uses a single weighted sum for each user, we can use  \\(\\tilde{n}\\)  global vertical filters to produce  \\(\\tilde{n}\\)  weighted sums  \\(\\tilde{\\pmb{o}} \\in \\mathbb{R}^{d\\tilde{n}}\\)  for all users:\n\n\\[\n\\tilde {\\boldsymbol {o}} = \\left[ \\tilde {\\boldsymbol {c}} ^ {1} \\tilde {\\boldsymbol {c}} ^ {2} \\dots \\tilde {\\boldsymbol {c}} ^ {\\tilde {n}} \\right]. \\tag {8}\n\\]\n\nSince their usage is aggregation, vertical filters have some differences from horizontal ones: (1) The size of each vertical filter is fixed to be  \\(L \\times 1\\) . This is because each column of  \\(E\\)  is latent for us, it is meaningless to interact with multiple successive columns at one time. (2) There is no need to apply max pooling operation over the vertical convolution results, as we want to keep the aggregation for every latent dimension. Thus, the output of this layer is  \\(\\tilde{\\mathbf{o}}\\) .\n\n### 3.3 Fully-connected Layers\n\nWe concatenate the outputs of the two convolutional layers and feed them into a fully-connected neural network layer to get more\n\nhigh-level and abstract features:\n\n\\[\n\\boldsymbol {z} = \\phi_ {a} (\\boldsymbol {W} \\left[ \\begin{array}{l} \\boldsymbol {0} \\\\ \\tilde {\\boldsymbol {0}} \\end{array} \\right] + \\boldsymbol {b}), \\tag {9}\n\\]\n\nwhere  \\(W \\in \\mathbb{R}^{d \\times (n + d\\tilde{n})}\\)  is the weight matrix that projects the concatenation layer to a  \\(d\\) -dimensional hidden layer,  \\(b \\in \\mathbb{R}^d\\)  is the corresponding bias term and  \\(\\phi_a(\\cdot)\\)  is the activation function for fully-connected layer.  \\(z \\in \\mathbb{R}^d\\)  is what we called convolutional sequence embedding, which encodes all kinds of sequential features of the  \\(L\\)  previous items.\n\nTo capture user's general preferences, we also look-up the user embedding  \\(P_{u}\\)  and concatenate the two  \\(d\\) -dimensional vectors,  \\(z\\)  and  \\(P_{u}\\) , together and project them to an output layer with  \\(|I|\\)  nodes, written as\n\n\\[\n\\boldsymbol {y} ^ {(u, t)} = \\boldsymbol {W} ^ {\\prime} \\left[ \\begin{array}{l} z \\\\ \\boldsymbol {P} _ {u} \\end{array} \\right] + \\boldsymbol {b} ^ {\\prime}, \\tag {10}\n\\]\n\nwhere  \\(\\pmb{b}^{\\prime} \\in \\mathbb{R}^{|\\mathcal{I}|}\\)  and  \\(W^{\\prime} \\in \\mathbb{R}^{|\\mathcal{I}| \\times 2d}\\)  are the bias term and weight matrix for output layer, respectively. As explained in Section 3.4, the value  \\(\\pmb{y}_i^{(u,t)}\\)  in the output layer is associated with the probability of how likely user  \\(u\\)  will interact with item  \\(i\\)  at time step  \\(t\\) .  \\(z\\)  intends to capture short term sequential patterns, whereas the user embedding  \\(P_u\\)  captures user's long-term general preferences. Here we put the user embedding  \\(\\mathrm{Pu}\\)  in the last hidden layer for several reasons: (1) As we shall see in Section 3.6, it can have the ability to generalize other models. (2) we can pre-train our model's parameters with other generalized models' parameters. As stated in [7], such pretraining is critical to model performance\n\n### 3.4 Network Training\n\nTo train the network, we transform the values of the output layer,  \\(\\pmb{y}^{(u,t)}\\) , to probabilities by:\n\n\\[\np \\left(\\mathcal {S} _ {t} ^ {u} \\mid \\mathcal {S} _ {t - 1} ^ {u}, \\mathcal {S} _ {t - 2} ^ {u}, \\dots , \\mathcal {S} _ {t - L} ^ {u}\\right) = \\sigma \\left(\\boldsymbol {y} _ {\\mathcal {S} _ {t} ^ {u}} ^ {(u, t)}\\right), \\tag {11}\n\\]\n\nwhere  \\(\\sigma(x) = 1 / (1 + e^{-x})\\)  is the sigmoid function. Let  \\(C^u = \\{L + 1, L + 2, \\dots, |S^u|\\}\\)  be the collection of time steps for which we would like to make predictions for user  \\(u\\) . The likelihood of all sequences in the dataset is:\n\n\\[\np (\\mathcal {S} | \\Theta) = \\prod_ {u} \\prod_ {t \\in \\mathcal {C} ^ {u}} \\sigma \\left(\\boldsymbol {y} _ {\\mathcal {S} _ {t} ^ {u}} ^ {(u, t)}\\right) \\prod_ {j \\neq \\mathcal {S} _ {t} ^ {u}} (1 - \\sigma \\left(\\boldsymbol {y} _ {j} ^ {(u, t)}\\right)). \\tag {12}\n\\]\n\nTo further capture skip behaviors, we could consider the next  \\(T\\)  target items,  \\(\\mathcal{D}_t^u = \\{\\mathcal{S}_t^u,\\mathcal{S}_{t + 1}^u,\\dots,\\mathcal{S}_{t + T}^u\\}\\) , at once by replacing the immediate next item  \\(\\mathcal{S}_t^u\\)  in the above equation with  \\(\\mathcal{D}_t^u\\) . Taking the negative logarithm of likelihood, we get the objective function, also known as binary cross-entropy loss:\n\n\\[\n\\ell = \\sum_ {u} \\sum_ {t \\in C ^ {u}} \\sum_ {i \\in \\mathcal {D} _ {u} ^ {u}} - \\log \\left(\\sigma \\left(\\boldsymbol {y} _ {i} ^ {(u, t)}\\right)\\right) + \\sum_ {j \\neq i} - \\log \\left(1 - \\sigma \\left(\\boldsymbol {y} _ {j} ^ {(u, t)}\\right)\\right). \\tag {13}\n\\]\n\nFollowing previous works [6, 21, 32], for each target item  \\(i\\) , we randomly sample several (3 in our experiments) negative instances  \\(j\\)  in the second term.\n\nThe model parameters  \\(\\Theta = \\{P, Q, F, \\tilde{F}, W, W', b, b'\\}\\)  are learned by minimizing the objective function in Eqn (13) on the training set, whereas the hyperparameters (e.g.,  \\(d, n, \\tilde{n}, L, T\\) ) are tuned on the validation set via grid search. We adopt an variant of Stochastic Gradient Descent (SGD) called Adaptive Moment Estimation\n\n(Adam) [13] for faster convergence, with a batch size of 100. To control model complexity and avoid over-fitting, we use two kinds of regularization methods: the  \\(L2\\)  Norm is applied for all model parameters and Dropout [27] technique with  \\(50\\%\\)  drop ratio is used on fully-connected layers. We implemented Caser with MatConvNet [28]. The whole training time is proportional to the number of training instances. For example, it took around 1 hour for MovieLens data and 2 hours for Gowalla data, 2 hours for Foursquare and 1 hour for Tmall on a 4-cores i7 CPU and 32GB RAM machine. These times are comparable to Fossil's [6] running time and can be further reduced by using GPU.\n\n### 3.5 Recommendation\n\nAfter obtaining the trained neural network, to make recommendations for a user  \\(u\\)  at time step  \\(t\\) , we take  \\(u\\) 's latent embedding  \\(P_{u}\\)  and extract his last  \\(L\\)  items' embeddings given by Eqn (2) as the neural network input. We recommend the  \\(N\\)  items that have the highest values in the output layer  \\(y\\) . The complexity for making recommendations to all users is  \\(O(|\\mathcal{U}||I|d)\\) , where the complexity of convolution operations is ignored. Note that the number of target items  \\(T\\)  is a hyperparameter used during the model training, whereas  \\(N\\)  is the number of items recommended after the model is trained.\n\n### 3.6 Connection to Existing Models\n\nWe show that Caser is a generalization of several previous models.\n\nCaser vs. MF. By discarding all convolutional layers and all bias terms, our model becomes a vanilla LFM with user embeddings as user latent factors and its associated weights as item latent factors. MF usually contains bias terms \\(^1\\) , which is  \\(b'\\)  in our model. After discarding all convolutional layers, the resulting model is the same as MF:\n\n\\[\n\\boldsymbol {y} _ {i} ^ {u} = W _ {i} ^ {\\prime} \\left[ \\begin{array}{l} 0 \\\\ P _ {u} \\end{array} \\right] + \\boldsymbol {b} _ {i} ^ {\\prime}. \\tag {14}\n\\]\n\nCaser vs. FPMC. FPMC fuses factorized first-order Markov chain with LFM and is optimized by Bayesian personalized ranking (BPR). Although Caser uses a different optimization criterion, i.e., the cross-entropy, it is able to generalize FPMC by copying the previous item's embedding to the hidden layer  \\(z\\)  and not using any bias terms:\n\n\\[\n\\boldsymbol {y} _ {i} ^ {(u, t)} = \\boldsymbol {W} _ {i} ^ {\\prime} \\left[ \\begin{array}{c} Q S _ {t - 1} ^ {u} \\\\ P _ {u} \\end{array} \\right]. \\tag {15}\n\\]\n\nAs FPMC uses BPR as the criterion, our model is not exactly the same as FPMC. However, BPR is limited to have only 1 target and negative sample at each time step. Our cross-entropy loss does not have these limitations.\n\nCaser vs. Fossil. By omitting the horizontal convolutional layer and using one vertical filter and copying the vertical convolution result  \\(\\tilde{c}\\)  to the hidden layer  \\(z\\) , we get\n\n\\[\n\\boldsymbol {y} _ {i} ^ {(u, t)} = \\boldsymbol {W} _ {i} ^ {\\prime} \\left[ \\begin{array}{l} \\tilde {\\boldsymbol {c}} \\\\ \\boldsymbol {P} _ {u} \\end{array} \\right] + \\boldsymbol {b} _ {i} ^ {\\prime}. \\tag {16}\n\\]\n\nAs discussed for Eqn (7), this vertical filter serves as the weighted sum of the embeddings of the  \\(L\\)  previous items, like in Fossil, though Fossil uses Similarity Model instead of LFM and factorizes it in the\n\nsame latent space as Markov model. Another difference is that Fossil uses one local weighting for each user while we use a number of global weighting through vertical filters.\n",
  "experiments": "# 4 EXPERIMENTS\n\nWe compare Caser with state-of-the-art methods. The source code of Caser and processed data sets are available online<sup>2</sup>.\n\n## 4.1 Experimental Setup\n\nDatasets. Sequential recommendation makes sense only when the data set contains sequential patterns. To identify such data sets, we applied sequential association rule mining to several public data sets and computed their sequential intensity defined by:\n\n\\[\n\\text {S e q u e n t i a l I n t e n s i t y} (S I) = \\frac {\\# \\text {r u l e s}}{\\# \\text {u s e r s}}. \\tag {17}\n\\]\n\nThe numerator is the total number of rules in the form of Eqn (1) found using a minimum threshold on support (i.e., 5) and confidence(i.e.,  \\(50\\%\\)  with Markov order  \\(L\\)  range from 1 to 5. The denominator is the total number of users. We use  \\(SI\\)  to estimate the intensity of sequential signals in a data set.\n\nThe four data sets with their  \\(SI\\)  are described in Table 1. MovieLens \\(^3\\)  is the widely used movie rating data. Gowalla \\(^4\\)  constructed by [3] and Foursquare obtained from [33] contain implicit feedback through user-venue check-ins. Tmall, the largest B2C platform in China, is a user-purchase data obtained from IJCAI 2015 competition \\(^5\\) , which aims to forecast repeated buyers. Following previous works [6, 20, 32], we converted all numeric ratings to implicit feedback of 1. We also removed cold-start users and items of having less than  \\(n\\)  feedbacks, as dealing with cold-start recommendation is usually treated as a separate issue in the literature [6, 7, 21, 32].  \\(n\\)  is 5,15,10,10 for MovieLens, Gowalla, Foursquare, and Tmall. The Amazon data previously used in [5, 6] was not used due to its  \\(SI\\)  (0.0026 for 'Office Products' category, 0.0019 for 'Clothing, Shoes, Jewelry' and 'Video Games' category), in other words, its sequential signals are much weaker than the above data sets.\n\nFollowing [17, 33, 35], we hold the first  \\(70\\%\\)  of actions in each user's sequence as the training set and use the next  \\(10\\%\\)  of actions as the validation set to search the optimal hyperparameter settings for all models. The remaining  \\(20\\%\\)  actions in each user's sequence are used as the test set for evaluating a model's performance.\n\nEvaluation Metrics. As in [19, 21, 29, 32], we evaluate a model by Precision@N, Recall@N, and Mean Average Precision (MAP). Given a list of top  \\(N\\)  predicted items for a user, denoted  \\(\\hat{R}_{1:N}\\) , and the last  \\(20\\%\\)  of actions in her/his sequence (i.e., denoted  \\(R\\)  (i.e., the test set), Precision@N and Recall@N are computed by\n\n\\[\n\\begin{array}{l} \\operatorname {P r e c} @ N = \\frac {\\left| R \\cap \\hat {R} _ {1 : N} \\right|}{N}, \\tag {18} \\\\ \\mathrm {R e c a l l} @ N = \\frac {| R \\cap \\hat {R} _ {1 : N} |}{| R |}. \\\\ \\end{array}\n\\]\n\nWe report the average of these values of all users.  \\(N\\in \\{1,5,10\\}\\)  The Average Precision (AP) is defined by\n\n\\[\n\\mathrm {A P} = \\frac {\\sum_ {N = 1} ^ {| \\hat {R} |} \\operatorname {P r e c} @ N \\times \\operatorname {r e l} (N)}{| \\hat {R} |}, \\tag {19}\n\\]\n\nwhere  \\(rel(N) = 1\\)  if the  \\(N\\) -th item in  \\(\\hat{R}\\)  is in  \\(R\\) . The Mean Average Precision (MAP) is the average of AP for all users.\n\n## 4.2 Performance Comparison\n\nWe compare our method, Caser, proposed in Section 3 with the following baselines.\n\n- POP. All items are ranked by their popularity in all users' sequences, and the popularity is determined by the number of interactions.  \n- BPR. Combined with Matrix Factorization model, Bayesian personalized ranking [20] is the state-of-the-art method for non-sequential item recommendation on implicit feedback data.  \n- FMC and FPMC. As introduced in [21], FMC factorizes the first-order Markov transition matrix into two low-dimensional sub-matrices, and FPMC is a fusion of FMC and LFM. These are the state-of-the-art sequential recommendation methods. FPMC allows a basket of several items at each step. For our sequential recommendation problem, each basket has a single item.  \n- Fossil. Fossil [6] models high-order Markov chains and uses Similarity Model instead of LFM for modeling general user preferences.  \n- GRU4Rec. This is the session-based recommendation proposed by [8]. This model uses RNN to capture sequential dependencies and make predictions.\n\nFor each method, the grid search is applied to find the optimal settings of hyperparameters using the validation set. These include latent dimensions  \\(d\\)  from  \\(\\{5, 10, 20, 30, 50, 100\\}\\) , regularization hyperparameters, and the learning rate from  \\(\\{1, 10^{-1}, \\dots, 10^{-4}\\}\\) . For Fossil, Caser and GRU4Rec, the Markov order  \\(L\\)  is from  \\(\\{1, \\dots, 9\\}\\) . For Caser itself, the height  \\(h\\)  of horizontal filters is from  \\(\\{1, \\dots, L\\}\\) , the target number  \\(T\\)  is from  \\(\\{1, 2, 3\\}\\) , the activation functions  \\(\\phi_{a}\\)  and  \\(\\phi_{c}\\)  are from  \\(\\{\\text{identity}, \\text{sigmoid}, \\text{tanh}, \\text{relu}\\}\\) . For each height  \\(h\\) , the number of horizontal filters is from  \\(\\{4, 8, 16, 32, 64\\}\\) . The number of vertical filters is from  \\(\\{1, 2, 4, 8, 16\\}\\) . We report the result of each method under its optimal hyperparameter settings.\n\nThe best results of the six baselines and Caser are summarized in Table 2. The best performer on each row is highlighted in bold face. The last column is the improvement of Caser relative to the best baseline, defined as  \\(\\frac{\\text{Caser-baseline}}{\\text{baseline}}\\) . Except for MovieLens, Caser improved the best baseline on all  \\(N\\)  tested by a large margin w.r.t. the three metrics. Among the baseline methods, the sequential recommenders (e.g., FPMC and Fossil) usually outperform non-sequential recommenders (i.e., BPR) on all data sets, suggesting the importance of considering sequential information. FPMC and Fossil outperform FMC on all data sets, suggesting the effectiveness of personalization. On MovieLens, GRU4Rec achieved a performance close to Caser's, but got a much worse performance on the other three data sets. In fact, MovieLens has more sequential signals than\n\nTable 1: Statistics of the datasets  \n\n<table><tr><td>Datasets</td><td>Sequential \nIntensity</td><td>#users</td><td>#items</td><td>avg. actions \nper user</td><td>Sparsity</td></tr><tr><td>MovieLens</td><td>0.3265</td><td>6.0k</td><td>3.4k</td><td>165.50</td><td>95.16%</td></tr><tr><td>Gowalla</td><td>0.0748</td><td>13.1k</td><td>14.0k</td><td>40.74</td><td>99.71%</td></tr><tr><td>Foursquare</td><td>0.0378</td><td>10.1k</td><td>23.4k</td><td>30.16</td><td>99.87%</td></tr><tr><td>Tmall</td><td>0.0104</td><td>23.8k</td><td>12.2k</td><td>13.93</td><td>99.89%</td></tr></table>\n\nTable 2: Performance comparison on the four data sets.  \n\n<table><tr><td>Dataset</td><td>Metric</td><td>POP</td><td>BPR</td><td>FMC</td><td>FPMC</td><td>Fossil</td><td>GRU4Rec</td><td>Caser</td><td>Improv.</td></tr><tr><td rowspan=\"7\">MovieLens</td><td>Prec@1</td><td>0.1280</td><td>0.1478</td><td>0.1748</td><td>0.2022</td><td>0.2306</td><td>0.2515</td><td>0.2502</td><td>-0.5%</td></tr><tr><td>Prec@5</td><td>0.1113</td><td>0.1288</td><td>0.1505</td><td>0.1659</td><td>0.2000</td><td>0.2146</td><td>0.2175</td><td>1.4%</td></tr><tr><td>Prec@10</td><td>0.1011</td><td>0.1193</td><td>0.1317</td><td>0.1460</td><td>0.1806</td><td>0.1916</td><td>0.1991</td><td>4.0%</td></tr><tr><td>Recall@1</td><td>0.0050</td><td>0.0070</td><td>0.0104</td><td>0.0118</td><td>0.0144</td><td>0.0153</td><td>0.0148</td><td>-3.3%</td></tr><tr><td>Recall@5</td><td>0.0213</td><td>0.0312</td><td>0.0432</td><td>0.0468</td><td>0.0602</td><td>0.0629</td><td>0.0632</td><td>0.5%</td></tr><tr><td>Recall@10</td><td>0.0375</td><td>0.0560</td><td>0.0722</td><td>0.0777</td><td>0.1061</td><td>0.1093</td><td>0.1121</td><td>2.6%</td></tr><tr><td>MAP</td><td>0.0687</td><td>0.0913</td><td>0.0949</td><td>0.1053</td><td>0.1354</td><td>0.1440</td><td>0.1507</td><td>4.7%</td></tr><tr><td rowspan=\"7\">Gowalla</td><td>Prec@1</td><td>0.0517</td><td>0.1640</td><td>0.1532</td><td>0.1555</td><td>0.1736</td><td>0.1050</td><td>0.1961</td><td>13.0%</td></tr><tr><td>Prec@5</td><td>0.0362</td><td>0.0983</td><td>0.0876</td><td>0.0936</td><td>0.1045</td><td>0.0721</td><td>0.1129</td><td>8.0%</td></tr><tr><td>Prec@10</td><td>0.0281</td><td>0.0726</td><td>0.0657</td><td>0.0698</td><td>0.0782</td><td>0.0571</td><td>0.0833</td><td>6.5%</td></tr><tr><td>Recall@1</td><td>0.0064</td><td>0.0250</td><td>0.0234</td><td>0.0256</td><td>0.0277</td><td>0.0155</td><td>0.0310</td><td>11.9%</td></tr><tr><td>Recall@5</td><td>0.0257</td><td>0.0743</td><td>0.0648</td><td>0.0722</td><td>0.0793</td><td>0.0529</td><td>0.0845</td><td>6.6%</td></tr><tr><td>Recall@10</td><td>0.0402</td><td>0.1077</td><td>0.0950</td><td>0.1059</td><td>0.1166</td><td>0.0826</td><td>0.1223</td><td>4.9%</td></tr><tr><td>MAP</td><td>0.0229</td><td>0.0767</td><td>0.0711</td><td>0.0764</td><td>0.0848</td><td>0.0580</td><td>0.0928</td><td>9.4%</td></tr><tr><td rowspan=\"7\">Foursquare</td><td>Prec@1</td><td>0.1090</td><td>0.1233</td><td>0.0875</td><td>0.1081</td><td>0.1191</td><td>0.1018</td><td>0.1351</td><td>13.4%</td></tr><tr><td>Prec@5</td><td>0.0477</td><td>0.0543</td><td>0.0445</td><td>0.0555</td><td>0.0580</td><td>0.0475</td><td>0.0619</td><td>6.7%</td></tr><tr><td>Prec@10</td><td>0.0304</td><td>0.0348</td><td>0.0309</td><td>0.0385</td><td>0.0399</td><td>0.0331</td><td>0.0425</td><td>6.5%</td></tr><tr><td>Recall@1</td><td>0.0376</td><td>0.0445</td><td>0.0305</td><td>0.0440</td><td>0.0497</td><td>0.0369</td><td>0.0565</td><td>13.7%</td></tr><tr><td>Recall@5</td><td>0.0800</td><td>0.0888</td><td>0.0689</td><td>0.0959</td><td>0.0948</td><td>0.0770</td><td>0.1035</td><td>7.9%</td></tr><tr><td>Recall@10</td><td>0.0954</td><td>0.1061</td><td>0.0911</td><td>0.1200</td><td>0.1187</td><td>0.1011</td><td>0.1291</td><td>7.6%</td></tr><tr><td>MAP</td><td>0.0636</td><td>0.0719</td><td>0.0571</td><td>0.0782</td><td>0.0823</td><td>0.0643</td><td>0.0909</td><td>10.4%</td></tr><tr><td rowspan=\"7\">Tmall</td><td>Prec@1</td><td>0.0010</td><td>0.0111</td><td>0.0197</td><td>0.0210</td><td>0.0280</td><td>0.0139</td><td>0.0312</td><td>11.4%</td></tr><tr><td>Prec@5</td><td>0.0009</td><td>0.0081</td><td>0.0114</td><td>0.0120</td><td>0.0149</td><td>0.0090</td><td>0.0179</td><td>20.1%</td></tr><tr><td>Prec@10</td><td>0.0007</td><td>0.0063</td><td>0.0084</td><td>0.0090</td><td>0.0104</td><td>0.0070</td><td>0.0132</td><td>26.9%</td></tr><tr><td>Recall@1</td><td>0.0004</td><td>0.0046</td><td>0.0079</td><td>0.0082</td><td>0.0117</td><td>0.0056</td><td>0.0130</td><td>11.1%</td></tr><tr><td>Recall@5</td><td>0.0019</td><td>0.0169</td><td>0.0226</td><td>0.0245</td><td>0.0306</td><td>0.0180</td><td>0.0366</td><td>19.6%</td></tr><tr><td>Recall@10</td><td>0.0026</td><td>0.0260</td><td>0.0333</td><td>0.0364</td><td>0.0425</td><td>0.0278</td><td>0.0534</td><td>25.6%</td></tr><tr><td>MAP</td><td>0.0030</td><td>0.0145</td><td>0.0197</td><td>0.0212</td><td>0.0256</td><td>0.0164</td><td>0.0310</td><td>21.1%</td></tr></table>\n\n![](images/ebd15c51e7cff975a66bae8f3a88799cddc3de0c8f531621fed63a41e0d7444a.jpg)  \nFigure 5: MAP (y-axis) vs. the number of latent dimensions  \\(d\\)  (x-axis).\n\n![](images/b596069ee017ab8b58e22235b7f5741349efa97dc1c3aa83a25ed5b9e2575aba.jpg)\n\n![](images/5c668c63442916307fb2503442b87a7f0310bbaf4ac92d9fa5b67c795fd47fae.jpg)\n\n![](images/4cbe0f93faab82ff5499f3452436611481252fdaf5a56c053390b2903c8af384.jpg)\n\nthe other three data sets, thus, the RNN-based GRU4Rec could perform well on MovieLens but can easily get biased on training sets of the other three data sets despite the use of regularization and dropout as described in [8]. In addition, GRU4Rec's recommendation is session-based, instead of personalized, which enlarge the generalization error to some extent.\n\nIn the following studies, we examine the impact of the hyperparameters  \\(d, L, T\\)  on a time by holding the remaining hyperparameters at their optimal settings. We focus on MAP as it is an overall performance indicator and consistent with other metrics.\n\n4.2.1 Influence of Latent Dimensionality  \\(d\\) . Figure 5 shows MAP for various  \\(d\\)  while keeping the other optimal hyperparameters unchanged. On the denser MovieLens, a larger  \\(d\\)  does not always lead to a better model performance. A model achieves its best performance when  \\(d\\)  is chosen properly and gets worse for a larger  \\(d\\)  because of over-fitting. But for the other three sparser data sets, each model requires more latent dimensions to achieve their best results. For all data sets, Caser beats the strongest baseline performance by using a relatively small number of latent dimensions.\n\n![](images/e087ea0da0f9253040d5206ae78f44f94b28fd6988b076b8f8115e7e4486da42.jpg)  \nFigure 6: MAP (y-axis) vs. the Markov order  \\(L\\)  (x-axis). Caser-1, Caser-2, and Caser-3 denote Caser with the number of targets  \\(T\\)  set to 1, 2, 3.\n\n![](images/114b017b470fde17b3cec172dfff0642b06faa73ef6a2314f59ff9def0c1e2cf.jpg)\n\n![](images/b6c76ce86a8c72ca6a9faf495b2b006aa96adb8205a224f33e83d7a5835ecad6.jpg)\n\n![](images/444690d3683566a061c8d2d5076504c9c6b1824cccf06ba0f4f664772c519684.jpg)\n\n4.2.2 Influence of Markov Order  \\(L\\)  and Target Number  \\(T\\) . We vary  \\(L\\)  to explore how much of Fossil, GRU4Rec and Caser can gain from high-order information while keeping other optimal hyperparameters unchanged. Caser-1, Caser-2, and Caser-3 denote Caser with the target number  \\(T\\)  at 1, 2, 3 to study the effect of skip behaviors. The results are shown in Figure 6. On the dense MovieLens, Caser best utilizes the extra information provided by a larger  \\(L\\)  and Caser-3 performs the best, suggesting the benefits of skip steps. However, for the sparser data sets, all models do not consistently benefit from a larger  \\(L\\) . This is reasonable, because for a sparse data set, a higher order Markov chain tends to introduce both extra information and more noises. In most cases, Caser-2 slightly outperforms the other models on these three data sets.\n\nTable 3: MAP vs. Caser Components  \n\n<table><tr><td></td><td>MovieLens</td><td>Gowalla</td></tr><tr><td>Caser-p</td><td>0.0935</td><td>0.0777</td></tr><tr><td>Caser-h</td><td>0.1304</td><td>0.0805</td></tr><tr><td>Caser-v</td><td>0.1403</td><td>0.0841</td></tr><tr><td>Caser-vh</td><td>0.1448</td><td>0.0856</td></tr><tr><td>Caser-ph</td><td>0.1372</td><td>0.0911</td></tr><tr><td>Caser-pv</td><td>0.1494</td><td>0.0921</td></tr><tr><td>Caser-pvh</td><td>0.1507</td><td>0.0928</td></tr></table>\n\n4.2.3 Analysis of Caser Components. Finally, we evaluate the contribution of each of Caser's components, the horizontal convolutional layer (i.e.,  \\(o\\) ), the vertical convolutional layer (i.e.,  \\(\\tilde{o}\\) ), and personalization (i.e.,  \\(P_u\\) ), to the overall performance while keeping all hyperparameters at their optimal settings. The result is shown in Table 3 for MovieLens and Gowalla; the results of the other two data sets are similar. For  \\(x \\in \\{p, h, v, vh, ph, pv, pvh\\}\\) , Caser- \\(x\\)  denotes Caser with the components  \\(x\\)  enabled.  \\(h\\)  denotes horizontal convolutional layer;  \\(v\\)  denotes vertical convolutional layer;  \\(p\\)  denotes personalization, which is similar to BPR and uses LFM only. Any missing component is represented by setting its corresponding  \\(o\\) ,  \\(\\tilde{o}\\) , and  \\(P_u\\)  to zero. For example,  \\(vh\\)  denotes both vertical convolutional layer and horizontal convolutional layer by setting  \\(P_u\\)  to all zeros, and  \\(pv\\)  denotes vertical convolutional layer and personalization by setting  \\(o\\)  to all zeros. Caser- \\(p\\)  performs the worst whereas Caser-h, Caser- \\(v\\) , and Caser- \\(vh\\)  improve the performance significantly, suggesting that treating top- \\(N\\)  sequential recommendation as the conventional top- \\(N\\)  recommendation will lose useful information, and that modeling both sequential patterns at the union-level and point-level is useful for improving the prediction. For both data\n\n![](images/bdcdad6a2a79a14fb1d387e7fb52b5804c7fc3dcdc7453a326fb55288c585198.jpg)  \nFigure 7: Visualization for four vertical convolutional filters of a trained model on MovieLens data when  \\(L = 9\\) .\n\nsets, the best performance is achieved by jointly using all parts of Caser, i.e., Caser-pvh.\n\n## 4.3 Network Visualization\n\nWe have a closer look at some trained networks and prediction. Figure 7 shows the values of four vertical convolutional filters after training Caser on MovieLens with  \\(L = 9\\) . In the micro perspective, the four filters are trained to be diverse, but in the macro perspective, they follow an ascending trend from past positions to recent positions. With each vertical filter serving as a way of weighting the embeddings of previous actions (see the related discussion in Section 3), this trend indicates that Caser puts more emphasis on recent actions, demonstrating a major difference from the conventional top- \\(N\\)  recommendation.\n\nTo see the effectiveness of horizontal filters, Figure 8(a) shows top  \\(N = 3\\)  ranked movies recommended by Caser, i.e.,  \\(\\hat{R}_1\\)  (Mad Max),  \\(\\hat{R}_2\\)  (Star War),  \\(\\hat{R}_3\\)  (Star Trek) in that order, for a user with  \\(L = 5\\)  previous movies, i.e.,  \\(S_1\\)  (13th Warrior),  \\(S_2\\)  (American Beauty),  \\(S_3\\)  (Star Trek),  \\(S_4\\)  (Star Trek III), and  \\(S_5\\)  (Star Trek IV).  \\(\\hat{R}_3\\)  is the ground truth (i.e., the next movie in the user sequence). Note that  \\(\\hat{R}_1\\)  and  \\(\\hat{R}_2\\)  are quite similar to  \\(\\hat{R}_3\\) , i.e., all being action and science fiction movies, so are also recommended to the user. Figure 8(b) shows the new rank of  \\(\\hat{R}_3\\)  after masking some of the  \\(L\\)  previous movies by setting their item embeddings to zeros in the trained network. Masking  \\(S_1\\)  and  \\(S_2\\)  actually increases the rank of  \\(\\hat{R}_3\\)  to 2 (from 3); in fact,  \\(S_1\\)  and  \\(S_2\\)  are history or romance movies and act like noises for recommending  \\(\\hat{R}_3\\) . Masking each of  \\(S_3\\) ,  \\(S_4\\)  and  \\(S_5\\)  decreases the rank of  \\(\\hat{R}_3\\)  because these movies are in the same category as  \\(\\hat{R}_3\\) . The most decrease occurs after masking  \\(S_3\\) ,  \\(S_4\\)  and  \\(S_5\\)  all together. This study clearly indicates that our model correctly captures the dependence\n\n![](images/d73df6d819f7cc8de68838295e065d1b95705e6fba010a61b980f09d1d311649.jpg)  \n(a)  \n\\(S_{1}\\)  \n(b)  \nFigure 8: Horizontal convolutional filters's effectiveness of capturing union-level sequential patterns on MovieLens data.\n\n![](images/1c816506ab24c520e7fb4515e286359c023c04530e3810ef8a7c91cd4b168608.jpg)  \n  \nPrevious Sequence  \n\\(S_{2}\\)\n\n![](images/94064762a31ea149be848c76f4e28f3957f3f0ed4e30948acc2da582319f0fff.jpg)  \n\\(S_{3}\\)\n\n![](images/bf68cd49ce2a4cff2cb0adb9802b67892d5e59023d000cad3c9efa3b75bdf29d.jpg)  \n\\(S_{4}\\)\n\n![](images/95f1620fc71118ec962856b6cc3d881f7013fe03e1ded348c55405aab088a742.jpg)  \n\\(S_{5}\\)\n\n![](images/d9b1f13a40adceaa266b09744cccc75894d4f7d867a5129a1f29fee58a27c6a2.jpg)  \nPredictions  \n\\(\\hat{R}_1\\)\n\n![](images/0106cf4a306e8294a9782301df527ed65fbc99a9d94052512bb5103fcc3f56ae.jpg)  \n\\(\\hat{R}_2\\)\n\n![](images/9388dfe09500712b78a1cd714a7c32722da07bcec3e33927f97faa168bedd616.jpg)  \n\\(\\hat{R}_3\\)\n\n<table><tr><td>Masking Items</td><td>New Rank of R3 after masking</td></tr><tr><td>S1,S2</td><td>2</td></tr><tr><td>S3</td><td>32</td></tr><tr><td>S4</td><td>117</td></tr><tr><td>S5</td><td>77</td></tr><tr><td>S3,S4,S5</td><td>513</td></tr></table>\n\nof  \\(\\hat{R}_3\\)  on the related  \\(\\{S_3, S_4, S_5\\}\\)  as a union-level sequential feature for recommending  \\(\\hat{R}_3\\) .\n",
  "hyperparameter": "Latent dimensions d: {5, 10, 20, 30, 50, 100} with optimal values varying by dataset (MovieLens prefers smaller d around 20-50, sparser datasets prefer larger d around 50-100); Markov order L: {1, 2, ..., 9} with optimal L=5 for MovieLens, L=2-3 for sparser datasets; Target number T: {1, 2, 3} with T=3 best for MovieLens, T=2 for sparser datasets; Number of horizontal filters per height h: {4, 8, 16, 32, 64}; Number of vertical filters: {1, 2, 4, 8, 16}; Filter height h: {1, ..., L}; Learning rate: {1, 0.1, 0.01, 0.001, 0.0001}; Batch size: 100; Dropout ratio: 50%; Negative samples: 3; Activation functions φ_a and φ_c: {identity, sigmoid, tanh, relu}; Optimizer: Adam (Adaptive Moment Estimation); Regularization: L2 norm applied to all parameters"
}