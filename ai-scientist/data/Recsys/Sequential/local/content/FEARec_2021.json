{
  "id": "FEARec_2021",
  "paper_title": "FEARec: Frequency Enhanced Hybrid Attention Network for Sequential Recommendation",
  "alias": "FEARec",
  "year": 2021,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "",
  "introduction": "# 1 INTRODUCTION\n\nIn recent years, sequential recommendation [1, 2] has attracted increasing attention from both industry and academic communities. Essentially, the key advantage of sequential recommender models lies in the explicit modeling of item chronological correlations. To capture users' dynamic sequential information more accurately, recent years have witnessed lots of efforts based on either Markov chains [3] or Recurrent Neural Networks (RNNs) [2]. Meanwhile, Transformer [4] has emerged as a powerful architecture and a dominant force in various research fields and outperformed RNN-based models in the recommendation tasks. Attributed to its strong capability of modeling long-range dependencies in data, various sequential recommender models [1, 5-12] adopt Transformers as the sequence encoder to capture item correlations via assigning attention weights to items at different positions and obtain high-quality sequence representations.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/376d8d2b-b898-465f-a300-bea4a70ea992/a82068ce70e373291f19fbca8d33fedd5d9967edf6110e8470f4d957210e300a.jpg)  \nFigure 1: Illustration of our motivations. (a) By shifting the sequential behaviors from the time domain to the frequency domain, the historical item sequence of each user decomposed into multiple behavioral patterns with different frequencies and periods along the  $\\omega$ -axis. (b) By constructing  $s_u$ 's  $\\tau$  lag sequences  $\\mathrm{Roll}(s_u,\\tau)$ , the periodic pattern can be distinguished from the most similar sequences with the help of discrete Fourier transform.\n\nDespite their effectiveness, self-attention used in current Transformer based models is constantly a low-pass filter, which continuously erases high-frequency information according to the theoretical justification in [13, 14]. That means, SASRec [5] and its variants are highly capable of capturing low-frequencies in the sequential data [15], which helps capture a global view of user's interaction data and the overall evolution of user preference. But owing to its non-local operation [16], they are incompetent in learning high-frequencies information, including those items that interact frequently within a short period.\n\nTo alleviate these issues, existing methods import local constraints in different ways to complement Transformer-based models. Such as LSAN [17] adopts a novel twin-attention paradigm to capture the global and local preference signals via a self-attention branch and a convolution branch module, respectively. LOCKER [16] combines five local encoders with existing global attention heads to enhance short-term user dynamics modeling. However, the above-mentioned models almost process the historical interactions from the perspective of the time domain, but seldom consider tackling this challenge in the frequency domain, where the reduced high-frequency information can be easily obtained. Take Figure 1(a) for an example, from the viewpoint of the time domain, all the items are chronologically ordered and intertwined along the  $t$ -axis. With a Discrete Fourier Transform (DFT) [18, 19], the historical item sequence features of each user can be decomposed into multiple behavioral patterns with different frequencies along the  $\\omega$ -axis, and the input time features are then converted to the frequency domain. Eventually, improving the self-attention operation's capability for capturing high-frequency information in the frequency domain could become a more direct and effective way.\n\nMoreover, users' behaviors on the Internet tend to show certain periodic trends [20-22]. Take Figure 1(b) for an example, there is a periodic behavior pattern hidden in Bob's low-frequency interactive item, i.e., a watch and laptop are brought after buying a mobile phone. When he recently bought a product with similar\n\ncharacteristics again, a laptop may be a good recommendation to him. However, it is difficult to find the periodic behavior patterns hidden in the sequence by directly calculating the overall attention scores of items in the time domain. But in the frequency domain, there emerges some methods [23] constructing models to recognize the periodic characterize with the help of the Fourier transform, which inspires us to tackle this challenge from a new perspective for recommendation.\n\nFor these reasons, we shift the perspective to the frequency domain and propose a novel Frequency Enhanced Hybrid Attention Network for Sequential Recommendation (FEARec). Firstly, we improve the time domain self-attention with an adaptive frequency ramp structure. After DFT, we select a specific frequency component as the input feature for each time domain self-attention layer. That enables each layer to focus on different frequency ranges (including not only low-frequency but also high-frequency) to address the problem that self-attention only concentrates on low-frequency. Furthermore, to disentangle the temporal sequence and highlight the inherent periodicity of user behaviors, we design a novel autocorrelation based frequency attention, which is an efficient method to discover the period-based dependencies by calculating the autocorrelation in the frequency domain based on the Wiener-Khinchin theorem [24]. Autocorrelation is used to compare a sequence with a time-delayed version of itself. Specifically, given a user's behavior sequence  $s_u$  and its  $\\tau$  lag sequences  $\\mathrm{Roll}(s_u, \\tau)$  in Figure 1, the sequential-level connection can be constructed by aggregating the top-  $k$  relative sequences  $(\\mathrm{Roll}(s_u, \\tau_1), \\dots, \\mathrm{Roll}(s_u, \\tau_k))$  based on the auto-correlation. Then, we combine the time domain self-attention with frequency domain attention in a union hybrid attention framework. With the help of contrastive and frequency domain regularize loss, a multi-task learning method is applied to increase the supervision information and regularize the training process. The main contributions of this paper can be summarized as follows:\n\n- We shift the perspective to the frequency domain and design a frequency ramp structure to improve existing time domain self-attention.  \n- We propose a novel frequency domain attention based on an autocorrelation mechanism, which discovers similar period-based dependencies by aggregating most relative time delay sequences.  \n- We unify the frequency ramp structure with vanilla self-attention and frequency domain attention in one framework and design a frequency domain loss to regularize the model training.  \n- We conduct extensive experiments on four public datasets, and the experimental results imply the superiority of the FEARec compared to state-of-the-art baselines.",
  "method": "# 3 PROPOSED METHOD\n\nIn this section, we present the details of the proposed Frequency Enhanced Hybrid Attention Network for Sequential Recommendation namely (FEARec). As shown in Figure 2, after the embedding layer we first transform the embeddings of item sequences from the\n\ntime domain to the frequency domain by using Fast Fourier Transform (FFT) algorithm mentioned in Appendix A.1.2. Then hybrid attention is conducted on the sampled frequency components, which captures attention scores and periodic behavior patterns of different frequency bands simultaneously. Finally, we apply contrastive learning and frequency domain loss to improve representations in both time and frequency domains\n\n# 3.1 Embedding Layer\n\nSequential Recommendation focuses on modeling the user behavior sequence of implicit feedback, which is a list of item IDs in SR. In this paper, the item ID set is denoted by  $\\mathcal{I} = \\{i_1, i_2, \\dots, i_{|\\mathcal{I}|}\\}$  and user ID set is represented as  $\\mathcal{U} = \\{u_1, u_2, \\dots, u_{|\\mathcal{U}|}\\}$ , where  $i \\in \\mathcal{I}$  denotes an item and  $u \\in \\mathcal{U}$  denotes a user. In this way, the set of user behavior can be represented as  $S = \\{s_1, s_2, \\dots, s_{|\\mathcal{U}|}\\}$ . In SR, the user's behavior sequence is usually chronologically ordered, i.e.,  $s_u = [i_1^{(u)}, i_2^{(u)}, \\dots, i_t^{(u)}, \\dots, i_N^{(u)}]$ , where  $s_u \\in S$ ,  $u \\in \\mathcal{U}$ , and  $i_t^{(u)} \\in \\mathcal{I}$  is the item that user  $u$  interacts at time step  $t$  and  $N$  is the sequence length. For  $s_u$ , it is embedded as:\n\n$$\n\\mathbf {E} _ {u} = \\left[ \\mathbf {e} _ {1} ^ {(u)}, \\mathbf {e} _ {2} ^ {(u)}, \\dots , \\mathbf {e} _ {N} ^ {(u)} \\right] \\tag {1}\n$$\n\nwhere  $\\mathbf{e}_t^{(u)}$  is the embedding of item  $i_t^{(u)}$ . To make our model sensitive to the positions of items, we adopt positional embedding to inject additional positional information while maintaining the same embedding dimensions of the item embedding. Moreover, dropout and layer normalization operations are also implemented:\n\n$$\n\\mathbf {E} _ {u} = \\operatorname {D r o p o u t} \\left(\\operatorname {L a y e r N o r m} \\left(\\mathbf {E} _ {u} + \\mathbf {P}\\right)\\right) \\tag {2}\n$$\n\nIt is worth noting that each item has a unique ID different from each other, but after the embedding layer, similar items may have the same feature value.\n\n# 3.2 Frequency Enhanced Hybrid Attention Encoder\n\nBased on the embedding layer, we develop the item encoder by stacking  $L$  Frequency Enhanced hybrid Attention (FEA) blocks, which generally consists of three modules, i.e., frequency ramp structure, hybrid attention layer, and the point-wise Feed Forward Network (FFN). Since FEARc focuses on capturing specific frequency spectrum at different layers, we first introduce the frequency ramp structure for each layer, and then present other modules for the sampled frequencies at a certain layer.\n\n3.2.1 Frequency Ramp Structure. In FEARc, instead of preserving all frequency components, we only extract a subset of frequencies for each layer to guarantee that different attention blocks focus on different spectrums. This strategy is used in both time domain attention and frequency domain attention as shown in Figure 2.\n\nFirst, given the input item representation matrix  $\\mathbf{H}^l\\in \\mathbb{R}^{N\\times D}$  of the  $l$  -th layer and  $\\mathbf{H}^0 = \\mathbf{E}_u$ , we could execute FFT denoted as  $\\mathcal{F}(\\cdot)$  along the item dimension to transform the input item representation matrix  $\\mathbf{H}^l$  to the frequency domain:\n\n$$\n\\mathcal {F} (\\mathbf {H} ^ {l}) \\rightarrow \\mathbf {X} ^ {l} \\in \\mathbb {C} ^ {N \\times D} \\tag {3}\n$$\n\nAs said in Appendix A.1.1, due to the conjugate symmetric property in the frequency domain, only half of the spectrum is used in FEARec\n\n$$\nM = \\lceil N / 2 \\rceil + 1 \\tag {4}\n$$\n\nAs a result, the sequence length of  $\\mathbf{X}^l$  almost equals half of  $\\mathbf{H}^l$ . Note that  $\\mathbf{X}^l$  is a complex tensor and represents the spectrum of  $\\mathbf{H}^l$ .\n\nThen, as shown in Figure 3(a), we gradually select a specific frequency range for each layer and formulate this select operator as\n\n$$\n\\tilde {X} ^ {l} = \\operatorname {S a m p l e} _ {\\alpha} ^ {l} \\left(X ^ {l}\\right) = X ^ {l} \\left[ p ^ {l}: q ^ {l}, : \\right] \\tag {5}\n$$\n\nwhere  $\\tilde{X}^l\\in \\mathbb{C}^{F\\times D}$  and  $F\\in \\mathbb{N}$  denotes the length of sampled frequency features. We set  $\\alpha = \\frac{F}{M}$  as the initial sampling ratio and all frequency components are retained when  $\\alpha = 1$ . The indexes of the sampled frequency components ( $p^l$  and  $q^l$ ) are determined by the position of the current layer in the model. Taking into account that top layers focus more on modeling low-frequency global information while bottom layers are more capable of capturing high-frequency details [15], we choose the direction from high frequency to low frequency. For example, for  $l$ -th layer:\n\n$$\np ^ {l} = M (1 - \\alpha) \\left(1 - \\frac {l - 1}{L - 1}\\right) \\tag {6}\n$$\n\n$$\nq ^ {l} = p ^ {l} + \\alpha M \\tag {7}\n$$\n\nFrom Figure 3(b), when  $\\alpha >\\frac{1}{L}$ , the frequencies of different layers are overlapped. To avoid the problem of missing frequencies when  $\\alpha \\leq \\frac{1}{L}$ , we adopt average sampling to ensure that all frequencies are retained:\n\n$$\np ^ {l} = M \\left(1 - \\frac {l}{L}\\right) \\tag {8}\n$$\n\n$$\nq ^ {l} = p ^ {l} + \\frac {M}{L} \\tag {9}\n$$\n\nIn this way, the whole spectrum is split into several frequency components and preprocessed in different layers. That convenient for us to capture different frequency patterns, and explicitly model\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/376d8d2b-b898-465f-a300-bea4a70ea992/6ef03ebb58a9cf1be6a981f00a78b404099a627e6f865d414a1446bd47de6534.jpg)  \n(a)  $\\alpha \\leq \\frac{1}{L}$  \nFigure 3: Frequency ramp sampling. When  $\\alpha \\leq \\frac{1}{L}$  sampling mode1 is adopted, and  $\\alpha > \\frac{1}{L}$  sampling mode2 is adopted.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/376d8d2b-b898-465f-a300-bea4a70ea992/aba51d10bcf2b888d021136e72ae5888671e0a14c2dc5f62298f7d6d7d8c5d7c.jpg)  \nLow  \n(b)  $\\alpha >\\frac{1}{L}$\n\nhigh-frequency information that is overlooked by classical self-attention operation.\n\n3.2.2 Time Domain Self-Attention Layer. As shown in Figure 3, we propose the time domain self-attention to expand the information utilization. For the embedding  $\\mathbf{H}^l$ , after linear projector, we get the queries  $Q^{l}\\in \\mathbb{R}^{N\\times D}$ , keys  $K^l\\in \\mathbb{R}^{N\\times D}$ , and values  $V^{l}\\in \\mathbb{R}^{N\\times D}$ . Then we convert  $Q^{l}$ ,  $K^{l}$ ,  $V^{l}$  to frequency by FFT and sample a Specific frequency components for each layer. Note that, before computing the attention weights in the time domain, the sampled frequencies need to be zero-padded from  $\\mathbb{C}^{\\alpha M\\times D}$  to  $\\mathbb{C}^{M\\times D}$ . The whole process is represented as:\n\n$$\n\\tilde {Q} _ {t} ^ {l} = \\mathcal {F} ^ {- 1} (\\text {P a d d i n g} (\\text {S a m p l e} _ {\\alpha} ^ {l} (\\mathcal {F} (Q ^ {l}))))\n$$\n\n$$\n\\tilde {\\boldsymbol {K}} _ {t} ^ {l} = \\mathcal {F} ^ {- 1} (\\text {P a d d i n g} (\\text {S a m p l e} _ {\\alpha} ^ {l} (\\mathcal {F} (\\boldsymbol {K} ^ {l}))) \\tag {10}\n$$\n\n$$\n\\tilde {V} ^ {l} = \\mathcal {F} ^ {- 1} (\\text {P a d d i n g} (\\text {S a m p l e} _ {\\alpha} ^ {l} (\\mathcal {F} (V ^ {l})))\n$$\n\nand\n\n$$\n\\operatorname {A t t e n t i o n} \\left(\\tilde {\\boldsymbol {Q}} _ {t} ^ {l}, \\tilde {\\boldsymbol {K}} _ {t} ^ {l}, \\tilde {\\boldsymbol {V}} ^ {l}\\right) = \\operatorname {s o f t m a x} \\left(\\frac {\\tilde {\\boldsymbol {Q}} _ {t} ^ {l} \\left(\\tilde {\\boldsymbol {K}} _ {t} ^ {l}\\right) ^ {\\top}}{\\sqrt {D}}\\right) \\tilde {\\boldsymbol {V}} ^ {l} \\tag {11}\n$$\n\nIn this way, our time domain self-attention can learn not only the low-frequency information in the top layers but also the high-frequency in the bottom layers, thus boosting the model's capability of capturing local behaviors. For the multi-head version used in FEARec, given hidden variables of  $D$  channels,  $h$  heads, the query, key and value for  $i$ -th head  $Q_{i}$ ,  $\\mathcal{K}_{i}$ ,  $\\mathcal{V}_{i} \\in \\mathbb{R}^{N \\times \\frac{D}{h}}$ ,  $i \\in \\{1, \\dots, h\\}$ , we have:\n\n$$\n\\begin{array}{r l} \\operatorname {M u l t i H e a d} (Q, \\mathcal {K}, \\mathcal {V}) & = \\operatorname {C o n c a t} \\left(\\operatorname {h e a d} _ {1}, \\dots , \\operatorname {h e a d} _ {h}\\right) \\mathcal {W} \\\\ \\text {w h e r e} & \\operatorname {h e a d} _ {i} = \\operatorname {A t t e n t i o n} \\left(Q _ {i}, \\mathcal {K} _ {i}, \\mathcal {V} _ {i}\\right) \\end{array} \\tag {12}\n$$\n\nWhere learnable output matrix  $\\mathcal{W} \\in \\mathbb{R}^{\\mathcal{D} \\times \\mathcal{D}}$ . Hence, the multihead attention operation for  $\\tilde{Q}_t^l, \\tilde{K}_t^l, \\tilde{V}^l$  in time domain equals to MultiHead_time( $\\tilde{Q}_t^l, \\tilde{K}_t^l, \\tilde{V}^l$ ).\n\n3.2.3 Frequency Domain Attention Layer. Existing self-attentive sequential recommenders tend to capture a global view of user interactions at the item level, missing the periodic similar behavior patterns captured at the sequence level. As discussed in Section 1, by calculating the auto-correlation, we can find the most related time-delay sequences in the frequency domain and thus discover the periodicity hidden in the behaviors. Specifically, as shown as Figure 1, given a finite sequence of user  $s_u = [i_1^{(u)}, i_2^{(u)}, i_3^{(u)}, \\dots, i_{N - 1}^{(u)}, i_N^{(u)}]$ , the\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/376d8d2b-b898-465f-a300-bea4a70ea992/56f4da76be6390c1fb65e3a72ed06156ac443c7c7bd8a80eccb49b34cee4b463.jpg)  \nFigure 4: The architecture of hybrid attention layer.\n\ntime delay operation is defined as  $\\mathrm{Roll}(s_u,\\tau)$ , where  $\\tau \\in \\{1,\\dots ,N\\}$  indicates the time lag and  $\\mathrm{Roll}(s_u,\\tau)$  can be formulated as:\n\n$$\n\\operatorname {R o l l} \\left(s _ {u}, \\tau\\right) = \\left[ i _ {\\tau + 1} ^ {(u)}, \\dots , i _ {N} ^ {(u)}, i _ {1} ^ {(u)}, \\dots , i _ {\\tau} ^ {(u)} \\right] \\tag {13}\n$$\n\nwhere  $\\operatorname{Roll}(s_u, N) = s_u$ . Its corresponding embedding matrix is denoted as  $\\tilde{\\boldsymbol{V}}_{\\tau}^{l}$ . We perform a similar attention mechanism via autocorrelation in the frequency domain. For the queries  $Q^{l} \\in \\mathbb{R}^{N \\times D}$ , keys  $\\boldsymbol{K}^{l} \\in \\mathbb{R}^{N \\times D}$ , and values  $\\boldsymbol{V}^{l} \\in \\mathbb{R}^{N \\times D}$ , the auto-correlation is defined as\n\n$$\n\\tilde {Q} _ {f} ^ {l} = \\operatorname {S a m p l e} _ {\\alpha} ^ {l} (\\mathcal {F} (Q ^ {l}))\n$$\n\n$$\n\\tilde {\\boldsymbol {K}} _ {f} ^ {l} = \\operatorname {S a m p l e} _ {\\alpha} ^ {l} (\\mathcal {F} (\\boldsymbol {K} ^ {l})) \\tag {14}\n$$\n\n$$\n\\mathcal {R} _ {\\tilde {Q} _ {f} ^ {l}, \\tilde {\\boldsymbol {K}} _ {f} ^ {l}} (\\tau) = \\mathcal {F} ^ {- 1} (\\text {P a d d i n g} (\\tilde {\\boldsymbol {Q}} _ {f} ^ {l} \\odot (\\tilde {\\boldsymbol {K}} _ {f} ^ {l}) ^ {*}))\n$$\n\nwhere  $\\odot$  represents the element-wise product and  $^*$  means the conjugate operation. We denote the sampled feature as  $\\tilde{Q}_f^l,\\tilde{K}_f^l\\in$ $\\mathbb{C}^{\\alpha M\\times D}$ $\\tilde{Q}_f^l\\odot (\\tilde{K}_f^l)^* \\in \\mathbb{C}^{\\alpha M\\times D}$  needs to be zero-padded to  $\\mathbb{C}^{M\\times D}$  before performing inverse Fourier transform. Since the information used to compute the attention scores is coming from within the same sequence, here we consider it to be auto-correlation rather than cross-correlation. The auto-correlation  $\\mathcal{R}_{\\tilde{Q}_f^l,\\tilde{K}_f^l}(\\tau)\\in \\mathbb{R}^{N\\times D}$  is based on the Wiener-Khinchin theorem (more details in the Appendix A.2). We further conduct MEAN operation on  $\\mathcal{R}_{\\tilde{Q}_f^l,\\tilde{K}_f^l}(\\tau)$  to transfer its dimension  $\\mathbb{R}^{N\\times D}$  into  $\\mathbb{R}^N$\n\nThe auto-correlation refers to the correlation of a time sequence with its own past and future, we choose the most related  $\\tau_{i},\\dots,\\tau_{k}$  from  $\\mathcal{R}_{\\tilde{\\boldsymbol{Q}}_f^l,\\tilde{\\boldsymbol{K}}_f^l}(\\tau)$ :\n\n$$\n\\tau_ {1}, \\dots , \\tau_ {k} = \\underset {\\tau \\in \\{1, \\dots , N \\}} {\\arg \\operatorname {T o p k}} \\left(\\mathcal {R} _ {\\tilde {Q} _ {f} ^ {l}, \\tilde {K} _ {f} ^ {l}} (\\tau)\\right) \\tag {15}\n$$\n\nwhere  $\\arg \\operatorname{Topk}(\\cdot)$  is to get the arguments of the Top  $k$  auto-correlation and  $k = \\lfloor m \\times \\log N \\rfloor$ . In this way, we get the most relevant time delay sequences  $\\mathrm{Roll}(s_u, \\tau_i)$ ,  $\\tau_i \\in \\{\\tau_1, \\dots, \\tau_k\\}$ . Then, the attention weights of different sequences are calculated as:\n\n$$\n\\widehat {\\mathcal {R}} _ {\\tilde {Q} _ {f} ^ {l}, \\tilde {K} _ {f} ^ {l}} (\\tau_ {1}), \\dots , \\widehat {\\mathcal {R}} _ {\\tilde {Q} _ {f} ^ {l}, \\tilde {K} _ {f} ^ {l}} (\\tau_ {k}) = \\tag {16}\n$$\n\n$$\n\\operatorname {S o f t M a x} (\\mathcal {R} _ {\\tilde {Q} _ {f} ^ {l}, \\tilde {K} _ {f} ^ {l}} (\\tau_ {1}), \\dots , \\mathcal {R} _ {\\tilde {Q} _ {f} ^ {l}, \\tilde {K} _ {f} ^ {l}} (\\tau_ {k}))\n$$\n\nHence, the  $k$  most similar sequences can be aggregated by multiplying with their corresponding auto-correlations, which is called time delay aggregation in Figure 4:\n\n$$\n\\text {A u t o - C o r r e l a t i o n} \\left(\\tilde {\\boldsymbol {Q}} _ {f} ^ {l}, \\tilde {\\boldsymbol {K}} _ {f} ^ {l}, \\tilde {\\boldsymbol {V}} ^ {l}\\right) = \\sum_ {i = 1} ^ {k} \\tilde {\\boldsymbol {V}} _ {\\tau_ {i}} ^ {l} \\widehat {\\mathcal {R}} _ {\\tilde {\\boldsymbol {Q}} _ {f} ^ {l}, \\tilde {\\boldsymbol {K}} _ {f} ^ {l}} \\left(\\tau_ {i}\\right) \\tag {17}\n$$\n\nwhere Auto-Correlation  $(\\tilde{Q}_f^l,\\tilde{K}_f^l,\\tilde{\\mathbf{V}}^l)\\in \\mathbb{R}^{N\\times D}$ . The multi-head attention operation is also conducted for  $\\tilde{Q}_f^l,\\tilde{K}_f^l,\\tilde{\\mathbf{V}}^l$  in frequency domain, which equals to MultiHeadFrequency  $(\\tilde{Q}_f^l,\\tilde{K}_f^l,\\tilde{\\mathbf{V}}^l)$ . Finally, we sum the output of the frequency domain attention module and the output of the time domain attention module with the hyperparameter  $\\gamma$ .\n\n$$\n\\widehat {\\mathbf {H}} ^ {l} = \\gamma \\operatorname {M u l t i H e a d} _ {T i m e} \\left(\\tilde {\\boldsymbol {Q}} _ {t} ^ {l}, \\tilde {\\boldsymbol {K}} _ {t} ^ {l}, \\tilde {\\boldsymbol {V}} ^ {l}\\right) + \\tag {18}\n$$\n\n$$\n(1 - \\gamma) \\operatorname {M u l t i H e a d} _ {F r e q u e n c y} (\\tilde {\\boldsymbol {Q}} _ {f} ^ {l}, \\tilde {\\boldsymbol {K}} _ {f} ^ {l}, \\tilde {\\boldsymbol {V}} ^ {l})\n$$\n\n3.2.4 Point-wise Feed Forward Network. The frequency domain attention and time domain self-attention are still linear operations, which fail to model complex non-linear relations. To make the network non-linear, we use a two-layer MLP with a GELU activation function. The process is defined as follows:\n\n$$\n\\tilde {\\mathbf {H}} ^ {l} = \\operatorname {F F N} \\left(\\widehat {\\mathbf {H}} ^ {l}\\right) = \\left(\\operatorname {G E L U} \\left(\\widehat {\\mathbf {H}} ^ {l} \\mathbf {W} _ {1} ^ {l} + \\mathbf {b} _ {1} ^ {l}\\right)\\right) \\mathbf {W} _ {2} ^ {l} + \\mathbf {b} _ {2} ^ {l} \\tag {19}\n$$\n\nwhere  $\\mathbf{W}_1^l, \\mathbf{W}_2^l \\in \\mathbb{R}^{d \\times d}$  and  $\\mathbf{b}_1^l, \\mathbf{b}_2^l \\in \\mathbb{R}^{1 \\times d}$  are learnable parameters. To avoid overfitting, dropout layer, residual connection structure, and layer normalization operations are applied on the obtained output  $\\mathbf{H}^{l+1}$ , as shown below:\n\n$$\n\\mathbf {H} ^ {l + 1} = \\operatorname {L a y e r N o r m} \\left(\\mathbf {H} ^ {l} + \\widehat {\\mathbf {H}} ^ {l} + \\operatorname {D r o p o u t} \\left(\\tilde {\\mathbf {H}} ^ {l}\\right)\\right) \\tag {20}\n$$\n\n# 3.3 Prediction Layer\n\nAfter  $L$  FEA blocks that hierarchically extract behavior pattern information of previously interacted items, we get the final combined representation of behavior sequences. Based on the preference representation  $\\mathbf{H}^L$ , we multiply it by the item embedding matrix  $\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{I}| \\times D}$  to predict the relevance of the candidate item and use softmax to convert it into recommendation probability:\n\n$$\n\\hat {\\mathbf {y}} = \\operatorname {s o f t m a x} \\left(\\mathbf {E} ^ {\\top} \\mathbf {H} ^ {L}\\right) \\tag {21}\n$$\n\nHence, we expect that the true item  $i$  adopted by user  $u$  can lead to a higher score  $\\hat{y}_i$ . Therefore, we adopt the cross-entropy loss to optimize the model parameter. The objective function of SR can be formulated as:\n\n$$\n\\mathcal {L} _ {R e c} = - \\sum_ {i = 1} ^ {| I |} y _ {i} \\log (\\hat {y} _ {i}) + (1 - y _ {i}) \\log (1 - \\hat {y} _ {i}) \\tag {22}\n$$\n\n# 3.4 Multi-Task Learning\n\nTo enhance the training of hybrid attention to capture attention scores and periodic behavior patterns of different frequency bands of user sequence, we leverage a multi-task training strategy to jointly optimize the main recommendation loss with auxiliary dual domain regularization.\n\n3.4.1 Contrastive Learning. Contrastive learning aims to minimize the difference between differently augmented views of the same user and maximize the difference between the augmented sequences derived from different users.\n\nAlthough previous augmentations methods [8] including item cropping, masking, and reordering help to enhance the performance of SR models, the data-level augmentations cannot guarantee a high level of semantic similarity [7]. Instead of using typical data augmentations, we use a dropout-based augmentations methods as shown in the right part of Figure 2, which is proposed in [7, 49]. We let  $\\mathbf{E}_u$  and  $\\mathbf{E}_u'$  pass through the FEA encoder twice for two output views  $\\mathbf{H}_u^L$  and  $(\\mathbf{H}_u^L)'$  respectively and model the frequency components to construct harder positive samples by mixing the frequency feature extract from time domain self-attention and frequency autocorrelation attention. Since there are different dropout layers in the embedding layer and hybrid attention module, we will get two different numerical features but similar semantics. Besides, in order to increase the supervision signal of contrast learning, we follow DuoRec [7] and use a sequence  $s_{u,s}$  with the same target as  $s_u$  as the positive sample of supervised contrast learning. All the other augmented samples in the training batch are treated as negative samples in order to efficiently create the negative samples for an augmented pair of samples (represented as neg).\n\nFor the batch  $\\mathcal{B}$ , the contrastive regularization is defined as:\n\n$$\n\\mathcal {L} _ {\\mathrm {C L R e g}} = \\mathcal {L} _ {\\mathrm {C L R e g}} \\left(\\mathbf {h} _ {u} ^ {\\prime}, \\mathbf {h} _ {u, s} ^ {\\prime}\\right) + \\mathcal {L} _ {\\mathrm {C L R e g}} \\left(\\mathbf {h} _ {u, s} ^ {\\prime}, \\mathbf {h} _ {u} ^ {\\prime}\\right) \\tag {23}\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {C L R e g}} \\left(\\mathbf {h} _ {u} ^ {\\prime}, \\mathbf {h} _ {u, s} ^ {\\prime}\\right) = - \\log \\frac {\\exp \\left(\\operatorname {s i m} \\left(\\mathbf {h} _ {u} ^ {\\prime} , \\mathbf {h} _ {u , s} ^ {\\prime}\\right)\\right)}{\\sum_ {n e q} \\exp \\left(\\operatorname {s i m} \\left(\\mathbf {h} _ {u} ^ {\\prime} , \\mathbf {h} _ {n e q}\\right)\\right)} \\tag {24}\n$$\n\nwhere  $sim(\\cdot)$  is dot product and  $\\mathbf{h}_u^{\\prime}$  and  $\\mathbf{h}_{u,s}^{\\prime}$  represent unsupervised and supervised augmented views, respectively, defined as follows:\n\n$$\n\\mathbf {h} _ {u} ^ {\\prime} = \\left(\\mathbf {H} _ {u} ^ {L}\\right) ^ {\\prime} [ - 1 ], \\quad \\mathbf {h} _ {u, s} ^ {\\prime} = \\left(\\mathbf {H} _ {u, s} ^ {L}\\right) ^ {\\prime} [ - 1 ] \\tag {25}\n$$\n\n3.4.2 Frequency Domain Regularization. The contrastive loss is intuitively performing the push and pull game according to Equation 24 in the time domain. Since time-domain and frequency-domain features represent the same semantics, but only in different domains, we assume that the frequency spectrum of similar time-domain features should also be similar. To ensure the alignment of the representation of different augmented views in the frequency domain, we suggest an L1 regularization in the frequency domain as a complement to FEARec, which contributes to enriching the regularization of the spectrum of the augmented views.\n\n$$\n\\mathcal {L} _ {F R e g} = \\left\\| \\mathcal {F} \\left(\\mathbf {h} _ {u} ^ {\\prime}\\right) - \\mathcal {F} \\left(\\mathbf {h} _ {u, s} ^ {\\prime}\\right) \\right\\| _ {1} \\tag {26}\n$$\n\n3.4.3 Train and Inference. Thus, the overall objective of FEARec with  $\\lambda$  scale weight is:\n\n$$\n\\ell = \\ell_ {\\text {R e c}} + \\lambda_ {1} \\ell_ {\\text {C R e g}} + \\lambda_ {2} \\ell_ {\\text {F R e g}} \\tag {27}\n$$\n\nwhere  $\\lambda$  is the hyperparameter to control the strengths of contrastive regularization.",
  "experiments": "# 4 EXPERIMENT\n\nIn this section, we first briefly describe the settings in our experiments and then conduct extensive experiments to evaluate our proposed model by answering the following research questions:\n\nTable 1: Statistics of the datasets after preprocessing.  \n\n<table><tr><td>Specs.</td><td>Beauty</td><td>Clothing</td><td>Sports</td><td>ML-1M</td></tr><tr><td># Users</td><td>22,363</td><td>39,387</td><td>35,598</td><td>6,041</td></tr><tr><td># Items</td><td>12,101</td><td>23,033</td><td>18,357</td><td>3,417</td></tr><tr><td># Avg.Length</td><td>8.9</td><td>7.1</td><td>8.3</td><td>165.5</td></tr><tr><td># Actions</td><td>198,502</td><td>278,677</td><td>296,337</td><td>999,611</td></tr><tr><td>Sparsity</td><td>99.93%</td><td>99.97%</td><td>99.95%</td><td>95.16%</td></tr></table>\n\n- RQ1: How does FEARc perform compared with state-of-the-art SR models?  \n- RQ2: How do key components, such as frequency sampling, time domain self-attention and frequency domain attention affect the performance of FEARc respectively?  \n- RQ3: What is the influence of different hyper-parameters in FEARec?  \n- RQ4: Do frequency domain attention and time domain attention focus on the same features?\n\n# 4.1 Experimental Setup\n\n4.1.1 Dataset. We conduct experiments on four publicly available benchmark datasets. Beauty, Clothing, and Sports $^2$  are three subsets of Amazon Product dataset, which is known for high sparsity and short sequence lengths. MovieLens-1M $^3$  is a large and dense dataset consisting of long item sequences collected from the movie recommendation site MovieLens. While ML-1M only contains about 1 million interactions. For all datasets, users/items interacted with less than 5 items/users were removed[1, 50]. The statistics of the four datasets after preprocessing are summarized in Table 1.\n\n4.1.2 Evaluation Metrics. In evaluation, we adopt the leave-one-out strategy for each user's item sequence. As suggested by [51], we rank the predictions over the whole item set without negative sampling. We report the widely used Top- $n$  metrics  $\\mathbf{HR}@\\bar{n}$  (Hit Rate) and  $\\mathbf{NDCG}@\\bar{n}$  (Normalized Discounted Cumulative Gain) to evaluate the recommended lists, where  $n$  is set to 5, 10.\n\n4.1.3 Baselines. We compare our methods with three types of representative SR models: Non-sequential models: BPR-MF [52] is a classic non-sequential method for learning personalized ranking from implicit feedback and optimizing the matrix factorization through a pair-wise Bayesian Personalized Ranking (BPR) loss. Standard sequential models: GRU4Rec [2] is the first recurrent model to apply Gated Recurrent Unit (GRU) to model sequences of user behavior for sequential recommendation. Caser [25] is a CNN-based method capturing user dynamic patterns by using convolutional filters. SASRec [5] is a strong Transformer-based model with a multi-head self-attention mechanism. FMLP-Rec [18] is an allMLP model using a learnable filter-enhanced block to remove noise in the embedding matrix. Sequential models with contrastive learning: CL4SRec [8] generates different contrastive views of the same user interaction sequence for the auxiliary contrastive learning task. CoSeRec [6] improves the robustness of data augmentation under the contrastive framework by leveraging item-correlations.\n\nTable 2: Overall performance over four datasets. Bold scores represent the highest results of all methods. Underlined scores stand for the highest results from previous methods. The FEARc achieves the state-of-the-art result among all baseline models.  \n\n<table><tr><td>Datasets</td><td>Metric</td><td>BPR-MF</td><td>GRU4Rec</td><td>Caser</td><td>SASRec</td><td>BERT4Rec</td><td>FMLP-Rec</td><td>CL4SRec</td><td>CoSeRec</td><td>DuoRec</td><td>FEARec</td><td>Improv.</td></tr><tr><td rowspan=\"4\">Beauty</td><td>HR@5</td><td>0.0120</td><td>0.0164</td><td>0.0259</td><td>0.0365</td><td>0.0193</td><td>0.0398</td><td>0.0401</td><td>0.0537</td><td>0.0546</td><td>0.0597</td><td>9.34%</td></tr><tr><td>HR@10</td><td>0.0299</td><td>0.0365</td><td>0.0418</td><td>0.0627</td><td>0.0401</td><td>0.0632</td><td>0.0683</td><td>0.0752</td><td>0.0845</td><td>0.0884</td><td>4.61%</td></tr><tr><td>NDCG@5</td><td>0.0040</td><td>0.0086</td><td>0.0127</td><td>0.0236</td><td>0.0187</td><td>0.0258</td><td>0.0223</td><td>0.0361</td><td>0.0352</td><td>0.0366</td><td>3.97%</td></tr><tr><td>NDCG@10</td><td>0.0053</td><td>0.0142</td><td>0.0253</td><td>0.0281</td><td>0.0254</td><td>0.0333</td><td>0.0317</td><td>0.0430</td><td>0.0443</td><td>0.0459</td><td>3.61%</td></tr><tr><td rowspan=\"4\">Clothing</td><td>HR@5</td><td>0.0067</td><td>0.0095</td><td>0.0108</td><td>0.0168</td><td>0.0125</td><td>0.0173</td><td>0.0168</td><td>0.0175</td><td>0.0193</td><td>0.0214</td><td>10.88%</td></tr><tr><td>HR@10</td><td>0.0094</td><td>0.0165</td><td>0.0174</td><td>0.0272</td><td>0.0208</td><td>0.0277</td><td>0.0266</td><td>0.279</td><td>0.0302</td><td>0.0323</td><td>6.95%</td></tr><tr><td>NDCG@5</td><td>0.0052</td><td>0.0061</td><td>0.0067</td><td>0.0091</td><td>0.0075</td><td>0.0098</td><td>0.0090</td><td>0.0095</td><td>0.0113</td><td>0.0121</td><td>7.08%</td></tr><tr><td>NDCG@10</td><td>0.0069</td><td>0.0083</td><td>0.0098</td><td>0.0124</td><td>0.0102</td><td>0.0127</td><td>0.0121</td><td>0.0131</td><td>0.0148</td><td>0.0156</td><td>5.41%</td></tr><tr><td rowspan=\"4\">Sports</td><td>HR@5</td><td>0.0092</td><td>0.0137</td><td>0.0139</td><td>0.0218</td><td>0.0176</td><td>0.0218</td><td>0.0227</td><td>0.0287</td><td>0.0326</td><td>0.0353</td><td>8.28%</td></tr><tr><td>HR@10</td><td>0.0188</td><td>0.0274</td><td>0.0231</td><td>0.0336</td><td>0.0326</td><td>0.0344</td><td>0.0374</td><td>0.0437</td><td>0.0498</td><td>0.0547</td><td>9.84%</td></tr><tr><td>NDCG@5</td><td>0.0040</td><td>0.0096</td><td>0.0085</td><td>0.0127</td><td>0.0105</td><td>0.0144</td><td>0.0129</td><td>0.0196</td><td>0.0208</td><td>0.0216</td><td>3.85%</td></tr><tr><td>NDCG@10</td><td>0.0051</td><td>0.0137</td><td>0.0126</td><td>0.0169</td><td>0.0153</td><td>0.0185</td><td>0.0184</td><td>0.0242</td><td>0.0262</td><td>0.0272</td><td>3.82%</td></tr><tr><td rowspan=\"4\">ML-1M</td><td>HR@5</td><td>0.0078</td><td>0.0763</td><td>0.0816</td><td>0.1087</td><td>0.0733</td><td>0.1109</td><td>0.1147</td><td>0.1262</td><td>0.2038</td><td>0.2212</td><td>8.54%</td></tr><tr><td>HR@10</td><td>0.0162</td><td>0.1658</td><td>0.1593</td><td>0.1904</td><td>0.1323</td><td>0.1932</td><td>0.1975</td><td>0.2212</td><td>0.2946</td><td>0.3123</td><td>6.01%</td></tr><tr><td>NDCG@5</td><td>0.0052</td><td>0.0385</td><td>0.0372</td><td>0.0638</td><td>0.0432</td><td>0.0657</td><td>0.0662</td><td>0.0761</td><td>0.1390</td><td>0.1523</td><td>9.57%</td></tr><tr><td>NDCG@10</td><td>0.0079</td><td>0.0671</td><td>0.0624</td><td>0.0910</td><td>0.0619</td><td>0.0918</td><td>0.0928</td><td>0.1021</td><td>0.1680</td><td>0.1861</td><td>10.77%</td></tr></table>\n\nDuoRec [7] uses unsupervised model-level augmentation and supervised semantic positive samples for contrastive learning. It is the most recent and strongest baseline for the sequential recommendation.\n\n4.1.4 Implementation Details. For all baseline models, we implement its model and report the result of each model with its optimal hyperparameter settings reported in the original paper. We implement our FEARec model in PyTorch. All the experiments are conducted on an NVIDIA V100 GPU with 32GB memory. For all datasets, the maximum sequence length  $N$  is set to 50. The dimension of the feed-forward network used in the filer mixer blocks and item embedding size  $d$  are both set to 64. The number of hybrid attention blocks  $L = 2$ . The model is optimized by Adam optimizer with a learning rate of 0.001.\n\n# 4.2 Overall Performance Comparison (Q1)\n\nTo prove the sequential recommendation performance of our model FEARec, we compare it with other state-of-the-art methods (RQ1). Table 2 presents the detailed evaluation results of each model.\n\nFirst, it is no doubt that the non-sequential recommendation method BPR-MF displays the lowest results across all datasets since it ignores the sequential information. Second, compared with the previous RNN-based and CNN-based methods, the advanced Transformer-based method (e.g., SASRec) shows a stronger capability of modeling interaction sequences in SR. More recently, a filter-enhanced MLP structure achieved better performance than SASRec by denoising with a learnable filter. Third, compared with the vanilla method, the model with auxiliary self-supervised learning tasks gains decent improvement. The strongest baseline DuoRec outperforms all the previous methods by a large margin by model\n\nTable 3: Ablation study of FEARec in terms of HR@5 and NDCG@5 on Beauty, Clothing and ML-1M datasets.  \n\n<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Beauty</td><td colspan=\"2\">Clothing</td><td colspan=\"2\">ML-1M</td></tr><tr><td>HR@5</td><td>NDCG@5</td><td>HR@5</td><td>NDCG@5</td><td>HR@5</td><td>NDCG@5</td></tr><tr><td>FEARec</td><td>0.0597</td><td>0.0366</td><td>0.0208</td><td>0.0119</td><td>0.2212</td><td>0.1523</td></tr><tr><td>(a) w/o STD</td><td>0.0578</td><td>0.0356</td><td>0.0214</td><td>0.0117</td><td>0.2141</td><td>0.1482</td></tr><tr><td>(b) w/o SFD</td><td>0.0566</td><td>0.0348</td><td>0.0204</td><td>0.0115</td><td>0.2134</td><td>0.1457</td></tr><tr><td>(c) w/o TDA</td><td>0.0589</td><td>0.0358</td><td>0.0203</td><td>0.0113</td><td>0.2123</td><td>0.1441</td></tr><tr><td>(d) w/o FDA</td><td>0.0590</td><td>0.0361</td><td>0.0202</td><td>0.0116</td><td>0.2116</td><td>0.1470</td></tr><tr><td>(e) w/o FReg</td><td>0.0582</td><td>0.0360</td><td>0.0208</td><td>0.0119</td><td>0.2114</td><td>0.1433</td></tr><tr><td>(f) w/o CReg</td><td>0.0567</td><td>0.0342</td><td>0.0191</td><td>0.0106</td><td>0.2028</td><td>0.1408</td></tr></table>\n\naugmentation and semantic augmentation, which verify the effectiveness of the combination of supervised contrastive learning. Finally, our model outperforms other competing methods on both sparse and dense datasets with a significant margin across all the metrics, demonstrating the superiority of our model. In addition, the average improvement on ML-1M is actually larger than that on the Amazon dataset, probably because the average length of users' interaction is extremely short in Amazon. This observation demonstrates the effectiveness of FEARec in both sparse and dense dataset and shows that transforming user sequence from the time domain to the frequency domain and improving the original attention mechanism with frequency ramp structure and extra frequency domain attention is promising to extract useful information for accurate recommendation.\n\n# 4.3 Ablation Study (Q2)\n\nIn this section, we conduct ablation studies to evaluate the effectiveness of each key component. Table 3 shows the performance of our default method and its 6 variants on three datasets.\n\nTable 4: Performance (HR@5) of time-domain self-attention (TDA) and frequency-domain attention (FDA) module mixed in different proportions on four datasets.  \n\n<table><tr><td>Hybrid</td><td>TDA</td><td>FDA</td><td>Beauty</td><td>Clothing</td><td>Sports</td><td>ML-1M</td></tr><tr><td>(1)</td><td>0.1</td><td>0.9</td><td>0.0596</td><td>0.0214</td><td>0.0349</td><td>0.1863</td></tr><tr><td>(2)</td><td>0.3</td><td>0.7</td><td>0.0584</td><td>0.0204</td><td>0.0335</td><td>0.2113</td></tr><tr><td>(3)</td><td>0.5</td><td>0.5</td><td>0.0557</td><td>0.0207</td><td>0.0346</td><td>0.2154</td></tr><tr><td>(4)</td><td>0.7</td><td>0.3</td><td>0.0591</td><td>0.0203</td><td>0.0342</td><td>0.2169</td></tr><tr><td>(5)</td><td>0.9</td><td>0.1</td><td>0.0592</td><td>0.0211</td><td>0.0339</td><td>0.2212</td></tr></table>\n\nFrequency Ramp Structure. In order to verify the effectiveness of the frequency ramp structure sampling in FEARec, we remove the sampling structure from the time-domain attention module (w/o STD) and frequency-domain (w/o SFD), which means that sets the sampling ratio  $\\alpha$  equals 1, that is, all frequency components are retained. Compared with (a)-(b), we find that without frequency ramp sampling in time domain or frequency domain, the performance drops significantly, which verifies the structure can trade-off high-frequency and low-frequency components across all layers. In summary, high-frequency information is captured at the bottom layer, then low-frequency information is gradually captured, and finally, the overall evolution of user preferences is obtained.\n\nHybrid Attention Module. From (c)-(d) we can see that each attention module plays a crucial role in modeling user preference, and removing time domain attention (w/o TDA) or frequency domain attention (w/o FDA) leads to a performance decrease. The time-domain self-attention layer captures the attention score in the item level. However, the frequency domain attention layer computes the auto-correlation to recognize the periodic characterize in the sub-sequence level. The hybrid attention module combines two attention modules into a unified structure as illustrated in Figure 4, which is promising to better capture user preference.\n\nContrastive Learning and regularization. From (e)-(f) we can see that combining contrastive learning regularization and a simple frequency domain regularization will improve the performance. When removing the contrastive auxiliary task of contrastive learning will hurt performance, which is consistent with the previous observation that sequential recommendation benefits from contrastive learning [6-8]. As a complement to contrastive learning, frequency domain regularization can implicitly reduce the distance between the spectrum of two augmented perspectives, but adopting it alone will yield poor results.\n\n# 4.4 Hyper-parameter Sensitivity (Q3)\n\nIn this section, we study the influence of three important hyperparameters in FEARec, including sampling ratio  $\\alpha$ , hybrid ratio  $\\gamma$  and autocorrelation top  $k$ . We keep all other hyperparameters optimal when investigating the current hyperparameter.\n\nModule Hybrid Ratio  $\\gamma$ . In order to simultaneously model user preferences at the item level and sub-sequence level, we mix the original TDA and FDA through Figure 4, and the weight of the frequency domain and time domain is controlled by Eq. (18). From the table 4, we can see that on the Beauty, Clothing and Sports dataset, the best effect is achieved when the FDA module is assigned greater weight than the TDA module, which verifies the\n\neffectiveness of frequency domain attention. While on the dense ML-1M dataset, the TDA module plays a more important role.\n\nSampling Ratio  $\\alpha$ . The ratio of frequency components is controlled by  $\\alpha$ , which significantly affects how much of the frequency component is retained. Although the average length of user interactions varies across data sets, the number of all frequency components in the frequency domain is  $M$ . We conduct experiments under different  $\\alpha$  on four datasets. As shown in Figure 5, we observe that with the increase of  $\\alpha$  the performance of FEARec starts to increase at the beginning, and it gradually reaches its peak when  $\\alpha$  equals 0.8 on Amazon-Beauty and Amazon-Clothing, and 0.6 on Amazon-Sports. Afterward, it starts to decline. While on dense datasets like ML-1M, to capture more complex sequential patterns of users, the hybrid attention structure requires more frequency components. Our method FEARec always performs better than DuoRec, regardless of the value of  $\\alpha$ .\n\nAuto-correlation Top-  $k$ . In frequency domain attention, we need to aggregate the Top-  $k$  most relevant time delay sequence, where  $k$  is determined by the parameter  $m$  in section 3.2.3. We gradually increase the parameter  $m$  to aggregate more time delayed sequences with high autocorrelation. As shown in Figure 6, we can observe that the performance of FEARec consistently improves as we increase  $m$  up to a certain point. However, it is worth noting that further increasing  $m$  beyond a certain value will result in performance degradation, which is not shown in the figure. This is because when  $m$  is too large, the frequency domain attention module may start to include irrelevant time delayed sequences, which can negatively impact the model's performance.\n\n# 4.5 Attention Visualization (Q4)\n\nTo evaluate how the hybrid attention capture sequential behavior in both item-level and sub-sequence level, the attention score and autocorrelation score learned in TDA and FAD in each layer are visualized on the ML-1M dataset. The results are displayed in Figure 7. Based on our observation, we can derive the following results: the time-domain attention in the hybrid attention module tends to assign more weight to the items initially interacted with because the users established their basic preference among these items, which constitute a critical sequential pattern of the user. Different from the time-domain attention of item level, the visualization results of frequency-domain attention represent the autocorrelation weights of different time delay sequences. It can be seen from Figure 6(b) and (d) that if the time delay is small or large for the original sequence, a higher autocorrelation score will be obtained, which effectively show periodic characteristics on ML-1M datasets.",
  "hyperparameter": ""
}