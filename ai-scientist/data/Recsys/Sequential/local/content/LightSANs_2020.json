{
  "id": "LightSANs_2020",
  "paper_title": "LightSANs: Lightweight Self-Attention Networks for Sequential Recommendation",
  "alias": "LightSANs",
  "year": 2020,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "FDSA (Feature-level Deeper Self-Attention Network) proposes using two separate self-attention blocks to model sequential recommendation: an item-based self-attention block to capture item-level transition patterns and a feature-based self-attention block to learn feature-level (e.g., category, brand) transition patterns. The key innovation is modeling item sequences and feature sequences independently through separate self-attention mechanisms, rather than concatenating item and feature representations as input to a single self-attention network. A vanilla attention mechanism is also employed to adaptively weight different heterogeneous features (categorical and textual) of items before feeding them into the feature-based self-attention block.",
  "introduction": "# 1 Introduction\n\nWith the quick development of the Internet, sequential recommendation has become essential in various applications, such as ad click prediction, purchase recommendation and web page recommendation. In such applications, each user behavior can be modeled as a sequence of activities in chronological order, with his/her following activity influenced by the previous activities. And sequential recommendation aims to recommend the next item that a user will likely interact by\n\ncapturing useful sequential patterns from user historical behaviors.\n\nIncreasing research interests have been put in sequential recommendation with various models proposed. For modeling sequential patterns, the classic Factorizing Personalized Markov Chain (FPMC) model has been introduced to factorize the user-specific transition matrix by considering the Markov Chains [Rendle et al., 2010]. However, the Markov assumption has difficulty in constructing a more effective relationship among factors [Huang et al., 2018]. With the success of deep learning, Recurrent Neural Network (RNN) methods have been widely adopted in sequential recommendation [Hidasi et al., 2016; Zhao et al., 2019]. These RNN methods usually employ the last hidden state of RNN as the user representation, which is used to predict the next action. Despite the success, these RNN models are difficult to preserve long-range dependencies even using the advanced memory cell structures like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) [Chung et al., 2014]. Besides, RNN-based methods need to learn to pass relevant information forward step by step, which makes RNN hard to parallelize [Al-Rfou et al., 2019]. Recently, self-attention networks (SANs) have shown promising empirical results in various NLP tasks, such as machine translation [Vaswani et al., 2017], natural language inference [Shen et al., 2018], and question answering [Li et al., 2019]. One strong point of self-attention networks is the strength of capturing long-range dependencies by calculating attention weights between each pair of items in a sequence. Inspired by self-attention networks, Kang et al. [Kang and McAuley, 2018] proposed Self-Attentive Sequential Recommendation model (SASRec) that applied a self-attention mechanism to replace traditional RNNs for sequential recommendation and achieved the state-of-the-art performance. However, it only considers the sequential patterns between items, ignoring the sequential patterns between features that are beneficial for capturing the user's fine-grained preferences.\n\nActually, our daily activities usually present transition patterns at the item feature level, i.e., explicit features like category or other implicit features. For example, a user is more likely to buy shoes after buying clothes, indicating that the\n\nnext product's category is highly related to the category of the current product. Here we refer to the user's evolving appetite for structured attributes (e.g., categories) as explicit feature transition. Moreover, an item may also contain some other unstructured attributes, like description texts or image, which present more details of the item. Therefore, we want to mine the user's potential feature-level patterns from these unstructured attributes, which we call implicit feature transition. However, explicit and implicit feature transitions among item features are often overlooked by existing methods. We argue that only the item-level sequences cannot reveal the full sequential patterns, while the feature-level sequences can help achieve this goal better. To this end, in this work, we propose a novel feature-level deeper self-attention network for sequential recommendation. For capturing explicit feature-level transition patterns, instead of using the combined representation of item and its features, we apply separated self-attention blocks on item sequences and feature sequences, respectively, to capture the item-item and feature-feature relationships. Then, we combine the contexts at the item-level and the feature-level to make a recommendation. Moreover, we further investigate how to capture meaningful implicit feature-level transition patterns from heterogeneous attributes of items. We additionally utilize vanilla attention to assist feature-based self-attention block to adaptively select essential features from the various types of attributes of items and further learn potential implicit feature transition patterns. Then, we combine item transition patterns with implicit feature transition patterns to a fully-connected layer for the recommendation. Finally, we conduct extensive experiments on two real-world datasets of a famous E-commerce platform. Experimental results demonstrate that considering feature-level transition patterns can significantly improve the performance of recommendation.\n\nThe main contributions of this paper are summarized as follows:\n\n- We propose a novel framework, Feature-level Deeper Self-Attention Network (FDSA), for sequential recommendation. FDSA applies self-attention networks to integrate item-level transitions with feature-level transitions for modeling user's sequential intents.  \n- Explicit and implicit feature transitions are modeled by applying different self-attention blocks on item sequences and feature sequences, respectively. For obtaining implicit feature transitions, a vanilla attention mechanism is added to assist feature-based self-attention block to adaptively select important features from various item attributes.  \n- We conduct extensive experiments on two real-world datasets to demonstrate the effectiveness of our proposed method.\n",
  "method": "# 3 Feature-level Deeper Self-Attention Network for Sequential Recommendation\n\nIn this section, we first describe the problem statement in our work, and then present the architecture of our feature-level deeper self-attention network (FDSA) for next item recommendation.\n\n## 3.1 Problem Statement\n\nBefore going into the details of our proposed model, we first introduce notations used in this paper and define the sequential recommendation problem. We denote a set of users as  \\(\\mathcal{U} = \\{u_1, u_2, \\dots, u_N\\}\\)  and a set of items as  \\(\\mathcal{I} = \\{i_1, i_2, \\dots, i_M\\}\\) , where  \\(N\\)  and  \\(M\\)  are the numbers of users and items, respectively. We use  \\(\\mathcal{S} = \\{s_1, s_2, \\dots, s_{|\\mathcal{S}|}\\}\\)  to denote a sequence of items in chronological order that a user has interacted with before, where  \\(s_i \\in \\mathcal{I}\\) . Each item  \\(i\\)  has some attributes, such as category, brand, and description text. Here we take category as an example, the category of item  \\(i\\)  is denoted as  \\(c_i \\in \\mathcal{C}\\) , where  \\(\\mathcal{C}\\)  is the set of categories. The goal of sequential recommendation is to recommend the next item that the user may act on, given the user historical activities on items.\n\n## 3.2 The Network Architecture of Feature-level Deeper Self-Attention (FDSA)\n\nAs we mentioned before, daily human activities usually present feature-level (e.g., category-level) transition patterns. In this paper, we propose a novel feature-level deeper self-attention network for sequential recommendation (FDSA). FDSA utilizes not only the item-based self-attention block to learn item-level sequence patterns but a feature-based self-attention block to search for feature-level transition patterns. As shown in Figure 1 FDSA consists of five components, i.e., Embedding layer, Vanilla Attention layer, Item-based self-attention block, Feature-based self-attention block, and Fully-connected layer. Specifically, we first project the sparse representation of items and discrete attributes of items (i.e., one-hot representation) into low-dimensional dense vectors. For text attributes of items, we employ a topic model to extract the topical keywords of these texts, and then apply Word2vector to gain the word vector representation of these keywords. Due to the features (attributes) of item are often heterogeneous and come in different domains and data types. Hence, we utilize a vanilla attention mechanism to assist the self-attention network in selecting important features from the various features of items adaptively. After that, a user's sequence patterns are learned through two self-attention networks, in which the item-based self-attention block is applied to learn item-level sequence patterns, and the feature-based self-attention block is used to capture feature-level transition patterns. Finally, we integrate the outputs of these two blocks to a fully-connected layer for getting the final prediction. Next, we will introduce the details of each component of FDSA.\n\nEmbedding layer. Due to the number of user's action sequence is not fixed, we take a fixed-length sequence  \\(s = (s_1, s_2, \\dots, s_n)\\)  from user's history sequence to calculate user's historical preferences, where  \\(n\\)  denotes the maximum length that our model handles. If a user's action sequence is less than  \\(n\\) , we add zero-padding to the left side of the sequence to convert the user' action sequence to a fixed-length. If a user's sequence length is greater than  \\(n\\) , we take the most recent  \\(n\\)  behaviors. Similarly, we process the feature sequence in the same way. Let us use the category information as an example. Since each item corresponds to a category, we get a fixed-length category sequence  \\(c = (c_1, c_2, \\dots, c_n)\\) . Then, we apply a lookup layer to transform the one-hot vec\n\ntors of action sequence  \\(s\\)  and its corresponding category sequence  \\(c\\)  into dense vector representations. For other categorical features (such as brand, seller), the same way is applied. For the textual features (i.e., description text, title), we first utilize the widely-used topic model to extract the topical keywords of texts, then apply Word2vector model to learn textual semantic representations. In this paper, we extract five topical keywords from the description text and title of each item, and then apply the Mean Pooling method to fuse five topical keyword vectors into a vector representation.\n\nVanilla attention layer. Since the characteristics of items are often heterogeneous, it is difficult to know which features will determine a user's choice. Therefore, we employ vanilla attention to assist the feature-based self-attention block in capturing the user's varying appetite toward attributes (e.g., categories, brands). Given an item  \\(i\\) , its attributes can be embedded as  \\(A_{i} = \\{vec(c_{i}), vec(b_{i}), vec(item_{i}^{text})\\}\\) , where  \\(vec(c_{i})\\)  and  \\(vec(b_{i})\\)  represent the dense vector representation of category and brand of item  \\(i\\) , respectively. Also,  \\(vec(item_{i}^{text})\\)  denotes the textual feature representation of item  \\(i\\) . Formally, the attention network is defined as follows.\n\n\\[\n\\boldsymbol {\\alpha} _ {i} = \\operatorname {s o f t m a x} \\left(\\mathbf {W} ^ {f} \\mathbf {A} _ {i} + \\mathbf {b} ^ {f}\\right), \\tag {1}\n\\]\n\nwhere  \\(\\mathbf{W}^f\\)  is  \\(d\\times d\\)  matrice and  \\(\\mathbf{b}^f\\)  is  \\(d\\) -dimensional vector. Finally, we compute the feature representation of item  \\(i\\)  as a sum of the item  \\(i\\) 's attribute vector representations weighted by the attention scores as follows.\n\n\\[\n\\mathbf {f} _ {i} = \\boldsymbol {\\alpha} _ {i} \\mathbf {A} _ {i}. \\tag {2}\n\\]\n\nIt is worth noting that if item  \\(i\\)  only considers one feature (e.g., category), then the feature representation of item  \\(i\\)  is  \\(\\text{vec}(c_i)\\) .\n\nFeature-based self-attention block. Since the item-based self-attention block and the feature-based self-attention block only differ in their inputs, we focus on illustrating the process of the feature-based self-attention block in detail. From the above attention layer, we can get a feature representation  \\(\\mathbf{f}_i\\)  for item  \\(i\\) . Thus, given a user, we get the feature sequence  \\(f = \\{f_1, f_2, \\dots, f_n\\}\\) . To model category-level transition patterns, we utilize the self-attention network proposed by [Vaswani et al., 2017], which can keep the sequential contextual information and capture the relationships between categories in the category sequence, regardless of their distance. Though the self-attention network can ensure computational efficiency and derive long-term dependencies, it ignores the positional information of the sequential input [Gehring et al., 2017]. Hence, we inject a positional matrix  \\(\\mathbf{P} \\in \\mathbb{R}^{n \\times d}\\)  into the input embedding. Namely, the input matrix of the feature-based self-attention block is\n\n\\[\n\\mathbf {F} = \\left[ \\begin{array}{c} \\mathbf {f} _ {1} + \\mathbf {p} _ {1} \\\\ \\mathbf {f} _ {2} + \\mathbf {p} _ {2} \\\\ \\dots \\\\ \\mathbf {f} _ {n} + \\mathbf {p} _ {n} \\end{array} \\right]. \\tag {3}\n\\]\n\nThe scaled dot-product attention (SDPA) proposed by [Vaswani et al., 2017] is defined as below:\n\n\\[\nS D P A (\\mathbf {Q}, \\mathbf {K}, \\mathbf {V}) = \\operatorname {s o f t m a x} \\left(\\frac {\\mathbf {Q} \\mathbf {K} ^ {T}}{\\sqrt {d}}\\right) \\mathbf {V}, \\tag {4}\n\\]\n\n![](images/370137501573c0ca7e886a268d67ff30180c72e976c97329353e7d8421ced432.jpg)  \nFigure 1: The Network Architecture of FDSA.\n\nwhere  \\(\\mathbf{Q}\\) ,  \\(\\mathbf{K}\\) ,  \\(\\mathbf{V}\\)  represent query, key, and value, respectively,  \\(d\\)  denotes feature dimension of each feature. Here, query, key and value in the feature-based self-attention block equal to  \\(\\mathbf{F}\\) , we first convert it to three matrices through linear transformation, and then feed them into the SDPA as follows.\n\n\\[\n\\mathbf {H} _ {f} = S D P A \\left(\\mathbf {F W} ^ {Q}, \\mathbf {F W} ^ {K}, \\mathbf {F W} ^ {V}\\right), \\tag {5}\n\\]\n\nwhere  \\(\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d \\times d}\\)  are the projection matrices. In order to enable the model to jointly attend to information from different representation subspaces at different positions [Vaswani et al., 2017], the self-attention adopts multi-head attention (MH). The multi-head attention is defined as follows.\n\n\\[\n\\mathbf {M} _ {f} = M H (\\mathbf {F}) = \\operatorname {C o n c a t} \\left(h _ {1}, h _ {2}, \\dots , h _ {l _ {f}}\\right) \\mathbf {W} ^ {O}, \\tag {6}\n\\]\n\n\\[\nh _ {i} = S D P A \\left(\\mathbf {F W} _ {i} ^ {Q}, \\mathbf {F W} _ {i} ^ {K}, \\mathbf {F W} _ {i} ^ {V}\\right),\n\\]\n\nwhere  \\(\\mathbf{W}^O, \\mathbf{W}_i^Q, \\mathbf{W}_i^K, \\mathbf{W}_i^V\\)  are parameters to be learned and  \\(l_f\\)  is the number of heads in the feature-based self-attention block. Also, the self-attention network employs a residual connection, a layer normalization and two-layer fully-connected layer with a ReLU activation function to strengthen the performance of the self-attention network. Finally, the output of the feature-based self-attention block is defined as follows.\n\n\\[\n\\mathbf {M} _ {f} = \\text {L a y e r N o r m} (\\mathbf {M} _ {f} + \\mathbf {F}),\n\\]\n\n\\[\n\\mathbf {O} _ {f} = \\operatorname {R e L U} \\left(\\left(\\mathbf {M} _ {f} \\mathbf {W} _ {1} + \\mathbf {b} _ {1}\\right) \\mathbf {W} _ {2} + \\mathbf {b} _ {2}\\right), \\tag {7}\n\\]\n\n\\[\n\\mathbf {O} _ {f} = \\operatorname {L a y e r N o r m} \\left(\\mathbf {O} _ {f} + \\mathbf {M} _ {f}\\right),\n\\]\n\nwhere  \\(\\mathbf{W}_{*},\\mathbf{b}_{*}\\)  are model parameters. For the sake of simplicity, we define the entire self-attention block as follows.\n\n\\[\n\\mathbf {O} _ {f} = S A B (\\mathbf {F}). \\tag {8}\n\\]\n\nAfter the first self-attention block,  \\(\\mathbf{O}_f\\)  essentially aggregates all previous features' embedding. However, it may need to capture more complex feature transitions via another self-attention block based on  \\(\\mathbf{O}_f\\) . Thus, we stack the self-attention block and the  \\(q\\) -th  \\((q > 1)\\)  block is defined as follows.\n\n\\[\n\\mathbf {O} _ {f} ^ {(q)} = S A B \\left(\\mathbf {O} _ {f} ^ {(q - 1)}\\right), \\tag {9}\n\\]\n\nwhere  \\(\\mathbf{O}_f^{(0)} = \\mathbf{F}\\)\n\nItem-based self-attention block. The goal of the item-based self-attention block is to learn meaningful item-level transition patterns. Given a user, we can get an item action sequence  \\(s\\)  whose corresponding matrix is  \\(\\mathbf{S}\\) . Thus, the output of the stack item-based self-attention block is constructed as follows.\n\n\\[\n\\mathbf {O} _ {s} ^ {(q)} = S A B \\left(\\mathbf {O} _ {s} ^ {(q - 1)}\\right), \\tag {10}\n\\]\n\nwhere  \\(\\mathbf{O}_s^{(0)} = \\mathbf{S}\\)\n\nFully-connected layer. To capture the transition patterns of items and categories simultaneously, we concatenate the output of item-based self-attention block  \\(\\mathbf{O}_s^{(q)}\\)  and the output of feature-based self-attention block  \\(\\mathbf{O}_f^{(q)}\\)  together and project them into a fully-connected layer.\n\n\\[\n\\mathbf {O} _ {s f} = \\left[ \\mathbf {O} _ {s} ^ {(q)}; \\mathbf {O} _ {f} ^ {(q)} \\right] \\mathbf {W} _ {s f} + \\mathbf {b} _ {s f}, \\tag {11}\n\\]\n\nwhere  \\(\\mathbf{W}_{sf} \\in \\mathbb{R}^{2d \\times d}\\) ,  \\(\\mathbf{b}_{sf} \\in \\mathbb{R}^d\\) . Finally, we calculate the user's preference for items through a dot product operation.\n\n\\[\ny _ {t, i} ^ {u} = \\mathbf {O} _ {s f _ {i}} \\mathbf {N} _ {i} ^ {T}, \\tag {12}\n\\]\n\nwhere  \\(\\mathbf{O}_{sft}\\)  denotes the  \\(t\\) -th line of  \\(\\mathbf{O}_{sf}\\) ,  \\(\\mathbf{N} \\in \\mathbb{R}^{M \\times d}\\)  is an item embedding matrix,  \\(y_{t,i}\\)  is the relevance of item  \\(i\\)  being the next item given the previous  \\(t\\)  items  \\((i.e., s_1, s_2, \\ldots, s_t)\\) . It is worth noting that the model inputs a sequence  \\((i_1, i_2, \\ldots, i_{n-1})\\)  and its expected output is a 'shifted' version of the same sequence:  \\((i_2, i_3, \\ldots, i_n)\\)  during training process. In the test process, we take the last row of matrix  \\(\\mathbf{O}_{sf}\\)  to predict the next item.\n\n### 3.3 The Loss Function for Optimization\n\nIn this subsection, to effectively learn from the training process, we adopt the binary cross-entropy loss as the optimization objective function of our FDSA model, which is defined as:\n\n\\[\nL = - \\sum_ {i \\in s} \\sum_ {t \\in [ 1, 2, \\dots , n ]} \\left[ \\log \\left(\\sigma \\left(y _ {t, i}\\right)\\right) + \\sum_ {j \\notin s} \\log \\left(1 - \\sigma \\left(y _ {t, j}\\right)\\right) \\right]. \\tag {13}\n\\]\n\nMoreover, for each target item  \\(i\\)  in each action sequence, we randomly sample a negative item  \\(j\\) .\n",
  "experiments": "# 4 Experiments\n\nIn this section, we conduct experiments to evaluate the performance of our proposed method FDSA on two real-world datasets. We first briefly introduce the datasets and baseline methods, then we compare FDSA with these baseline methods. Finally, we analyze our experimental results.\n\n<table><tr><td>Dataset</td><td>Tmall</td><td>Toys and Games</td></tr><tr><td># users</td><td>16,257</td><td>35,124</td></tr><tr><td># items</td><td>18,678</td><td>28,351</td></tr><tr><td># avg. actions/user</td><td>15.98</td><td>5.51</td></tr><tr><td># Ratings</td><td>276,117</td><td>228,650</td></tr></table>\n\nTable 1: Datasets statistics\n\n## 4.1 Dataset\n\nWe perform experiments on two publicly available datasets, i.e., Amazon  \\({}^{1}\\)  [Zhou et al.,2018] and Tmall  \\({}^{2}\\)  [Tang and Wang,2018]. Amazon is an E-commerce platform and is widely used for product recommendation evaluation. We adopt a sub-category: Toys and Games. For Toys and Games dataset, we filter users who rated less than 5 items and items that are rated by less than 10 users [Kang and McAuley, 2018]. The feature set of each item contains category, brand, description text on Toys and Games dataset. Tmall, the largest B2C platform in China, is a user-purchase data obtained from IJCAI 2015 competition. We remove items that are observed by less than 30 users and eliminate users who rated less than 15 items [Kang and McAuley, 2018]. The characteristics of each item are category, brand, and seller on Tmall dataset. The statistics of two datasets are summarized in Table 1.\n\n## 4.2 Evaluation Metrics and Implementation Details\n\nTo evaluate the performance of each model for sequential recommendation, we apply two widely used evaluation metrics, i.e., hit ratio (Hit) and normalized discounted cumulative gain (NDCG). Hit ratio measures the accuracy of the recommendation, and NDCG is a position-aware metric which assigns larger weights on higher positions[Yuan et al., 2019]. In our experiments, we choose  \\(K = \\{5,10\\}\\)  to illustrate different results of Hit@K and NDCG@K. Without a special mention in this text, we fix the embedding size of all models to 100 and the batch size to 10. Also, the maximum sequence length  \\(n\\)  is set to 50 on the two datasets.\n\n## 4.3 Baseline Methods\n\nWe will compare our model FDSA with following baseline methods, which are briefly described as follows.\n\n- PopRec ranks items according to their popularity. The most popular items are recommended to users.  \n- BPR [Rendle et al., 2009] is a classic method for building recommendation from implicit feedback data, which\n\nproposes a pair-wise loss function to model the relative preferences of users.\n\n- FPMC [Rendle et al., 2010] fuses matrix factorization and first-order Markov Chains to capture long-term preferences and short-term item-item transitions, respectively, for next item recommendation.  \n- TransRec [He et al., 2017] regards users as a relational vector acting as the junction between items.  \n- GRU4Rec [Hidasi et al., 2016] applies GRU to model user click sequences for session-based recommendation.  \n- CSAN [Huang et al., 2018] can model multi-type actions and multi-modal contents based on the self-attention network. Here we only consider content and behavior in datasets.  \n- SASRec [Kang and McAuley, 2018] is a self-attention-based sequential model, and it can consider consumed items for next item recommendation.  \n- SASRec+ is our extension to the SASRec method, which concatenates item vector representations and category vector representations together as the input of the item-level self-attention network.  \n- SASRec++ is our extension of SASRec method, which splices item representations and various heterogeneous features of items together as the input of the item-level self-attention mechanism.  \n- CFSA is a simplified version of our proposed method, which only considers a category feature. It applies separated self-attention blocks on the item-level sequences and the category-level sequences, respectively.\n\n## 4.4 Performance Comparison\n\nWe compare the performance of FDSA with ten baselines regarding Hit and NDCG with cutoffs at 5 and 10. Table 2 reports their overall experimental performances on the two datasets. We summarize the experimental analysis as follows.\n\nFirstly, both BPR and GRU4Rec outperform PopRec on the two datasets. This suggests the effectiveness of personalized recommendation methods. Among the baseline methods, the sequential model (e.g., FPMC and TransRec) usually perform better than the non-sequential model (i.e., BPR) on the two datasets. This demonstrates the importance of considering sequential information in next item recommendation.\n\nSecondly, compared with FPMC and TransRec, SASRec performs better performance in terms of the two metrics. This confirms the advantages of using a self-attention mechanism to model a sequence pattern. Although CSAN splices the heterogeneous features of the item in the item representation to help the self-attention mechanism learn the sequential patterns, the self-attention mechanism may only be able to better model temporal order information. However, SASRec employs not only self-attention mechanism to capture long-term preferences but also considers short-term preferences (i.e., last action) through a residual connection.\n\nThirdly, SASRec+ and SASRec++ achieve a better result than SASRec on the Toys and Games dataset and perform worse than SASRec on the Tmall dataset. This phenomenon\n\n<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Method</td><td colspan=\"2\">@5</td><td colspan=\"2\">@10</td></tr><tr><td>Hit</td><td>NDCG</td><td>Hit</td><td>NDCG</td></tr><tr><td rowspan=\"11\">Tmall</td><td>PopRec</td><td>0.1532</td><td>0.0988</td><td>0.2397</td><td>0.1267</td></tr><tr><td>BPR</td><td>0.1749</td><td>0.1129</td><td>0.2647</td><td>0.1418</td></tr><tr><td>FPMC</td><td>0.2731</td><td>0.2034</td><td>0.3680</td><td>0.2339</td></tr><tr><td>TransRec</td><td>0.2652</td><td>0.1854</td><td>0.3773</td><td>0.2214</td></tr><tr><td>GRU4Rec</td><td>0.1674</td><td>0.1217</td><td>0.2446</td><td>0.1465</td></tr><tr><td>CSAN</td><td>0.3481</td><td>0.2440</td><td>0.4787</td><td>0.2863</td></tr><tr><td>SASRec</td><td>0.3572</td><td>0.2531</td><td>0.4840</td><td>0.2940</td></tr><tr><td>SASRec+</td><td>0.3427</td><td>0.2415</td><td>0.4714</td><td>0.2829</td></tr><tr><td>SASRec++</td><td>0.3550</td><td>0.2534</td><td>0.4785</td><td>0.2932</td></tr><tr><td>CFSA</td><td>0.3836</td><td>0.2724</td><td>0.5152</td><td>0.3149</td></tr><tr><td>FDSA</td><td>0.3940</td><td>0.2820</td><td>0.5197</td><td>0.3226</td></tr><tr><td rowspan=\"11\">Toys and Games</td><td>PopRec</td><td>0.1952</td><td>0.1287</td><td>0.3058</td><td>0.1643</td></tr><tr><td>BPR</td><td>0.2096</td><td>0.1394</td><td>0.3219</td><td>0.1756</td></tr><tr><td>FPMC</td><td>0.2983</td><td>0.2261</td><td>0.3833</td><td>0.2535</td></tr><tr><td>TransRec</td><td>0.3135</td><td>0.2255</td><td>0.4206</td><td>0.2600</td></tr><tr><td>GRU4Rec</td><td>0.2039</td><td>0.1359</td><td>0.3118</td><td>0.1705</td></tr><tr><td>CSAN</td><td>0.2327</td><td>0.1601</td><td>0.3404</td><td>0.1947</td></tr><tr><td>SASRec</td><td>0.3292</td><td>0.2334</td><td>0.4441</td><td>0.2705</td></tr><tr><td>SASRec+</td><td>0.3367</td><td>0.2410</td><td>0.4510</td><td>0.2776</td></tr><tr><td>SASRec++</td><td>0.3394</td><td>0.2428</td><td>0.4544</td><td>0.2799</td></tr><tr><td>CFSA</td><td>0.3391</td><td>0.2411</td><td>0.4538</td><td>0.2782</td></tr><tr><td>FDSA</td><td>0.3571</td><td>0.2572</td><td>0.4738</td><td>0.2949</td></tr></table>\n\ncan be explained that the sequential patterns may not be stably modeled by concatenating items' representations and items' feature representations together as input vectors of the self-attention mechanism. Moreover, the performance of CFSA is better than SASRec+, and FDSA surpasses SASRec++. This demonstrates that applying separated self-attention blocks on item-level sequences and feature-level sequences, respectively, to capture item transition patterns and feature transition patterns (i.e., CFSA and FDSA) is more effective than splicing item representations and its feature representations as the input to a self-attention mechanism (i.e., SASRec+ and SASRec++.). The above experiments demonstrate that modeling item and feature transition patterns through two separate independent item-level and feature-level sequences is valuable and meaningful for sequential recommendation.\n\nFinally, regardless of the datasets and the evaluation metrics, our proposed FDSA achieves the best performance. Our degenerated model CFSA consistently beats most baseline methods. This shows the effectiveness of modeling independent category-level sequences by the self-attention network. FDSA performs better than CFSA, indicating the effectiveness of modeling more features in feature-level sequences.\n\nTable 2: Experimental results of FDSA and baselines. The best performance of each column (the larger is the better) is in bold.  \n\n<table><tr><td>Dataset</td><td>model</td><td>NDCGlf/s</td><td>lf=2</td><td>lf=4</td></tr><tr><td rowspan=\"4\">Tmall</td><td rowspan=\"2\">CFSA</td><td>ls=2</td><td>0.3058</td><td>0.3060</td></tr><tr><td>ls=4</td><td>0.3149</td><td>0.3146</td></tr><tr><td rowspan=\"2\">FDSA</td><td>ls=2</td><td>0.3120</td><td>0.3176</td></tr><tr><td>ls=4</td><td>0.3226</td><td>0.3211</td></tr><tr><td rowspan=\"4\">Toys and Games</td><td rowspan=\"2\">CFSA</td><td>ls=2</td><td>0.2600</td><td>0.2782</td></tr><tr><td>ls=4</td><td>0.2764</td><td>0.2729</td></tr><tr><td rowspan=\"2\">FDSA</td><td>ls=2</td><td>0.2759</td><td>0.2949</td></tr><tr><td>ls=4</td><td>0.2799</td><td>0.2791</td></tr></table>\n\nTable 3: The Performance of FDSA and CFSA with varying  \\(l_{s}\\)  and  \\(l_{f}\\)  in terms of NDCG@10 on two datasets.\n\n### 4.5 Influence of Hyper-parameters\n\nWe investigate the influence of hyper-parameters, such as the embedding size  \\(d\\) , the number of heads in item-based self-attention block  \\(l_{s}\\)  and the number of heads in feature-based\n\nself-attention block  \\(l_{f}\\) . Due to space limitation, we only show the experimental results of NDCG@10. We have obtained similar experimental results on the Hit@10 metric.\n\n![](images/0a30c52a3696fed791a7c8fc64684ef11113b5995f93e090639330cdb29edce2.jpg)  \nFigure 2: The performance of FDSA and CFSA under difference choices of  \\(d\\) .\n\n![](images/611979d8eeb0ce1fe0e12447e6c0da97a9c3d63a26247d771c784dc9a8cab0d9.jpg)\n\nInfluence of embedding size  \\(d\\) . Figure 2 shows the performance of our model with different embedding sizes  \\(d\\)  on the two datasets. As we can see from Figure 2, high dimensions can model more information for items, but when the dimension exceeds 100, the performance of FDSA and CFSA degrade. This demonstrates that over-fitting may occur when the implicit factor dimension of the model is too high.\n\nInfluence of the number of heads  \\(l_{s}\\)  and  \\(l_{f}\\) . We conduct experiments to study the performance of our model with varying  \\(l_{s}\\)  and  \\(l_{f}\\)  on the two datasets. Table 3 demonstrates the experimental result in term of NDCG@10. We can observe that CFSA and FDSA achieve the best performance with the setting  \\(l_{s} = 4\\) ,  \\(l_{f} = 2\\)  on the Tmall dataset, while they get the best result with the setting  \\(l_{s} = 2\\) ,  \\(l_{f} = 4\\)  on the Toys and Games dataset. This may be because our model needs more heads to capture the transition relationships between features due to each item contains a descriptive text and a title in the Toys and Games dataset, while the single data type of the features of these items on Tmall dataset may not require too complicated structures to model the relationships between the features.\n",
  "hyperparameter": "Embedding size d=100; Batch size=10; Maximum sequence length n=50; Number of heads in item-based self-attention block ls varies by dataset (ls=4 for Tmall, ls=2 for Toys and Games); Number of heads in feature-based self-attention block lf varies by dataset (lf=2 for Tmall, lf=4 for Toys and Games); Number of stacked self-attention blocks q (not explicitly specified but model supports stacking); Number of topical keywords extracted from text=5; Negative sampling: 1 negative item per target item; Loss function: binary cross-entropy with negative sampling"
}