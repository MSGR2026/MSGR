{
  "id": "CORE_2020",
  "paper_title": "Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation",
  "alias": "CORE",
  "year": 2020,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "DIF-SR proposes a Decoupled Side Information Fusion mechanism that calculates separate attention matrices for items and each attribute type, then fuses them together, rather than early fusion of embeddings. This approach breaks the rank bottleneck of attention matrices (rank limited by head projection size d_h in early fusion methods), avoids mixed correlations between different attributes, and enables flexible training gradients. The method is complemented by Auxiliary Attribute Predictors (AAP) that apply multiple predictors directly on final item representations to force meaningful interaction between side information and item representation learning.",
  "introduction": "# 1 INTRODUCTION\n\nSequential recommendation (SR) aims to model users' dynamic preferences from their historical behaviors and make next item recommendations. SR has become an increasingly appealing research topic with wide and practical usage in online scenarios. Multiple deep learning-based solutions [8, 17, 31] are proposed, and self-attention [27] based methods [11, 25, 28] become mainstream solutions with competitive performances. Among the recent improvements on self-attention based methods, one important branch is related to side information fusion [16, 33, 34, 37]. Instead of using item IDs as only item attribute as prior solutions, the side information, such as other item attributes and ratings, is taken into consideration. Intuitively, the highly-related information can benefit recommendations. However, how to effectively fuse side information into the recommendation process remains a challenging open issue.\n\nMany research efforts have been devoted to fusing side information in various stages of recommendation. Specifically, an early trial FDSA [34] combines two separate branches of self-attention blocks for item and feature and fuses them in the final stage.  $S^3$ -Rec [37] uses self-supervised attribute prediction tasks in the pretraining stage. However, the independent learning of item and side information representation in FDSA and the pretraining strategy in  $S^3$ -Rec are hard to allow side information to directly interact with item self-attention.\n\nRecently, several studies design solutions integrating side information embedding into the item representation before the attention layer to enable side information aware attention. ICAI-SR [33] utilizes attribute-to-item aggregation layer before attention layer to integrate side information into item representation with separate attribute sequential models for training. NOVA [16] proposes to feed both the pure item id representation and side information integrated representation to the attention layer, where the latter is only used to calculate the attention key and query and keeps the value non-invasive.\n\nDespite the remarkable improvement, several drawbacks remain for the current early-integration based solutions [16, 33]. First, we observe that integrating embedding before the attention layer suffers from a rank bottleneck of attention matrices, which leads to inferior attention score representation capacity. It is because of the fact that the rank of attention matrices of prior solutions is inherently bounded by multi-head query-key down-projection size  $d_h$ , which is usually smaller than the matrices could reach. We further theoretically explain such phenomenon in Sec 4.2.4. Second, the attention performed on compound embedding space may lead to random disturbance, where the mixed embeddings from various information resources inevitably attend to unrelated information. The similar drawback for positional encoding in input layer is discussed [5, 12]. Third, since the integrated embedding remains impartible in the whole attention block, early-integrating forces the model to develop complex and heavy integration solutions and training schemes to enable flexible gradients for various side information. With a simple fusion solution, such as widely-used addition fusion, all embeddings share the same gradient for training, which limits the model from learning the relative importance of side-information encodings with respect to item embeddings.\n\nTo overcome the limitations, we propose Decoupled Side Information Fusion for Sequential Recommendation (DIF-SR). Inspired by the success of decoupled positional embedding [2, 5], we propose to thoroughly explore and analyze the effect of decoupled embedding for side information fusion in the sequential recommendation. Specifically, instead of early integration, we move the fusion process from the input to the attention layer. We decouple various side information as well as item embedding through generating key and query separately for every attribute and item in the attention layer. Afterward, we fuse all the attention matrices with fusion function. This simple and effective strategy directly enables our solution to break the rank bottleneck, thus enhancing the modeling capacity of the attention mechanism. Fig. 1 shows an illustration of rank comparison between current early-integration based solutions and ours with the same embedding size  $d$  and head projection size  $d_h$ . Our solution avoids unnecessary randomness of attention caused by the mixed correlation of heterogeneous embeddings. Also, it enables flexible gradients to adaptively learn various side information in different scenarios. We further propose to utilize the light Auxiliary Attribute Predictors (AAP) in a multi-task training scheme to better activate side information to cast beneficial influence on learning final representation.\n\nExperimental results show that our proposed method outperforms the existing basic SR methods [8, 11, 25, 26] and competitive side information integrated SR methods [16, 33, 37] on four widely-used datasets for sequential recommendation, including Amazon\n\nBeauty, Sports, Toys, and Yelp. Moreover, our proposed solution can be incorporated into self-attention based basic SR models easily. Further study on two representative models [11, 25] shows that significant improvement is achieved when basic SR models are incorporated with our modules. Visualization on attention matrices also offers an interpretation of the rationality of decoupled attention calculation and attention matrices fusion.\n\nOur contribution can be summarized as follows:\n\n- We present the DIF-SR framework, which can effectively leverage various side information for sequential recommendation tasks with higher attention representation capacity and flexibility to learn the relative importance of side information\n\n- We propose the novel DIF attention mechanism and AAP-based training scheme, which can be easily incorporated into attention-based recommender systems and boost performance.\n\n- We theoretically and empirically analyze the effectiveness of the proposed solution. We achieve state-of-the-art performance on multiple real-world datasets. Comprehensive ablation studies and in-depth analysis demonstrate the robustness and interpretability of our method.",
  "method": "# 4 METHODOLOGY\n\nIn this section, we present our DIF-SR to effectively and flexibly fuse side information to help next-item prediction. The overall architecture of DIF-SR is illustrated in Fig. 2, consisting of three main modules: Embedding Module (Sec. 4.1), Decoupled Side Information Fusion Module (Sec. 4.2), and Prediction Module with AAP (Sec. 4.3).\n\n# 4.1 Embedding Module\n\nIn the embedding module, the input sequence  $\\mathbb{S}_u = [v_1,v_2,\\dots ,v_n]$  is fed into the item embedding layer and various attribute embedding layers to get the item embedding  $E^{ID}$  and side information embeddings  $E^{f1},\\ldots ,E^{fp}$ :\n\n$$\nE ^ {I D} = \\mathcal {E} _ {i d} ([ I _ {1}, I _ {2}, \\dots , I _ {n} ]),\n$$\n\n$$\n\\begin{array}{c} E ^ {f 1} = \\mathcal {E} _ {f 1} ([ f _ {1} ^ {(1)}, f _ {2} ^ {(1)}, \\dots , f _ {n} ^ {(1)} ]), \\\\ \\dots \\end{array} \\tag {1}\n$$\n\n$$\nE ^ {f p} = \\mathcal {E} _ {f p} ([ f _ {1} ^ {(p)}, f _ {2} ^ {(p)}, \\dots , f _ {n} ^ {(p)} ]),\n$$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b383b849-5f8f-48de-a3c1-2bb6e1f07ba6/ebefca639b6b0b0ba3ee5ebe01f49e219490e63e50e21aef8018e97c8d0b6cd4.jpg)  \n(a) SASRecF.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b383b849-5f8f-48de-a3c1-2bb6e1f07ba6/d50e72bd1b79ba19a44313e0d79f8bc278cd1dba6ba227a1ee78b040b76371a3.jpg)  \n(b) NOVA-SR.  \nFigure 3: The comparison of item representation learning process of various solutions. (a)  $\\mathrm{SASRec}_{\\mathrm{F}}$ :  $\\mathrm{SASRec}_{\\mathrm{F}}$  fuses side information into item representation and uses the fused item representation to calculate key, query and value. (b) NOVA-SR: NOVA-SR uses the fused item representation for the calculation of key and query, and keeps value non-invasive. (c) DIF-SR: Instead of early fusion to get fused item representation, the proposed DIF-SR decouples the attention calculation process of various side information to generate fused attention matrices for higher representation power, avoidance of mixed correlation, and flexible training gradient.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b383b849-5f8f-48de-a3c1-2bb6e1f07ba6/cc776fd10551c1db1e6a109ff62df6bd6ba1193813fa73ac3db4f147f4ec25a7.jpg)  \n(c) DIF-SR.\n\nwhere  $\\mathcal{E}$  represents the corresponding embedding layer that encodes the item and different item attributes into vectors. The look-up embedding matrices can be represented as  $M_{id} \\in \\mathbb{R}^{|I| \\times d}, M_{f1} \\in \\mathbb{R}^{|f1| \\times d_{f1}}, \\ldots, M_{fp} \\in \\mathbb{R}^{|fp| \\times d_{fp}}$ , and  $|\\cdot|$  represents the corresponding total number of different items and various side information, and  $d$  and  $d_{f1}, \\ldots, d_{fp}$  denote the dimension of embedding of item and various side information. Note that supported by the operations in proposed DIF module, the embedding dimensions are flexible for different types of attributes. It is further verified in Sec. 5.4.3 that we can apply much smaller dimension for attribute than item to greatly improve efficiency of the network without harming the performance. Then the embedding module gets the output embeddings  $E^{ID} \\in \\mathbb{R}^{n \\times d}, E^{f1} \\in \\mathbb{R}^{n \\times d_{f1}}, \\ldots, E^{fp} \\in \\mathbb{R}^{n \\times d_{fp}}$ .\n\n# 4.2 Decoupled Side Information Fusion Module\n\nWe first specify the overall layer structure of the module in Sec. 4.2.1. To better illustrate our proposed DIF attention, we discuss self-attentive learning process of the prior solutions [11, 16] in Sec. 4.2.2. Afterward, we comprehensively introduce the proposed DIF attention in Sec. 4.2.3. Finally, we provide theoretical analysis on the enhancement of DIF on expressiveness of the model regarding the rank of attention matrices and flexibility of gradients in Sec. 4.2.4.\n\n4.2.1 Layer Structure. As shown in Fig. 2, the Decoupled Side Information Fusion Module contains several stacked blocks of sequential combined DIF Attention Layer and Feed Forward Layer. The block structure is the same as SASRec [11], except that we replace the original multi-head self-attention with the multi-head DIF attention mechanism. Each DIF block involves two types of input, namely current item representation and auxiliary side information embeddings, and then output updated item representation. Note that the auxiliary side information embeddings are not updated per layer to save computation while avoiding overfitting. Let  $R_{i}^{(ID)} \\in \\mathbb{R}^{n \\times d}$  denote the input item representation of block  $i$ . The process can be\n\nformulated as:\n\n$$\nR _ {i + 1} ^ {(I D)} = \\operatorname {L N} \\left(\\operatorname {F F N} \\left(\\operatorname {D I F} \\left(R _ {i} ^ {(I D)}, E ^ {f 1}, \\dots , E ^ {f p}\\right)\\right)\\right), \\tag {2}\n$$\n\n$$\nR _ {1} ^ {(I D)} = E ^ {I D}, \\tag {3}\n$$\n\nwhere FFN represents a fully connected feed-forward network and LN denotes layer normalization.\n\n4.2.2 Prior Attention Solutions. Fig. 3 shows the comparison of the prior solutions to fuse side information to the updating process of item representation. Here we focus on the self-attention calculation, which is the main difference of several solutions.\n\nSASRecF: As shown in Fig. 3(a), the solution directly fuses side information embedding to the item representation and performs vanilla self-attention upon the integrated embedding, which is extended from original SASRec [11].\n\nWith the input length  $n$ , hidden size  $d$ , multi-head query-key down-projection size  $d_h$ , we can let  $R \\in \\mathbb{R}^{n \\times d}$  denote the integrated embedding, and  $W_{Q}^{i}, W_{K}^{i}, W_{V}^{i} \\in \\mathbb{R}^{d \\times d_h}, i \\in [h]$  denote the query, key, and value projection matrices for  $h$  heads ( $d_h = d / h$ ), then the calculation of attention score can be formalized as:\n\n$$\n\\mathrm {S A S} _ {-} \\mathrm {a t t} ^ {i} = \\left(R W _ {Q} ^ {i}\\right) \\left(R W _ {K} ^ {i}\\right) ^ {\\top}. \\tag {4}\n$$\n\nThen the output for each head can be represented as:\n\n$$\n\\mathrm {S A S} \\_ \\text {h e a d} ^ {i} = \\sigma \\left(\\frac {\\mathrm {S A S} _ {-} \\mathrm {a t t} ^ {i}}{\\sqrt {d}}\\right) \\left(R W _ {V} ^ {i}\\right), \\tag {5}\n$$\n\nwhere  $\\sigma$  denotes Softmax function.\n\nDespite that this solution allows side information to directly influence the item representation learning process, it is observed that such method has the drawback of invasion of item representation [16].\n\nNOVA: To solve the mentioned problem, the work [16] proposes to utilize the non-invasive fusion of side information. As shown in Fig. 3(b), NOVA calculates  $Q$  and  $K$  from the integrated embeddings  $R \\in \\mathbb{R}^{n \\times d}$ , and  $V$  from the pure item ID embeddings  $R^{(ID)} \\in \\mathbb{R}^{n \\times d}$ . Similarly,  $W_{Q}^{i}, W_{K}^{i}, W_{V}^{i} \\in \\mathbb{R}^{d \\times dh}, i \\in [h]$  denote the query,key, and\n\nvalue projection matrices:\n\n$$\n\\text {N O V A} _ {-} \\text {a t t} ^ {i} = \\left(R W _ {Q} ^ {i}\\right) \\left(R W _ {K} ^ {i}\\right) ^ {\\top} \\tag {6}\n$$\n\nThen the output for each head can be represented as:\n\n$$\n\\mathrm {N O V A} _ {\\text {h e a d}} ^ {i} = \\sigma \\left(\\frac {\\mathrm {N O V A} _ {\\text {a t t}} ^ {i}}{\\sqrt {d}}\\right) \\left(R ^ {(I D)} W _ {V} ^ {i}\\right) \\tag {7}\n$$\n\n4.2.3 DIF Attention. We argue that although NOVA solves the invasion problem of value, using integrated embedding for the calculation of key and value still suffers from compound attention space as well as the degradation of expressiveness regarding the rank of attention matrix and training gradient flexibility. The supportive theoretical analysis is shown in Sec. 4.2.4.\n\nTherefore, in contrast to the prior studies that inject attribute embedding into item representation for a mixed representation, we propose to leverage decoupled side information fusion solution. As shown in Fig. 3(c), in our proposed solution, all attributes attend upon themselves to generate the decoupled attention matrices, which are then fused to the final attention matrices. The decoupled attention calculation improves the model's expressiveness through breaking the rank bottleneck of attention matrices bounded by head projection size  $d_h$ . It also avoids inflexible gradients and uncertain cross relationships among different attributes and the item to enable a reasonable and stable self-attention.\n\nGiven the input length  $n$ , item hidden size  $d$ , multi-head query-key down-projection size  $d_h$ , we have  $W_{Q}^{i}, W_{K}^{i}, W_{V}^{i} \\in \\mathbb{R}^{d \\times d_h}, i \\in [h]$  denote the query, key, and value projection matrices for  $h$  heads  $(d_h = d / h)$  for item representation  $R^{(ID)} \\in \\mathbb{R}^{n \\times d}$ . Then attention score for item representation is then calculated with:\n\n$$\n\\operatorname {a t t} _ {I D} ^ {i} = \\left(R ^ {(I D)} W _ {Q} ^ {i}\\right) \\left(R ^ {(I D)} W _ {K} ^ {i}\\right) ^ {\\top}. \\tag {8}\n$$\n\nDifferent from prior works, we also generate multi-head attention matrices for every attribute with attribute embeddings  $E^{f1} \\in \\mathbb{R}^{n \\times d_{f1}}, \\ldots, E^{fp} \\in \\mathbb{R}^{n \\times d_{fp}}$ . Note that we have  $d_{fj} \\leq d, j \\in [p]$  to avoid over-parameterization and reduce computation overhead. Then we have corresponding  $W_{Q}^{(fj)i}, W_{K}^{(fj)i}, W_{V}^{(fj)i} \\in \\mathbb{R}^{d \\times d_{hj}}, i \\in [h], j \\in [p]$  denote the query, key, and value projection matrices for  $h$  heads ( $d_{hj} = d_{fj} / h$ ):\n\n$$\n\\mathrm {a t t} _ {f 1} ^ {i} = (E ^ {f 1} W _ {Q} ^ {(f 1) i}) (E ^ {f 1} W _ {K} ^ {(f 1) i}) ^ {\\top},\n$$\n\n(9)\n\n$$\n\\mathrm {a t t} _ {f p} ^ {i} = (E ^ {f p} W _ {Q} ^ {(f p) i}) (E ^ {f p} W _ {K} ^ {(f p) i}) ^ {\\top}.\n$$\n\nThen our DIF attention fuses all the attention matrices with the fusion function  $\\mathcal{F}$  explored in the prior work [16], including addition, concatenation, and gating, and gets the output for each head as:\n\n$$\n\\begin{array}{l} \\operatorname {D I F} _ {-} \\operatorname {a t t} ^ {i} = \\mathcal {F} \\left(\\operatorname {a t t} _ {I D} ^ {i}, \\operatorname {a t t} _ {f 1} ^ {i}, \\dots , \\operatorname {a t t} _ {f p} ^ {i}\\right), \\\\ \\operatorname {D I F} _ {-} \\text {h e a d} ^ {i} = \\sigma \\left(\\frac {\\operatorname {D I F} _ {-} \\operatorname {a t t} ^ {i}}{\\sqrt {d}}\\right) \\left(R ^ {(I D)} W _ {V} ^ {i}\\right). \\tag {10} \\\\ \\end{array}\n$$\n\nFinally, the outputs of all attention heads are concatenated and fed to the feed-forward layer.\n\n4.2.4 Theoretical Analysis. In this section, we extend the analysis for positional embedding in [2] to theoretically analyze the proposed DIF and early fusion solution in the previous models [11, 16]. See Appendix A for the proof.\n\nWe first discuss the expressiveness of the model regarding the rank of attention matrices for DIF and the prior solutions.\n\nTheorem 4.1. Let  $W_{Q}, W_{K} \\in \\mathbb{R}^{d \\times d_{h}}$  denote the projection matrices with head projection size  $d_{h}$ ,  $W_{Q}^{fj}, W_{K}^{fj}$  denote projection matrices for attribute  $j$  with head projection size  $d_{hj}$ , and  $d_{hj} \\leq d_{h}$ ,  $d$  and  $n \\geq d_{h} + \\Sigma_{j=1}^{p} d_{hj}$ . Let  $att = RW_{Q}W_{K}^{\\top}R^{\\top}$  be the attention matrices for integrated representation  $R \\in \\mathbb{R}^{n \\times d}$ . Let  $DIF\\_att = R^{(ID)}W_{Q}W_{K}^{\\top}R^{(ID)\\top} + \\Sigma_{j=1}^{p}(Ef^{fj}W_{Q}^{fj}W_{K}^{fj\\top}Ef^{fj\\top})$  be the attention matrices for decoupled representation  $R^{(ID)} \\in \\mathbb{R}^{n \\times d}$ ,  $Ef^{f1} \\in \\mathbb{R}^{n \\times df_{1}}, \\ldots, Ef^{fp} \\in \\mathbb{R}^{n \\times df_{p}}$ . Then, for any  $R, W_{Q}, W_{K}$\n\n$$\nr a n k (a t t) \\leq d _ {h}.\n$$\n\nThere exists a choice of parameters such that\n\n$$\nr a n k (D I F \\_ a t t) = d _ {h} + \\Sigma_ {j = 1} ^ {p} d _ {h j} > d _ {h}.\n$$\n\nRemarks. This theorem shows that early fusion of side information limits the rank of attention matrices by  $d_h$ , which is usually a smaller value than the attention matrices could reach in multi-head self-attention. And our solution breaks such rank bottleneck through fusing the decoupled attention scores for items and attributes. Fig. 1 also offers an experimental illustration of rank comparison. Higher-rank attention matrices inherently enhance DIF-SR with stronger model expressiveness.\n\nThen, we discuss the training flexibility of integrated embedding based solution. We claim that with simple addition fusion solution, both  $\\mathrm{SASRec}_{\\mathrm{F}}$  and NOVA limit the flexibility of gradients. Let  $E^{ID}, E^{f1}, \\ldots, E^{fp} \\in \\mathbb{R}^{n \\times d}$  denote the input item and attribute embeddings. For  $\\mathrm{SASRec}_{\\mathrm{F}}$ , all the embeddings are added up and fed into the model  $G$ . For given input and label  $y$ , the loss function can be denoted as:\n\n$$\nL = \\ell (G (E ^ {I D} + \\Sigma_ {i = 1} ^ {p} E ^ {f i}), y). \\tag {11}\n$$\n\nFor NOVA, the attribute embeddings are first added up as inseparable and then fused to item representation in every layer, thus for model  $G$  and label  $y$ , the loss function can be denoted as:\n\n$$\nL = \\ell (G (E ^ {I D}, \\Sigma_ {i = 1} ^ {p} E ^ {f i}), y). \\tag {12}\n$$\n\nTheorem 4.2. Let  $E^{ID}, E^{f1}, \\ldots, E^{fp} \\in \\mathbb{R}^{n \\times d}$  denote the input item and attribute embeddings. For the loss function in Equ.(11), the gradients for  $E^{ID}, E^{f1}, \\ldots, E^{fp}$  remain same for any function  $\\ell$  and model  $G$ , and any input and label  $y$ . For the loss function in Equ.(12), the gradients for  $E^{f1}, \\ldots, E^{fp}$  remain same for any function  $\\ell$  and model  $G$ , and any input and label  $y$ .\n\nRemarks. This theorem shows that with simple addition fusion, the gradients are the same for the input item embeddings and attribute embeddings for SASRec, and all types of attribute embeddings share the same gradient for NOVA. This suggests that to enable flexible gradients, more complex and heavy fusion solutions need to be involved for early-integration based methods compared with ours.\n\n# 4.3 Prediction Module with AAP\n\nWith the final representation  $R_{L}^{(ID)}$  that encodes the sequence information with the help of side information, we take the last element of  $R_{L}^{(ID)}$ , denoted as  $R_{L}^{(ID)}[n]$ , to estimate the probability of user  $u$  to interact with each item in the item vocabulary. The item prediction layer can be formulated as:\n\n$$\n\\hat {y} = \\operatorname {s o f t m a x} \\left(M _ {i d} R _ {L} ^ {(I D)} [ n ] ^ {\\top}\\right), \\tag {13}\n$$\n\nwhere  $\\hat{y}$  represents the  $|\\mathcal{I}|$ -dimensional probability, and  $M_{id} \\in \\mathbb{R}^{|\\mathcal{I}| \\times d}$  is the item embedding table in the embedding layer.\n\nDuring training, we propose to use Auxiliary Attribute Predictors (AAP) for attributes (except position information) to further activate the interaction between auxiliary side information and item representation. Note that different from prior solutions that make predictions with separate attribute embedding [33] or only use attributes for pretraining [37], we propose to apply multiple predictors directly on final representation to force item representation to involve useful side information. As verified in Sec. 5.4.1, AAP can help further improve the performance, especially when combining with DIF. We attribute this to the fact that AAP is designed to enhance the attributes' informative influence on self-attentive item representation learning, while early-integration based solutions do not support such influence.\n\nThe prediction for attribute  $j$  can be represented as:\n\n$$\n\\hat {y} ^ {(f j)} = \\sigma \\left(W _ {f j} R _ {L} ^ {(I D)} [ n ] ^ {\\top} + b _ {f j}\\right), \\tag {14}\n$$\n\nwhere  $\\hat{y}^{(fj)}$  represents the  $|fj|$ -dimensional probability,  $W_{fj} \\in \\mathbb{R}^{|fj| \\times d_{fj}}$  and  $b_{fj} \\in \\mathbb{R}^{|fj| \\times 1}$  are learnable parameters, and  $\\sigma$  is the sigmoid function. Here we compute the item loss  $L_{id}$  using cross entropy to measure the difference between prediction  $\\hat{y}$  and ground truth  $y$ :\n\n$$\nL _ {i d} = - \\sum_ {i = 1} ^ {| I |} y _ {i} \\log \\left(\\hat {y} _ {i}\\right) \\tag {15}\n$$\n\nAnd following [24], we compute the side information loss  $L_{fj}$  for  $j$ th type using binary cross entropy to support multi-labeled attributes:\n\n$$\nL _ {f j} = - \\sum_ {i = 1} ^ {| f j |} y _ {i} ^ {(f j)} \\log \\left(\\hat {y} _ {i} ^ {f j}\\right) + \\left(1 - y _ {i} ^ {(f j)}\\right) \\log \\left(1 - \\hat {y} _ {i} ^ {(f j)}\\right), \\tag {16}\n$$\n\nThen, the combined loss function with balance parameter  $\\lambda$  can be formulated as:\n\n$$\nL = L _ {i d} + \\lambda \\Sigma_ {j = 1} ^ {p} L _ {f j}. \\tag {17}\n$$",
  "experiments": "# 5 EXPERIMENTS\n\nWe conduct extensive experiments on four real-world and widely-used datasets to answer the following research questions.\n\n- RQ1: Does DIF-SR outperform the current state-of-the-art basic SR methods and side information integrated SR methods?  \n- RQ2: Can the proposed DIF and AAP be readily incorporated into the state-of-the-art self-attention based models and boost the performance?  \n- RQ3: What is the effect of different components and hyperparameters in the DIF-SR framework?\n\nTable 1: Statistics of the datasets after preprocessing.  \n\n<table><tr><td>Dataset</td><td>Beauty</td><td>Sports</td><td>Toys</td><td>Yelp</td></tr><tr><td># Users</td><td>22,363</td><td>35,598</td><td>19,412</td><td>30,499</td></tr><tr><td># Items</td><td>12,101</td><td>18,357</td><td>11,924</td><td>20,068</td></tr><tr><td># Avg. Actions / User</td><td>8.9</td><td>8.3</td><td>8.6</td><td>10.4</td></tr><tr><td># Avg. Actions / Item</td><td>16.4</td><td>16.1</td><td>14.1</td><td>15.8</td></tr><tr><td># Actions</td><td>198,502</td><td>296,337</td><td>167,597</td><td>317,182</td></tr><tr><td>Sparsity</td><td>99.93%</td><td>99.95%</td><td>99.93%</td><td>99.95%</td></tr></table>\n\n- RQ4: Does the visualization of attention matrices fusion of DIF provide evidence for superior performance?\n\n# 5.1 Experimental Settings\n\n5.1.1 Dataset. The experiments are conducted on four real-world and widely used datasets.\n\n- Amazon Beauty, Sports and Toys<sup>1</sup>. These datasets are constructed from Amazon review datasets [20]. Following baseline [33], we utilize fine-grained categories of the goods and position information as attributes for all these three datasets.  \n- Yelp  $^{2}$ , which is a well-known business recommendation dataset. Following [37], we only retain the transaction records after Jan. 1st, 2019 in our experiment, and the categories of businesses and position information are regarded as attributes.\n\nFollowing the same pre-processing steps used in [11, 33, 37], we remove all items and users that occur less than five times in these datasets. All the interactions are regarded as implicit feedback. The statistics of all these four datasets after preprocessing are summarized in Tab. 1.\n\n5.1.2 Evaluation Metrics. In our experiments, following the prior works [11, 25], we use leave-one-out strategy for evaluation. Specifically, for each user-item interaction sequence, the last two items are reserved as validation and testing data, respectively, and the rest are utilized for training SR models. The performances of SR models are evaluated by top-K Recall (Recall@K) and top-K Normalized Discounted Cumulative Gain (NDCG@K) with  $K$  chosen from \\{10, 20\\}, which are two commonly used metrics. As suggested by [4, 13], we evaluate model performance in a full ranking manner for a fair comparison. The ranking results are obtained over the whole item set rather than sampled ones.\n\n5.1.3 Baseline Models. We choose two types of state-of-the-art methods for comparison, including strong basic SR methods and recent competitive side information integrated SR methods. The baselines are introduced as follows.\n\n- GRU4Rec [8]: A session-based recommender system that uses RNN to capture sequential patterns.  \n- GRU4RecF: An enhanced version of GRU4Rec that considers the side information to improve the performance.  \n- Caser [26]: A CNN-based model that uses horizontal and vertical convolutional filters to learn multi-level patterns and user preferences.\n\nTable 2: Overall performance. Bold scores represent the highest results of all methods. Underlined scores stand for the highest results from previous methods.  \n\n<table><tr><td>Dataset</td><td>Metric</td><td>GRU4Rec</td><td>Caser</td><td>BERT4Rec</td><td>GRU4RecF</td><td>SASRec</td><td>SASRecF</td><td>S3Rec</td><td>NOVA</td><td>ICAI</td><td>DIF-SR</td></tr><tr><td rowspan=\"4\">Beauty</td><td>Recall@10</td><td>0.0530</td><td>0.0474</td><td>0.0529</td><td>0.0587</td><td>0.0828</td><td>0.0719</td><td>0.0868</td><td>0.0887</td><td>0.0879</td><td>0.0908</td></tr><tr><td>Recall@20</td><td>0.0839</td><td>0.0731</td><td>0.0815</td><td>0.0902</td><td>0.1197</td><td>0.1013</td><td>0.1236</td><td>0.1237</td><td>0.1231</td><td>0.1284</td></tr><tr><td>NDCG@10</td><td>0.0266</td><td>0.0239</td><td>0.0237</td><td>0.0290</td><td>0.0371</td><td>0.0414</td><td>0.0439</td><td>0.0439</td><td>0.0439</td><td>0.0446</td></tr><tr><td>NDCG@20</td><td>0.0344</td><td>0.0304</td><td>0.0309</td><td>0.0369</td><td>0.0464</td><td>0.0488</td><td>0.0531</td><td>0.0527</td><td>0.0528</td><td>0.0541</td></tr><tr><td rowspan=\"4\">Sports</td><td>Recall@10</td><td>0.0312</td><td>0.0227</td><td>0.0295</td><td>0.0394</td><td>0.0526</td><td>0.0435</td><td>0.0517</td><td>0.0534</td><td>0.0527</td><td>0.0556</td></tr><tr><td>Recall@20</td><td>0.0482</td><td>0.0364</td><td>0.0465</td><td>0.0610</td><td>0.0773</td><td>0.0640</td><td>0.0758</td><td>0.0759</td><td>0.0762</td><td>0.0800</td></tr><tr><td>NDCG@10</td><td>0.0157</td><td>0.0118</td><td>0.0130</td><td>0.0199</td><td>0.0233</td><td>0.0235</td><td>0.0249</td><td>0.0250</td><td>0.0243</td><td>0.0264</td></tr><tr><td>NDCG@20</td><td>0.0200</td><td>0.0153</td><td>0.0173</td><td>0.0253</td><td>0.0295</td><td>0.0286</td><td>0.0310</td><td>0.0307</td><td>0.0302</td><td>0.0325</td></tr><tr><td rowspan=\"4\">Toys</td><td>Recall@10</td><td>0.0370</td><td>0.0361</td><td>0.0533</td><td>0.0492</td><td>0.0831</td><td>0.0733</td><td>0.0967</td><td>0.0978</td><td>0.0972</td><td>0.1013</td></tr><tr><td>Recall@20</td><td>0.0588</td><td>0.0566</td><td>0.0787</td><td>0.0767</td><td>0.1168</td><td>0.1052</td><td>0.1349</td><td>0.1322</td><td>0.1303</td><td>0.1382</td></tr><tr><td>NDCG@10</td><td>0.0184</td><td>0.0186</td><td>0.0234</td><td>0.0246</td><td>0.0375</td><td>0.0417</td><td>0.0475</td><td>0.0480</td><td>0.0478</td><td>0.0504</td></tr><tr><td>NDCG@20</td><td>0.0239</td><td>0.0238</td><td>0.0297</td><td>0.0316</td><td>0.0460</td><td>0.0497</td><td>0.0571</td><td>0.0567</td><td>0.0561</td><td>0.0597</td></tr><tr><td rowspan=\"4\">Yelp</td><td>Recall@10</td><td>0.0361</td><td>0.0380</td><td>0.0524</td><td>0.0361</td><td>0.0650</td><td>0.0413</td><td>0.0589</td><td>0.0681</td><td>0.0663</td><td>0.0698</td></tr><tr><td>Recall@20</td><td>0.0592</td><td>0.0608</td><td>0.0756</td><td>0.0578</td><td>0.0928</td><td>0.0675</td><td>0.0902</td><td>0.0964</td><td>0.0940</td><td>0.1003</td></tr><tr><td>NDCG@10</td><td>0.0184</td><td>0.0197</td><td>0.0327</td><td>0.0182</td><td>0.0401</td><td>0.0216</td><td>0.0338</td><td>0.0412</td><td>0.0400</td><td>0.0419</td></tr><tr><td>NDCG@20</td><td>0.0243</td><td>0.0255</td><td>0.0385</td><td>0.0236</td><td>0.0471</td><td>0.0282</td><td>0.0416</td><td>0.0483</td><td>0.0470</td><td>0.0496</td></tr></table>\n\n- BERT4Rec [25]: A bidirectional self-attention network that uses Cloze task to model user behavior sequences.  \n- SASRec [11]: An attention-based model that uses self-attention network for the sequential recommendation.  \n- SASRecF: An extension of SASRec, which fuses the item and attribute representation through concatenation operation before feeding them to the model.  \n-  $\\mathbf{S}^3$ -Rec [37]: A self-supervised learning based model with four elaborately designed optimization objectives to learn the correlation in the raw data.  \n- ICAI-SR [33]: A generic framework that carefully devised attention-based Item-Attribute Aggregation model (IAA) and Entity Sequential (ES) models for exploiting various relations between items and attributes. For a fair comparison, we instantiate ICAI-SR by taking SASRec as ES models in our experiments.  \n- NOVA [16]: A framework that adopts non-invasive self-attention (NOVA) mechanism for better attention distribution learning. Similar to ICAI-SR, we implement NOVA mechanism on SASRec for a fair comparison.\n\n5.1.4 Implementation Details. All the baselines and ours are implemented based on the popular recommendation framework RecBole [35] and evaluated with the same setting. For all baselines and our proposed method, we train them with Adam optimizer for 200 epochs, with a batch size of 2048 and a learning rate of 1e-4. The hidden size of our DIF-SR and other attention-based baselines are all set to 256. For other hyperparameters, we apply grid-search to find the best config for our model and baseline models that involve the following hyperparameters. The searching space is: attribute_embedding_size  $\\in$  {16,32,64,128,256}, num_heads  $\\in$  {2,4,8}, num_layers  $\\in$  {2,3,4}. Balance parameter  $\\lambda$  in Equ.(17) is chosen from {5,10,15,20,25}, and fusion function  $\\mathcal{F}$  in Equ.(10) for NOVA and ours is chosen from addition, concatenation and gating [16].\n\n# 5.2 Overall Performance (RQ1)\n\nThe overall performance of different methods on all datasets is summarized in Table 2. Based on these results, we can observe that: For four basic SR baselines, it can be observed that SASRec outperforms other methods by a large margin, while BERT4Rec is better than or close to GRU4Rec and Caser in most cases, demonstrating the superiority of attention-based methods on sequential data. Note that although BERT4Rec is proposed to be an advanced version of SASRec, its performance is not so good as SASRec under the full ranking evaluation setting, which is also found in prior works [14, 37]. We attribute this phenomenon to the mismatch between masked item prediction and the intrinsic autoregressive nature of SR. And the superior performance of BERT4Rec under the popularity-based sampling strategy in the original paper may be because the bidirectional encoder with Cloze task used by BERT4Rec could learn better representations for popular items for evaluation.\n\nBased on the aforementioned findings, we implement all attention-based side information aware baselines based on the same attentive structure from SASRec for fair comparison under the full ranking setting. The reimplementation details are discussed in Sec. 5.1.3. For the side information aware baselines, it can be found that the simple early fusion solution for GRU4Rec and SASRec does not always improve the performance comparing with the version without side information, which is in line with our analysis that early-integrated representation forces the model to design complex merging solution, otherwise it will even harm the prediction performance. Also, the recently proposed side information aware SR models, i.e., S $^3$ Rec, NOVA, ICAI, get better and comparative performance, suggesting that with carefully-designed fusion strategy, side information can improve prediction performance.\n\nFinally, it is clear to see that our proposed DIF-SR consistently outperforms both state-of-the-art SR models and side information integrated SR models on all four datasets in terms of all evaluation\n\nTable 3: Performance comparison of self-attention based sequential models with their DIF & AAP incorporated version on Beauty, Sports and Toys datasets.  \n\n<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Metric</td><td colspan=\"2\">DIF-SASRec</td><td colspan=\"2\">DIF-BERT4Rec</td><td colspan=\"2\">Improv.</td></tr><tr><td>w/o</td><td>w</td><td>w/o</td><td>w</td><td>DIF-SASRec</td><td>DIF-BERT4Rec</td></tr><tr><td rowspan=\"4\">Beauty</td><td>Recall@10</td><td>0.0828</td><td>0.0908</td><td>0.0529</td><td>0.0579</td><td>9.66%</td><td>9.45%</td></tr><tr><td>Recall@20</td><td>0.1197</td><td>0.1284</td><td>0.0815</td><td>0.0915</td><td>7.27%</td><td>12.27%</td></tr><tr><td>NDCG@10</td><td>0.0371</td><td>0.0446</td><td>0.0237</td><td>0.0279</td><td>20.22%</td><td>17.72%</td></tr><tr><td>NDCG@20</td><td>0.0464</td><td>0.0541</td><td>0.0309</td><td>0.0363</td><td>16.59%</td><td>17.48%</td></tr><tr><td rowspan=\"4\">Sports</td><td>Recall@10</td><td>0.0526</td><td>0.0556</td><td>0.0295</td><td>0.0394</td><td>5.70%</td><td>33.56%</td></tr><tr><td>Recall@20</td><td>0.0773</td><td>0.0800</td><td>0.0465</td><td>0.0611</td><td>3.49%</td><td>31.40%</td></tr><tr><td>NDCG@10</td><td>0.0233</td><td>0.0264</td><td>0.0130</td><td>0.0198</td><td>13.30%</td><td>52.31%</td></tr><tr><td>NDCG@20</td><td>0.0295</td><td>0.0325</td><td>0.0173</td><td>0.0252</td><td>10.17%</td><td>45.66%</td></tr><tr><td rowspan=\"4\">Toys</td><td>Recall@10</td><td>0.0831</td><td>0.1013</td><td>0.0533</td><td>0.0599</td><td>21.90%</td><td>12.38%</td></tr><tr><td>Recall@20</td><td>0.1168</td><td>0.1382</td><td>0.0787</td><td>0.0851</td><td>18.32%</td><td>8.13%</td></tr><tr><td>NDCG@10</td><td>0.0375</td><td>0.0504</td><td>0.0234</td><td>0.0324</td><td>34.40%</td><td>38.46%</td></tr><tr><td>NDCG@20</td><td>0.0460</td><td>0.0597</td><td>0.0297</td><td>0.0387</td><td>29.78%</td><td>30.30%</td></tr></table>\n\nmetrics. Different from the baselines, we decouple the attention calculation process of side information and propose to add attribute predictors upon the learned item representation. DIF inherently enhances the expressiveness of self-attention with higher-rank attention matrices, avoidance of mixed correlation, and flexible training gradient, while AAP further strengthens the mutual interactions between side information and item information during the training period. These results demonstrate the effectiveness of the proposed solution to improve the performance by leveraging side information for the attention-based sequential recommendation.\n\n# 5.3 Enhancement Study (RQ2)\n\nWith the simple and effective design, we argue that DIF and AAP can be easily incorporated into any self-attention based SR model and boost the performance. To verify this, we conduct experiments on two representative models: SASRec and BERT4Rec, which represent the unidirectional model and bidirectional model, respectively.\n\nAs shown in Tab. 3, with our design, the enhanced models significantly outperform the original models. Specifically, DIF-BERT4Rec achieves  $18.46\\%$  and  $36.16\\%$  average relative improvements and DIF-SASRec achieves  $12.42\\%$  and  $22.64\\%$  average relative improvements on three datasets in terms of Recall@10 and NDCG@10. It demonstrates that the proposed solution can effectively fuse side information to help make next-item predictions for various attention-based SR models, indicating that it can potentially make more impact as a plug-in module in state-of-the-art SR models.\n\n# 5.4 Ablation and Hyper-parameter Study (RQ3)\n\n5.4.1 Effectiveness of Different Components. In order to figure out the contribution of different components in our proposed DIF-SR, we conduct ablation study for every proposed components on Sports and Yelp datasets, as shown in Tab. 4.:\n\n- (DIF-SR w/o DIF & AAP) DIF-SR without both DIF and AAP, which is identical to SASRecF.  \n- (DIF-SR w/o DIF) DIF-SR without DIF, which adopts early fusion as SASRec $_\\mathrm{F}$  and keep other settings including AAP same as the original model.  \n- (DIF-SR w/o AAP) DIF-SR without AAP, which only uses the item predictor for training.\n\nThen we have the following observations:\n\nTable 4: Ablation study of DIF and AAP on Yelp, Sports and Beauty datasets.  \n\n<table><tr><td colspan=\"2\">Settings</td><td colspan=\"2\">Yelp</td><td colspan=\"2\">Sports</td><td colspan=\"2\">Beauty</td></tr><tr><td>DIF</td><td>AAP</td><td>Recall@20</td><td>+Δ</td><td>Recall@20</td><td>+Δ</td><td>Recall@20</td><td>+Δ</td></tr><tr><td>X</td><td>X</td><td>0.0663</td><td>-</td><td>0.0621</td><td>-</td><td>0.0996</td><td>-</td></tr><tr><td>X</td><td>✓</td><td>0.0663</td><td>+0%</td><td>0.0754</td><td>+21.42%</td><td>0.0991</td><td>-0.50%</td></tr><tr><td>✓</td><td>X</td><td>0.0968</td><td>+46.00%</td><td>0.0767</td><td>+23.51%</td><td>0.1240</td><td>+24.50%</td></tr><tr><td>✓</td><td>✓</td><td>0.1003</td><td>+51.28%</td><td>0.0800</td><td>+28.82%</td><td>0.1284</td><td>+28.92%</td></tr></table>\n\nTable 5: Performance comparison of using different kinds of side information on Yelp dataset.  \n\n<table><tr><td>Side-info</td><td>Recall@10</td><td>Recall@20</td><td>NDCG@10</td><td>NDCG@20</td></tr><tr><td>Position</td><td>0.0655</td><td>0.0954</td><td>0.0405</td><td>0.048</td></tr><tr><td>Position + Categories</td><td>0.0698</td><td>0.1003</td><td>0.0419</td><td>0.0496</td></tr><tr><td>Position + City</td><td>0.0691</td><td>0.1001</td><td>0.0415</td><td>0.0493</td></tr><tr><td>Position + Categories + City</td><td>0.0699</td><td>0.1010</td><td>0.0421</td><td>0.0499</td></tr></table>\n\nFirst, DIF is the most effective component in DIF-SR framework. This could be demonstrated by the fact that  $DIF - SR$  w/o AAP outperforms  $DIF - SR$  w/o DIF & AAP by a large margin. This observation verifies that decoupled side information significantly improves the representation power of attention matrices.\n\nSecond, only using AAP on the prior integrated embedding based method can not always improve the performance, which is in line with our design: AAP is proposed to activate the attributes' influence on item-to-item attention in the self-attention layer, while prior solutions do not enable such influence.\n\nThird, the AAP-based training paradigm can further improve the model performance when combined with DIF, verifying its capacity to activate the beneficial interaction and improve the performance.\n\n5.4.2 Effectiveness of Different Kinds of Side Information. In order to figure out the contribution of different kinds of side information in our proposed DIF-SR, we conduct a study for various attributes. The position is the special and basic side information to enable order-aware self-attention, and other item-related side information of Yelp includes city and category. As shown in Tab. 5, both item-related attributes can help the prediction by a large margin, demonstrating the effectiveness of side information fusion. Also, the combination of two attributes can further improve the performance slightly, which shows that our proposed solution can jointly leverage useful information from various resources.\n\n5.4.3 Impact of Hyperparameters and Fusion Function. In this experiment, we aim to investigate the effect of two hyperparameters, i.e., the balance parameter  $\\lambda$  for loss function and the attribute embedding size  $d_{f}$  for category. We also experiment on the effect of fusion function  $\\mathcal{F}$ .\n\nFig. 4 presents the Recall@20 and NDCG@20 scores of DIF-SR with different  $\\lambda$ . For all these four datasets, the best performance is achieved when the balance parameter  $\\lambda$  is 5 or 10. For Beauty and Yelp datasets, the model performance varies in extremely small range and setting  $\\lambda$  to 10 is a better choice.\n\nFig. 5 shows the effect of attribute embedding size on DIF-SR's performance. We observe that, in most cases, the performance is consistent across different choices of the attribute embedding size  $d_{f}$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b383b849-5f8f-48de-a3c1-2bb6e1f07ba6/c2379664c06a24856b12abed6b72740d590de982d47eb690a9402007d53cfe7d.jpg)  \nFigure 4: Effect of balance parameter  $\\lambda$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-17/b383b849-5f8f-48de-a3c1-2bb6e1f07ba6/34ccae3f83bea7cd1b5a005ef67505f8b2a8f6e06c31a424bcbc649c4540fb08.jpg)  \nFigure 5: Effect of attribute embedding size  $d_{f}$\n\nfor category. Based on this, it is possible to greatly reduce the model complexity of DIF-SR by setting  $d_f$  to a smaller value (typically smaller than the dimension of item embedding).\n\nFig. 6 demonstrates that our method is also robust regarding different fusion functions, suggesting that simple fusion solutions, such as addition, do not harm the capacity of our model.\n\n# 5.5 Visualization of Attention Distribution (RQ4)\n\nTo discuss the interpretability of DIF-SR, we visualize the attention matrices of test samples from the Yelp dataset. Due to the limitation of space, we show one example in Fig. 7. The two rows represent the attention matrices of the same head in different layers of one sample. The first three columns are decoupled attention matrices for the item and the attributes, and the last one is the fused attention matrices which are used to calculate the output item representation for each layer.\n\nWe derive the following observations from the results: 1) the decoupled attention matrices for different attributes show a different preference for capturing data patterns. 2) the fused attention matrix could adaptively adjust the contribution of each kind of side information via the decoupled attention matrices and synthesize crucial patterns from them.",
  "hyperparameter": "Hidden size (d): 256; Batch size: 2048; Learning rate: 1e-4; Training epochs: 200; Optimizer: Adam; Attribute embedding size (d_f): {16, 32, 64, 128, 256} (can be much smaller than item embedding without harming performance, typically smaller than d=256); Number of attention heads: {2, 4, 8}; Number of layers: {2, 3, 4}; Balance parameter λ for loss function: {5, 10, 15, 20, 25} (best performance at λ=5 or 10); Fusion function F: {addition, concatenation, gating} (addition works well and is simple); Maximum sequence length n: varies by dataset preprocessing (leave-one-out with last 2 items for validation/test)"
}