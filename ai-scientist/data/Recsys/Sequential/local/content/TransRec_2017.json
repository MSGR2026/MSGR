{
  "id": "TransRec_2017",
  "paper_title": "TransRec: Translating User Behaviors for Recommendation",
  "alias": "TransRec",
  "year": 2017,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "TransRec proposes a translation-based embedding framework for sequential recommendation that unifies user preferences and sequential dynamics into a single metric space. The core innovation is modeling item transitions as translations in the latent space: the next item is predicted by translating the current item embedding with a combined user-specific and global translation vector (γ_i + T_u → γ_j), where T_u = t + t_u captures both shared sequential patterns and personalized preferences. This unified translation approach elegantly integrates matrix factorization and Markov chains into one geometric operation, outperforming existing methods that treat these components separately, especially on sparse datasets.",
  "introduction": "# Introduction\nSequential recommendation (next-item prediction) requires modeling third-order interactions between a user (u), their previously consumed item (i), and the next item (j). Traditional methods decompose these interactions into separate pairwise components:\n- Matrix Factorization (MF) captures user-item preferences but ignores sequential patterns.\n- Markov Chains (MC) model item-item sequential dynamics but lack personalization.\n- Hybrid methods (e.g., FPMC) combine MF and MC but treat the two components independently, limiting their ability to model inherent correlations.\n\nThis paper proposes **TransRec**, a unified translation-based framework that naturally models third-order interactions. Key ideas:\n1. Embed items into a \"transition space\" where each user is represented as a translation vector.\n2. Personalized transitions are modeled as a translation operation: \\( \\vec{\\gamma}_i + \\vec{t}_u \\approx \\vec{\\gamma}_j \\) (item i + user translation ≈ next item j).\n3. Leverages metric space properties (triangle inequality) for better generalization, especially on sparse data.\n\nTransRec outperforms state-of-the-art methods on large real-world datasets, and adapts to item-to-item recommendation tasks. It also introduces a new large-scale Google Local dataset for sequential/location-based recommendation.",
  "method": "# Method\n## 1. Problem Formulation\n- **Input**: For each user \\( u \\), a sequence of interacted items \\( S^u = (S_1^u, S_2^u, ..., S_{|S^u|}^u) \\).\n- **Task**: Predict the next item \\( S_{|S^u|+1}^u \\) for user \\( u \\) based on \\( S^u \\).\n\n## 2. Core Model: Translation-Based Embedding\n### 2.1 Embedding Space & Representations\n- **Transition Space**: \\( \\Phi = \\mathbb{R}^K \\) (K = latent dimension), with a subspace \\( \\Psi \\subseteq \\Phi \\) (e.g., unit \\( L_2 \\)-ball) to mitigate dimensionality issues.\n- **Item Embeddings**: Each item \\( i \\) is represented as \\( \\vec{\\gamma}_i \\in \\Psi \\) (latent vector).\n- **User Translation Vectors**: Each user \\( u \\) has a personalized translation vector \\( \\vec{t}_u \\in \\Phi \\); a global translation vector \\( \\vec{t} \\) captures shared sequential dynamics. Combined as \\( \\vec{T}_u = \\vec{t} + \\vec{t}_u \\).\n\n### 2.2 Prediction Rule\nThe probability that user \\( u \\) transitions from item \\( i \\) to item \\( j \\) is:\n\\[ Prob(j | u, i) \\propto \\beta_j - d(\\vec{\\gamma}_i + \\vec{T}_u, \\vec{\\gamma}_j) \\]\n- \\( \\beta_j \\): Item popularity bias.\n- \\( d(\\cdot, \\cdot) \\): Distance metric (squared \\( L_2 \\) or \\( L_1 \\); squared \\( L_2 \\) performs better empirically).\n\n### 2.3 Optimization with S-BPR\nOptimize for pairwise ranking (user prefers next item \\( j \\) over non-interacted item \\( j' \\)) using Sequential Bayesian Personalized Ranking (S-BPR):\n\\[ \\max_\\Theta \\sum_{u} \\sum_{j \\in S^u} \\sum_{j' \\notin S^u} \\ln \\sigma(\\hat{p}_{u,i,j} - \\hat{p}_{u,i,j'}) - \\Omega(\\Theta) \\]\n- \\( \\hat{p}_{u,i,j} = \\beta_j - d(\\vec{\\gamma}_i + \\vec{T}_u, \\vec{\\gamma}_j) \\) (prediction score).\n- \\( \\Theta = \\{\\beta_i, \\vec{\\gamma}_i, \\vec{t}_u, \\vec{t}\\} \\) (model parameters).\n- \\( \\Omega(\\Theta) \\): \\( L_2 \\) regularization to prevent overfitting.\n\n### 2.4 Learning Procedure\n1. **Initialization**: \\( \\vec{\\gamma}_i \\) and \\( \\vec{t} \\) as unit vectors; \\( \\beta_i \\) and \\( \\vec{t}_u \\) as 0.\n2. **Stochastic Gradient Ascent**: Sample (u, i, j, j'), compute gradients, update parameters, and re-normalize \\( \\vec{\\gamma}_i/\\vec{\\gamma}_j/\\vec{\\gamma}_{j'} \\) to \\( \\Psi \\).\n3. **Inference**: For test, absorb bias \\( \\beta_j \\) into item embeddings and perform nearest-neighbor search on \\( \\vec{\\gamma}_i + \\vec{T}_u \\).\n\n## 3. Connections to Existing Models\n- **FPMC**: Combines MF and MC as separate components; TransRec unifies them into a single translation operation.\n- **PRME**: Uses two metric spaces for user-item and item-item interactions; TransRec uses one unified metric space.\n- **HRM**: Aggregates user and item vectors but lacks metric space generalization; TransRec leverages translation and metric properties.",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\n11 large-scale real-world datasets (sequential recommendation) + 8 Amazon co-purchase datasets (item-to-item recommendation):\n| Dataset       | #Users   | #Items   | #Actions  | Domain                  |\n|---------------|----------|----------|-----------|-------------------------|\n| Epinions      | 5,015    | 8,335    | 26,932    | Consumer reviews        |\n| Automotive    | 34,316   | 40,287   | 183,573   | Amazon products         |\n| Google Local  | 4.57M    | 3.12M    | 11.45M    | Local businesses        |\n| Foursquare    | 43,110   | 13,335   | 306,553   | Venue check-ins         |\n| Flixter       | 69,485   | 25,759   | 8.00M     | Movie ratings          |\n\n### 1.2 Baselines\n- **PopRec**: Popularity-based (non-personalized).\n- **BPR-MF**: Matrix factorization with BPR loss (no sequential modeling).\n- **FMC**: Factorized Markov Chain (sequential, no personalization).\n- **FPMC**: Hybrid of MF and FMC (separate components).\n- **PRME**: Personalized ranking metric embedding (two metric spaces).\n- **HRM**: Hierarchical representation model (max/average pooling).\n\n### 1.3 Evaluation Metrics\n- **AUC**: Measures ranking quality (higher = better).\n- **Hit@50**: Fraction of test items in top-50 recommendations (higher = better).\n- For item-to-item recommendation: AUC and Hit@10.\n\n### 1.4 Hyperparameters\n- Latent dimension \\( K = 10 \\) (default); sensitivity tested up to 100.\n- Learning rate = 0.05; regularization \\( \\lambda \\in \\{0, 0.001, 0.01, 0.1, 1\\} \\).\n- Distance metric: Squared \\( L_2 \\) (primary) and \\( L_1 \\) (ablation).\n\n## 2. Main Results\n### 2.1 Sequential Recommendation Performance\nTable 4 (core result table) shows TransRec outperforms all baselines on most datasets:\n- **Key Findings**:\n  1. TransRec achieves 2.7–32.5% Hit@50 improvement over the strongest baseline (HRM/PRME) on sparse datasets (Google Local, Clothing).\n  2. On dense datasets (Flixter), TransRec closes the gap with PRME when increasing \\( K \\) to 100.\n  3. Squared \\( L_2 \\) distance outperforms \\( L_1 \\) due to better alignment with metric space properties.\n  4. TransRec’s advantage correlates with dataset sparsity/variability—its translation-based unified model generalizes better on sparse data.\n\n### 2.2 Convergence\n- Sparse datasets (e.g., Automotive): TransRec converges slower than simple baselines (FMC/BPR-MF) but achieves higher final accuracy.\n- Dense datasets (e.g., Flixter): All methods converge at similar speeds, with TransRec maintaining superior performance.\n\n### 2.3 Sensitivity to Latent Dimension\n- Increasing \\( K \\) (10→100) improves TransRec’s performance on dense datasets (Flixter), closing the gap with PRME.\n- On sparse datasets (Electronics, Foursquare), TransRec remains dominant even at \\( K=10 \\).\n\n### 2.4 Item-to-Item Recommendation\nTransRec outperforms state-of-the-art baselines (WNN, LMT, Monomer) on 8 Amazon co-purchase datasets:\n- Average 4.2–312.0% improvement in Hit@10, demonstrating its ability to model item-item relationships via translation.\n\n## 3. Case Study\nTransRec successfully captures both long-term and short-term user dynamics:\n- Long-term: Recommends a tripod to a user with a history of photography equipment.\n- Short-term: Recommends a desktop case after a user purchases a motherboard.",
  "hyperparameter": "Latent dimension K=10 (default, tested up to 100 for dense datasets); Learning rate=0.05; Regularization λ ∈ {0, 0.001, 0.01, 0.1, 1} (selected via validation); Distance metric: squared L2 norm (primary choice, outperforms L1); Embedding constraint: item embeddings normalized to unit L2-ball (Ψ ⊆ ℝ^K); Optimization: Stochastic Gradient Ascent with Sequential BPR (S-BPR) loss; Initialization: item embeddings and global translation as unit vectors, biases and user translations as zeros."
}