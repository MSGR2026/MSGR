{
  "id": "GRU4RecF_2019",
  "paper_title": "Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations",
  "alias": "GRU4RecF",
  "year": 2019,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "",
  "introduction": "# 1. INTRODUCTION\n\nIn traditional recommender systems algorithms it is often assumed that user history logs (e.g. clicks, purchases or views) are available. We show that this assumption does not hold in many real-world recommendation use cases: (1)\n\n*The author worked at Telefonica Research as an intern during the time of this research.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nRecSys '16, September 15-19, 2016, Boston, MA, USA\n\nÂ© 2016 ACM. ISBN 978-1-4503-4035-9/16/09...$15.00\n\nDOI: http://dx.doi.org/10.1145/2959100.2959167\n\nmany e-commerce sites do not require user authentication even for purchase; (2) in video streaming services users also rarely log in; (3) many sites have a small percentage of returning users; (4) or the user intent can change between different sessions, typical for e.g. classified sites. User tracking can partly solve the user identification problem across sessions (e.g. through fingerprinting technology, cookies), but this is often unreliable and also raises privacy concerns. So, in such cases the typical solution is to resort to item-to-item recommendations. Here we investigate how we can exploit session data to improve recommendations.\n\nGiven the absence of user profiles, it is vital to draw as much information as possible from the session clicks. Besides the click-stream data of the session (called item-IDs), one can also take into account the characteristics of the items that have been clicked. Items often come with rich feature representation such as detailed text description or (thumb-nail) image. One can expect that e.g. users shopping for a particular type of item will click on items that have similar text descriptions and similar visual features. Images, text or potentially even richer features such as animated gifs are eventually what the user sees and what will determine the appeal of an item to a user. Features become particularly important in a session modeling setting where historical user specific data is missing or has no importance. Such features should be utilized to aid the session modeling process. Item features are also a very good way to deal with item cold start i.e. when a new item enters the pool of selectable items. The joint modeling of the sequence of clicked item-IDs and item features poses some interesting problems.\n\nThe algorithms in this work utilize deep learning techniques both to extract high quality features from visual information and to model the sessions. (We also extract features from text via bag-of-words.) Individual user sessions can be seen as sequence of clicks. We use Recurrent Neural Networks (RNNs) to model the session data. RNNs have been shown to perform excellent in modeling sequence data [15]. We introduce a number of parallel $^{1}$  RNN (p-RNN) ar\n\nchitectures to model the session clicks concurrently with the features (text or images) of the clicked items.\n\nInstead of using a single RNN where all the data is used at the input in a concatenated form, we apply parallel architectures because of the very different nature of the data: image features tend to be much denser than the one-hot representation of the item-ID or the bag-of-words representation of text. Parallel training also allows us to fine-tune the individual networks with respect to their hyper-parameters while also maintaining a connection between the networks through shared parameters and optimization. We introduce 3 different architectures of p-RNNs that combine the ID-click data with the features of the clicked items. The architectures vary with regards to the shared model parameters and interactions of the hidden states (Section 3).\n\nWe also point out that training p-RNNs is not trivial. Standard simultaneous training can waste the capacity of the network, because different parts of the same network may learn the same relations from the data. Therefore we devise 3 alternative optimization procedures to train p-RNNs. We thoroughly evaluate the proposed p-RNN architectures on (1) video recommendations using the thumbnail and (2) product recommendations using text description on a classified site. We compare against the industry standard session-based solution, that is today the item-kNN.",
  "method": "# 3. P-RNN ARCHITECTURES\n\nIn this section we describe the proposed parallel RNN (p-RNN) architectures that utilize item representations (features) for session modeling. A p-RNN consists of multiple RNNs, one for each representation/aspect of the item (e.g. one for ID, one for image and one for text). The hidden states of these networks are merged to produce the score for all items. We also introduce baseline architectures, naive approaches for using the different item representations.\n\nAs a basis, we take the best RNN setting from [7]: a single GRU layer without feedforward layers and the TOP1 pairwise loss function along with session-parallel mini-batching. The input of the networks is the item ID of a transaction.\n\nThe input then translates to either (a) a one-hot ID vector $^2$ ; or (b) a precomputed dense image feature vector; or (c) a sparse unigram + bigram text feature vector. See details on feature extraction in Section 4. The proposed architectures use a subset of the above 3 item representations. The output is a score for every item indicating the likelihood of being the next item in the session. During training scores are compared to a one-hot vector created from the item ID of the next event in the session to compute the loss. In order to reduce computational costs, only the score of the target item and that of a small subset of \"negative\" items - sampled in each step (see [7]) - are computed during training.\n\nThe TOP1 loss is the regularized approximation of the relative rank of the relevant item. The relative rank of the relevant item is given by  $\\frac{1}{N_S} \\cdot \\sum_{j=1}^{N_S} I\\{\\hat{r}_{s,j} > \\hat{r}_{s,i}\\}$  where  $N_S$  is the sample size,  $\\hat{r}_{s,i}$  is the predicted score of the target item,  $\\hat{r}_{s,j}$  is the score of negative (other) items in the sample and  $I\\{\\cdot\\}$  is approximated with a sigmoid. Optimizing for this loss modifies the parameters so that the score for  $i$  is high. This loss though is unstable as some positive items also act as negative examples and so scores tend to become increasingly higher. To avoid this, the scores of the negative examples are pushed towards zero through a regularization term. The loss function thus takes the following form:  $L_s = \\frac{1}{N_S} \\cdot \\sum_{j=1}^{N_S} \\sigma(\\hat{r}_{s,j} - \\hat{r}_{s,i}) + \\sigma(\\hat{r}_{s,j}^2)$ . Given that a typical item set in recommender systems is in the 100,000s, evaluating this loss over all items is computationally prohibitive. We thus use a sampling procedure whereby the loss is evaluated on a sample of the items. For a given session we use the items in the other sessions of the mini-batch as negative samples. This is computationally cheap and also allows us to get samples from the real distribution of the data.\n\nWe devised the following architectures (see Figure 1). Due to limited space, we only present architectures with ID and image features; for text features one can proceed analogously as with image ones. The parallel architectures can also deal with ID, image and text features simultaneously. The architectures can be separated into two groups:\n\n# 1. Baseline architectures:\n\nID only: This architecture only uses the one-hot ID vectors and is identical to the one used in [7]. It serves as a baseline in our experiments.\n\nFeature only: The input of this variant is one of the content feature vectors (image or text). Otherwise it works similarly to the previous network.\n\nConcatenated input: The easiest way to combine different item representations is to concatenate them. This network uses the concatenated representations as its input.\n\n# 2. p-RNN architectures:\n\nParallel: The first parallel architecture trains one GRU network for each of the representations. Outputs are computed from the concatenation of the hidden layers of the subnets. Training can be done in different ways (see training strategies below).\n\nParallel shared-W: This architecture differs from the previous one by having a shared hidden to output weight matrix. Scores are not computed for each subnetwork separately. Instead, the weighted sum of the hidden states is\n\nmultiplied by a single weight matrix to produce the output. Having a shared weight matrix greatly reduces the number of parameters and thus decrease training times and overfitting. This model is also analogous to the pairwise model from context-aware factorization research. $^4$\n\nParallel interaction: In this architecture, the hidden state of the item feature subnet(s) is multiplied by the hidden state of the ID subnet in an element-wise manner before computing the score of the subnet(s). Mixing different aspects of the session to compute item scores is analogous to context-aware preference modeling. For that task [9] found that the interaction model, i.e. the sum of the user-item and user-context-item interaction to perform the best. This architecture mimics that model with the ID subnet being promoted to the primary representation of the session. The main difference to the context-aware task is that all of our representations are session representations and not (mostly) independent dimensions. Also note that contrary to the original interaction model, the output weight matrix (item feature matrix) is not shared in our model.\n\n# 3.1 Training p-RNNs\n\nTraining parallel architectures is not trivial. Standard backpropagation across the whole architecture can produce suboptimal results due to different components of the architecture learning the same relations from the data. This can be avoided by pretraining some parts of the network and training the rest afterwards. This cycle can be done several times, motivated by the success of alternating methods like ALS for matrix factorization. Note that while the parameters of fixed networks remain unchanged they still participate in the forward pass and they are only excluded from the backpropagation. We developed the following training strategies for p-RNNs:\n\nSimultaneous: Every parameter of every subnet is trained simultaneously. Serves as the baseline.\n\nAlternating: Subnets are trained in an alternating fashion per epoch. For example, the ID subnet is trained in the first epoch, while the others are fixed; then we fix the ID subnet and train the image subnet for one epoch; and so on. The cycle restarts after each subnet was trained.\n\nResidual: Subnets are trained one after the other, on the residual error of the ensemble of the previously trained subnets. The cycle does not start over, but the individual training of a subnet is longer compared to the alternating method. For example, the ID subnet is trained for 10 epochs, then the image subnet is trained on the residual error of the ID subnet and so on.\n\nInterleaving: Alternating training per mini-batch. For each mini-batch of training examples, the first subnet is trained, the second subnet is trained on the residual error for the current mini-batch and so on. The more frequent alternation allows for a more balanced training across subnets without the drawbacks of the simultaneous training.\n\n# 4. FEATURE EXTRACTION\n\nIn this Section we describe the feature extraction process\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f3ab7331-b61b-4b59-988a-baf22e12b077/53163671998d95e45e7174ec64009e71811ddabb4ccb11b70126b25bde9da46b.jpg)  \nFigure 1: RNN architectures for feature inclusion. Architectures are presented with ID and image features only, but text features can be used in place of the image features as well. The parallel architectures can also be extended to have networks for ID, image and text features simultaneously. f() is the nonlinearity applied to the network output (tanh in our case). Top row (left to right): ID only; Concatenated input; Parallel. Bottom row (left to right): Feature only; Parallel with interaction model; Parallel with shared weight matrix.\n\nused to create item representations from images and text. Image features were extracted from video thumbnails, while text features are based on product descriptions.\n\nEncoding images. Recently, deep learning research on convolutional neural networks (CNNs) achieved breakthroughs in a variety of different image-related tasks, like object recognition, image segmentation, video classification, etc. [12, 13] even surpassing human performance on the task of object recognition [5]. Unlike other approaches, CNNs don't require prior feature extraction, since they are capable of working on the raw image data. CNNs trained on millions of images produce image features that can then be used as input in other algorithms e.g. clustering [3, 26]. These models generalize well and also perform well on images that the CNN has never encountered during training and can thus be used as generic feature extractors. This makes CNNs ideal for extracting high quality image features.\n\nWe used the GoogLeNet [27] implementation of the Caffe deep learning framework [12] to extract features from the thumbnails of the videos. The network was pre-trained as an image classifier on the ImageNet ILSVRC 2014 dataset [19] that contains 1.2M images organized into 1000 cate\n\ngories. The video thumbnails first had to be scaled down and cropped in order to fit the input of the network. Features were extracted from the last average pooling layer.5 The feature vectors were normalized to an  $l2$  norm of 1. The image feature representation we end up with is a real-valued vector of length 1024.\n\nFigure 2 demonstrates the feature quality by showing the 3 most similar images to two query images, where similarity is defined as the cosine similarity between the image feature vectors. Given the good quality, we do not plug the CNN directly into the RNN, as it would introduce unnecessary complexity to the training and is also unpractical, because (a) this network would converge much slower as it needs to learn the model on incomplete/changing item representations; (b) the network would not be suitable for datasets with lower number of items, as 10,000s of items are not enough to leverage the full potential of the CNN; (c) retraining would take much longer. Another possibility is to use the pretrained network and fine tune the item representations during the training of the RNN. This did not make much difference in our experiments, therefore we did not use fine tuning.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f3ab7331-b61b-4b59-988a-baf22e12b077/907f86c9a588ff3020172bd442814d6af247f84f08e556cc6884cf03674f5465.jpg)  \nFigure 2: Top3 similar images to query images, based on cosine similarity of image feature vectors.\n\nEncoding text. Given the strict limitation on the length of the descriptions imposed by online classified advertisement platforms, advertisers usually provide rather concise text for their items. The main goal of the description is to attract the attention of potentially interested users. Therefore, descriptions often contain only the main characteristics of the item and use syntactically incorrect sentences. Moreover, it is not rare to have descriptions written in multiple languages to capture a broader audience. The majority of descriptions of our dataset used a subset of 3 main languages with a handful of less frequent ones also present.\n\nGiven the inherent noise in user generated, unstructured text and multiple languages in our data, we adopted the classical bag-of-words representation to encode product descriptions. First we concatenated the title and the description of the items. We filtered stopwords and extracted unigrams and bigrams from text, and discarded all the entries that appear only once. Finally, the resulting bag-of-words was weighted using TF-IDF [23]. The item representation is a sparse vector of length 1,099,425 with an average of 5.44 non-zero coordinates.\n\nWe tried other methods to extract features from unstructured text, e.g. distributed bag-of-words [18] and Language Modeling with RNNs [17]. However the classical bag-of-words with TF-IDF was found to work best with our data. We attribute this to the noisiness of user generated content. The lack of English text and the presence of multiple languages prevented us from using pre-trained word representations, e.g. from word2vec.<sup>7</sup>\n\nExperiments with adding an embedding layer between the features and the network resulted in worse performance, therefore, the classical bag-of-words/TF-IDF features were used as item representations and were used directly as the input of the RNNs.",
  "experiments": "# 5. RESULTS\n\nThe evaluation was done on two proprietary datasets. The first dataset - coined VIDXL - was collected over a 2-month period from a Youtube-like video site, and contains video watching events having at least a predefined length. During the collection item-to-item recommendations were displayed next to the featured video, generated by a selection of different algorithms. The second dataset consists of product view events of an online classified site. We refer to this dataset as CLASS. The site also had recommendations displayed with different algorithms during the collection period.\n\nDuring the preprocessing of the raw event streams we filtered out unrealistically long sessions as these are likely due\n\n<sup>6</sup>We explicitly avoided to repeat the title if this was already written at the beginning of the text\n\n<sup>7</sup>https://code.google.com/p/word2vec\n\nto bot traffic. We removed sessions of one (single click) event because they are not useful for session-based recommendations and also removed items whose support is below five, as items with low support are not ideal for modeling. The sessions of the last day of each dataset are put into the test set. Each session is assigned to either the training or the test set, we do not split the data mid-session. We also filter items from the test dataset if they were not in the training set. This affects only a tiny fraction of the items. The datasets are summarized in Table 1.\n\nTable 1: Properties of the datasets.  \n\n<table><tr><td rowspan=\"2\">Data</td><td colspan=\"2\">Train set</td><td colspan=\"2\">Test set</td><td rowspan=\"2\">Items</td></tr><tr><td>Sessions</td><td>Events</td><td>Sessions</td><td>Events</td></tr><tr><td>VIDXL</td><td>17,419,964</td><td>69,312,698</td><td>216,725</td><td>921,202</td><td>712,824</td></tr><tr><td>CLASS</td><td>1,173,094</td><td>9,011,321</td><td>35,741</td><td>254,857</td><td>339,055</td></tr></table>\n\nWe evaluate wrt. the (sequential) next-event prediction task, i.e. given an event of the session how well can the algorithms predict the next event of the session. The trained model is fed with the events of a session one after another and we check the rank of the selected item of the next event. The hidden state of the network is reset to zero after a session ends.\n\nAs recommender systems recommend only a few items at once, the relevant item should be amongst the first few items of the list. Therefore, our first evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items of all test cases. Recall does not consider the actual rank of the item as long as it is below 20. This is an accurate model for certain practical scenarios where no recommendation is highlighted and their absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)[8]. The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). MRR is the average of the reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes the rank of the item into account, which is important in cases where the order of recommendations matters (e.g. the lower ranked items are only visible after scrolling).\n\n# 5.1 Thumbnail based video recommendation\n\nWe extracted image features from the thumbnails of the videos, see Section 4 for the details of the feature extraction. We experimented with different architectures and training strategies described in Section 3 to see how image data can contribute to recommendation accuracy.\n\nSimilarly to [7], the networks optimize for TOP1 loss using adagrad. The parameters - such as dropout, learning rate, momentum, batch size, etc. - of the ID only and the feature only networks were optimized on a hold out validation set. Then the networks were retrained on the full training set (validation set included) and the final results were measured on the test set. Due to the size of the VIDXL dataset, the more complex networks used the optimal parameters of the ID and image networks in their corresponding subnets. The weights of the subnets were set to be equal as we did not get significantly different results until either of the subnet weights was set to zero.\n\nTo speed up evaluation, we computed the rank of the rel-\n\nevant item compared to the 50,000 most supported items. While this evaluation methodology somewhat overestimates the rank and thus the evaluated metrics are a little bit higher, the comparison of the algorithms remains fair [1].\n\nTable 2 summarizes the results with different architectures and training methods. In this experiment the number of hidden units was set to 100 for the baseline architectures and 100 per subnetwork for p-RNNs. The networks are trained for 10 epochs as there is no significant change in the loss after that. The networks are compared to the item-kNN algorithm, the de-facto solution for item-to-item and item-to-session recommendation tasks in the industry. p-RNNs with  $100 + 100$  hidden units can easily outperform the ID only network with 100 units, due to the additional information source and the increase of the overall capacity of the network. Therefore we also measured the accuracy of the ID only network with 200 units. Note that this is a very strong baseline, because having 200 hidden units increases the capacity of the network  $\\sim 4$  times, while having  $100 + 100$  only doubles it. Also, the doubled capacity of p-RNN is split between two information sources, therefore it is clearly in disadvantage to even an RNN with doubled capacity. Nevertheless we show that p-RNNs can often beat this strong baseline as well, while they are typically better than the ID only network with 100 units.\n\nTable 2: Results on VIDXL, using image features extracted from thumbnails. The best results are typeset in bold. p-RNNs use  $100 + 100$  hidden units, others use 100 unless stated otherwise. Performance gain over item-kNN is shown in parentheses.  \n\n<table><tr><td>Method</td><td>Recall@20</td><td>MRR@20</td></tr><tr><td>Item-kNN</td><td>0.6263</td><td>0.3740</td></tr><tr><td>ID only</td><td>0.6831 (+9.07%)</td><td>0.3847 (+2.85%)</td></tr><tr><td>ID only (200)</td><td>0.6963 (+11.17%)</td><td>0.3881 (+3.77%)</td></tr><tr><td>Feature only</td><td>0.5367 (-14.30%)</td><td>0.3065 (-18.05%)</td></tr><tr><td>Concatenated</td><td>0.6766 (+8.03%)</td><td>0.3850 (+2.94%)</td></tr><tr><td>Parallel (sim)</td><td>0.6765 (+8.01%)</td><td>0.4014 (+7.34%)</td></tr><tr><td>Parallel (alt)</td><td>0.6874 (+9.76%)</td><td>0.4331 (+15.81%)</td></tr><tr><td>Parallel (res)</td><td>0.7028 (+12.21%)</td><td>0.4440 (+18.72%)</td></tr><tr><td>Parallel (int)</td><td>0.7040 (+12.41%)</td><td>0.4361 (+16.60%)</td></tr><tr><td>Shared-W (sim)</td><td>0.6681 (+6.66%)</td><td>0.4007 (+7.13%)</td></tr><tr><td>Shared-W (alt)</td><td>0.6804 (+8.63%)</td><td>0.4035 (+7.88%)</td></tr><tr><td>Shared-W (res)</td><td>0.6425 (+2.58%)</td><td>0.3541 (-5.31%)</td></tr><tr><td>Shared-W (int)</td><td>0.6658 (+6.31%)</td><td>0.3715 (-0.66%)</td></tr><tr><td>Int. model (sim)</td><td>0.6751 (+7.78%)</td><td>0.3998 (+6.90%)</td></tr><tr><td>Int. model (alt)</td><td>0.6847 (+9.32%)</td><td>0.4104 (+9.74%)</td></tr><tr><td>Int. model (res)</td><td>0.6749 (+7.76%)</td><td>0.4098 (+9.56%)</td></tr><tr><td>Int. model (int)</td><td>0.6843 (+9.25%)</td><td>0.4040 (+8.02%)</td></tr></table>\n\nSimilar to [7], the RNN outperforms the item-kNN baseline by a large margin. The recall for the RNN on this task is very high, therefore it is very hard for the more advanced architectures to significantly improve on this result. The network that is trained on the image features only is significantly worse than the ID only network and even worse than the item-kNN, demonstrating that the sequence of item features in and of itself is not enough to model the session well. Feeding the network with the concatenated input of IDs and image features, because the stronger input dominates during the training, thus the performance hardly differs from that of the ID only network. It is hard for a single GRU layer\n\nto handle two types of inputs at once, resulting in a performance very similar to that of the ID only network. Adding item features using the naive approach has no observable benefits. Therefore we propose to use p-RNNs instead.\n\nMoving on to the proposed p-RNN architectures, one can see that several configurations perform significantly better than the strong ID only baseline. Due to the originally high recall of the network, these novel architectures mostly increase the MRR, i.e. they don't find more relevant items, but they rank them better. The best performing architecture is the classic parallel one. With the naive simultaneous training it is significantly better than the strong ID only baseline wrt. MRR, but slightly worse wrt. recall. With simultaneous training, different components of the p-RNN learn the same relations from the data, thus the full capacity of the network is not leveraged. Therefore we propose using alternative training strategies.\n\nThe best of the alternative training methods is residual training, closely followed by the interleaving one, but the alternating training is also not far behind. The p-RNN with residual training outperforms the strong ID only baseline by  $14.07\\%$  in MRR, while achieving similar recall. The improvement is even greater over the industry de facto item-kNN solution:  $12.21\\%$  in recall and  $18.72\\%$  in MRR.\n\nTable 3: Results on VIDXL, using image features extracted from thumbnails. The best results are typeset in bold. p-RNN architectures use  $500 + 500$  hidden units, others use 1000. Performance gain over item-kNN is shown in parentheses.  \n\n<table><tr><td>Network variant</td><td>Recall@20</td><td>MRR@20</td></tr><tr><td>ID only, 10 epochs</td><td>0.7279 (+16.21%)</td><td>0.4350 (+16.32%)</td></tr><tr><td>ID only, 20 epochs</td><td>0.7207 (+15.07%)</td><td>0.4287 (+14.63%)</td></tr><tr><td>Feature only</td><td>0.6479 (+3.45%)</td><td>0.4089 (+9.34%)</td></tr><tr><td>Concatenated</td><td>0.7216 (+15.22%)</td><td>0.4291 (+14.72%)</td></tr><tr><td>Parallel (sim)</td><td>0.7084 (+13.10%)</td><td>0.4420 (+18.19%)</td></tr><tr><td>Parallel (alt)</td><td>0.7142 (+14.03%)</td><td>0.4456 (+19.15%)</td></tr><tr><td>Parallel (res)</td><td>0.7165 (+14.40%)</td><td>0.4513 (+20.67%)</td></tr><tr><td>Parallel (int)</td><td>0.7262 (+15.59%)</td><td>0.4587 (+22.64%)</td></tr></table>\n\nBy increasing the number of hidden units, the capacity of the RNN increases, thus this parameter has a large effect on performance. However this parameter also obeys the law of diminishing returns. We found that results do not improve significantly above  $\\sim 1000$  hidden units on this problem. We ran experiments with 1000 units on non-parallel and  $500 + 500$  units on the best performing p-RNN architecture (i.e. classic parallel) to confirm that adding item features can also benefit session modeling when increasing the network capacity and/or the number of epochs has diminishing returns. Table 3 depicts the results.\n\nWith more hidden units, the performance increases and even the feature only network outperforms the item-kNN baseline as the capacity of the network is enough to leverage the information in the image features. But otherwise the relation between the results is similar to that of the previous experiments. This further underpins that p-RNNs with alternative training strategies are vital for efficiently incorporating item features into learning session models. Further increasing the number of hidden units and/or the number of epochs did not increase the performance of any network significantly, but p-RNN architectures significantly outperform\n\nthe ID only network with more than 2 times larger capacity in terms of MRR ( $\\sim 7\\%$  with interleaving training) and have similar recall. This means that the proposed architectures with the proposed training strategies can significantly increase performance, even when increasing the capacity of the network has diminishing returns. In other words, adding additional data sources (item features) can increase the accuracy of recommendations beyond the maximum achievable just from item IDs. However handling multiple sources requires special architectures and training: p-RNNs and alternative training strategies.\n\n# 5.2 Using product descriptions\n\nWe repeated the last experiment - i.e. baseline RNNs had 1000 hidden units; the classic p-RNN had 500 per subnet - on the CLASS dataset with features extracted from product descriptions instead of images. See the detailed feature extraction process in Section 4. The experimental setup was the same as before, except that we opted for ranking all items during evaluation, because it is possible to do the full evaluation within reasonable time due to the significantly smaller size of this dataset (compared to VIDXL).\n\nTable 4: Results on CLASS, using textual features extracted from product descriptions. The best results are typeset in bold. p-RNN architectures use  $500 + 500$  hidden units, others use 1000. Performance gain over item-kNN is shown in parentheses.  \n\n<table><tr><td>Training info</td><td>Recall@20</td><td>MRR@20</td></tr><tr><td>Item-kNN baseline</td><td>0.2387</td><td>0.0839</td></tr><tr><td>ID only, 10 epochs</td><td>0.2849 (+19.38%)</td><td>0.1062 (+26.56%)</td></tr><tr><td>ID only, 20 epochs</td><td>0.2783 (+16.60%)</td><td>0.1051 (+25.26%)</td></tr><tr><td>Feature only</td><td>0.2397 (+0.47%)</td><td>0.0937 (+11.70%)</td></tr><tr><td>Conc. input</td><td>0.2844 (+19.17%)</td><td>0.1029 (+22.64%)</td></tr><tr><td>Parallel (sim)</td><td>0.2741 (+14.85%)</td><td>0.1019 (+21.53%)</td></tr><tr><td>Parallel (alt)</td><td>0.2877 (+20.97%)</td><td>0.1096 (+30.62%)</td></tr><tr><td>Parallel (res)</td><td>0.2946 (+23.44%)</td><td>0.1119 (+33.36%)</td></tr><tr><td>Parallel (int)</td><td>0.2854 (+19.57%)</td><td>0.1058 (+26.17%)</td></tr></table>\n\nThe results (see Table 4) concur with that of the earlier experiments with image features. The text only network significantly outperforms the item-kNN baseline in terms of MRR. This confirms that textual features can be effectively exploited to generate better rankings. However it falls short when compared with the ID only network. This suggests that text features alone are not enough. With concatenated input, the network performs similarly to the ID only network analogously to previous experiments.\n\nThe proposed alternative training strategies are of crucial importance when training p-RNNs, the simultaneous training is clearly suboptimal wrt. recommendation accuracy. The classic p-RNN with alternative training strategies significantly outperformed both the ID only RNN and item-kNN in both recall and MRR. Residual training proved to be the best strategy in this experiment with  $\\sim 6\\%$  improvement in recall and  $\\sim 6.5\\%$  in MRR over the ID only network. Note that further increasing the number of hidden units or number of epochs for the baseline RNNs did not improve the results any further. Thus using text based item features in p-RNNs with proper training can also increase recommendation accuracy beyond what is achievable from IDs only.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f3ab7331-b61b-4b59-988a-baf22e12b077/286b45f2806250dee03bf13f66deda2d7f5aa70ab465a81fc45045d1135e6a74.jpg)  \nFigure 3: Comparing best performing p-RNN against the ID only RNN and item-kNN.\n\nWe demonstrate the power of the proposed solution (500 units per subnet) by comparing it to item-kNN and the ID only network with 1000 hidden units on Figure 3.",
  "hyperparameter": ""
}