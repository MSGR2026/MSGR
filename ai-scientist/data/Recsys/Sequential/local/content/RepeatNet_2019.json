{
  "id": "RepeatNet_2019",
  "paper_title": "RepeatNet: A Repeat Aware Neural Recommendation Machine for Session-Based Recommendation",
  "alias": "RepeatNet",
  "year": 2019,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "RepeatNet explicitly models repeat consumption in session-based recommendation by decomposing the prediction probability into two modes: repeat mode (for re-clicking items already in the session) and explore mode (for clicking new items). The model uses a probabilistic framework P(i_{t+1}|I_S) = P(r|I_S)P(i_{t+1}|r,I_S) + P(e|I_S)P(i_{t+1}|e,I_S), where a repeat-explore mechanism acts as a soft switch between modes, a repeat decoder predicts probabilities over clicked items using an attention-based copying mechanism, and an explore decoder predicts probabilities over unclicked items using item-level attention.",
  "introduction": "# 1 Introduction\n\nSession-based recommendations have received increasing interest recently, due to their broad applicability in many online services (e.g., e-commerce, video watching, music listening) (Cheng et al. 2017). Here, a session is a group of interactions that take place within a given time frame. Sessions from a user can occur on the same day, or over several days, weeks, or even months (Quadrana et al. 2017).\n\nConventional recommendation methods tackle session-based recommendations based on either the last interaction or the last session. Zimdars, Chickering, and Meek (2001) and Shani, Heckerman, and Brafman (2005) investigate how to extract sequential patterns to predict the next item using Markov models. Then, Chen et al. (2012) propose logistic Markov embeddings to learn the representations of songs for playlist prediction. A major issue for these models is that the state space quickly becomes unmanageable when trying to include all possible sequences of potential\n\nuser selections over all items. Recurrent neural networks (RNNs) have recently been used for the purpose of session-based recommendations and attracted significant attention. Hidasi et al. (2016a) introduce RNNs with gated recurrent units (GRUs) for session-based recommendation. They introduce a number of parallel RNN (p-RNN) architectures to model sessions based on both clicks and features (images and text) of clicked items (Hidasi et al. 2016b). Quadrana et al. (2017) personalize RNN models with cross-session information transfer and devise a Hierarchical RNN model that relays and evolves latent hidden states of the RNNs across user sessions. Li et al. (2017b) introduce an attention mechanism into session-based recommendations and outperform (Hidasi et al. 2016a). Though the number of studies that apply deep learning to session-based recommendation is increasing, none has emphasized so-called repeat consumptions, which are a common phenomenon in many recommendation scenarios (e.g., e-commerce, music, and TV program recommendations), where the same item is consumed repeatedly over time.\n\nRepeat consumption exists because people have regular habits. For example, we all buy the same things repeatedly, we eat at the same restaurants regularly, we listen to the same songs and artists frequently (Anderson et al. 2014). Table 1 shows the repeat consumption ratio for three benchmark datasets that are commonly used in related studies (Hidasi et al. 2016a; Li et al. 2017b). Repeat consumption not only\n\nTable 1: Repeat ratio (%) on three benchmark datasets.  \n\n<table><tr><td>Datasets</td><td>Train</td><td>Validation</td><td>Test</td></tr><tr><td>YOOCHOOSE 1/4</td><td>25.52</td><td>25.51</td><td>26.02</td></tr><tr><td>DIGINETICA</td><td>19.94</td><td>20.06</td><td>20.49</td></tr><tr><td>LASTFM</td><td>20.72</td><td>20.42</td><td>20.95</td></tr></table>\n\nexists but also accounts for a large proportion of the interactions in some applications. In this paper, we investigate repeat consumption by incorporating a repeat-exlore mechanism into neural networks and propose a new model called RepeatNet with an encoder-decoder structure. Unlike existing work that evaluates a score for each item using a single decoder, RepeatNet evaluates the recommendation probabilities of each item with two decoders in a repeat mode and an\n\nexplore mode, respectively. In the repeat mode we recommend an old item from the user's history while in the explore mode we recommend a new item. Specifically, we first encode each session into a representation. Then, we use a repeat-exlore mechanism to learn the switch probabilities between repeat and explore modes. After that, we propose a repeat recommendation decoder to learn the probabilities of recommending old items in the repeat mode and an explore recommendation decoder to learn the probabilities of recommending new items under the explore mode. Finally, we determine the recommendation score for an item by combining the mode switch probabilities and the recommendation probabilities of each item under the two modes in a probabilistic way. The mode prediction and item recommendation are jointly learned in an end-to-end back-propagation training paradigm within a unified framework.\n\nWe carry out extensive experiments on three benchmark datasets. The results show that RepeatNet outperforms state-of-the-art baselines on all three datasets in terms of MRR and Recall. Furthermore, we find that as the dataset size and the repeat ratio increase, the improvements of RepeatNet over the baselines also increase, which demonstrates its advantages in handling repeat recommendation scenarios.\n\nTo sum up, the main contributions in this paper are:\n\n- We propose a novel deep learning-based model named RepeatNet that takes into account the repeat consumption phenomenon. To the best of our knowledge, we are the first to consider this in the context of session-based recommendation with a neural model.\n\n- We introduce a repeat-exlore mechanism for session-based recommendation to automatically learn the switch probabilities between repeat and explore modes. Unlike existing works that use a single decoder, we propose two decoders to learn the recommendation probability for each item in the two modes.\n\n- We carry out extensive experiments and analyses on three benchmark datasets. The results show that RepeatNet can improve the performance of session-based recommendation over state-of-the-art methods by explicitly modeling repeat consumption.",
  "method": "# 3 RepeatNet\n\nGiven an action (e.g., clicking, shopping) session  $I_{S} = \\{i_{1}, i_{2}, \\ldots, i_{\\tau}, \\ldots, i_{t}\\}$ , where  $i_{\\tau}$  refers to an item, session-based recommendation tries to predict what the next event would be, as shown in Eq. 1. Without loss of generality, we take click actions as our running example in the paper:\n\n$$\nP \\left(i _ {t + 1} \\mid I _ {S}\\right) \\sim f \\left(I _ {S}\\right), \\tag {1}\n$$\n\nwhere  $P(i_{t + 1} \\mid I_S)$  denotes the probability of recommending  $i_{t + 1}$  given  $I_S$ . Conventional methods usually model  $f(I_S)$  directly as a discriminant or probability function.\n\n# 3.1 Framework\n\nWe propose RepeatNet to model  $P(i_{t+1} \\mid I_S)$  from a probabilistic perspective by explicitly taking repeat consumption into consideration, as shown in Eq. 2:\n\n$$\n\\begin{array}{l} P \\left(i _ {t + 1} \\mid I _ {S}\\right) = P \\left(r \\mid I _ {S}\\right) P \\left(i _ {t + 1} \\mid r, I _ {S}\\right) + \\tag {2} \\\\ P (e \\mid I _ {S}) P (i _ {t + 1} \\mid e, I _ {S}), \\\\ \\end{array}\n$$\n\nwhere  $r$  and  $e$  denote repeat mode and explore mode, respectively. Here,  $P(r \\mid I_S)$  and  $P(e \\mid I_S)$  represent the probabilities of executing in repeat mode and explore mode, respectively.  $P(i_{t+1} \\mid r, I_S)$  and  $P(i_{t+1} \\mid e, I_S)$  refer to the probabilities of recommending  $i_{t+1}$  in repeat mode and in explore mode, respectively, given  $I_S$ .\n\nAs illustrated in Fig. 1, RepeatNet consists of four main components, a session encoder, a repeat-exlore mechanism, a repeat recommendation decoder, and an explore recommendation decoder. The session encoder encodes the given session  $I_S$  into latent representations  $H = \\{h_1, h_2, \\dots, h_\\tau, \\dots, h_t\\}$ , where  $h_t$  represents the session representation at timestamp  $t$ . The repeat-exlore mechanism takes  $H$  as input and predicts the probabilities of executing repeat mode or explore mode, corresponding to  $P(r \\mid I_S)$  and  $P(e \\mid I_S)$  in Eq. 2. The repeat recommendation decoder takes  $H$  as input and predicts the repeat recommendation probabilities over clicked items in  $I_S$ , corresponding to  $P(i_{t+1} \\mid r, I_S)$  in Eq. 2. The explore recommendation decoder takes  $H$  as input and predicts the explore recommendation probabilities over unCLICKed items in  $I - I_S$ , where  $I$  refers to all items, corresponding to  $P(i_{t+1} \\mid e, I_S)$  in Eq. 2.\n\n# 3.2 Session encoder\n\nLike previous studies (Hidasi et al. 2016a; Li et al. 2017b), we use a GRU to encode  $I_{S}$ , where the GRU is defined as:\n\n$$\n\\begin{array}{l} z _ {\\tau} = \\sigma \\left(W _ {z} \\left[ e m b \\left(i _ {\\tau}\\right), h _ {\\tau - 1} \\right]\\right) \\\\ r _ {\\tau} = \\sigma \\left(W _ {r} \\left[ e m b \\left(i _ {\\tau}\\right), h _ {\\tau - 1} \\right]\\right) \\\\ \\widetilde {h _ {\\tau}} = \\tanh  \\left(W _ {h} \\left[ e m b \\left(i _ {\\tau}\\right), r _ {\\tau} \\odot h _ {\\tau - 1} \\right]\\right) \\tag {3} \\\\ h _ {\\tau} = \\left(1 - z _ {\\tau}\\right) \\odot h _ {\\tau - 1} + z _ {\\tau} \\odot \\widetilde {h _ {\\tau}}, \\\\ \\end{array}\n$$\n\nwhere  $W_{z}, W_{r}$ , and  $W_{h}$  are weight matrices;  $\\text{emb}(i_{\\tau})$  is the item embedding of  $i_{\\tau}$ ;  $\\sigma$  denotes the sigmoid function. The initial state of the GRU is set to zero vectors, i.e.,  $h_0 = 0$ . After the session encoder, each session  $I_{S}$  is encoded into  $H = \\{h_1, h_2, \\dots, h_\\tau, \\dots, h_t\\}$ .\n\n# 3.3 Repeat-exlore mechanism\n\nThe repeat-exlore mechanism can be seen as a binary classifier that predicts the recommendation mode based on  $H = \\{h_1, h_2, \\dots, h_\\tau, \\dots, h_t\\}$ . To this end, we first apply an attention mechanism (Bahdanau, Cho, and Bengio 2015) to\n\n$H$  to get a fixed-length vector representation of  $I_{S}$ . Specifically, we first use the last hidden state  $h_t$  to match with each encoder hidden state  $h_\\tau \\in H$  to get an importance score:\n\n$$\ne _ {\\tau} ^ {r e} = v _ {r e} ^ {\\top} \\tanh  \\left(W _ {r e} h _ {t} + U _ {r e} h _ {\\tau}\\right), \\tag {4}\n$$\n\nwhere  $v_{re}$ ,  $W_{re}$ , and  $U_{re}$  are parameters. The importance scores are then normalized to get the context vector for  $I_S$  as a weighted sum in Eq. 5:\n\n$$\n\\alpha_ {\\tau} ^ {r e} = \\frac {\\exp \\left(e _ {\\tau} ^ {r e}\\right)}{\\sum_ {t} ^ {t} \\exp \\left(e _ {\\tau} ^ {r e}\\right)} \\tag {5}\n$$\n\n$$\nc _ {I _ {S}} ^ {r e} = \\sum_ {\\tau = 1} ^ {t} \\alpha_ {\\tau} ^ {r e} h _ {\\tau}.\n$$\n\nWe then employ a softmax regression to transform  $c_{IS}^{re}$  into a mode probability distribution, corresponding to  $P(r \\mid I_S)$  and  $P(e \\mid I_S)$  respectively, as shown in Eq. 6:\n\n$$\n\\left[ P (r \\mid I _ {S}), P (e \\mid I _ {S}) \\right] = \\operatorname {s o f t m a x} \\left(W _ {r e} ^ {c} c _ {I _ {S}} ^ {r e}\\right), \\tag {6}\n$$\n\nwhere  $W_{re}^{c}$  is the weight matrix.\n\n# 3.4 Repeat recommendation decoder\n\nThe repeat recommendation decoder evaluates the probability of re-clicking an item in  $I_{S}$ . Inspired by CopyNet (Gu et al. 2016), we use a modification of the attention model to achieve this. The probability of re-clicking item  $i_{\\tau} \\in I_{S}$  is computed as follows:\n\n$$\ne _ {\\tau} ^ {r} = v _ {r} ^ {\\top} \\tanh  \\left(W _ {r} h _ {t} + U _ {r} h _ {\\tau}\\right) \\tag {7}\n$$\n\n$$\nP (i \\mid r, I _ {S}) = \\left\\{ \\begin{array}{l l} \\frac {\\sum_ {i} \\exp \\left(e _ {\\tau} ^ {r}\\right)}{\\sum_ {\\tau = 1} ^ {t} \\exp \\left(e _ {\\tau} ^ {r}\\right)} & \\text {i f} i \\in I _ {S} \\\\ 0 & \\text {i f} i \\in I - I _ {S}, \\end{array} \\right. \\tag {8}\n$$\n\nwhere  $v_{r}$ ,  $W_{r}$ , and  $U_{r}$  are parameters;  $\\sum_{i}\\exp (e_{\\tau}^{r})$  denotes the sum of all occurrences of item  $i\\in I_S$ , because the same item might occur multiple times in different positions of  $I_{S}$ .\n\n# 3.5 Explore recommendation decoder\n\nThe explore recommendation decoder evaluates the probability of clicking a new item that does not exist in  $I_S$ . To better capture the user's interest in session  $I_S$ , we employ an item-level attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence (Li et al. 2017b):\n\n$$\ne _ {\\tau} ^ {e} = v _ {e} ^ {\\top} \\tanh (W _ {e} h _ {t} + U _ {e} h _ {\\tau})\n$$\n\n$$\n\\alpha_ {\\tau} ^ {e} = \\frac {\\exp \\left(e _ {\\tau} ^ {e}\\right)}{\\sum_ {\\tau = 1} ^ {t} \\exp \\left(e _ {\\tau} ^ {e}\\right)} \\tag {9}\n$$\n\n$$\nc _ {I s} ^ {e} = \\sum_ {\\tau = 1} ^ {t} \\alpha_ {\\tau} ^ {e} h _ {\\tau},\n$$\n\nwhere  $v_{e}$ ,  $W_{e}$ , and  $U_{e}$  are parameters. The factors  $\\alpha_h^e$  determine which part of the input sequence should be emphasized or ignored when making predictions. We then combine the last hidden state and the attentive state into a hybrid representation  $c_{I_S}$  for  $I_S$ , which is the concatenation of vectors  $h_t$  and  $c_{I_S}^e$ :  $c_{I_S} = [h_t, c_{I_S}^e]$ .\n\nFinally, the probability of clicking item  $i_{\\tau} \\in I - I_{S}$  is computed as follows:\n\n$$\nf _ {i} = \\left\\{ \\begin{array}{l l} - \\infty & \\text {i f} i \\in I _ {S} \\\\ W _ {e} ^ {c} c _ {I _ {S}} & \\text {i f} i \\in I - I _ {S} \\end{array} \\right. \\tag {10}\n$$\n\n$$\nP (i \\mid e, I _ {S}) = \\frac {\\exp \\left(f _ {i}\\right)}{\\sum_ {\\tau = 1} ^ {t} \\exp \\left(f _ {\\tau}\\right)}, \\tag {11}\n$$\n\nwhere  $W_{e}^{c}$  is the weight matrix and  $-\\infty$  means negative infinity. Since  $\\exp (-\\infty) = 0$ , we assume that if an item exists in  $I_{S}$ , then the probability of recommending it in the explore mode is zero.\n\n# 3.6 Objective function\n\nOur goal is to maximize the output prediction probability given the input session. Therefore, we optimize the negative log-likelihood loss function as follows:\n\n$$\nL _ {r e c} (\\theta) = - \\frac {1}{| \\mathbb {I} _ {\\mathbb {S}} |} \\sum_ {I _ {S} \\in \\mathbb {I} _ {\\mathbb {S}}} \\sum_ {\\tau = 1} ^ {| I _ {S} |} \\log P (i _ {\\tau} \\mid I _ {S}), \\tag {12}\n$$\n\nwhere  $\\theta$  are all the parameters of RepeatNet,  $\\mathbb{I}_{\\mathbb{S}}$  is the set of all sessions in the training set, and  $P(i_{\\tau} \\mid I_S)$  is the item prediction probability as defined in Eq. 2.\n\nRepeatNet incorporates an extra repeat-exlore mechanism to softly switch between repeat mode and explore mode. We assume that if the next item exists in  $I_{S}$ , then it is generated under the repeat mode, otherwise explore mode. Here, we can jointly train another mode prediction loss as follows, which is also the negative log-likelihood loss:\n\n$$\nL _ {m o d e} (\\theta)\n$$\n\n$$\n= - \\frac {1}{\\left| \\mathbb {I} _ {\\mathbb {S}} \\right|} \\sum_ {I _ {S} \\in \\mathbb {I} _ {\\mathbb {S}}} \\sum_ {\\tau = 1} ^ {| I _ {S} |} \\mathbb {1} \\left(i _ {\\tau} \\in I _ {S}\\right) \\log P (r \\mid I _ {S}) + \\tag {13}\n$$\n\n$$\n(1 - \\mathbb {1} (i _ {\\tau} \\in I _ {S})) \\log P (e \\mid I _ {S}),\n$$\n\nwhere  $\\mathbb{1}(i_{\\tau}\\in I_S)$  is an indicator function that equals 1 if  $i_{\\tau}\\in I_S$  and 0 otherwise.\n\nIn the case of joint training, the final loss is a linear combination of both losses:\n\n$$\nL (\\theta) = L _ {r e c} (\\theta) + L _ {m o d e} (\\theta). \\tag {14}\n$$\n\nAll parameters of RepeatNet as well as the item embeddings are learned in an end-to-end back-propagation training paradigm. Due to the full probability term in Eq. 2, the two modes probabilities  $P(r \\mid I_S)$ ,  $P(e \\mid I_S)$  and the item prediction probabilities  $P(i \\mid r, I_S)$ ,  $P(i \\mid e, I_S)$  are basically competing through a unified function.",
  "experiments": "# 4 Experiments\n\n# 4.1 Datasets and evaluation metrics\n\nWe carry out experiments on three standard datasets, i.e., YOOCHOOSE, DIGINETICA, and LASTFM. YOOCHOOSE and DIGINETICA are frequently used in session-based recommendation studies (Hidasi et al. 2016a; Tan, Xu, and Liu 2016; Li et al. 2017b; Jannach and Ludewig\n\nTable 2: Statistics of three datasets (number of sessions and items).  \n\n<table><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td><td>Items</td></tr><tr><td>YOOCHOOSE</td><td>5,325,971</td><td>591,775</td><td>55,898</td><td>30,470</td></tr><tr><td>DIGINETICA</td><td>647,532</td><td>71,947</td><td>60,858</td><td>43,097</td></tr><tr><td>LASTFM</td><td>2,690,424</td><td>333,537</td><td>338,115</td><td>40,000</td></tr></table>\n\n2017). Since they are both for e-commerce, we choose a third dataset in a different domain, music, Last.fm. See Table 2. The splitting of the datasets are the same as (Li et al. 2017b).\n\n- YOOCHOOSE $^2$  is a public dataset released by the RecSys Challenge 2015. We follow (Hidasi et al. 2016a; Li et al. 2017b) and filter out sessions of length 1 and items that appear less than 5 times. They note that the 1/4 version of the dataset is enough for the task and increasing the amount of data will not further improve the performance.  \n- DIGINETICA<sup>3</sup> is released by the CIKM Cup 2016. We again follow (Li et al. 2017b) and filter out sessions of length 1 and items that appear less than 5 times.  \nLASTFM<sup>4</sup> is released by (Celma 2010) and widely used in recommendation tasks (Cheng et al. 2017). We use the dataset for music artist recommendation; we keep the top 40,000 most popular artists and filter out sessions that are longer than 50 or shorter than 2 items.\n\nRecommender systems can only recommend a few items at a time, the actual item a user might pick should be amongst the first few items of the list (He et al. 2018b; Cheng et al. 2018). Therefore, commonly used metrics are MRR@20 and Recall@20 (He et al. 2018a; Mei et al. 2018). In this paper, we also report MRR@10 and Recall@10.\n\n- Recall@k: The primary evaluation metric is Recall@k, which is the proportion of cases when the desired item is amongst the top-k items in all test cases.  \n- MRR@k: Another used metric is MRR@k (Mean Reciprocal Rank), which is the average of reciprocal ranks of the desire items. The reciprocal rank is set to zero if the rank is larger than k.\n\n# 4.2 Implementation details\n\nWe set the item embedding size and GRU hidden state sizes to 100. We use dropout (Srivastava et al. 2014) with drop ratio  $p = 0.5$ . We initialize model parameters randomly using the Xavier method (Glorot and Bengio 2010). We use Adam as our optimizing algorithm. For the hyper-parameters of the Adam optimizer, we set the learning rate  $\\alpha = 0.001$ , two momentum parameters  $\\beta 1 = 0.9$  and  $\\beta 2 = 0.999$ , respectively, and  $\\epsilon = 10^{-8}$ . We halve the learning rate  $\\alpha$  every 3 rounds. We also apply gradient clipping (Pascanu,\n\nMikolov, and Bengio 2013) with range  $[-5, 5]$  during training. To speed up the training and converge quickly, we use mini-batch size 1024 by grid search. We test the model performance on the validation set for every epoch. The model is written in Chainer (Tokui et al. 2015) and trained on a GeForce GTX TitanX GPU.\n\n# 4.3 Methods used for comparison\n\nConventional methods We select the following conventional methods which are commonly used as baselines in session based recommendations (Hidasi et al. 2016a; Tan, Xu, and Liu 2016; Li et al. 2017b).\n\n- POP: POP always recommends the most popular items in the training set. It is frequently used as baselines in recommender system domains (He et al. 2017).  \n- S-POP: S-POP recommends the most popular items of the current session. Ties are broken using global popularity values (Hidasi et al. 2016a).  \n- Item-KNN: Items similar to the actual item are recommended by this baseline. Similarity is defined as the cooccurrence number of two items in sessions divided by the square root of the product of the number of sessions in which either item occurs. Regularization is also included to avoid coincidental high similarities between rarely visited items (Davidson et al. 2010).  \n- BPR-MF: BPR-MF (Rendle et al. 2009) is a commonly used matrix factorization method. We apply it to session-based recommendation by representing a new session with the average latent factors of items that occurred in the session so far.  \n- FPMC: FPMC (Rendle, Freudenthaler, and Schmidt-Thieme 2010) is a state-of-the-art hybrid model for next-basket recommendation. To adapt it to session-based recommendation, we ignore the user latent representations when computing recommendation scores.  \n- PDP: Benson, Kumar, and Tomkins (2016) propose PDP and claim that they are the first to model sequential repeat consumption. This is the only recommendation model that considers sequential repeat consumption, to the best of our knowledge.\n\nDeep learning methods No previous studies propose neural models that consider sequential repeat consumption. We select recent state-of-the-art neural session based recommendation models as baselines.\n\n- GRU4REC: GRU4REC (Hidasi et al. 2016a) uses session-parallel mini-batch training process and also employs ranking-based loss functions for learning the model.  \n- Improved-GRU4REC: Improved GRU4REC (Tan, Xu, and Liu 2016) improves GRU4REC with two techniques, data augmentation and a method to account for shifts in the input data distribution.  \n- GRU4REC-TOPK: Hidasi and Karatzoglou (2017) further improve GRU4REC with a top-k based ranking loss.  \n- NARM: NARM (Li et al. 2017b) further improves Improved-GRU4REC with a neural attention mechanism.\n\nTable 3: Experimental results (%) on the three datasets.  \n\n<table><tr><td rowspan=\"3\">Methods</td><td colspan=\"4\">YOOCHOOSE</td><td colspan=\"4\">DIGINETICA</td><td colspan=\"4\">LASTFM</td></tr><tr><td colspan=\"2\">MRR</td><td colspan=\"2\">Recall</td><td colspan=\"2\">MRR</td><td colspan=\"2\">Recall</td><td colspan=\"2\">MRR</td><td colspan=\"2\">Recall</td></tr><tr><td>@10</td><td>@20</td><td>@10</td><td>@20</td><td>@10</td><td>@20</td><td>@10</td><td>@20</td><td>@10</td><td>@20</td><td>@10</td><td>@20</td></tr><tr><td>POP</td><td>0.26</td><td>0.30</td><td>0.81</td><td>1.33</td><td>0.18</td><td>0.20</td><td>0.53</td><td>0.89</td><td>1.09</td><td>1.26</td><td>2.90</td><td>5.26</td></tr><tr><td>S-POP</td><td>17.70</td><td>17.79</td><td>25.96</td><td>27.11</td><td>13.64</td><td>13.68</td><td>20.56</td><td>21.06</td><td>8.36</td><td>8.71</td><td>18.08</td><td>22.59</td></tr><tr><td>Item-KNN</td><td>20.89</td><td>21.72</td><td>41.56</td><td>52.35</td><td>10.77</td><td>11.57</td><td>25.04</td><td>35.75</td><td>4.48</td><td>4.85</td><td>9.77</td><td>14.84</td></tr><tr><td>BPR-MF</td><td>1.90</td><td>1.97</td><td>3.07</td><td>4.05</td><td>1.86</td><td>1.98</td><td>3.60</td><td>5.24</td><td>4.88</td><td>5.19</td><td>9.87</td><td>14.05</td></tr><tr><td>FPMC</td><td>16.59</td><td>17.50</td><td>38.87</td><td>51.86</td><td>6.30</td><td>6.95</td><td>17.07</td><td>26.53</td><td>4.58</td><td>4.99</td><td>11.67</td><td>17.68</td></tr><tr><td>PDP</td><td>18.44</td><td>19.15</td><td>40.03</td><td>52.98</td><td>6.75</td><td>7.24</td><td>19.57</td><td>28.77</td><td>4.86</td><td>5.05</td><td>12.11</td><td>18.09</td></tr><tr><td>GRU4REC</td><td>21.64</td><td>22.60</td><td>46.67</td><td>59.56</td><td>7.59</td><td>8.33</td><td>19.09</td><td>29.45</td><td>4.92</td><td>5.39</td><td>11.56</td><td>17.90</td></tr><tr><td>Improved-GRU4REC</td><td>28.36</td><td>29.15</td><td>57.91</td><td>69.20</td><td>13.63</td><td>14.69</td><td>33.48</td><td>46.16</td><td>9.60</td><td>10.15</td><td>20.98</td><td>29.04</td></tr><tr><td>GRU4REC-TOPK</td><td>29.76</td><td>30.69</td><td>58.15</td><td>70.30</td><td>13.14</td><td>14.16</td><td>31.54</td><td>45.23</td><td>7.44</td><td>7.95</td><td>15.73</td><td>22.61</td></tr><tr><td>NARM</td><td>28.52</td><td>29.23</td><td>58.70</td><td>69.73</td><td>15.25</td><td>16.17</td><td>33.62</td><td>49.70</td><td>10.31</td><td>10.85</td><td>22.04</td><td>29.94</td></tr><tr><td>RepeatNet (no repeat)</td><td>30.02</td><td>30.76</td><td>59.62</td><td>70.21</td><td>12.71</td><td>13.52</td><td>30.96</td><td>42.56</td><td>9.92</td><td>10.47</td><td>21.81</td><td>29.96</td></tr><tr><td>RepeatNet</td><td>30.50†</td><td>31.03†</td><td>59.76†</td><td>70.71</td><td>16.90†</td><td>17.66†</td><td>36.86†</td><td>47.79</td><td>11.46†</td><td>12.03†</td><td>24.18†</td><td>32.38†</td></tr></table>\n\nBold face indicates the best result in terms of the corresponding metric. Significant improvements over the best baseline results are marked with  $\\dagger$  (t-test,  $p < .05$ ). The scores reported in (Li et al. 2017b) on the DIGINETICA dataset differ because they did not sort the session items according to the \"timeframe\" field, which ignores the sequential information. We run the code released by (Hidasi et al. 2016a; Tan, Xu, and Liu 2016; Hidasi and Karatzoglou 2017; Li et al. 2017b) to obtain the results of GRU4REC, Improved-GRU4REC, GRU4REC-TOPK, and NARM.",
  "hyperparameter": "Item embedding size: 100; GRU hidden state size: 100; Dropout ratio: 0.5; Learning rate (α): 0.001 (halved every 3 epochs); Adam optimizer parameters: β1=0.9, β2=0.999, ε=10^-8; Gradient clipping range: [-5, 5]; Mini-batch size: 1024; Xavier initialization for model parameters"
}