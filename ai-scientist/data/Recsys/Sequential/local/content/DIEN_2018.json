{
  "id": "DIEN_2018",
  "paper_title": "Deep Interest Evolution Network for Click-Through Rate Prediction",
  "alias": "DIEN",
  "year": 2018,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "DIEN (Deep Interest Evolution Network) introduces two key innovations for CTR prediction: (1) An Interest Extractor Layer that uses GRU with auxiliary loss to extract sequential interest states from user behaviors, where the auxiliary loss supervises each hidden state by predicting the next behavior, making interest representations more expressive. (2) An Interest Evolving Layer with AUGRU (GRU with Attentional Update Gate) that models the evolution of interests relative to the target item by using attention scores to modulate the update gate of GRU, effectively capturing interest dynamics while filtering out irrelevant interest drifting.",
  "introduction": "## Introduction\n\nCost per click (CPC) billing is one of the commonest billing forms in the advertising system, where advertisers are charged for each click on their advertisement. In CPC advertising system, the performance of click-through rate (CTR) prediction not only influences the final revenue of whole platform, but also impacts user experience and satisfaction. Modeling CTR prediction has drawn more and more attention from the communities of academia and industry.\n\nIn most non-searching e-commerce scenes, users do not express their current intention actively. Designing model to\n\ncapture user's interests as well as their dynamics is the key to advance the performance of CTR prediction. Recently, many CTR models transform from traditional methodologies (Friedman 2001; Rendle 2010) to deep CTR models (Guo et al. 2017; Qu et al. 2016; Lian et al. 2018). Most deep CTR models focus on capturing interaction between features from different fields and pay less attention to user interest representation. Deep Interest Network (DIN) (Zhou et al. 2018c) emphasizes that user interests are diverse, it uses attention based model to capture relative interests to target item, and obtains adaptive interest representation. However, most interest models including DIN regard the behavior as the interest directly, while latent interest is hard to be fully reflected by explicit behavior. Previous methods neglect to dig the true user interest behind behavior. Moreover, user interest keeps evolving, capturing the dynamic of interest is important for interest representation.\n\nBased on all these observations, we propose Deep Interest Evolution Network (DIEN) to improve the performance of CTR prediction. There are two key modules in DIEN, one is for extracting latent temporal interests from explicit user behaviors, and the other one is for modeling interest evolving process. Proper interest representation is the footstone of interest evolving model. At interest extractor layer, DIEN chooses GRU (Chung et al. 2014) to model the dependency between behaviors. Following the principle that interest leads to the consecutive behavior directly, we propose auxiliary loss which uses the next behavior to supervise the learning of current hidden state. We call these hidden states with extra supervision as interest states. These extra supervision information helps to capture more semantic meaning for interest representation and push hidden states of GRU to represent interests effectively. Moreover, user interests are diverse, which leads to interest drifting phenomenon: user's intentions can be very different in adjacent visitings, and one behavior of a user may depend on the behavior that takes long time ago. Each interest has its own evolution track. Meanwhile, the click actions of one user on different target items are effected by different parts of interests. At interest evolving layer, we model the interest evolving trajectory that is relative to target item. Based on the interest sequence obtained from interest extractor layer, we design GRU with attentional update gate (AUGRU). Using interest state and target item to compute relevance, AUGRU strengthens rela\n\ntive interests' influence on interest evolution, while weakens. irrelative interests' effect that results from interest drifting. With the introduction of attentional mechanism into update gate, AUGRU can lead to the specific interest evolving processes for different target items. The main contributions of DIEN are as following:\n\n- We focus on interest evolving phenomenon in e-commerce system, and propose a new structure of network to model interest evolving process. The model for interest evolution leads to more expressive interest representation and more precise CTR prediction.  \n- Different from taking behaviors as interests directly, we specially design interest extractor layer. Pointing at the problem that hidden state of GRU is less targeted for interest representation, we propose one auxiliary loss. Auxiliary loss uses consecutive behavior to supervise the learning of hidden state at each step. which makes hidden state expressive enough to represent latent interest.  \n- We design interest evolving layer novelly, where GPU with attentional update gate (AUGRU) strengthens the effect from relevant interests to target item and overcomes the inference from interest drifting.\n\nIn the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. It is notable that DIEN has been deployed in Taobao display advertisement system and obtains significant improvement under various metrics.\n",
  "method": "## Deep Interest Evolution Network\n\nIn this section, we introduce Deep Interest Evolution Network (DIEN) in detail. First, we review the basic Deep CTR\n\nmodel, named BaseModel. Then we show the overall structure of DIEN and introduce the techniques that are used for capturing interests and modeling interest evolution process.\n\n### Review of BaseModel\n\nThe BaseModel is introduced from the aspects of feature representation, model structure and loss function.\n\nFeature Representation In our online display system, we use four categories of feature: User Profile, User Behavior, Ad and Context. It is notable that the ad is also item. For generation, we call the ad as the target item in this paper. Each category of feature has several fields, User Profile's fields are gender, age and so on; The fields of User Behavior's are the list of user visited goods id; Ad's fields are ad_id, shop_id and so on; Context's fields are time and so on. Feature in each field can be encoded into one-hot vector, e.g., the female feature in the category of User Profile are encoded as [0,1]. The concat of different fields' one-hot vectors from User Profile, User Behavior, Ad and Context form  \\(\\mathbf{x}_p, \\mathbf{x}_b, \\mathbf{x}_a, \\mathbf{x}_c\\) , respectively. In sequential CTR model, it's remarkable that each field contains a list of behaviors, and each behavior corresponds to a one-hot vector, which can be represented by  \\(\\mathbf{x}_b = [\\mathbf{b}_1; \\mathbf{b}_2; \\dots; \\mathbf{b}_T] \\in \\mathbb{R}^{K \\times T}, \\mathbf{b}_t \\in \\{0,1\\}^K\\) , where  \\(\\mathbf{b}_t\\)  is encoded as one-hot vector and represents  \\(t\\) -th behavior,  \\(T\\)  is the number of user's history behaviors,  \\(K\\)  is the total number of goods that user can click.\n\nThe Structure of BaseModel Most deep CTR models are built on the basic structure of embedding & MLR. The basic structure is composed of several parts:\n\n- Embedding Embedding is the common operation that transforms the large scale sparse feature into low-dimensional dense feature. In the embedding layer, each field of feature is corresponding to one embedding matrix, e.g., the embedding matrix of visited goods can be represented by  \\(\\mathrm{E}_{\\text{goods}} = [\\mathbf{m}_1; \\mathbf{m}_2; \\dots; \\mathbf{m}_K] \\in \\mathbb{R}^{n_E \\times K}\\) , where  \\(\\mathbf{m}_j \\in \\mathbb{R}^{n_E}\\)  represents a embedding vector with dimension  \\(n_E\\) . Especially, for behavior feature  \\(\\mathbf{b}_t\\) , if  \\(\\mathbf{b}_t[j_t] = 1\\) , then its corresponding embedding vector is  \\(\\mathbf{m}_{j_t}\\) , and the ordered embedding vector list of behaviors for one user can be represented by  \\(\\mathbf{e}_b = [\\mathbf{m}_{j_1}; \\mathbf{m}_{j_2}; \\dots, \\mathbf{m}_{j_T}]\\) . Similarly,  \\(\\mathbf{e}_a\\)  represents the concatenated embedding vectors of fields in the category of advertisement.  \n- Multilayer Perceptron (MLP) First, the embedding vectors from one category are fed into pooling operation. Then all these pooling vectors from different categories are concatenated. At last, the concatenated vector is fed into the following MLP for final prediction.\n\nLoss Function The widely used loss function in deep CTR models is negative log-likelihood function, which uses the label of target item to supervise overall prediction:\n\n\\[\nL _ {t a r g e t} = - \\frac {1}{N} \\sum_ {(\\mathbf {x}, y) \\in \\mathcal {D}} ^ {N} (y \\log p (\\mathbf {x}) + (1 - y) \\log (1 - p (\\mathbf {x}))), \\tag {1}\n\\]\n\nwhere  \\(\\mathbf{x} = [\\mathbf{x}_p, \\mathbf{x}_a, \\mathbf{x}_c, \\mathbf{x}_b] \\in \\mathcal{D}\\) ,  \\(\\mathcal{D}\\)  is the training set of size  \\(N\\) .  \\(y \\in \\{0, 1\\}\\)  represents whether the user clicks target\n\nitem.  \\(p(\\mathbf{x})\\)  is the output of network, which is the predicted probability that the user clicks target item.\n\n### Deep Interest Evolution Network\n\nDifferent from sponsored search, in many e-commerce platforms like online display advertisement, users do not show their intention clearly, so capturing user interest and their dynamics is important for CTR prediction. DIEN devotes to capture user interest and models interest evolving process. As shown in Fig. 1, DIEN is composed by several parts. First, all categories of features are transformed by embedding layer. Next, DIEN takes two steps to capture interest evolving: interest extractor layer extracts interest sequence based on behavior sequence; interest evolving layer models interest evolving process that is relative to target item. Then final interest's representation and embedding vectors of ad, user profile, context are concatenated. The concatenated vector is fed into MLP for final prediction. In the remaining of this section, we will introduce two core modules of DIEN in detail.\n\nInterest Extractor Layer In e-commerce system, user behavior is the carrier of latent interest, and interest will change after user takes one behavior. At the interest extractor layer, we extract a series of interest states from sequential user behaviors.\n\nThe click behaviors of user in e-commerce system are rich, where the length of history behavior sequence is long even in a short period of time, like two weeks. For the balance between efficiency and performance, we take GRU to model the dependency between behaviors, where the input of GRU is ordered behaviors by their occur time. GRU overcomes the vanishing gradients problem of RNN and is faster than LSTM (Hochreiter and Schmidhuber 1997), which is suitable for e-commerce system. The formulations of GRU are listed as follows:\n\n\\[\n\\mathbf {u} _ {t} = \\sigma \\left(W ^ {u} \\mathbf {i} _ {t} + U ^ {u} \\mathbf {h} _ {t - 1} + \\mathbf {b} ^ {u}\\right), \\tag {2}\n\\]\n\n\\[\n\\mathbf {r} _ {t} = \\sigma \\left(W ^ {r} \\mathbf {i} _ {t} + U ^ {r} \\mathbf {h} _ {t - 1} + \\mathbf {b} ^ {r}\\right), \\tag {3}\n\\]\n\n\\[\n\\tilde {\\mathbf {h}} _ {t} = \\tanh  \\left(W ^ {h} \\mathbf {i} _ {t} + \\mathbf {r} _ {t} \\circ U ^ {h} \\mathbf {h} _ {t - 1} + \\mathbf {b} ^ {h}\\right), \\tag {4}\n\\]\n\n\\[\n\\mathbf {h} _ {t} = \\left(\\mathbf {1} - \\mathbf {u} _ {t}\\right) \\circ \\mathbf {h} _ {t - 1} + \\mathbf {u} _ {t} \\circ \\tilde {\\mathbf {h}} _ {t}, \\tag {5}\n\\]\n\nwhere  \\(\\sigma\\)  is the sigmoid activation function,  \\(\\circ\\)  is elementwise product,  \\(W^{u}, W^{r}, W^{h} \\in \\mathbb{R}^{n_{H} \\times n_{I}}, U^{z}, U^{r}, U^{h} \\in n_{H} \\times n_{H}, n_{H}\\)  is the hidden size, and  \\(n_{I}\\)  is the input size.  \\(\\mathbf{i}_{t}\\)  is the input of GRU,  \\(\\mathbf{i}_{t} = \\mathbf{e}_{b}[t]\\)  represents the  \\(t\\) -th behavior that the user takes,  \\(\\mathbf{h}_{t}\\)  is the  \\(t\\) -th hidden states.\n\nHowever, the hidden state  \\(\\mathbf{h}_t\\)  which only captures the dependency between behaviors can not represent interest effectively. As the click behavior of target item is triggered by final interest, the label used in  \\(L_{target}\\)  only contains the ground truth that supervises final interest's prediction, while history state  \\(\\mathbf{h}_t\\)  ( \\(t < T\\) ) can't obtain proper supervision. As we all know, interest state at each step leads to consecutive behavior directly. So we propose auxiliary loss, which uses behavior  \\(\\mathbf{b}_{t+1}\\)  to supervise the learning of interest state  \\(\\mathbf{h}_t\\) . Besides using the real next behavior as positive instance, we also use negative instance that samples from item set except the clicked item. There are  \\(N\\)  pairs of behavior embedding sequence:  \\(\\{\\mathbf{e}_b^i, \\hat{\\mathbf{e}}_b^i\\} \\in \\mathcal{D}_{\\mathcal{B}}, i \\in 1,2,\\dots,N\\) , where  \\(\\mathbf{e}_b^i \\in \\mathbb{R}^{T \\times n_E}\\)  represents the clicked behavior sequence, and\n\n![](images/81017dac5d039d53820ba87d16d0671720663f7ee6f9c0235e366ab9dcb382c5.jpg)  \nFigure 1: The structure of DIEN. At the behavior layer, behaviors are sorted by time, the embedding layer transforms the one-hot representation  \\(\\mathbf{b}[t]\\)  to embedding vector  \\(\\mathbf{e}[t]\\) . Then interest extractor layer extracts each interest state  \\(\\mathbf{h}[t]\\)  with the help of auxiliary loss. At interest evolving layer, AUGRU models the interest evolving process that is relative to target item. The final interest state  \\(\\mathbf{h}'[T]\\)  and embedding vectors of remaining feature are concatenated, and fed into MLR for final CTR prediction.\n\n\\(\\hat{\\mathbf{e}}_b^i \\in \\mathbb{R}^{T \\times n_E}\\)  represent the negative sample sequence.  \\(T\\)  is the number of history behaviors,  \\(n_E\\)  is the dimension of embedding,  \\(\\mathbf{e}_b^i[t] \\in \\mathcal{G}\\)  represents the  \\(t\\) -th item's embedding vector that user  \\(i\\)  click,  \\(\\mathcal{G}\\)  is the whole item set.  \\(\\hat{\\mathbf{e}}_b^i[t] \\in \\mathcal{G} - \\mathbf{e}_b^i[t]\\)  represents the embedding of item that samples from the item set except the item clicked by user  \\(i\\)  at  \\(t\\) -th step. Auxiliary loss can be formulated as:\n\n\\[\n\\begin{array}{l} L _ {a u x} = - \\frac {1}{N} \\left(\\sum_ {i = 1} ^ {N} \\sum_ {t} \\log \\sigma \\left(\\mathbf {h} _ {t} ^ {i}, \\mathbf {e} _ {b} ^ {i} [ t + 1 ]\\right)\\right) \\tag {6} \\\\ + \\log (1 - \\sigma \\left(\\mathbf {h} _ {t} ^ {i}, \\hat {\\mathbf {e}} _ {b} ^ {i} [ t + 1 ])\\right)), \\\\ \\end{array}\n\\]\n\nwhere  \\(\\sigma (\\mathbf{x_1},\\mathbf{x_2}) = \\frac{1}{1 + \\exp(-[\\mathbf{x_1},\\mathbf{x_2}])}\\)  is sigmoid activation function,  \\(\\mathbf{h}_t^i\\)  represents the  \\(t\\) -th hidden state of GRU for user  \\(i\\) . The global loss we use in our CTR model is:\n\n\\[\nL = L _ {\\text {t a r g e t}} + \\alpha * L _ {\\text {a u x}}, \\tag {7}\n\\]\n\nwhere  \\(\\alpha\\)  is the hyper-parameter which balances the interest representation and CTR prediction.\n\nWith the help of auxiliary loss, each hidden state  \\(\\mathbf{h}_t\\)  is expressive enough to represent interest state after user takes behavior  \\(\\mathbf{i}_t\\) . The concat of all  \\(T\\)  interest points  \\([\\mathbf{h}_1,\\mathbf{h}_2,\\dots ,\\mathbf{h}_T]\\)  composes the interest sequence that interest evolving layer can model interest evolving on.\n\nOverall, the introduction of auxiliary loss has several advantages: from the aspect of interest learning, the introduction of auxiliary loss helps each hidden state of GRU represent interest expressively. As for the optimization of GRU, auxiliary loss reduces the difficulty of back propagation when GRU models long history behavior sequence. Last but not the least, auxiliary loss gives more semantic information for the learning of embedding layer, which leads to a better embedding matrix.\n\nInterest Evolving Layer As the joint influence from external environment and internal cognition, different kinds of user interests are evolving over time. Using the interest on clothes as an example, with the changing of population trend and user taste, user's preference for clothes evolves. The evolving process of the user interest on clothes will directly decides CTR prediction for candidate clothes. The advantages of modeling the evolving process is as follows:\n\n- Interest evolving module could supply the representation of final interest with more relative history information;  \n- It is better to predict the CTR of target item by following the interest evolution trend.\n\nNotably, interest shows two characteristics during evolving:\n\n- As the diversity of interests, interest can drift. The effect of interest drifting on behaviors is that user may interest in kinds of books during a period of time, and need clothes in another time.  \n- Though interests may affect each other, each interest has its own evolving process, e.g. the evolving process of books and clothes is almost individually. We only concerns the evolving process that is relative to target item.\n\nIn the first stage, with the help of auxiliary loss, we has obtained expressive representation of interest sequence. By analyzing the characteristics of interest evolving, we combine the local activation ability of attention mechanism and sequential learning ability from GRU to model interest evolving. The local activation during each step of GRU can intensify relative interest's effect, and weaken the disturbance from interest drifting, which is helpful for modeling interest evolving process that relative to target item.\n\nSimilar to the formulations shown in Eq. (2-5), we use  \\(\\mathbf{i}_t^\\prime\\) \\(\\mathbf{h}_t^\\prime\\)  to represent the input and hidden state in interest evolving module, where the input of second GRU is the corresponding interest state at Interest Extractor Layer:  \\(\\mathbf{i}_t^\\prime = \\mathbf{h}_t\\) . The last hidden state  \\(\\mathbf{h}_T^\\prime\\)  represents final interest state.\n\nAnd the attention function we used in interest evolving module can be formulated as:\n\n\\[\na _ {t} = \\frac {\\exp \\left(\\mathbf {h} _ {t} W \\mathbf {e} _ {a}\\right)}{\\sum_ {j = 1} ^ {T} \\exp \\left(\\mathbf {h} _ {j} W \\mathbf {e} _ {a}\\right)}, \\tag {8}\n\\]\n\nwhere  \\(\\mathbf{e}_a\\)  is the concat of embedding vectors from fields in category ad,  \\(W\\in \\mathbb{R}^{n_H\\times n_A}\\) ,  \\(n_H\\)  is the dimension of hidden state and  \\(n_A\\)  is the dimension of advertisement's embedding vector. Attention score can reflect the relationship between advertisement  \\(\\mathbf{e}_a\\)  and input  \\(\\mathbf{h}_t\\) , and strong relativeness leads to a large attention score.\n\nNext, we will introduce several approaches that combine the mechanism of attention and GRU to model the process of interest evolution.\n\n- GRU with attentional input (AIGRU) In order to activate relative interests during interest evolution, we propose a naive method, named GRU with attentional input (AIGRU). AIGRU uses attention score to affect the input of interest evolving layer. As shown in Eq. (9):\n\n\\[\n\\mathbf {i} _ {t} ^ {\\prime} = \\mathbf {h} _ {t} * a _ {t} \\tag {9}\n\\]\n\nWhere  \\(\\mathbf{h}_t\\)  is the  \\(t\\) -th hidden state of GRU at interest extractor layer,  \\(\\mathbf{i}_t^\\prime\\)  is the input of the second GRU which is for interest evolving, and  \\(*\\)  means scalar-vector product.\n\nIn AIGRU, the scale of less related interest can be reduced by the attention score. Ideally, the input value of less related interest can be reduced to zero. However, AIGRU works not very well. Because even zero input can also change the hidden state of GRU, so the less relative interests also affect the learning of interest evolving.\n\n- Attention based GRU(AGRU) In the area of question answering (Xiong, Merity, and Socher 2016), attention based GRU (AGRU) is firstly proposed. After modifying the GRU architecture by embedding information from the attention mechanism, AGRU can extract key information in complex queries effectively. Inspired by the question answering system, we transfer the using of AGRU from extracting key information in query to capture relative interest during interest evolving novelly. In detail, AGRU uses the attention score to replace the update gate of GRU, and changes the hidden state directly. Formally:\n\n\\[\n\\mathbf {h} _ {t} ^ {\\prime} = \\left(1 - a _ {t}\\right) * \\mathbf {h} _ {t - 1} ^ {\\prime} + a _ {t} * \\tilde {\\mathbf {h}} _ {t} ^ {\\prime}, \\tag {10}\n\\]\n\nwhere  \\(\\mathbf{h}_t^\\prime\\) \\(\\mathbf{h}_{t - 1}^{\\prime}\\)  and  \\(\\tilde{\\mathbf{h}}_t^\\prime\\)  are the hidden state of AGRU.\n\nIn the scene of interest evolving, AGRU makes use of the attention score to control the update of hidden state directly. AGRU weakens the effect from less related interest during interest evolving. The embedding of attention into GRU improves the influence of attention mechanism, and helps AGRU overcome the defects of AIGRU.\n\n- GRU with attentional update gate (AUGRU) Although AGRU can use attention score to control the update of hidden state directly, it uses a scalar (the attention score  \\(a_{t}\\) ) to replace a vector (the update gate  \\(u_{t}\\) ), which ignores the\n\nTable 1: The statistics of datasets  \n\n<table><tr><td>Dataset</td><td>User</td><td>Goods</td><td>Categories</td><td>Samples</td></tr><tr><td>Books</td><td>603,668</td><td>367,982</td><td>1,600</td><td>603,668</td></tr><tr><td>Electronics</td><td>192,403</td><td>63,001</td><td>801</td><td>192,403</td></tr><tr><td>Industrial dataset</td><td>0.8 billion</td><td>0.82 billion</td><td>18,006</td><td>7.0 billion</td></tr></table>\n\ndifference of importance among different dimensions. We propose the GRU with attentional update gate (AUGRU) to combine attention mechanism and GRU seamlessly:\n\n\\[\n\\tilde {\\mathbf {u}} _ {t} ^ {\\prime} = a _ {t} * \\mathbf {u} _ {t} ^ {\\prime}, \\tag {11}\n\\]\n\n\\[\n\\mathbf {h} _ {t} ^ {\\prime} = \\left(1 - \\tilde {\\mathbf {u}} _ {t} ^ {\\prime}\\right) \\circ \\mathbf {h} _ {t - 1} ^ {\\prime} + \\tilde {\\mathbf {u}} _ {t} ^ {\\prime} \\circ \\tilde {\\mathbf {h}} _ {t} ^ {\\prime}, \\tag {12}\n\\]\n\nwhere  \\(\\mathbf{u}_t^\\prime\\)  is the original update gate of AUGRU,  \\(\\tilde{\\mathbf{u}}_t^\\prime\\)  is the attentional update gate we design for AUGRU,  \\(\\mathbf{h}_t^\\prime ,\\mathbf{h}_{t - 1}^\\prime\\)  and  \\(\\tilde{\\mathbf{h}}_t^\\prime\\)  are the hidden states of AUGRU.\n\nIn AUGRU, we keep original dimensional information of update gate, which decides the importance of each dimension. Based on the differentiated information, we use attention score  \\(a_{t}\\)  to scale all dimensions of update gate, which results that less related interest makes less effects on the hidden state. AUGRU avoids the disturbance from interest drifting more effectively, and pushes the relative interest to evolve smoothly.\n",
  "experiments": "## Experiments\n\nIn this section, we compare DIEN with the state of the art on both public and industrial datasets. Besides, we design experiments to verify the effect of auxiliary loss and AUGRU, respectively. For observing the process of interest evolving, we show the visualization result of interest hidden states. At last, we share the results and techniques for online serving.\n\n### Datasets\n\nWe use both public and industrial datasets to verify the effect of DIEN. The statistics of all datasets are shown in Table 1.\n\npublic Dataset Amazon Dataset (McAuley et al. 2015) is composed of product reviews and metadata from Amazon. We use two subsets of Amazon dataset: Books and Electronics, to verify the effect of DIEN. In these datasets, we regard reviews as behaviors, and sort the reviews from one user by time. Assuming there are  \\(T\\)  behaviors of user  \\(u\\) , our purpose is to use the  \\(T - 1\\)  behaviors to predict whether user  \\(u\\)  will write reviews that shown in  \\(T\\) -th review.\n\nIndustrial Dataset Industrial dataset is constructed by impression and click logs from our online display advertising system. For training set, we take the ads that clicked at last 49 days as the target item. Each target item and its corresponding click behaviors construct one instance. Using one target item  \\(a\\)  as example, we set the day that  \\(a\\)  is clicked as the last day, the behaviors that this user takes in previous 14 days as history behaviors. Similarly, the target item in test set is choose from the following one day, and the behaviors are built as same as training data.\n\nTable 2: Results (AUC) on public datasets  \n\n<table><tr><td>Model</td><td>Electronics (mean±std)</td><td>Books (mean ± std)</td></tr><tr><td>BaseModel (Zhou et al. 2018c)</td><td>0.7435 ± 0.00128</td><td>0.7686 ± 0.00253</td></tr><tr><td>Wide&amp;Deep (Cheng et al. 2016)</td><td>0.7456 ± 0.00127</td><td>0.7735 ± 0.00051</td></tr><tr><td>PNN (Qu et al. 2016)</td><td>0.7543 ± 0.00101</td><td>0.7799 ± 0.00181</td></tr><tr><td>DIN (Zhou et al. 2018c)</td><td>0.7603 ± 0.00028</td><td>0.7880 ± 0.00216</td></tr><tr><td>Two layer GRU Attention</td><td>0.7605 ± 0.00059</td><td>0.7890 ± 0.00268</td></tr><tr><td>DIEN</td><td>0.7792 ± 0.00243</td><td>0.8453 ± 0.00476</td></tr></table>\n\n### Compared Methods\n\nWe compare DIEN with some mainstream CTR prediction methods:\n\n- BaseModel BaseModel takes the same setting of embedding and MLR with DIEN, and uses sum pooling operation to integrate behavior embeddings.  \n- Wide&Deep (Cheng et al. 2016) Wide & Deep consists of two parts: its deep model is the same as Base Model, and its wide model is a linear model.  \n- PNN (Qu et al. 2016) PNN uses a product layer to capture interactive patterns between interfield categories.  \n- DIN (Zhou et al. 2018c) DIN uses the mechanism of attention to activate related user behaviors.  \n- Two layer GRU Attention Similar to (Parsana et al. 2018), we use two layer GRU to model sequential behaviors, and takes an attention layer to active relative behaviors.\n\n### Results on Public Datasets\n\nOverall, as shown in Fig. 1, the structure of DIEN consists GRU, AUGRU and auxiliary loss and other normal components. In public dataset. Each experiment is repeated 5 times.\n\nFrom Table 2, we can find Wide & Deep with manually designed features performs not well, while automotive interaction between features (PNN) can improve the performance of BaseModel. At the same time, the models aiming to capture interests can improve AUC obviously: DIN activates the interests that relative to ad, Two layer GRU attention further activates relevant interests in interest sequence, all these explorations obtain positive feedback. DIEN not only captures sequential interests more effectively, but also models the interest evolving process that is relative to target item. The modeling for interest evolving helps DIEN obtain better interest representation, and capture dynamic of interests precisely, which improves the performance largely.\n\n### Results on Industrial Dataset\n\nWe further conduct experiments on the dataset of real display advertisement. There are 6 FCN layers used in industrial dataset, the dimensions area 600, 400, 300, 200, 80, 2, respectively, the max length of history behaviors is set as 50.\n\nAs shown in Table 3, Wide & Deep and PNN obtain better performance than BaseModel. Different from only one category of goods in Amazon dataset, the dataset of online advertisement contains all kinds of goods at the same time. Based on this characteristic, attention-based methods improve the performance largely, like DIN. DIEN captures the interest evolving process that is relative to target item, and obtains best performance.\n\nTable 3: Results (AUC) on industrial dataset  \n\n<table><tr><td>Model</td><td>AUC</td></tr><tr><td>BaseModel (Zhou et al. 2018c)</td><td>0.6350</td></tr><tr><td>Wide&amp;Deep (Cheng et al. 2016)</td><td>0.6362</td></tr><tr><td>PNN (Qu et al. 2016)</td><td>0.6353</td></tr><tr><td>DIN (Zhou et al. 2018c)</td><td>0.6428</td></tr><tr><td>Two layer GRU Attention</td><td>0.6457</td></tr><tr><td>BaseModel + GRU + AUGRU</td><td>0.6493</td></tr><tr><td>DIEN</td><td>0.6541</td></tr></table>\n\n### Application Study\n\nIn this section, we will show the effect of AUGRU and auxiliary loss, respectively.\n\nTable 4: Effect of AUGRU and auxiliary loss (AUC)  \n\n<table><tr><td>Model</td><td>Electronics (mean ± std)</td><td>Books (mean ± std)</td></tr><tr><td>BaseModel</td><td>0.7435 ± 0.00128</td><td>0.7686 ± 0.00253</td></tr><tr><td>Two layer GRU attention</td><td>0.7605 ± 0.00059</td><td>0.7890 ± 0.00268</td></tr><tr><td>BaseModel + GRU + AIGRU</td><td>0.7606 ± 0.00061</td><td>0.7892 ± 0.00222</td></tr><tr><td>BaseModel + GRU + AGRU</td><td>0.7628 ± 0.00015</td><td>0.7890 ± 0.00268</td></tr><tr><td>BaseModel + GRU + AUGRU</td><td>0.7640 ± 0.00073</td><td>0.7911 ± 0.00150</td></tr><tr><td>DIEN</td><td>0.7792 ± 0.00243</td><td>0.8453 ± 0.00476</td></tr></table>\n\nEffect of GRU with attentional update gate (AUGRU) Table 4 shows the results of different methods for interest evolving. Compared to BaseMode, Two layer GRU Attention obtains improvement, while the lack for modeling evolution limits its ability. AIGRU takes the basic idea to model evolving process, though it has advances, the splitting of attention and evolving lost information during interest evolving. AGRU further tries to fuse attention and evolution, as we proposed previously, its attention in GRU can't make fully use the resource of update gate. AUGRU obtains obvious improvements, which reflects it fuses the attention mechanism and sequential learning ideally, and captures the evolving process of relative interests effectively.\n\nEffect of auxiliary loss Based on the model that obtained with AUGRU, we further explore the effect of auxiliary loss. In the public datasets, the negative instance used in the auxiliary loss is randomly sampled from item set except the item shown in corresponding review. As for industrial dataset, the ads shown while not been clicked are as negative instances. As shown in Fig. 2, the loss of both whole loss  \\(L\\)  and aux\n\n![](images/1cf00fb88ac16575458f1351f860cd53b81e3429a2da8876b63c4f1a5527555d.jpg)  \n(a) Books\n\n![](images/fdf90c337b4a1aca188528ecee97b5ec3cd676a3f7d407e057f16e2d047a9b41.jpg)  \n(b) Electronics  \nFigure 2: Learning curves on public datasets.  \\(\\alpha\\)  is set as 1.\n\niliary loss  \\(L_{-}\\) aux keep similar descend trend, which means\n\nTable 5: Results from Online A/B testing  \n\n<table><tr><td>Model</td><td>CTR Gain</td><td>PPC Gain</td><td>eCPM Gain</td></tr><tr><td>BaseModel</td><td>0%</td><td>0%</td><td>0%</td></tr><tr><td>DIN (Zhou et al. 2018c)</td><td>+ 8.9%</td><td>- 2.0%</td><td>+ 6.7%</td></tr><tr><td>DIEN</td><td>+ 20.7%</td><td>- 3.0%</td><td>+ 17.1%</td></tr></table>\n\nboth global loss for CTR prediction and auxiliary loss for interest representation make effect.\n\nAs shown in Table 4, auxiliary loss bring great improvements for both public datasets, it reflects the importance of supervision information for the learning of sequential interests and embedding representation. For industrial dataset shown in Table 3, model with auxiliary loss improves performance further. However, we can see that the improvement is not as obvious as that in public dataset. The difference derives from several aspects. First, for industrial dataset, it has a large number of instances to learn the embedding layer, which makes it earn less from auxiliary loss. Second, different from all items from one category in amazon dataset, the behaviors in industrial dataset are clicked goods from all scenes and all categories in our platform. Our goal is to predict CTR for ad in one scene. The supervision information from auxiliary loss may be heterogeneous from the target item, so the effect of auxiliary loss for the industrial dataset may be less for public datasets, while the effect of AUGRU is magnified.\n\n#### Visualization of Interest Evolution\n\nThe dynamic of hidden states in AUGRU can reflect the evolving process of interest. In this section, we visualize these hidden states to explore the effect of different target item for interest evolution.\n\nThe selective history behaviors are from category Computer Speakers, Headphones, Vehicle GPS, SD & SDHC Cards, Micro SD Cards, External Hard Drives, Headphones, Cases, successively. The hidden states in AUGRU are projected into a two dimension space by principal component analysis (PCA) (Wold, Esbensen, and Geladi 1987). The projected hidden states are linked in order. The moving routes of hidden states activated by different target item are shown in Fig. 3(a). The yellow curve which is with None target represents the attention score used in eq. (12) are equal, that is the evolution of interest are not effected by target item. The blue curve shows the hidden states are activated by item from category Screen Protectors, which is less related to all history behaviors, so it shows similar route to yellow curve. The red curve shows the hidden states are activated by item from category Cases, the target item is strong related to the last behavior, which moves a long step shown in Fig. 3(a). Correspondingly, the last behavior obtains a large attention score showed in Fig. 3(b).\n\n### Online Serving & A/B testing\n\nFrom 2018-06-07 to 2018-07-12, online A/B testing was conducted in the display advertising system of Taobao. As shown in Table 5, compared to the BaseModel, DIEN has improved CTR by  \\(20.7\\%\\)  and effective cost per\n\n![](images/9e0aede336519fba28cf5f965bec8f6385f4fe6422f8d768ef012b046f8b42c6.jpg)  \n(a) Visualization of hidden states in AUGRU\n\n![](images/c918b9b4c0947a26c724345d0ee942e001e618fb9a7b273e8948b9921f4eab6e.jpg)  \n(b) Attention score of different history behaviors  \nFigure 3: Visualization of interest evolution, (a) The hidden states of AUGRU reduced by PCA into two dimensions. Different curves show the same history behaviors are activated by different target items. None means interest evolving is not effected by target item. (b) Faced with different target item, attention scores of all history behaviors are shown.\n\nmille (eCPM) by  \\(17.1\\%\\) . Besides, DIEN has decayed pay per click (PPC) by  \\(3.0\\%\\) . Now, DIEN has been deployed online and serves the main traffic, which contributes a significant business revenue growth.\n\nIt is worth noticing that online serving of DIEN is a great challenge for commercial system. Online system holds really high traffic in our display advertising system, which serves more than 1 million users per second at traffic peak. In order to keep low latency and high throughput, we deploy several important techniques to improve serving performance: i) element parallel GRU & kernel fusion (Wang, Lin, and Yi 2010), we fuse as many independent kernels as possible. Besides, each element of the hidden state of GRU can be calculated in parallel. ii) Batching: adjacent requests from different users are merged into one batch to take advantage of GPU. iii) Model compressing with Rocket Launching (Zhou et al. 2018b): we use the method proposed in (Zhou et al. 2018b) to train a light network, which has smaller size but performs close to the deeper and more complex one. For instance, the dimension of GRU hidden state can be compressed from 108 to 32 with the help of Rocket Launching. With the help of these techniques, latency of DIEN serving can be reduced from  \\(38.2\\mathrm{ms}\\)  to  \\(6.6\\mathrm{ms}\\)  and the QPS (Query Per Second) capacity of each worker can be improved to 360.\n",
  "hyperparameter": "Embedding dimension (n_E): varies by dataset; GRU hidden size (n_H): 108 for full model, compressed to 32 with Rocket Launching for online serving; Auxiliary loss weight (α): 1.0 for balancing interest representation and CTR prediction; Maximum behavior sequence length: 50 for industrial dataset; MLP layers for industrial dataset: 6 layers with dimensions [600, 400, 300, 200, 80, 2]; Number of historical behavior days: 14 days for industrial dataset; Training period: 49 days for industrial dataset training set"
}