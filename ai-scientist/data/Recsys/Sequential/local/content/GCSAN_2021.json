{
  "id": "GCSAN_2021",
  "paper_title": "Graph Contextual Self-Attention Network for Session-Based Recommendation",
  "alias": "GCSAN",
  "year": 2021,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "GC-SAN combines graph neural networks (GNN) with multi-layer self-attention mechanisms for session-based recommendation. The model first constructs directed session graphs and uses GNN to learn contextual node embeddings by propagating information through incoming/outgoing edges. It then applies stacked self-attention layers to capture long-range dependencies between items regardless of their sequential distances. The final session representation is a weighted combination of the global self-attention representation and the local last-clicked item embedding, enabling the model to leverage both long-term preferences and short-term interests for next-item prediction.",
  "introduction": "# 1 Introduction\n\nRecommender systems play an important role in helping users alleviate the problem of information overload and select interesting contents in many applications domains, e.g., ecommerce, music, and social media. Most of existing recommender systems are based on user historical interactions. However, in many application scenarios, user identification may be unknown and there are only user historical actions during an ongoing session. To solve this problem, session-based recommendation is proposed to predict the next action (e.g., click on an item) that a user may take based on the sequence of the user's previous behaviors in the current session.\n\nDue to its highly practical value, many kinds of approaches for session-based recommendation have been proposed. Markov Chain (MC) is a classic example, which assumes that the next action is based on the previous ones [Rendle et al., 2010]. With such a strong assumption, an independent combination of the past interactions may limit the accuracy of recommendation. Recent studies have highlighted the importance of using recurrent neural network (RNN) in session-based recommender systems and obtained promising results [Zhao et al., 2019]. For instance, Hidasi et al. [Hidasi et al., 2016] proposed to model short-term preferences with GRU (a variant of RNN), and then an improved version [Tan et al., 2016] is proposed to further boost its recommendation performance. Recently, NARM [Li et al., 2017] is designed to capture the user's sequential pattern and main purpose simultaneously by employing a global and local RNN. However, the existing methods usually model single-way transitions between consecutive items and neglect complex transitions among the entire session sequence.\n\nMore recently, a new sequential model, Transformer [Vaswani et al., 2017], has achieved state-of-the-art performance and efficiency in various translation tasks. Instead of using recurrence or convolution, Transformer utilizes an encoder-decoder structure composed of stacked self-attention network to draw global dependencies between input and output. Self-attention, as a special attention mechanism, has been widely used to model the sequential data and achieved remarkable results in many applications, e.g., machine translation [Vaswani et al., 2017], sentiment analysis [Lin et al., 2017], and sequential recommendation [Kang and McAuley, 2018; Zhou et al., 2018]. The success of the Transformer model can be attributed to its self-attention network, which takes full account of all signals with a weighted averaging operation. Despite its success, such an operation disperses the distribution of attention, which results in lacking local dependencies over adjacent items and limiting its capacity for learning contextualized representations of items [Liu et al., 2019]. While the local contextual information of adjacent items has been shown that it can enhance the ability of modeling dependencies among neural representations, especially for the attention models [Yang et al., 2018; Liu et al., 2019].\n\nIn this work, we propose to strengthen self-attention network through graph neural network (GNN). On the one hand, the strength of self-attention is to capture long-range dependencies by explicitly attending to all the positions. On the other hand, GNN is capable of providing rich local contextual information by encoding edge or node attribute features [Battaglia et al., 2018]. Specifically, we introduce a graph contextual self-attention network, named GC-SAN, for session-based recommendation, which benefits from the complementary strengths of GNN and self-attention. We first construct a directed graph from all historical session sequences. Based on the session graph, GC-SAN is able to capture transitions of neighbor items and generate the latent vectors for all nodes involved in the graph correspondingly. Then we apply the self-attention mechanism to model long-range dependencies regardless of the distance, where session embedding vectors are composed by the latent vectors of all nodes in the graph. Finally, we use the linear weighted sum of the user's global interests and his/her local interests in that session as the embedding vector to predict the probability of clicking on the next item.\n\nThe main contributions of this work are summarized as follows.\n\n- To improve the representation of session sequences, we present a novel graph contextual self-attention model based on graph neural network (GC-SAN). GC-SAN utilizes the complementarity between self-attention network and graph neural network to enhance the recommendation performance.  \n- Graph neural network is used to model local graph-structured dependencies of separated session sequences, while multi-layer self-attention network is designed to obtain contextualized non-local representations.  \n- We conduct extensive experiments on two benchmark datasets. Our experimental results show the effectiveness and superiority of GC-SAN, comparing with the state-of-the-art methods via comprehensive analysis.",
  "method": "# 3 Graph Contextualized Self-Attention Network\n\nIn this section, we introduce the proposed contextualized self-attention recommendation model based on graph neural network (GC-SAN). We first formulate the problem of session-based recommendation, and then describe the architecture of our model in detail (As shown in Figure 1).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/63928bc9bca1b160f1838a9fcfafadb3750046448ec6851d19fec26d05e5fcc8.jpg)  \nFigure 1: The general architecture of the proposed model. We first construct a directed graph of all session sequences. Based on the graph, we apply graph neural network to obtain all node vectors involved in the session graph. After that, we use a multi-layer self-attention network to capture long-range dependencies between items in the session. In prediction layer, we represent each session as a linear of the global preference and the current interest of that session. Finally, we compute the ranking scores of each candidate item for recommendation.\n\n# 3.1 Problem Statement\n\nSession-based recommendation aims to predict which item a user would like to click next, only based upon his/her current interaction sequence. Here we give a formulation of the session-based recommendation problem as below.\n\nLet  $V = \\{v_{1}, v_{2}, \\dots, v_{|V|}\\}$  denote a set of all unique items involved in all sessions. For each anonymous session, a sequence of clicked actions by the user are denoted as  $S = \\{s_{1}, s_{2}, \\dots, s_{n}\\}$  in time order, where  $s_{t} \\in V$  represents a clicked item of the user at time step  $t$ . Formally, our model aims to predict the next possible click (i.e.,  $s_{t+1}$ ) for a given prefix of the action sequence truncated at time  $t$ ,  $S_{t} = \\{s_{1}, s_{2}, \\dots, s_{t-1}, s_{t}\\}$  ( $1 \\leq t < n$ ). To be exact, our model generates a ranking list over all candidate items that may occur in that session.  $\\hat{\\mathbf{y}} = \\{\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{|V|}\\}$  denotes the output probability for all items, where  $\\hat{y}_{i}$  corresponds to the recommendation score of item  $v_{i}$ . Since a recommender typically makes more than one recommendation for the user, thus we choose the top-N items from  $\\hat{\\mathbf{y}}$  for recommendation.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/fad32f9041e4ca60a0c5ffea544fa365989e13204bef2d6abb5814800f835240.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/89611b45aa8be919c4c3f70145e5accbec258d4214fea6cb9dea99864fd3b94b.jpg)  \nFigure 2: An example of a session graph structure and the connection matrices  $\\mathbf{M}^I$  and  $\\mathbf{M}^O$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/e692b63d945ab2c45532dcbf284a2de073a3dbd841da0273db221e74cac2aab4.jpg)\n\n# 3.2 Dynamic Graph Structure\n\nGraph Construction. The first part of GNN is to a construct meaningful graph from all sessions. Given a session\n\n$S = \\{s_{1}, s_{2}, \\dots, s_{n}\\}$ , we treat each item  $s_{i}$  as a node and  $(s_{i-1}, s_{i})$  as an edge which represents a user clicks item  $s_{i}$  after  $s_{i-1}$  in the session  $S$ . Therefore, each session sequence can be modeled as a directed graph. The graph structure is updated by promoting communication between different nodes. Specifically, let  $\\mathbf{M}^{I}, \\mathbf{M}^{O} \\in \\mathbb{R}^{n \\times n}$  denote weighted connections of outgoing and incoming edges in the session graph, respectively. For example, considering a session  $S = \\{s_{1}, s_{3}, s_{2}, s_{4}, s_{3}\\}$ , the corresponding graph and the matrix (i.e.,  $\\mathbf{M}^{I}$  and  $\\mathbf{M}^{O}$ ) are shown in Figure2. Since several items may appear in the session sequence repeatedly, we assign each edge with normalized weight, which is calculated as the occurrence of the edge divided by the outdegree of that edge's start node. Note that our model can support various strategies of constructing session graph and generate the corresponding connection matrices. Then we can apply the two weighted connection matrices with graph neural network to capture the local information of the session sequence.\n\nNode Vectors Updating. Next, we present how to obtain latent feature vectors of nodes via graph neural network. We first convert every item  $v \\in V$  into an unified low-dimensional latent space and the node vector  $\\mathbf{s} \\in \\mathbb{R}^d$  denotes a  $d$ -dimensional real-valued latent vector of item  $v$ . For each node  $s_t$  at time  $t$  in the graph session, given by the connection matrices  $\\mathbf{M}^I$  and  $\\mathbf{M}^O$ , the information propagation between different nodes can be formalized as:\n\n$$\n\\begin{array}{l} \\mathbf {a} _ {t} = \\operatorname {C o n c a t} \\left(\\mathbf {M} _ {t} ^ {I} \\left([ \\mathbf {s} _ {1}, \\dots , \\mathbf {s} _ {n} ] \\mathbf {W} _ {a} ^ {I} + \\mathbf {b} ^ {I}\\right), \\right. \\\\ \\mathbf {M} _ {t} ^ {O} \\left(\\left[ \\mathbf {s} _ {1}, \\dots , \\mathbf {s} _ {n} \\right] \\mathbf {W} _ {a} ^ {O} + \\mathbf {b} ^ {O}\\right)), \\tag {1} \\\\ \\end{array}\n$$\n\nwhere  $\\mathbf{W}_a^I$ ,  $\\mathbf{W}_a^O \\in \\mathbb{R}^{d \\times d}$  are the parameter matrices.  $\\mathbf{b}^I, \\mathbf{b}^O \\in \\mathbb{R}^d$  are the bias vectors.  $\\mathbf{M}_t^I, \\mathbf{M}_t^O \\in \\mathbb{R}^{1 \\times n}$  are  $t$ -th row of each matrix corresponding to node  $s_t$ , respectively.  $\\mathbf{a}_t$  extracts the contextual information of neighborhoods for node  $s_t$ . Then we take them and the previous state  $\\mathbf{s}_{t-1}$  as input and feed into the graph neural network. Thus, the final\n\noutput  $\\mathbf{h}_t$  of GNN layer is computed as follows.\n\n$$\n\\mathbf {z} _ {t} = \\sigma \\left(\\mathbf {W} _ {z} \\mathbf {a} _ {t} + \\mathbf {P} _ {z} \\mathbf {s} _ {t - 1}\\right),\n$$\n\n$$\n\\mathbf {r} _ {t} = \\sigma (\\mathbf {W} _ {r} \\mathbf {a} _ {t} + \\mathbf {P} _ {r} \\mathbf {s} _ {t - 1}),\n$$\n\n$$\n\\tilde {\\mathbf {h}} _ {t} = \\tanh  \\left(\\mathbf {W} _ {h} \\mathbf {a} _ {t} + \\mathbf {P} _ {h} \\left(\\mathbf {r} _ {t} \\odot \\mathbf {s} _ {t - 1}\\right)\\right), \\tag {2}\n$$\n\n$$\n\\mathbf {h} _ {t} = \\left(1 - \\mathbf {z} _ {t}\\right) \\odot \\mathbf {s} _ {t - 1} + \\mathbf {z} _ {t} \\odot \\tilde {\\mathbf {h}} _ {t}.\n$$\n\nwhere  $\\mathbf{W}_z, \\mathbf{W}_r, \\mathbf{W}_h \\in \\mathbb{R}^{2d \\times d}$ ,  $\\mathbf{P}_z, \\mathbf{P}_r, \\mathbf{P}_h \\in \\mathbb{R}^{d \\times d}$ , are learnable parameters.  $\\sigma(\\cdot)$  represents the logistic sigmoid function and  $\\odot$  denotes element-wise multiplication.  $\\mathbf{z}_t, \\mathbf{r}_t$  are update gate and reset gate, which decide what information to be preserved and discarded, respectively.\n\n# 3.3 Self-Attention Layers\n\nSelf-attention is a special case of the attention mechanism and has been successfully applied in lots of research topics including NLP [Vaswani et al., 2017] and QA [Li et al., 2019]. The self-attention mechanism can draw global dependencies between input and output, and capture item-item transitions across the entire input and output sequence itself without regard to their distances.\n\nSelf-Attention Layer. After feeding a session sequence into the graph neural network, we can obtain the latent vectors of all nodes involved in the session graph, i.e.,  $\\mathbf{H} = [\\mathbf{h}_1,\\mathbf{h}_2,\\dots,\\mathbf{h}_n]$ . Next, we feed them into the self-attention layer to better capture the global session preference.\n\n$$\n\\mathbf {F} = \\operatorname {s o f t m a x} \\left(\\frac {\\left(\\mathbf {H W} ^ {Q}\\right) \\left(\\mathbf {H W} ^ {K}\\right) ^ {T}}{\\sqrt {d}}\\right) \\left(\\mathbf {H W} ^ {V}\\right) \\tag {3}\n$$\n\nwhere the projection matrices  $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{2d \\times d}$ .\n\nPoint-Wise Feed-Forward Network. After that, we apply two linear transformations with a ReLU activation function to endow the model with nonlinearity and consider interactions between different latent dimensions. However, transmission loss may occur in self-attention operations. Thus we add a residual connection after the feed-forward network, which makes the model much easier to leverage low-layer information inspired by [Vaswani et al., 2017].\n\n$$\n\\mathbf {E} = R e L U \\left(\\mathbf {F W} _ {1} + \\mathbf {b} _ {1}\\right) \\mathbf {W} _ {2} + \\mathbf {b} _ {2} + \\mathbf {F} \\tag {4}\n$$\n\nwhere  $\\mathbf{W}_1$  and  $\\mathbf{W}_2$  are  $d\\times d$  matrices,  $\\mathbf{b}_1$  and  $\\mathbf{b}_2$  are  $d$ -dimensional bias vectors. Moreover, to alleviate overfitting problems in deep neural networks, we apply \"Dropout\" regularization techniques during training. For simplicity, we define the above whole self-attention mechanism as:\n\n$$\n\\mathbf {E} = S A N (\\mathbf {H}) \\tag {5}\n$$\n\nMulti-layer Self-Attention. Recent work shows that different layers capture different types of features. In this work, we investigate which levels of layers benefit most from the features modeling to learn more complex item transitions. The 1-st layer is defined as  $\\mathbf{E}^{(1)} = \\mathbf{E}$ . The  $k$ -th  $(k > 1)$  self-attention layer is defined as:\n\n$$\n\\mathbf {E} ^ {(k)} = S A N \\left(\\mathbf {E} ^ {(k - 1)}\\right) \\tag {6}\n$$\n\nwhere  $\\mathbf{E}^{(k)}\\in \\mathbb{R}^{n\\times d}$  is the final output of the multi-layer selfattention network.\n\n# 3.4 Prediction Layer\n\nAfter several self-attention blocks that adaptively extract sequential information of sessions, we achieve the long-term self-attentive representation  $\\mathbf{E}^{(k)}$ . To better predict the user's next clicks, we combine the long-term preference and the current interest of the session, and then use this combined embedding as the session representation. For a session  $S = \\{s_1,s_2,\\dots ,s_n\\}$ , we take the last dimensions of  $\\mathbf{E}^{(k)}$  as the global embedding following [Kang and McAuley, 2018]. The local embedding can be simply defined as the last clicked-item vector, i.e.,  $\\mathbf{h}_n$ . Then we weight them together as the final session embedding.\n\n$$\n\\mathbf {S} _ {f} = \\omega \\mathbf {E} _ {n} ^ {(k)} + (1 - \\omega) \\mathbf {h} _ {n} \\tag {7}\n$$\n\nwhere  $\\mathbf{E}_n^{(k)}\\in \\mathbf{R}^d$  represent  $n$ -th row of the matrix. Finally, we predict the next click for each candidate item  $v_{i}\\in V$  given session embedding  $\\mathbf{S}_f$  as follows:\n\n$$\n\\hat {\\mathbf {y}} _ {i} = \\operatorname {s o f t m a x} \\left(\\mathbf {S} _ {f} ^ {T} \\mathbf {v} _ {i}\\right). \\tag {8}\n$$\n\nwhere  $\\hat{\\mathbf{y}}_i$  denotes the recommendation probability of item  $v_i$  to be the next click in session  $S$ . Finally, we train our model by minimizing the following objective function:\n\n$$\n\\mathcal {J} = - \\sum_ {i = 1} ^ {n} \\mathbf {y} _ {i} \\log (\\hat {\\mathbf {y}} _ {i}) + (1 - \\mathbf {y} _ {i}) \\log (1 - \\hat {\\mathbf {y}} _ {i}) + \\lambda | | \\theta | | ^ {2}. \\tag {9}\n$$\n\nwhere  $\\mathbf{y}$  denotes the one-hot encoding vector of the ground truth item,  $\\theta$  is the set of all learnable parameters.",
  "experiments": "# 4 Experiments and Analysis\n\nIn this section, we first set up the experiment. And then we conduct experiments to answer the following questions:\n\nRQ1: Does the proposed graph contextualized self-attention session-based recommendation model (GC-SAN) achieve state-of-the-art performance?\n\nRQ2: How do the key hyper-parameters affect model performance, such as the weight factor and embedding size?\n\n# 4.1 Experimental Setup\n\nDatasets. We study the effectiveness of our proposed approach GC-SAN on two real-world datasets, i.e., Diginetica and Retailrocket. Diginetica dataset comes from CIKM Cup 2016, where only the transactional data is used in this study. Retailrocket dataset is published by a personalized e-commerce company, which contains six months of user browsing activities. To filter noisy data, we filter out items appearing less than 5 times and then remove all sessions with fewer than 2 items on both datasets. Furthermore, for session-based recommendation, we set the sessions data of last week as the test data, and the remaining for training. Similar to [Tan et al., 2016; Yuan et al., 2019], for a session sequence  $S = \\{s_1, s_2, \\dots, s_n\\}$ , we generate the input and corresponding labels ( $\\{s_1\\}, s_2$ ,  $\\{s_1, s_2\\}, s_3$  ...  $\\{s_1, \\dots, s_{(n-1)}\\}, s_n$ ) for training and testing on both datasets. After preprocessing, the statistics of the datasets are shown in Table 2.\n\n<table><tr><td>Datasets</td><td colspan=\"6\">Diginetica</td><td colspan=\"6\">Retailrocket</td></tr><tr><td>Measures</td><td>HR@5</td><td>HR@10</td><td>MRR@5</td><td>MRR@10</td><td>NDCG@5</td><td>NDCG@10</td><td>HR@5</td><td>HR@10</td><td>MRR@5</td><td>MRR@10</td><td>NDCG@5</td><td>NDCG@10</td></tr><tr><td>Pop</td><td>0.0036</td><td>0.0077</td><td>0.0019</td><td>0.0025</td><td>0.0023</td><td>0.0037</td><td>0.0133</td><td>0.0208</td><td>0.0066</td><td>0.0076</td><td>0.0082</td><td>0.0107</td></tr><tr><td>BPR-MF</td><td>0.1060</td><td>0.1292</td><td>0.0789</td><td>0.0842</td><td>0.0586</td><td>0.0672</td><td>0.2106</td><td>0.2719</td><td>0.1356</td><td>0.1407</td><td>0.1138</td><td>0.1322</td></tr><tr><td>IKNN</td><td>0.1407</td><td>0.2083</td><td>0.0776</td><td>0.0867</td><td>0.0693</td><td>0.0902</td><td>0.1709</td><td>0.2248</td><td>0.0972</td><td>0.1043</td><td>0.0855</td><td>0.1020</td></tr><tr><td>FPMC</td><td>0.1855</td><td>0.2309</td><td>0.0875</td><td>0.0986</td><td>0.0811</td><td>0.1037</td><td>0.1732</td><td>0.2319</td><td>0.1013</td><td>0.1152</td><td>0.0901</td><td>0.1095</td></tr><tr><td>GRU4Rec</td><td>0.2577</td><td>0.3657</td><td>0.1434</td><td>0.1577</td><td>0.1276</td><td>0.1607</td><td>0.2196</td><td>0.2869</td><td>0.1286</td><td>0.1489</td><td>0.1076</td><td>0.1323</td></tr><tr><td>STAMP</td><td>0.3998</td><td>0.5014</td><td>0.2357</td><td>0.2469</td><td>0.2039</td><td>0.2394</td><td>0.3287</td><td>0.3972</td><td>0.2241</td><td>0.2334</td><td>0.1758</td><td>0.1970</td></tr><tr><td>SR-GNN</td><td>0.4082</td><td>0.5269</td><td>0.2439</td><td>0.2599</td><td>0.2078</td><td>0.2443</td><td>0.3502</td><td>0.4268</td><td>0.2422</td><td>0.2525</td><td>0.1885</td><td>0.2121</td></tr><tr><td>GC-SAN</td><td>0.4280</td><td>0.5351</td><td>0.2694</td><td>0.2838</td><td>0.2223</td><td>0.2552</td><td>0.3644</td><td>0.4380</td><td>0.2506</td><td>0.2604</td><td>0.1956</td><td>0.2181</td></tr><tr><td>Improv.</td><td>4.84%</td><td>1.56%</td><td>10.46%</td><td>9.20%</td><td>6.98%</td><td>4.44%</td><td>4.07%</td><td>2.62%</td><td>3.49%</td><td>3.15%</td><td>3.79%</td><td>2.85%</td></tr></table>\n\nTable 1: The performance of different methods on the two datasets. We generate Top-5 and 10 items for recommendation. The best performance in each column is boldfaced (the higher, the better). Improvements over the best baseline are shown in the last row.  \n\n<table><tr><td>Dataset</td><td># clicks</td><td># train</td><td># test</td><td># items</td><td>avg.len</td></tr><tr><td>Diginetica</td><td>858,107</td><td>526,134</td><td>44,279</td><td>40,840</td><td>5.97</td></tr><tr><td>Retailrocket</td><td>710,856</td><td>433,648</td><td>15,132</td><td>36,968</td><td>5.43</td></tr></table>\n\nTable 2: Statistics of datasets.\n\nEvaluation Metrics. To evaluate the recommendation performance of all models, we adopt three common metrics, i.e., Hit Rate (HR@N), Mean Reciprocal Rank (MRR@N) and Normalized Discounted Cumulative Gain (NDCG@N). The former one is an evaluation of unranked retrieval results, while the latter two are evaluations of ranked lists. Here, we consider Top-N ( $N = \\{5, 10\\}$ ) for recommendation.\n\n# 4.2Baselines\n\nWe consider the following compared methods for performance comparisons:\n\n- Pop is a simple baseline that recommends top rank items based on popularity in training data.  \n- BPR-MF [Rendle et al., 2009] is the state-of-the-art method for non-sequential recommendation, which optimizes matrix factorization using a pairwise ranking loss.  \n- IKNN [Sarwar et al., 2001] is a traditional item-to-item model, which recommends items similar to the candidate item within the session based on cosine similarity.  \n- FPMC³ [Rendle et al., 2010] is a classic hybrid model combing matrix factorization and first-order Markov chain for next-basket recommendation. Note that in our recommendation problem, each basket is a session.  \n- GRU4Rec $^{4}$  [Hidasi et al., 2016] is a RNN-based deep learning model for session-based recommendation. It utilizes a session-parallel mini-batch training process to model user action sequences.  \n- STAMP [Liu et al., 2018] is a novel short-term memory priority model to capture the user's long-term preference from previous clicks and the current interest of the last-clicks in a session.  \n- SR-GNN $^{5}$  [Wu et al., 2018] is recently proposed session-based recommendation model with graph neural\n\nnetwork, which applies GNN to generate latent vectors of items and then represent each session through traditional attention network.\n\n# 4.3 Comparisons of Performance\n\nTo demonstrate the recommendation performance of our model GC-SAN, we compare it with other state-of-the-art methods (RQ1). The experimental results of all methods on Diginetica and Retailrocket datasets are illustrated in Table1, and we have the following observations.\n\nThe non-personalized Popularity-based methods (i.e., Pop) has the most unfavorable performance on both datasets. By profiling users individually and optimizing the pairwise ranking loss function, BPR-MF performs better than Pop. This suggests the importance of personalization in recommendation tasks. IKNN and FPMC achieve better performance than BPR-MF on Diginetica dataset, while BPR-MF outperforms IKNN and FPMC on Retailrocket dataset. In fact, IKNN utilizes the similarity between items in the session and FPMC is based on first-order Markov Chain.\n\nAll of the neural network methods, such as GRU4Rec and STAMP, outperform the traditional baselines (e.g., FPMC and IKNN) in nearly all the cases, which verifies the power of deep learning technology in this field. GRU4Rec leverages the recurrent structure with GRU as a special form of RNN to capture the user's general preference, while STAMP improves the short-term memory through the last clicked item. Unsurprisingly, STAMP performs better than GRU4Rec, which indicates the effectiveness of short-term behavior for predicting the next item problem. On the other hand, by modeling every session as a graph and applying graph neural network and the attention mechanism, SR-GNN outperforms all other baselines on both datasets. This further proves the power of neural network in recommender systems.\n\nCompared to SR-GNN, our approach GC-SAN adopts the self-attention mechanism to adaptively assign weights to previous items regardless of their distances in the current session and captures long-range dependencies between items of a session. We combine the long-range self-attention representation and the short-term interest of the last-click in a linear way to generate the final session representation. As we can see, our method achieves the best performance among all the methods on both datasets in terms of HR, MRR, and NDCG. These results demonstrate the efficacy and validity of GC-SAN for session-based recommendation.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/5b6cc35e8d49d2dce66826911bddc6017f4606c2efe23fe9ae93325336547e7f.jpg)  \n(a) The weight factor  $\\omega$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/ddc1d5cea611a2816e94f8d438f6b95e94b53a5203ede4c280af58b6a2884c0b.jpg)  \n(b) Number of SA blocks  $k$  \nFigure 3: Effects of the weight factor  $\\omega$  and effects of the number of stacked self-attention blocks  $k$  on both datasets.\n\n# 4.4 Model Analysis and Discussion\n\nIn this subsection, we take an in-depth model analysis study, aiming to further understand the framework of GC-SAN (RQ2). Due to the space limit, we only show the analysis results in terms of HR@10 and NDCG@10. We have obtained similar experimental results in terms of other metrics.\n\n<table><tr><td>Dataset</td><td colspan=\"2\">Diginetica</td><td colspan=\"2\">Retailrocket</td></tr><tr><td>Measures</td><td>HR@10</td><td>NDCG@10</td><td>HR@10</td><td>NDCG@10</td></tr><tr><td>w/GC-SAN</td><td>0.5351</td><td>0.2552</td><td>0.4380</td><td>0.2181</td></tr><tr><td>w/o GC-SAN</td><td>0.4199</td><td>0.2060</td><td>0.4191</td><td>0.2065</td></tr></table>\n\nTable 3: The performance of GC-SAN with and without graph neural network in terms of HR@10 and NDCG@10.\n\nImpact of graph neural network. Although we can infer the effectiveness of graph neural network implicitly from Table 1, we would like to verify the contribution of graph neural network in GC-SAN. We remove the graph neural network module from GC-SAN, replace it with a randomly initialized item embedding, and feed into the self-attention layer. Table 3 displays the comparisons between with and without GNN. From Table 1 and Table 3, we find that even without GNN, GC-SAN can still outperform STAMP on Retailrocket dataset, while it was beaten by GRU4Rec on Diginetica dataset. In fact, the maximum session length of Retailrocket dataset is almost four times that of Diginetica dataset. A possible reason is that short sequence lengths can construct more dense session graphs that provide richer contextual information, while the self-attention mechanism performs better with long sequence lengths. This further demonstrates that the self-attention mechanism and graph neural network play important roles in improving recommendation performance.\n\nImpact of weight factor  $\\omega$ . The weight parameter  $\\omega$  controls the contribution of self-attention representation and the last-clicked action. Observing from Figure 3(a), taking only global self-attention dependencies  $(\\omega = 1.0)$  as final session embedding usually achieves a better performance than considering only current interests  $(\\omega = 0)$ . Setting  $\\omega$  to a value from 0.4 to 0.8 is more desirable. This indicates that while the self-attention mechanism with graph neural network can adaptively assign weights to focus on long-range dependencies or more recent actions, the short-term interest is also indispensable for improving the recommendation performance.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/85b9ca130f4c2b06b19d651ec459beaf3a7022fee1092b77ab227803c82758a9.jpg)  \n(a) Diginetica  \nFigure 4: The performance under different embedding sizes  $d$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/42ed15bd-5422-4a60-b448-beac13ca0c57/4a58234e99b0c661f21c3f326d8111e3686e660879839308259dec0d5d24da80.jpg)  \n(b) Retailrocket\n\nImpact of the number of self-attention blocks  $k$ . As aforementioned, we investigate which levels of self-attention layers benefit most from GC-SAN. Figure 3(b) displays the experimental results of applying different self-attention blocks with  $k$  varying from 1 to 6. On both datasets, we can observe that increasing  $k$  can boost the performance of GC-SAN. However, it achieves the best performance when  $k$  is chosen properly and gets worse for a larger  $k$ . This may be because using more blocks ( $k \\geq 4$ ) would make GC-SAN easier to lose low-layer information.\n\nImpact of the embedding size  $d$ . In Figure 4, we investigate the effectiveness of the embedding size  $d$  ranging from 10 to 120 on both datasets. Among all the baselines, STAMP and SR-GNN perform well and stable. Hence, we use S-TAMP and SR-GNN as two baselines for ease of comparisons. From figure 4, we can observe that our model GC-SAN consistently outperforms STAMP on all latent dimensions. When  $d$  is less than a certain value, SR-GNN performs better than GC-SAN. Once this value is exceeded, the performance of GC-SAN still grows and eventually stabilizes with  $d \\geq 100$ , while the performance of SR-GNN slightly reduces. This may be because that a relatively small  $d$  limits GC-SAN to capture complex transitions between item latent factors, while SR-GNN may suffer from overfitting with a larger  $d$ .",
  "hyperparameter": "Embedding dimension d: 100-120 (stable performance at d≥100); Weight factor ω: 0.4-0.8 (balances global self-attention and local interest); Number of self-attention blocks k: 3-4 (k≥4 causes information loss); L2 regularization λ is used but value not specified; Dropout is applied during training (rate not specified); Learning rate and batch size not explicitly mentioned in the method/experiments sections"
}