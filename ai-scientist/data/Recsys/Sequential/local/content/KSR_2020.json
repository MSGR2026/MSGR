{
  "id": "KSR_2020",
  "paper_title": "Knowledge-Aware Sequential Recommendation with Relational Memory",
  "alias": "KSR",
  "year": 2020,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "The paper proposes KSR (Knowledge-Aware Sequential Recommendation), a model that combines knowledge graph embeddings with sequential modeling for news recommendation. The core innovation is: (1) enriching news representations by integrating word embeddings, entity embeddings from knowledge graphs (using TransE/H/R/D), and context entities from the knowledge graph; (2) using LSTM to capture sequential relationships in both word sequences and entity sequences; (3) applying an attention mechanism to weight historical clicked news based on their relevance to the candidate news for prediction.",
  "introduction": "# 1. Introduction\n\nWith the booming popularity of the Internet and the changes in the way people read, traditional media such as newspapers, radios, and other devices are gradually out of sight. At the same time, some online news reading systems have emerged, such as Google News and Bing News. People can see news from the world on their mobile phones or the web. Massive news gives users a lot of opportunities to choose the information they want. However, the overload of information makes it impossible for users to obtain the information they want quickly. The appearance of the recommendation system dramatically slows down the information overload. It can personalize the recommendation according to the user's historical interest [1] to help user find information quickly.\n\nDifferent from recommendation systems in other fields, such as movies, music, and e-commerce recommendations. Recommendation systems in the area of news often face the following challenges. First, the timeliness of news recommendation is very high. [2] shows that in business news dataset news will soon become obsolete, and the average life of news is only 4.1h. In the recommendation field, collaborative filtering has achieved great success, and the collaborative filtering method is based on the interaction record of user history, which is heavily dependent on the user's item scoring matrix. It will\n\nface the problem of sparse and cold start. The problem in news domain makes the traditional collaborative filtering recommendation [3] invalid. Second, when users read the news, interest tends to shift significantly with time flows. Third, the news field has a unique feature that the language used in the title is often highly concise and contains many entity words.\n\nIn order to solve these problems, researchers are committed to introducing more side information to make recommendations valid, such as users-item attributes [4] and social networks [5]. Among this side information, knowledge graph appears as a relatively new knowledge information in the sight of researchers. Knowledge graph is a graph network that contains rich semantic information. The nodes in the knowledge graph represent the entity, and the edges represent the relationships between the entities. In recent years, researchers have proposed many knowledge graphs, such as DBPedia, Google Knowledge Graph, and Microsoft Satori. These knowledge graphs have been widely applied to the question-and-answer system [6] and the study of the word vector [7].\n\nInspired by the idea of applying knowledge graphs to other tasks, as shown in Figure 1, we can also apply the knowledge graph to the recommendation system. Applying the knowledge graph to the recommendation system has the following benefits. First, the knowledge graph contains a large number of semantic relationships between items which can find hidden deep relationships between items and can improve the accuracy of recommendations. Second, the knowledge graph is a heterogeneous graph, which contains many different types of relationships, which can enrich the user's interest portraits and provide more diverse recommendation results. Third, the knowledge graph connects the user history and the objective item, which provides interpretability for recommendations. So we introduce the knowledge graph into the recommendation system and propose the KSR model (Knowledge-based Sequential Recommendation). The KSR model aligns the entity in the news with its entities in the knowledge graph, extracts the subgraphs containing the entities in the knowledge graph, and uses the knowledge graph to represent the learning method for encoding. Since the title's words of the news are sequential, the vector which training from the knowledge graph is introduced into the recurrent neural network for learning the sequential relationships. The user's feature is represented by his historical clicked news. Since the relationship between the news to be predicted and the news that the user has seen has different weights, we use the attention model to calculate those weights.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/d87d02b4-4121-4191-aa10-ceccb9bbb002/2ff7a03906b2a3899a32b99b591e20f31d3f35eb1d97dd8c9f62ebd9316cd01f.jpg)  \nFigure 1. This picture briefly describes how the knowledge graph works.\n\nIn summary, this article contributes as follows:\n\n(1) We introduce the knowledge graph into the recommendation system. That is, the feature of the news data is represented as its own word vector, and the feature representation of the entity appearing in the news in the knowledge graph which can enrich the feature representation of the news. There are different degrees of correlations between the news clicked by the user history and the news to be predicted, so we introduced the attention mechanism to calculate that weight.\n\nIOP Conf. Series: Materials Science and Engineering 799 (2020) 012042 doi:10.1088/1757-899X/799/1/012042\n\n(2) The words in the news's title contain sequential relationships. The existing knowledge graph-based recommendation model does not capture these relationships. our KSR model uses a recurrent neural network to learn the sequential relationship. Compared with some existing models, it achieves better results. Moreover, in the subsequent comparative experiments, it is also proved that the module for the sequential relationship has a specific improvement for the recommendation effect.",
  "method": "# 3. Model Define\n\nIn this section, we will introduce the KSR model in detail. The framework of the model is shown in Figure 2. The model takes the news that the user has clicked and the news that is to be predicted as input. For each piece of new we use the KGE module to process its information level representation, and get the embedding of each piece of news. We enrich the news embedding vector by capturing sequential relationship through LSTM, which is described in detail in 3.1. The user's embedding is represented by the news that he has clicked. In order to obtain the final embedding of the news, we use the attention mechanism to calculate the weight of user history vector and the news vector to be predicted which will be described in detail in 3.2. Finally, we put the user's embedding and the embedding of the news to be predicted into the neural network to get the probability that the user clicks on the news.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/d87d02b4-4121-4191-aa10-ceccb9bbb002/4ab43a2dfb2b463bd96c8e249c4388155ac61f82086dcb273c4766ad238beda7.jpg)  \nFigure 2. This is the framework of the KSR model.\n\n# 3.1. News embedding refining\n\nThe KGE and LSTM modules work primarily to get the embedding of the news, which is used to enrich the user's characteristics. First, we use the traditional word2vec for the title in the dataset. We use the Gensim library to calculate the word embedding  $w_{i}$ , as shown in Equation 1. Then we get the entities in the title, and extract all the relationship links from the original knowledge graph to construct a subgraph. For the extracted subgraphs, we use the knowledge graph embedding methods TransE, TransH, TransR, and TransD to get the entity embedding. The entity embedding is expressed as  $e_{i}$ , as shown in Equation 2. We found in the experiment that when the entities in the news titles are constructed into small subgraphs, although the structural information of some entities can be obtained, these subgraphs are often sparse. So we use the context entities to enrich the semantic information. The context entity is represented as the average of its neighbor entities, denoted here by  $c_{i}$ , as expressed by Equation 3. We complete the first step of feature refinement. Next, we will use the recurrent neural network to capture the sequential relationship between each part of the vector.\n\n$$\nw _ {i} = \\text {G e n s i m} (w o r d _ {i}) \\tag {1}\n$$\n\n$$\ne _ {i} = \\operatorname {T r a n s} \\left(G _ {i}\\right) \\tag {2}\n$$\n\n$$\nc _ {i} = \\frac {1}{| c o n t e x t (e _ {i}) |} \\sum_ {e \\in c o n t e x t (e _ {i})} e \\tag {3}\n$$\n\nWe can do this by using the recurrent neural network, as shown in Section 2.1. In subsequent section, we also used the GRU unit which is variants of LSTM for comparative experiments. We pass the word embedding in the title mentioned above and the entity embedding generated by Trans method into two different LSTM networks to obtain a embedding containing the sequential relationship, such as formula (4)(5), where  $[w_1w_2\\ldots w_n]$  is the words contained in the news. The embedding of the entity part is the same. Finally, the three parts of the embedding are connected as an input to the subsequent modules, as shown in Equation 6.\n\n$$\n\\left[ w _ {1} w _ {2} \\dots w _ {n} \\right] = L S T M \\left(w _ {1} w _ {2} \\dots w _ {n}\\right) \\tag {4}\n$$\n\n$$\n\\left[ e _ {1} e _ {2} \\dots e _ {n} \\right] = L S T M \\left(e _ {1} e _ {2} \\dots e _ {n}\\right) \\tag {5}\n$$\n\n$$\nW = \\left[ w _ {1} w _ {2} \\dots w _ {n} e _ {1} e _ {2} \\dots e _ {n} c _ {1} c _ {2} \\dots c _ {n} \\right] \\tag {6}\n$$\n\n# 3.2. Attention module\n\nThrough the processing in Section 3.1, we obtain the news embedding that the user history clicked. We use the news that the user history clicked to indicate the user's preference. We can average these vectors, but because of the variety of user interests, news that have been clicked by user history may have different effects on the predicted news. In order to indicate the user's preference for different news, we\n\nIOP Conf. Series: Materials Science and Engineering 799 (2020) 012042 doi:10.1088/1757-899X/799/1/012042\n\nuse the attention model [13] to calculate the weight of news clicked by users and news to be predicted. The attention model diagram is shown in Figure 3. We use the news to be predicted, and the news clicked by the user history as input to the deep neural network for training, and finally use the softmax function to learn the weight of each historical news.",
  "experiments": "# 4. Experiment\n\nIn this section, we will introduce the experimental part, including the introduction of the dataset and the results of the experiment and the comparative experiment.\n\n# 4.1. Dataset description\n\nOur data set comes from Microsoft Bing News (for publicly released news data). After desensitization, each log mainly includes the user ID, news title and click status (1 is clicked, 0 is unclicked), entity ID, entity name. The knowledge graph is a graph of all the entities present in the dataset in the Microsoft Satori knowledge graph and their neighbours within several hops.\n\n# 4.2. Baselines\n\nDMF [14] is a depth matrix factorization model that uses a multi-layer neural network to process user embedding and item embedding for recommendation. We only use the user's implicit feedback for input.\n\nFM [15] is a representative model based on use's history. We use the doc2vec model to encode the news titles for calculation.\n\nDKN [16] takes entity embedding and word embedding as input and encodes it into the KCNN network for encoding.\n\n# 4.3. Experimental setup\n\nOur experimental environment is Win7 64-bit, Python 3.6 and TensorFlow 1.14. KGE module is FastTransX. It is an effective way to implement TransE and its variants, compiling with g++. Our optimization algorithm is Adam. The loss function is the cross entropy loss function plus the loss of the regularization term. The optimization goal is shown in Equation 7. Where y is the label of the sample, and  $\\bar{y}$  is the probability that the model is predicted to be positive.\n\n$$\nL o s s = \\sum - [ y * \\log (\\bar {y}) + (1 - y) * \\log (1 - \\bar {y}) ] + \\theta * L o s s _ {\\text {r e g u l a r i z a t i o n}} \\tag {7}\n$$\n\nIOP Conf. Series: Materials Science and Engineering 799 (2020) 012042 doi:10.1088/1757-899X/799/1/012042\n\n# 4.4. Experimental results\n\nWe analyze the experimental results with AUC and MSE. AUC is an evaluation value to measure the pros and cons of the model, indicating the probability that the positive example of the prediction is in front of the negative example. Performance of the model is better when this value is higher. MSE is the mean square error, which is the expected value of the square of the difference between the predicted value and the real value, which we expect the lower, the better. As shown in Table 1, the KSR model obtains the highest AUC value and the lowest MSE value and it achieves best results than the baselines.\n\nTable 1. Comparison of different models  \n\n<table><tr><td></td><td>AUC</td><td>MSE</td></tr><tr><td>KSR</td><td>58.38</td><td>0.2406</td></tr><tr><td>FM</td><td>44.96</td><td>0.2428</td></tr><tr><td>DKN</td><td>57.65</td><td>0.2419</td></tr><tr><td>DMF</td><td>53.90</td><td>0.2591</td></tr></table>\n\nWe will analyze the model and conduct comparative experiments from the following four dimensions. 1. In Section 3.1, we mentioned that we not only use the vector of the entity in the news but also introduce the context entity. We will experimentally verify whether this works. 2. There is a well-known variant of LSTM--GRU, we will compare which effect is better. 3. We introduced the attention model into the model. We will verify this in our experiment. 4. In Section 2.2, we introduced four KGE methods, and we will figure out that which one has best effect. We performed experiments on these four cases on the KSR model, and the experimental results are shown in Table 2. The first row in the table is the standard KSR model, using LSTM to capture sequential relationships and using TransD model to get the knowledge graph embedding. The second and third rows in the table prove the necessity of introducing the context vector and attention module. The fourth row in the table is the result of sequential capture by using GRU method, which proves that LSTM achieves better results than GRU. The fifth, sixth, and seventh rows in the table are the results of using different Trans methods, which proves the superiority of using TransD.",
  "hyperparameter": "Optimization algorithm: Adam; Loss function: cross entropy loss + regularization loss (Loss = Σ-[y*log(ŷ) + (1-y)*log(1-ŷ)] + θ*Loss_regularization); Knowledge graph embedding: TransD (best performing among TransE, TransH, TransR, TransD); Sequential model: LSTM (better than GRU variant); Word embedding: word2vec using Gensim library; Context entity embedding: average of neighbor entities; Implementation: Python 3.6, TensorFlow 1.14, FastTransX for KGE module. Specific hyperparameter values (learning rate, embedding dimensions, LSTM hidden size, etc.) are not explicitly mentioned in the provided text."
}