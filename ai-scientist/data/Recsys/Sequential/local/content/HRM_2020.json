{
  "id": "HRM_2020",
  "paper_title": "Learning Hierarchical Representation Model for Next Basket Recommendation",
  "alias": "HRM",
  "year": 2020,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "",
  "introduction": "# 1. INTRODUCTION\n\nMarket basket analysis helps retailers gain a better understanding of users' purchase behavior which can lead to better decisions. One of its most important tasks is next basket recommendation [7, 8, 12, 20]. In this task, usually sequential transaction data is given per user, where a transaction is a set/basket of items (e.g. shoes or bags) bought at one point of time. The target is to recommend items that the user probably want to buy in his/her next visit.\n\nTypically, there are two modeling paradigms for this problem. One is sequential recommender [5, 25], mostly relying on Markov chains, which explores the sequential transaction data by predicting the next purchase based on the last actions. A major advantage of this model is its ability to capture sequential behavior for good recommendations, e.g. for a user who has recently bought a mobile phone, it may recommend accessories that other users have bought after buying that phone. The other is general recommender [1, 23], which discards any sequential information and learns what items a user is typically interested in. One of the most successful methods in this class is the model based collaborative filtering (i.e. matrix factorization models). Obviously, such general recommender is good at capturing the general taste of the user by learning over the user's whole purchase history.\n\nA better solution for next basket recommendation, therefore, is to take both sequential behavior and users' general taste into consideration. One step towards this direction is the factorizing personalized Markov chains (FPMC) model proposed by Steffen Rendle et al. [23]. FPMC can model both sequential behavior (by interaction between items in the last transaction and that in the next basket) and users' general taste (by interaction between the user and the item in the next basket), thus achieves better performance than either sequential or general recommender alone. However, a major problem of FPMC is that all the components are linearly combined, indicating that it makes strong independent assumption among multiple factors (i.e. each component influence users' next purchase independently).\n\nUnfortunately, from our analysis, we show that the independent assumption is not sufficient for good recommendations.\n\nTo tackle the above problems, we introduce a novel hierarchical representation model (HRM) for next basket recommendation. Specifically, HRM represents each user and item as a vector in continuous space, and employs a two-layer structure to construct a hybrid representation over user and items from last transaction: The first layer forms the trans\n\naction representation by aggregating item vectors from last transaction; While the second layer builds the hybrid representation by aggregating the user vector and the transaction representation. The resulting hybrid representation is then used to predict the items in the next basket. Note here the transaction representation involved in recommendation models the sequential behavior, while the user representation captures the general taste in recommendation.\n\nHRM allows us to flexibly use different types of aggregation operations at different layers. Especially, by employing nonlinear rather than linear operations, we can model more complicated interactions among different factors beyond independent assumption. For example, by using a max pooling operation, features from each factor are compared and only those most significant are selected to form the higher level representation for future prediction. We also show that by choosing proper aggregation operations, HRM subsumes several existing methods including markov chain model, matrix factorization model as well as a variation of FPMC model. For learning the model parameters, we employ the negative sampling procedure [27] as the optimization method.\n\nWe conducted experiments over three real-world transaction datasets. The empirical results demonstrated the effectiveness of our approach as compared with the state-of-the-art baseline methods.\n\nIn total the contributions of our work are as follows:\n\n- We introduce a general model for next basket recommendation which can capture both sequential behavior and users' general taste, and flexibly incorporate different interactions among multiple factors.  \n- We introduce two types of aggregation operations, i.e. average pooling and max pooling, into our hierarchical model and study the effect of different combinations of these operations.  \n- Theoretically we show that our model subsumes several existing recommendation methods when choosing proper aggregation operations.  \n- Empirically we show that our model, especially with nonlinear operations, can consistently outperform state-of-the-art baselines under different evaluation metrics on next basket recommendation.",
  "method": "# 4. OUR APPROACH\n\nIn this section, we first introduce the problem formalization of next basket recommendation. We then describe the proposed HRM in detail. After that, we talk about the learning and prediction procedure of HRM. Finally, we discuss the connections of HRM to existing methods.\n\n# 4.1 Formalization\n\nLet  $U = \\{u_{1}, u_{2}, \\ldots, u_{|U|}\\}$  be a set of users and  $I = \\{i_{1}, i_{2}, \\ldots, i_{|I|}\\}$  be a set of items, where  $|U|$  and  $|I|$  denote the total number of unique users and items, respectively. For each user  $u$ , a purchase history  $T^{u}$  of his transactions is given by  $T^{u} := (T_{1}^{u}, T_{2}^{u}, \\ldots, T_{t_{u}-1}^{u})$ , where  $T_{t}^{u} \\subseteq I$ ,  $t \\in [1, t_{u}-1]$ . The purchase history of all users is denoted as  $T := \\{T^{u_{1}}, T^{u_{2}}, \\ldots, T^{u_{|U|}}\\}$ . Given this history, the task is to recommend items that user  $u$  would probably buy at the next (i.e.  $t_{u}$ -th) visit. The next basket recommendation task can then be formalized as creating a personalized total ranking  $>_{u,t} \\subset I^{2}$  for user  $u$  and  $t_{u}$ -th transaction. With this ranking, we can recommend the top  $n$  items to the user.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/284c347b54c1380edda8499de9c835e7619e74890beb5bee44d92a312cca8ac1.jpg)  \nFigure 2: The HRM model architecture. A two-layer structure is employed to construct a hybrid representation over user and items from last transaction, which is used to predict the next purchased items.\n\n# 4.2 HRM Model\n\nTo solve the above recommendation problem, here we present the proposed HRM in detail. The basic idea of our work is to learn a recommendation model that can involve both sequential behavior and users' general taste, and meanwhile modeling complicated interactions among these factors in prediction.\n\nSpecifically, HRM represents each user and item as a vector in a continuous space, and employs a two-layer structure to construct a hybrid representation over user and items from last transaction: The first layer forms the transaction representation by aggregating item vectors from last transaction; While the second layer builds the hybrid representation by aggregating the user vector and the transaction representation. The resulting hybrid representation is then used to predict the items in the next basket. The hierarchical structure of HRM is depicted in Figure 2. As we can see, HRM captures the sequential behavior by modeling the consecutive purchases, i.e. constructing the representation of the last transaction from its items for predicting the next purchase. At the same time, by integrating a personalized user representation in sequential recommendation, HRM also models the user's general taste.\n\nMore formally, let  $V^{U} = \\{\\vec{v}_{u}^{U}\\in \\mathbb{R}^{n}|u\\in U\\}$  denote all the user vectors and  $V^{I} = \\{\\vec{v}_{i}^{I}\\in \\mathbb{R}^{n}|i\\in I\\}$  denote all the item vectors. Note here  $V^{U}$  and  $V^{I}$  are model parameters to be learned by HRM. Given a user  $u$  and two consecutive transactions  $T_{t - 1}^{u}$  and  $T_{t}^{u}$ , HRM defines the probability of buying next item  $i$  given user  $u$  and his/her last transaction  $T_{t - 1}^{u}$  via a softmax function:\n\n$$\np \\left(i \\in T _ {t} ^ {u} \\mid u, T _ {t - 1} ^ {u}\\right) = \\frac {\\exp \\left(\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {u , t - 1} ^ {H y b r i d}\\right)}{\\sum_ {j = 1} ^ {| I |} \\exp \\left(\\vec {v} _ {j} ^ {I} \\cdot \\vec {v} _ {u , t - 1} ^ {H y b r i d}\\right)} \\tag {1}\n$$\n\nwhere  $\\vec{v}_{u,t-1}^{Hybrid}$  denotes the hybrid representation obtained from the hierarchical aggregation which is defined as follows\n\n$$\n\\vec {v} _ {u, t - 1} ^ {H y b r i d} := f _ {2} (\\vec {v} _ {u} ^ {U}, f _ {1} (\\vec {v} _ {l} ^ {I} \\in T _ {t - 1} ^ {u}))\n$$\n\nwhere  $f_{1}(\\cdot)$  and  $f_{2}(\\cdot)$  denote the aggregation operation at the first and second layer, respectively.\n\nOne advantage of HRM is that we can introduce various aggregation operations in forming higher level representation from lower level. In this way, we can model differ\n\nent interactions among multiple factors at different layers, i.e. interaction among items forming the transaction representation at the first layer, as well as interaction between user and transaction representations at the second layer. In this work, we study two typical aggregation operations as follows.\n\n- average pooling: To aggregate a set of vector representations, average pooling construct one vector by taking the average value of each dimension. Let  $V = \\{\\vec{v}_l \\in \\mathbb{R}^n | l = 1, \\dots, |V|\\}$  be a set of input vectors to be aggregated, average pooling over  $V$  can be formalized as\n\n$$\nf _ {a v g} (V) = \\frac {1}{| V |} \\sum_ {l = 1} ^ {| V |} \\vec {v} _ {l}\n$$\n\nObviously, average pooling is a linear operation, which assumes the independence among input representations in forming higher level representation.\n\n- max pooling: To aggregate a set of vector representations, max pooling constructs one vector by taking the maximum value of each dimension, which can be formalized as\n\n$$\nf _ {m a x} (V) = \\left[ \\begin{array}{c} m a x (\\vec {v} _ {1} [ 1 ], \\dots , \\vec {v} _ {| V |} [ 1 ]) \\\\ m a x (\\vec {v} _ {1} [ 2 ], \\dots , \\vec {v} _ {| V |} [ 2 ]) \\\\ \\vdots \\\\ m a x (\\vec {v} _ {1} [ n ], \\dots , \\vec {v} _ {| V |} [ n ]) \\end{array} \\right]\n$$\n\nwhere  $\\vec{v}_l[k]$  denotes the  $k$ -th dimension in  $\\vec{v}_l$ . In Contrary to average pooling, max pooling is a nonlinear operation which models interactions among input representations, i.e. features from each input vector are compared and only those most significant features will be selected to the next level. Take the movie recommender mentioned in Section 3.1 for example, we suppose vector representations are used for both sequential and general factors. If there are two dimensions capturing the genre and actor/actress preference respectively, max pooling then selects the most significant feature in each dimension (e.g. science fiction and Scarlett Johansson) in aggregating the two vectors.\n\nNote that there are other ways to define the aggregation operations, e.g. top-k average pooling or Hadamard product. We may study these operations in the future work. Besides, one may also consider to introduce nonlinear hidden layers as in deep neural network [4]. However, we resort to simple models since previous work has demonstrated that such models can learn accurate representations from very large data set due to low computational complexity [17, 27].\n\nSince there are two-layer aggregations in HRM, we thus can obtain four versions of HRM based on different combinations of operations, namely  $\\mathrm{HRM}_{AvgAvg}$ ,  $\\mathrm{HRM}_{MaxAvg}$ ,  $\\mathrm{HRM}_{AvgMax}$ , and  $\\mathrm{HRM}_{MaxMax}$ , where the two abbreviations in subscript denote the first and second layer aggregation operation respectively. For example,  $\\mathrm{HRM}_{AvgMax}$  denotes the model that employs average pooling at the first layer and max pooling at second layer.\n\nAs we can see, these four versions of HRM actually assume different strength of interactions among multiple factors. By only using average pooling,  $\\mathrm{HRM}_{AvgAvg}$  assume independence among all the factors. We later show that  $\\mathrm{HRM}_{AvgAvg}$  can be viewed as some variation of FPMC.\n\nBoth  $\\mathrm{HRM}_{AvgMax}$  and  $\\mathrm{HRM}_{MaxAvg}$  introduce partial interactions, either among the items in last transaction or between the user and transaction representations. Finally, by using nonlinear operations at both layers,  $\\mathrm{HRM}_{MaxMax}$  assumes full interactions among all the factors.\n\n# 4.3 Learning and Prediction\n\nIn learning, HRM maximizes the log probability defined in Equation (1) over the transaction data of all users as follows\n\n$$\n\\ell_ {H R M} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\log p (i \\in T _ {t} ^ {u} | u, T _ {t - 1} ^ {u}) - \\lambda \\| \\Theta \\| _ {F} ^ {2}\n$$\n\nwhere  $\\lambda$  is the regularization constant and  $\\Theta$  are the model parameters (i.e.  $\\Theta = \\{V^U,V^I\\}$ ). As defined in Section 4.1, the goal of next basket recommendation is to derive a ranking  $>_{u,t}$  over items. HRM actually defines the ranking as\n\n$$\ni > _ {u, t} i ^ {\\prime}: \\Leftrightarrow p \\left(i \\in T _ {t} ^ {u} | u, T _ {t - 1} ^ {u}\\right) > p \\left(i ^ {\\prime} \\in T _ {t} ^ {u} | u, T _ {t - 1} ^ {u}\\right)\n$$\n\nand attempts to derive such ranking by maximizing the buying probability of next items over the whole purchase history.\n\nHowever, directly optimizing the above objective function is impractical because the cost of computing the full softmax is proportional to the size of items  $|I|$ , which is often extremely large. Therefore, we adopt the negative sampling technique [21, 27] for efficient optimization, which approximates the original objective  $\\ell_{HRM}$  with the following objective function\n\n$$\n\\begin{array}{l} \\ell_ {N E G} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\Big (\\log \\sigma (\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {u, t - 1} ^ {H y b r i d}) \\\\ \\left. + k \\cdot \\mathbb {E} _ {i ^ {\\prime} \\sim P _ {I}} [ \\log \\sigma (- \\vec {v} _ {i ^ {\\prime}} ^ {I} \\cdot \\vec {v} _ {u, t - 1} ^ {H y b r i d}) ]\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} \\\\ \\end{array}\n$$\n\nwhere  $\\sigma(x) = 1 / (1 + e^{-x})$ ,  $k$  is the number of \"negative\" samples, and  $i'$  is the sampled item, drawn according to the noise distribution  $P_I$  which is modeled by empirical unigram distribution over items. As we can see, the objective of HRM with negative sampling aims to derive the ranking  $>_{u,t}$  in a discriminative way by maximizing the probability of observed item  $i$  and meanwhile minimizing the probability of unobserved item  $i'$ 's.\n\nWe then apply stochastic gradient descent algorithm to maximize the new objective function for learning the model. Moreover, when learning the nonlinear models, we also adopt Dropout technique to avoid overfitting. In our work, we simply set a fixed drop ratio (50%) for each unit.\n\nWith the learned user and item vectors, the next basket recommendation with HRM is as follows. Given a user  $u$  and his/her last transaction  $T_{t_u - 1}^u$ , for each candidate item  $i \\in I$ , we calculate the probability  $p(i \\in I | u, T_{t_u - 1}^u)$  according to Equation (1). We then rank the items according to their probabilities, and select the top  $n$  results as the final recommendations to the user.\n\n# 4.4 Connection to Previous Models\n\nIn this section, we discuss the connection of the proposed HRM to previous work. We show that by choosing proper aggregation operations, HRM subsumes several existing methods including Markov chain model, matrix factorization model as well as a variation of FPMC model.\n\n# 4.4.1 HRM vs. Markov Chain Model\n\nTo show that HRM can be reduced to a certain type of Markov chain model, we first introduce a special aggregation\n\noperation, namely select-copy operation. When aggregating a set of vector representations, the select-copy operation selects one of the vectors according to some criterion, and copy it as the aggregated one. Now we apply this operation to both levels of HRM. Specifically, when constructing the transaction representation from item vectors, the operation randomly selects one item vector and copies it. When combining the user and transaction representations, the operation always selects and copies the transaction vector. We refer the HRM with this model architecture as  $\\mathrm{HRMCopyItem}$ . The new objective function of  $\\mathrm{HRMCopyItem}$  using negative sampling is as follows:\n\n$$\n\\begin{array}{l} \\ell_ {C o p y I t e m} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\left(\\log \\sigma (\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {s} ^ {I}) \\right. \\\\ \\left. + k \\cdot \\mathbb {E} _ {i ^ {\\prime} \\sim P _ {I}} \\left[ \\log \\sigma \\left(- \\vec {v} _ {i ^ {\\prime}} ^ {I} \\cdot \\vec {v} _ {s} ^ {I}\\right) \\right]\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} \\\\ \\end{array}\n$$\n\nwhere  $\\vec{v}_s^I$  denotes the vector of randomly selected item in last transaction.\n\nSimilar as the derivation in [21], we can show that the solution of  $\\mathrm{HRM}_{\\text{CopyItem}}$  follows that\n\n$$\n\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {s} ^ {I} = P M I (v _ {i} ^ {I}, v _ {s} ^ {I}) - \\log k\n$$\n\nwhich indicates that  $\\mathrm{HRM}_{CopyItem}$  is actually a factorized Markov chain model (FMC) [23], which factorizes a transition matrix between items from two consecutive transactions with the association measured by shifted PMI (i.e.  $PMI(x,y)-\\log k$ ). When  $k = 1$ , the transition matrix becomes a PMI matrix.\n\nIn fact, if we employ noise contrastive estimation [27] for optimization, the solution then follows that:\n\n$$\n\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {s} ^ {I} = \\log P (v _ {i} ^ {I} | v _ {s} ^ {I}) - \\log k\n$$\n\nwhich indicates the transition matrix factorized by  $\\mathrm{HRM}_{CopyItem}$  become a (shifted) log-conditional-probability matrix.\n\n# 4.4.2 HRM vs. Matrix Factorization Model\n\nNow we only apply the select-copy operation to the second layer (i.e. aggregation over user and transaction representations), and this time we always select and copy user vector. We refer this model as  $\\mathrm{HRM}_{CopyUser}$ . The corresponding objective function using negative sampling is as follows:\n\n$$\n\\begin{array}{l} \\ell_ {C o p y U s e r} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\left(\\log \\sigma (\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {u} ^ {U}) \\right. \\\\ \\left. + k \\cdot \\mathbb {E} _ {i ^ {\\prime} \\sim P _ {I}} \\left[ \\log \\sigma \\left(- \\vec {v} _ {i ^ {\\prime}} ^ {I} \\cdot \\vec {v} _ {u} ^ {U}\\right) \\right]\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} \\\\ \\end{array}\n$$\n\nAgain, we can show that  $\\mathrm{HRM}_{\\text{CopyUser}}$  has the solution in the following form:\n\n$$\n\\vec {v} _ {u} ^ {U} \\cdot \\vec {v} _ {i} ^ {I} = P M I (v _ {u} ^ {U}, v _ {i} ^ {I}) - \\log k\n$$\n\nIn this way,  $\\mathrm{HRM}_{\\text{CopyUser}}$  reduces to a matrix factorization model, which factorizes a user-item matrix where the association between a user and a item is measured by shifted PMI.\n\n# 4.4.3 HRM vs. FPMC\n\nFPMC conducts a tensor factorization over the transition cube constructed from the transition matrices of all users. It is optimized under the Bayesian personalized ranking (BPR) criterion and the objective function using MAP-estimator is\n\nTable 1: Statistics of the datasets used in our experiments.  \n\n<table><tr><td>dataset</td><td>users |U|</td><td>items |I|</td><td>transactions T</td><td>avg_transaction size</td><td>avg_transaction per user</td></tr><tr><td>Ta-Feng</td><td>9238</td><td>7982</td><td>67964</td><td>7.4</td><td>5.9</td></tr><tr><td>BeiRen</td><td>9321</td><td>5845</td><td>91294</td><td>9.7</td><td>5.8</td></tr><tr><td>T-Mall</td><td>292</td><td>191</td><td>1805</td><td>5.6</td><td>1.2</td></tr></table>\n\nas follows [23]:\n\n$$\n\\ell_ {F P M C} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\sum_ {i ^ {\\prime} \\notin T _ {t} ^ {u}} \\log \\sigma \\left(\\hat {x} _ {u, t, i} - \\hat {x} _ {u, t, i ^ {\\prime}}\\right)\\rightarrow \\lambda \\| \\Theta \\| _ {F} ^ {2} \\tag {2}\n$$\n\nwhere  $\\hat{x}_{u,t,i}$  denotes the prediction model\n\n$$\n\\begin{array}{l} \\hat {x} _ {u, t, i} \\quad := \\quad \\hat {p} (i \\in T _ {t} ^ {u} | u, T _ {t - 1} ^ {u}) \\\\ := \\vec {v} _ {u} ^ {U} \\cdot \\vec {v} _ {i} ^ {I} + \\frac {1}{\\left| T _ {t - 1} ^ {u} \\right|} \\sum_ {l \\in T _ {t - 1} ^ {u}} \\left(\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {l} ^ {I}\\right) \\tag {3} \\\\ \\end{array}\n$$\n\nTo see the connection between HRM and FPMC, we now set the aggregation operation as average pooling at both layers and apply negative sampling with  $k = 1$ . We denote this model as  $\\mathrm{HRM}_{AvgAvgNEG1}$  and its objective function is as follows\n\n$$\n\\begin{array}{l} \\ell_ {A v g A v g N E G 1} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\left(\\log \\sigma (\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {u, t - 1} ^ {H y b r i d}) \\right. \\\\ \\left. + \\mathbb {E} _ {i ^ {\\prime} \\sim P _ {I}} \\left[ \\log \\sigma \\left(- \\vec {v} _ {i ^ {\\prime}} ^ {I} \\cdot \\vec {v} _ {u, t - 1} ^ {H y b r i d}\\right) \\right]\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} \\\\ = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\sum_ {i ^ {\\prime} \\notin T _ {t} ^ {u}} \\left(\\log \\sigma (\\vec {v} _ {i} ^ {I} \\cdot \\vec {v} _ {u, t - 1} ^ {H y b r i d}) \\right. \\\\ \\left. + \\log \\sigma \\left(- \\vec {v} _ {i ^ {\\prime}} ^ {I} \\cdot \\vec {v} _ {u, t - 1} ^ {H h y b r i d}\\right)\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} \\tag {4} \\\\ \\end{array}\n$$\n\nwhere\n\n$$\n\\vec {v} _ {u, t - 1} ^ {H y b r i d} = \\frac {1}{2} \\left(\\vec {v} _ {u} ^ {U} + \\frac {1}{\\left| T _ {t - 1} ^ {u} \\right|} \\sum_ {l \\in T _ {t - 1} ^ {u}} \\vec {v} _ {l} ^ {I}\\right) \\tag {5}\n$$\n\nWith Equation (3) and (5), we can rewrite Equation (4) as follows\n\n$$\n\\begin{array}{l} \\ell_ {A v g A v g N E G 1} = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {\\prime u}} \\sum_ {i \\in T _ {t} ^ {u}} \\sum_ {i ^ {\\prime} \\notin T _ {t} ^ {u}} \\Big (\\log \\sigma \\left(\\hat {x} _ {u, t, i}\\right) \\\\ \\left. + \\log \\sigma \\left(- \\hat {x} _ {u, t, i ^ {\\prime}}\\right)\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} + C \\\\ = \\sum_ {u \\in U} \\sum_ {T _ {t} ^ {u} \\in T ^ {u}} \\sum_ {i \\in T _ {t} ^ {u}} \\sum_ {i ^ {\\prime} \\notin T _ {t} ^ {u}} \\left(\\log \\sigma \\left(\\hat {x} _ {u, t, i}\\right) \\right. \\\\ \\left. + \\log \\left(1 - \\sigma \\left(\\hat {x} _ {u, t, i ^ {\\prime}}\\right)\\right)\\right) - \\lambda \\| \\Theta \\| _ {F} ^ {2} + C (6) \\\\ \\end{array}\n$$\n\nBased on the above derivations, we can see that both  $\\mathrm{HRM}_{AvgAvgNEG1}$  and FPMC share the same prediction model denoted by Equation (3), but optimize with slightly different criteria. FPMC tries to maximize the pairwise rank, i.e. an observed item  $i$  ranks higher than an unobserved item  $i'$ , by defining the pairwise probability using a logistic function as shown in Equation (2). While  $\\mathrm{HRM}_{AvgAvgNEG1}$  also optimizes this pairwise rank by maximizing the probability of item  $i$  and minimizing the probability of item  $i'$ , each defined in a logistic form as shown in Equation (6). In fact, we can also adopt BPR criterion to define the objective function of  $\\mathrm{HRM}_{AvgAvg}$ , and obtain the same model as FPMC.\n\nBased on all the above analysis, we can see that the proposed HRM is actually a very general model. By introducing\n\ndifferent aggregation operations, we can produce multiple recommendation models well connected to existing methods. Moreover, HRM also allows us to explore other prediction functions as well as optimization criteria, showing large flexibility and promising potential.",
  "experiments": "# 5. EVALUATION\n\nIn this section, we conduct empirical experiments to demonstrate the effectiveness of our proposed HRM on next basket recommendation. We first introduce the dataset, baseline methods, and the evaluation metrics employed in our experiments. Then we compare the four versions of HRM to study the effect of different combinations of aggregation operations. After that, we compare our HRM to the state-of-the-art baseline methods to demonstrate its effectiveness. Finally, we conduct some analysis on our optimization procedure, i.e. negative sampling technique.\n\n# 5.1 Dataset\n\nWe evaluate different recommenders based on three real-world transaction datasets, i.e. two retail datasets Ta-Feng and BeiRen, and one e-commerce dataset T-Mall.\n\n- The Ta-Feng $^{1}$  dataset is a public dataset released by RecSys conference, which covers products from food, office supplies to furniture. It contains 817,741 transactions belonging to 32,266 users and 23,812 items.  \n- The BeiRen dataset comes from BeiGuoRenBaiÂ², a large retail enterprise in China, which records its supermarket purchase history during the period from Jan. 2013 to Sept. 2013. It contains 1,123,754 transactions belonging to 34,221 users and 17,920 items.  \n- The T-Mall<sup>3</sup> dataset is a public online e-commerce dataset released by Taobao<sup>4</sup>, which records the online transactions in terms of brands. It contains 4298 transactions belonging to 884 users and 9,531 brands.\n\nWe first conduct some pre-process on these transaction datasets similar as [23]. For both Ta-Feng and BeiRen dataset, we remove all the items bought by less than 10 users and users that has bought in total less than 10 items. For the T-Mall dataset, which is relatively smaller, we remove all the items bought by less than 3 users and users that has bought in total less than 3 items. The statistics of the three datasets after pre-processing are shown in Table 1.\n\nFinally, we split all the datasets into two non-overlapping set, i.e. a training set and a testing set. The testing set contains only the last transaction of each user, while all the remaining transactions are put into the training set.\n\n<sup>1</sup> http://recsyswiki.com/wiki/Grocery_shopping_datasets  \n<sup>2</sup>http://www.brtj.cn/  \n<sup>3</sup>http://102.alibaba.com/competition/addDiscovery/index.htm  \n<sup>4</sup>http://www.taobao.com\n\nTable 2: Performance comparison among four versions of HRM over three datasets  \n(a) Performance comparison on Ta-Feng  \n\n<table><tr><td rowspan=\"2\">Models</td><td colspan=\"3\">d=50</td><td colspan=\"3\">d=100</td><td colspan=\"3\">d=150</td><td colspan=\"3\">d=200</td></tr><tr><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td></tr><tr><td>HRMAvgAvg</td><td>0.051</td><td>0.240</td><td>0.073</td><td>0.060</td><td>0.276</td><td>0.082</td><td>0.063</td><td>0.283</td><td>0.080</td><td>0.063</td><td>0.286</td><td>0.086</td></tr><tr><td>HRMMaxAvg</td><td>0.059</td><td>0.275</td><td>0.080</td><td>0.064</td><td>0.279</td><td>0.087</td><td>0.065</td><td>0.290</td><td>0.083</td><td>0.067</td><td>0.298</td><td>0.086</td></tr><tr><td>HRMAvgMax</td><td>0.057</td><td>0.262</td><td>0.080</td><td>0.064</td><td>0.288</td><td>0.085</td><td>0.065</td><td>0.289</td><td>0.082</td><td>0.068</td><td>0.293</td><td>0.090</td></tr><tr><td>HRMMaxMax</td><td>0.062</td><td>0.282</td><td>0.089</td><td>0.065</td><td>0.293</td><td>0.088</td><td>0.068</td><td>0.298</td><td>0.085</td><td>0.070</td><td>0.312</td><td>0.093</td></tr></table>\n\n(b) Performance comparison on BeiRen  \n\n<table><tr><td rowspan=\"2\">Models</td><td colspan=\"3\">d=50</td><td colspan=\"3\">d=100</td><td colspan=\"3\">d=150</td><td colspan=\"3\">d=200</td></tr><tr><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td></tr><tr><td>HRMAvgAvg</td><td>0.100</td><td>0.463</td><td>0.119</td><td>0.107</td><td>0.475</td><td>0.128</td><td>0.112</td><td>0.505</td><td>0.137</td><td>0.113</td><td>0.509</td><td>0.137</td></tr><tr><td>HRMMaxAvg</td><td>0.105</td><td>0.485</td><td>0.131</td><td>0.113</td><td>0.498</td><td>0.138</td><td>0.115</td><td>0.509</td><td>0.139</td><td>0.115</td><td>0.505</td><td>0.141</td></tr><tr><td>HRMAvgMax</td><td>0.106</td><td>0.494</td><td>0.131</td><td>0.114</td><td>0.512</td><td>0.140</td><td>0.115</td><td>0.510</td><td>0.141</td><td>0.115</td><td>0.510</td><td>0.140</td></tr><tr><td>HRMMaxMax</td><td>0.111</td><td>0.501</td><td>0.134</td><td>0.115</td><td>0.515</td><td>0.144</td><td>0.117</td><td>0.516</td><td>0.146</td><td>0.118</td><td>0.515</td><td>0.145</td></tr></table>\n\n(c) Performance comparison on T-Mall  \n\n<table><tr><td rowspan=\"2\">Models</td><td colspan=\"3\">d=10</td><td colspan=\"3\">d=15</td><td colspan=\"3\">d=20</td><td colspan=\"3\">d=25</td></tr><tr><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td><td>F1-score</td><td>Hit-ratio</td><td>NDCG</td></tr><tr><td>HRMAvgAvg</td><td>0.052</td><td>0.154</td><td>0.119</td><td>0.055</td><td>0.139</td><td>0.146</td><td>0.061</td><td>0.180</td><td>0.146</td><td>0.063</td><td>0.186</td><td>0.151</td></tr><tr><td>HRMMaxAvg</td><td>0.062</td><td>0.186</td><td>0.133</td><td>0.063</td><td>0.148</td><td>0.157</td><td>0.066</td><td>0.196</td><td>0.154</td><td>0.068</td><td>0.202</td><td>0.158</td></tr><tr><td>HRMAvgMax</td><td>0.061</td><td>0.186</td><td>0.133</td><td>0.063</td><td>0.148</td><td>0.153</td><td>0.064</td><td>0.191</td><td>0.157</td><td>0.066</td><td>0.196</td><td>0.159</td></tr><tr><td>HRMMaxMax</td><td>0.065</td><td>0.191</td><td>0.142</td><td>0.066</td><td>0.197</td><td>0.163</td><td>0.070</td><td>0.207</td><td>0.163</td><td>0.071</td><td>0.212</td><td>0.168</td></tr></table>\n\n# 5.2 Baseline Methods\n\nWe evaluate our model by comparing with several state-of-the-art methods on next-basket recommendation:\n\n- TOP: The top popular items in training set are taken as recommendations for each user.  \n- MC: A Markov chain model (i.e. sequential recommender) which predicts the next purchase based on the last transaction of the user. The prediction model is as follows:\n\n$$\np \\left(i \\in T _ {t _ {u}} ^ {u} \\mid T _ {t _ {u} - 1} ^ {u}\\right) := \\frac {1}{\\left| T _ {t _ {u} - 1} ^ {u} \\right|} \\sum_ {l \\in T _ {t _ {u} - 1} ^ {u}} p \\left(i \\in T _ {t _ {u}} ^ {u} \\mid l \\in T _ {t _ {u} - 1} ^ {u}\\right)\n$$\n\nThe transition probability of buying an item based on the last purchase is estimated from the training set.\n\n- NMF: A state-of-the-art model based collaborative filtering method [14]. Here Nonnegative Matrix Factorization is applied over the user-item matrix, which is constructed from the transaction dataset by discarding the sequential information. For implementation, we adopt the publicly available codes from NMF:DTU Toolbox<sup>5</sup>.  \n- FPMC: A state-of-the-art hybrid model on next basket recommendation [23]. Both sequential behavior and users' general taste are taken into account for prediction.\n\nFor NMF, FPMC and our  $\\mathrm{HRM}^6$  methods, we run several times with random initialization by setting the dimensionality  $d\\in \\{50,100,150,200\\}$  on Ta-Feng and BeiRen datasets, and  $d\\in \\{10,15,20,25\\}$  on T-Mall dataset. We compare the best results of different methods and demonstrate the results in the following sections.\n\n# 5.3 Evaluation Metrics\n\nThe performance is evaluated for each user  $u$  on the transaction  $T_{t_u}^u$  in the testing dataset. For each recommendation method, we generate a list of  $N$  items  $(N = 5)$  for each user  $u$ , denoted by  $R(u)$ , where  $R_i(u)$  stands for the item recommended in the  $i$ -th position. We use the following quality measures to evaluate the recommendation lists against the actual bought items.\n\n- F1-score: F1-score is the harmonic mean of precision and recall, which is a widely used measure in recommendation [9, 15, 23]:\n\n$$\n\\operatorname {P r e c i s o n} \\left(T _ {t _ {u}} ^ {u}, R (u)\\right) = \\frac {\\left| T _ {t _ {u}} ^ {u} \\cap R (u) \\right|}{\\left| R (u) \\right|}\n$$\n\n$$\n\\mathrm {R e c a l l} (T _ {t _ {u}} ^ {u}, R (u)) = \\frac {| T _ {t _ {u}} ^ {u} \\bigcap R (u) |}{| T _ {t _ {u}} ^ {u} |}\n$$\n\n$$\n\\mathrm {F 1 - s c o r e} = \\frac {2 \\times \\text {P r e c i s i o n} \\times \\text {R e c a l l}}{\\text {P r e c i s i o n} + \\text {R e c a l l}}\n$$\n\n- Hit-Ratio: Hit-Ratio is a All-but-One measure used in recommendation [13, 28]. If there is at least one item in the test transaction also appears in the recommendation list, we call it a hit. The Hit-Ratio is calculated in the following way:\n\n$$\n\\text {H i t - R a t i o} = \\frac {\\sum_ {u \\in U} I \\left(T _ {t _ {u}} ^ {u} \\cap R (u) \\neq \\phi\\right)}{| U |}\n$$\n\nwhere  $I(\\cdot)$  is an indicator function and  $\\phi$  denotes the empty set. Hit-Ratio focuses on the recall of a recommender system, i.e. how many people can obtain at least one correct recommendation.\n\n- NDCG@k: Normalized Discounted Cumulative Gain (NDCG) is a ranking based measure which takes into\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/69b865b5517478c65210ecb187fdd676c90af4707b29a86b4e457de23ee24651.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/7c1da681bf435e53c04d06b5a7904b1e13d16d6fa42c61b1296a897edf716b94.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/0a738a7051a52a0a3ad24570a404cf7e6a62b0d0c1d899665d21d47a129fc444.jpg)  \n(a) Ta-Feng\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/db481dc7c642e2a3564af9823da881f59bef05a7e5bd7cd34ea44c7476235c59.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/954b74033c35c61802bfade4bfc26741df1a2243ceff0c58b9e79a5fddac51ae.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/db26d2ee7d21d308c999d298003f9668c553bb4777fc45dcc717997fac2635c0.jpg)  \n(b) BeiRen\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/036c0eafa0cec65355adb33bba78c9296c634830fee14b71d0eb22f720f86b25.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/643e705d235db0da904694d0f41ad77a61fe1d74f3d0c5e278e1b7c5bceb8385.jpg)  \nFigure 3: Performance comparison of HRM among TOP,MC,NMF, and FPMC over three datasets. The dimensionality is increased from 50 to 200 on Ta-Feng and BeiRen, and 10 to 25 on T-Mall.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/5c609cbaadeeb0f4eb71e0793006bf3d38a9666da9254a51313feb48775ba9bf.jpg)  \n(c) T-Mall\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/7729112eaf310804eef9ca0515f7617271c55facdf3e4138b31a54fc7843731c.jpg)\n\naccount the order of recommended items in the list[11], and is formally given by:\n\n$$\nN D C G @ k = \\frac {1}{N _ {k}} \\sum_ {j = 1} ^ {k} \\frac {2 ^ {I (R _ {j} (u) \\in T _ {t _ {u}} ^ {u})} - 1}{\\log_ {2} (j + 1)}\n$$\n\nwhere  $I(\\cdot)$  is an indicator function and  $N_{k}$  is a constant which denotes the maximum value of NDCG@k given  $R(u)$ .\n\n# 5.4 Comparison among Different HRMs\n\nWe first empirically compare the performance of the four versions of HRM, referred to as  $\\mathrm{HRM}_{AvgAvg}$ ,  $\\mathrm{HRM}_{MaxAvg}$ ,  $\\mathrm{HRM}_{AvgMax}$ ,  $\\mathrm{HRM}_{MaxMax}$ . The results over three datasets are shown in Table 2.\n\nAs we can see,  $\\mathrm{HRM}_{AvgAvg}$ , which only uses average pooling operations in aggregation, performs the worst among the four models. It indicates that by assuming independence among all the factors, we may not be able to learn a good recommendation model. Both  $\\mathrm{HRM}_{MaxAvg}$  and  $\\mathrm{HRM}_{AvgMax}$  introduce partial interactions by using max pooling either at the first or the second layer, and obtain better results than  $\\mathrm{HRM}_{AvgAvg}$ . Take the Ta-Feng dataset as an example, when compared with  $\\mathrm{HRM}_{AvgAvg}$  with dimensionality set as 50, the relative performance improvement by  $\\mathrm{HRM}_{MaxAvg}$  and  $\\mathrm{HRM}_{AvgMax}$  is around  $13.6\\%$  and  $9.8\\%$ , respectively.\n\nBesides, we also find that there is no consistent dominant between these two partial-interaction models, indicating that interactions at different layers may both help the recommendation in their own way. Finally, by applying max pooling at both layers (i.e. full interactions),  $\\mathrm{HRM}_{MaxMax}$  can outperform the other three variations in terms of all the three evaluation measures. The results demonstrate the advantage of modeling interactions among multiple factors in next basket recommendation.\n\n# 5.5 Comparison against Baselines\n\nWe further compare our HRM model to the state-of-the-art baseline methods on next basket recommendation. Here we choose the best performed  $\\mathrm{HRM}_{MaxMax}$  as the representative for clear comparison. The performance results over Ta-Feng, BeiRen, and T-Mall are shown in Figure 3.\n\nWe have the following observations from the results. (1) Overall, the Top method is the weakest. However, we find that the Top method outperforms MC on the T-Mall dataset. This might be due to the fact that the items in T-Mall dataset are actually brands. Therefore, the distributions of top popular brands on both training and testing datasets are very close, which accords with the assumption of the Top method and leads to better performance. (2) The NMF method outperforms the MC method in most cases. A major reason might be that the transition matrix estimated in the\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/df9e9b58e93c1a023f3eaabfd8706992c9bfa3290780bf4328eabe91c0bba28c.jpg)  \nFigure 4: Performance variation in terms of F1-score against the number of negative samples over three datasets with  $\\mathrm{HRM}_{MaxMax}$ . The number of negative samples is increased from 1 to 25 on Ta-Feng, 10 to 60 on BeiRen, and from 1 to 6 on T-Mall.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/cd4751ed9e4aa39a0ecfd054cba795e6484ce8ed3ecc8f7e0e3515827c2dac68.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/5df18959-83a5-43c3-a936-6b51899ede3b/b404412e1aae477b99000656734c2df347ae8e03c1eae58ee36b449f0e113a42.jpg)\n\nTable 3: Performance comparison on Ta-Feng over different user groups with dimensionality set as 50.  \n\n<table><tr><td>user \nactiveness</td><td>method</td><td>F1-score</td><td>Hit-Ratio</td><td>NDCG@5</td></tr><tr><td rowspan=\"5\">Inactive</td><td>Top</td><td>0.036</td><td>0.181</td><td>0.054</td></tr><tr><td>MC</td><td>0.042</td><td>0.206</td><td>0.058</td></tr><tr><td>NMF</td><td>0.037</td><td>0.198</td><td>0.046</td></tr><tr><td>FPMC</td><td>0.043</td><td>0.216</td><td>0.060</td></tr><tr><td>HRMMaxMax</td><td>0.048</td><td>0.236</td><td>0.062</td></tr><tr><td rowspan=\"5\">Medium</td><td>Top</td><td>0.051</td><td>0.230</td><td>0.084</td></tr><tr><td>MC</td><td>0.059</td><td>0.262</td><td>0.088</td></tr><tr><td>NMF</td><td>0.052</td><td>0.234</td><td>0.072</td></tr><tr><td>FPMC</td><td>0.059</td><td>0.263</td><td>0.087</td></tr><tr><td>HRMMaxMax</td><td>0.068</td><td>0.299</td><td>0.097</td></tr><tr><td rowspan=\"5\">Active</td><td>Top</td><td>0.045</td><td>0.207</td><td>0.074</td></tr><tr><td>MC</td><td>0.050</td><td>0.212</td><td>0.075</td></tr><tr><td>NMF</td><td>0.056</td><td>0.223</td><td>0.075</td></tr><tr><td>FPMC</td><td>0.054</td><td>0.224</td><td>0.080</td></tr><tr><td>HRMMaxMax</td><td>0.062</td><td>0.246</td><td>0.087</td></tr></table>\n\nMC method are rather sparse, and directly using it for recommendation may not work well. One way to improve the performance of the MC method is to factorize the transition matrix to alleviate the sparse problem [23]. (3) By combining both sequential behavior and users' general taste, FPM-C can obtain better results than both MC and NMF. This result is quite consistent with the previous finding in [23]. (4) By further introducing the interactions among multiple factors, the proposed  $\\mathrm{HRM}_{MaxMax}$  can consistently outperform all the baseline methods in terms of all the measures over the three datasets. Take the Ta-Feng dataset as an example, when compared with second best performed baseline method (i.e. FPMC) with dimensionality set as 200, the relative performance improvement by  $\\mathrm{HRM}_{MaxMax}$  is around  $13.1\\%$ $11.1\\%$  ,and  $12.5\\%$  in terms of F1-score, Hit-Ratio and NDCG@5, respectively.\n\nTo further investigate the performance of different methods, we split the users into three groups (i.e., inactive, medium and active) based on their activeness and conducted the comparisons on different user groups. Take the Ta-Feng dataset as an example, a user is taken as inactive if there are less than 5 transactions in his/her purchase history, and active if there are more than 20 transactions in the purchase history. The remaining users are taken as medium. In this way, the proportions of inactive, medium and active are  $40.8\\%$ ,  $54.5\\%$ , and  $4.7\\%$  respectively. Here we only report the comparison results on Ta-Feng dataset under one dimensionality (i.e.  $d = 50$ ) due to the page limitation. In fact, similar conclusions can be drawn from other datasets. The results are shown in Table 3.\n\nFrom the results we can see that, not surprisingly, the Top method is still the worst on all the groups. Furthermore, we find that MC works better than NMF on both inactive and medium users in terms of all the measures; While on active users, NMF can achieve better performance than MC. The results indicate that it is difficult for NMF to learn a good user representation with few transactions for recommendation. By combining both sequential behavior and users' general taste linearly, FPMC obtains better performance than MC on inactive and active users, and performs better than NMF on inactive and medium users. However, we can see the improvements are not very consistent on different user groups. Finally,  $\\mathrm{HRM}_{MaxMax}$  can achieve the best performance on all the groups in terms of all the measures. It demonstrates that modeling interactions among multiple factors can help generate better recommendations for different types of users.\n\n# 5.6 The Impact of Negative Sampling\n\nTo learn the proposed HRM, we employ negative sampling procedure for optimization. One parameter in this procedure is the number of negative samples we draw each time, denoted by  $k$ . Here we investigate the impact of the sampling number  $k$  on the final performance. Since the item size is different over the three datasets, we tried different ranges of  $k$  accordingly. Specifically, we tried  $k \\in \\{1, 5, 10, 15, 20, 25\\}$  on Ta-Feng,  $k \\in \\{10, 20, 30, 40, 50, 60\\}$  on BeiRen, and  $k \\in \\{1, 2, 3, 4, 5, 6\\}$  on T-Mall, respectively. We report the test performance of  $\\mathrm{HRM}_{MaxMax}$  in terms of F1-score against the number of negative samples over the three datasets in Figure 4. Here we only show the results on one dimension over each dataset (i.e.  $d = 50$  on Ta-Feng and BeiRen and  $d = 10$  on T-Mall) due to the space limitation.\n\nFrom the results we find that: (1) As the sampling number  $k$  increases, the test performance in terms of F1-score increases too. The trending is quite consistent over the three datasets. (2) As the sampling number  $k$  increases, the performance gain between two consecutive trials decreases. For example, on Ta-Feng dataset, when we increase  $k$  from 20 to 25, the relative performance improvement in terms of F1-score is about  $0.0011\\%$ . It indicates that if we continue to sample more negative samples, there will be less performance improvement but larger computational complexity. Therefore, in our performance comparison experiments, we set  $k$  as 25, 60, 6 on Ta-Feng, BeiRen and T-Mall, respectively.",
  "hyperparameter": ""
}