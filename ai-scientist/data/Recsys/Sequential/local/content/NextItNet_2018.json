{
  "id": "NextItNet_2018",
  "paper_title": "A Simple Convolutional Generative Network for Next Item Recommendation",
  "alias": "NextItNet",
  "year": 2018,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "NextItNet introduces a probabilistic generative model based on stacked 1D dilated convolutional layers for sequential recommendation. Unlike previous methods that only predict the next item, it explicitly models the joint distribution of all items in the sequence by factorizing p(x) as a product of conditional distributions (∏p(x_i|x_{0:i-1})). The model uses dilated convolutions with masked operations to prevent future information leakage, residual blocks for deeper networks, and 1D convolutions on reshaped embeddings to efficiently capture long-range dependencies while enabling parallel training.",
  "introduction": "# 1 INTRODUCTION\n\nLeveraging sequences of user-item interactions (e.g., clicks or purchases) to improve real-world recommender systems has become increasingly popular in recent years. These sequences are automatically generated when users interact with online systems in sessions (e.g., shopping session, or music listening session). For example, users on Last.fm<sup>1</sup> or Weishi<sup>2</sup> typically enjoy a series of songs/videos during a certain time period without any interruptions, i.e., a listening or watching session. The set of music videos played in one session usually have strong correlations [6], e.g., sharing the same album, writer, or genre. Accordingly, a good recommender system is supposed to generate recommendations by taking advantage of these sequential patterns in the session.\n\nA class of models often employed for these sequences of interactions are the Recurrent Neural Networks (RNNs). RNNs typically generate a softmax output where high probabilities represent the most relevant recommendations. While effective, these RNN-based models, such as [3, 15], depend on a hidden state of the entire past that cannot fully utilize parallel computation within a sequence [8]. Thus their speed is limited in both training and evaluation.\n\nBy contrast, training CNNs does not depend on the computations of the previous time step and therefore allow parallelization over every element in a sequence. Inspired by the successful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser [29], abandoned RNN structures, proposing instead a convolutional sequence embedding model, and demonstrated that this CNN-based recommender is able to achieve comparable or superior performance to the popular RNN model in the top- $N$  sequential recommendation task. The basic idea of the convolution processing is to treat the  $t \\times k$  embedding matrix as the \"image\" of the previous  $t$  interactions in  $k$  dimensional latent space and regard the sequential pattens as local features of the \"image\". A max pooling operation that only preserves the maximum value of the convolutional layer is performed to increase the receptive field, as well as dealing with the varying length of input sequences. Fig. 1 depicts the key architecture of Caser.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/0d5362be5e71bf917e8e6ff1b839edc99b29920fd52984b7058f57c62b55032a.jpg)  \nFigure 1: The basic structure of Caser [29]. The red, yellow and blue regions denotes a  $2 \\times k$ ,  $3 \\times k$  and  $4 \\times k$  convolution filter respectively, where  $k = 5$ . The purple row stands for the true next item.\n\nConsidering the training speed of networks, in this paper we follow the path of sequential convolution techniques for the next item recommendation task. We show that the typical network architecture used in  $\\text{Caser}$  has several obvious drawbacks - e.g., (1) the max pooling scheme that is safely used in computer vision may discard important position and recurrent signals when modeling long-range sequence data; (2) generating the softmax distribution only for the desired item fails to effectively use the compete set of dependencies. Both drawbacks become more severe as the length of the sessions and sequences increases. To address these issues, we introduce a simple but fundamentally different CNN-based sequential recommendation model that allows us to model the complex conditional distributions even in very long-range item sequences. To be more specific, first our generative model is designed to explicitly encode item inter-dependencies, which allows to directly estimates the distribution of the output sequence (rather than the desired item) over the raw item sequence. Second, instead of using inefficient huge filters, we stack the 1D dilated convolutional layers [31] on top of each other to increase the receptive fields when modeling long-range dependencies. The pooling layer can be safely removed in the proposed network structure. It is worth noting that although the dilated convolution was invented for dense prediction in image generation tasks [4, 26, 31], and has been applied in other fields (e.g., acoustic [22, 26] and translation [18] tasks), it is yet unexplored in recommender systems with huge sparse data. Furthermore, to ease the optimization of the deep generative architecture, we propose using residual network to wrap convolutional layer(s) by residual block. To the best of our knowledge, this is also the first work in terms of adopting residual learning to model the recommendation task. The combination of these choices enables us to tackle large-scale problems and attain state-of-the-art results in both short- and long-range sequential recommendation data sets. In summary, our main contributions include a novel recommendation generative model (Section 3.1) and a fundamentally different convolutional network architecture (Sections 3.2 ~ 3.4).",
  "method": "# 3 MODEL DESIGN\n\nTo address the above limitations, we introduce a new probabilistic generative model that is formed of a stack of 1D convolution layers. We first focus on the form of the distribution, and then the architectural innovations. Generally, our proposed model is fundamentally different from Caser in several key ways: (1) our probability estimator explicitly models the distribution transition of all individual items at once, rather than the final one, in the sequence; (2) our network has a deep, rather than shallow, structure; (3) our convolutional layers are based on the efficient 1D dilated convolution rather than standard 2D convolution; and (4) pooling layers are removed.\n\n# 3.1 A Simple Generative Model\n\nIn this section, we introduce a simple yet very effective generative model directly operating on the sequence of previous interacted items. Our aim is to estimate a distribution over the original item interaction sequences that can be used to tractably compute the\n\nlikelihood of the items and to generate the future items that users would like to interact. Let  $p(\\mathbf{x})$  be the joint distribution of item sequence  $\\mathbf{x} = \\{x_0, \\dots, x_t\\}$ . To model  $p(\\mathbf{x})$ , we can factorize it as a product of conditional distributions by the chain rule.\n\n$$\np (\\mathbf {x}) = \\prod_ {i = 1} ^ {t} p \\left(x _ {i} \\mid x _ {0: i - 1}, \\theta\\right) p \\left(x _ {0}\\right) \\tag {1}\n$$\n\nwhere the value  $p(x_{i}|x_{0:i - 1},\\theta)$  is the probability of  $i$ -th item  $x_{i}$  conditioned on all the previous items  $x_{0:i - 1}$ . A similar setup has been explored by NADE [19], PixelRNN/CNN [23, 24] in biological and image domains.\n\nOwing to the ability of neural networks in modeling complex nonlinear relations, in this paper we model the conditional distributions of user-item interactions by a stack of 1D convolutional networks. To be more specific, the network receives  $x_{0:t-1}$  as the input and outputs distributions over possible  $x_{1:t}$ , where the distribution of  $x_{t}$  is our final expectation. For example, as illustrated in Fig. 2, the output distribution of  $x_{15}$  is determined by  $x_{0:14}$ , while  $x_{14}$  is determined by  $x_{0:13}$ . It is worth noting that in previous sequential recommendation literatures, such as Caser, GRUREc and [20, 25, 28, 30], they only model a single conditional distribution  $p(x_i | x_{0:i-1}, \\theta)$  rather than all conditional probabilities  $\\prod_{i=1}^{t} p(x_i | x_{0:i-1}, \\theta) p(x_0)$ . Within the context of the above example, assuming  $\\{x_0, \\dots, x_{14}\\}$  is given, models like Caser only estimate the probability distribution (i.e., softmax) of the next item  $x_{15}$  (also see Fig. 1 (d)), while our generative method estimates the distributions of all individual items in  $\\{x_1, \\dots, x_{15}\\}$ . The comparison of the generating process is shown below.\n\n$$\n\\text {C a s e r / G R U R e c :} \\underbrace {\\{x _ {0} , x _ {1} , \\dots , x _ {1 4} \\}} _ {\\text {i n p u t}} \\Rightarrow \\underbrace {x _ {1 5}} _ {\\text {o u t p u t}}\n$$\n\n$$\nO u r s: \\underbrace {\\left\\{x _ {0} , x _ {1} , \\dots , x _ {1 4} \\right\\}} _ {\\text {i n p u t}} \\Rightarrow \\underbrace {\\left\\{x _ {1} , x _ {2} , \\dots , x _ {1 5} \\right\\}} _ {\\text {o u t p u t}} \\tag {2}\n$$\n\nwhere  $\\Rightarrow$  denotes 'predict'. Clearly, our proposed model is more effective in capturing the set of all sequence relations, whereas Caser and GRUREc fail to explicitly model the internal sequence features between  $\\{x_0,\\dots,x_{14}\\}$ . In practice, to address the drawback, such models will typically generate a number of sub-sequences (or sub-sessions) for training by means of data augmentation techniques [28] (e.g., padding, splitting or shifting the input sequence), such as shown in Eq. (3) (see [20, 25, 29, 30]).\n\n$$\n\\text {C a s e r} / \\text {G R U R e c} \\quad \\text {s u b} - \\text {s e s s i o n} - 1: \\left\\{x _ {- 1}, x _ {0},..., x _ {1 3} \\right\\} \\Rightarrow x _ {1 4}\n$$\n\n$$\n\\begin{array}{c} \\text {C a s e r / G R U R e c s u b - s e s s i o n - 2 :} \\left\\{x _ {- 1}, x _ {- 1}, \\dots , x _ {1 2} \\right\\} \\Rightarrow x _ {1 3} \\\\ \\dots \\dots \\end{array} \\tag {3}\n$$\n\n$$\n\\text {C a s e r / G R U R e c s u b - s e s s i o n - 1 2 :} \\left\\{x _ {- 1}, x _ {- 1}, \\dots , x _ {2} \\right\\} \\Rightarrow x _ {3}\n$$\n\nWhile effective, the above approach to generate sub-session cannot guarantee the optimal results due to the separate optimization for each sub-session. In addition, optimizing these sub-sessions separately will result in corresponding computational costs. Detailed comparison with empirical results has also been reported in our experimental sections.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/a323e8dab218215a763cd631650bfd0cb8e89f3cec00517e982ba21d4e93760d.jpg)  \n(a) Standard CNN\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/0f9626e8ce061cbeacd01b11f8f7f7577800a3b2403c9d739396dabd52db7431.jpg)  \n(b) Dilated CNN with 'Holes'  \nFigure 2: The proposed generative architecture with 1D standard CNNs (a) and efficient dilated CNNs (b). The blue lines are the identity map which exists only for residual block (b) in Fig. 3. An example of a standard 1D convolution filter and dilated filters are shown at the bottom of (a) and (b) respectively. We will refer to a dilated convolution with a dilation factor  $l$  as  $l$ -dilated convolution. Apparently, compared with the standard CNN that linearly increases the receptive field by the depth of the network, the dilated CNN has a much larger receptive field by the same stacks without introducing more parameters. It can be seen that the standard convolution is a special case of 1-dilated convolution.\n\n# 3.2 Network Architecture\n\n3.2.1 Embedding Look-up Layer: Given an item sequence  $\\{x_0, \\dots, x_t\\}$ , the model retrieves each of the first  $t$  items  $\\{x_0, \\dots, x_{t-1}\\}$  via a lookup table, and stacks these item embeddings together. Assuming the embedding dimension is  $2k$ , where  $k$  can be set as the number of inner channels in the convolutional network. This results in a matrix of size  $t \\times 2k$ . Note that unlike Caser that treats the input matrix as a 2D \"image\" during convolution, our proposed architecture learns the embedding layer by 1D convolutional filters, which we will describe later.\n\n3.2.2 Dilated layer: As shown in Fig. 2 (a), the standard filter is only able to perform convolution with the receptive field linearly by the depth of the network. This makes it difficult to handle long-range sequences. Similar to Wavenet [22], we employ the dilated convolution to construct the proposed generative model. The basic idea of dilation is to apply the convolutional filter over a field larger than its original length by dilating it with zeros. As such, it is more efficient since it utilizes fewer parameters. For this reason, a dilated filter is also referred to as a holed or sparse filter. Another benefit is that dilated convolution can preserve the spatial dimensions of the input, which makes the stacking operation much easier for both convolutional layers and residual structures.\n\nFig. 2 shows the network comparison between the standard convolution and dilated convolutions with the proposed sequential generative model. The dilation factor in (b) are 1, 2, 4 and 8. To describe the network architecture, we denote receptive field,  $j$ -th convolutional layer, channel and dilation as  $r$ ,  $F_{j}$ ,  $C$  and  $l$  respectively. By setting the width of convolutional filter  $f$  as 3, we can\n\nsee that the dilated convolutions (Fig. 2 (b)) allow for exponential increase in the size of receptive fields  $(r = 2^{j + 1} - 1)$ , while the same stacking structure for the standard convolution (Fig. 2 (a)) has only linear receptive fields  $(r = 2j + 1)$ . Formally, with dilation  $l$ , the filter window from location  $i$  is given as\n\n$$\n\\left[ \\begin{array}{c c c c c} x _ {i} & x _ {i + l} & x _ {i + 2 l} & \\ldots & x _ {i + (f - 1) \\cdot l} \\end{array} \\right]\n$$\n\nand the 1D dilated convolution operator  $*_{l}$  on element  $h$  of the item sequence is given below\n\n$$\n\\left(\\mathbf {x} * _ {l} g\\right) (h) = \\sum_ {i = 0} ^ {f - 1} \\mathbf {x} _ {h - l \\cdot i} \\cdot g (i) \\tag {4}\n$$\n\nwhere  $g$  is the filter function. Clearly, the dilated convolutional structure is more effective to model long-range item sequences, and thus more efficient without using larger filters or becoming deeper. In practice, to further increase the model capacity and receptive fields, one just need to repeat the architecture in Fig. 2 multiple times by stacking, e.g., 1, 2, 4, 8, 1, 2, 4, 8.\n\n3.2.3 One-dimensional Transformation: Although our dilated convolution operator depends on the 2D input matrix  $E$ , the proposed network architecture is actually composed of all 1D convolutional layers. To model the 2D embedding input, we perform a simple reshape operation, which serves as a prerequisite for performing 1D convolution. Specifically, the 2D matrix  $E$  is reshaped from  $t \\times 2k$  to a 3D tensor  $T$  of size  $1 \\times t \\times 2k$ , where  $2k$  is treated as the \"image\" channel rather than the width of the standard convolution filter in Casej. Fig. 3 (b) illustrates the reshaping process.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/ee84b44e3468e7053db262c83ea667e9e853d5f11e8c3c7039bd1bfe624c6b25.jpg)  \n(a)  \nFigure 3: Dilated residual blocks (a), (b) and one-dimensional transformation (c). (c) shows the transformation from the 2D filter  $(C = 1)$  (left) to the 1D 2-dilated filter  $(C = 2k)$  (right); the vertical black arrows represent the direction of the sliding convolution. In this work, the default stride for the dilated convolution is 1. Note the reshape operation in (b) is performed before each convolution in (a) and (b) (i.e.,  $1 \\times 1$  and masked  $1 \\times 3$ ), which is then followed by a reshape back step after convolution.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/0a8a6f0ce856a7c0699e9646a807e22f9ac73c1fac0a4bfe11ede0a9254b4e6e.jpg)  \n(b)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/64c0d02384dd37d867d188dcc51abed2f76b01ff912dc1d3da26a4b727177a9a.jpg)  \n(c) One-dimensional Transformation\n\n# 3.3 Masked Convolutional Residual Network\n\nAlthough increasing the depth of network layers can help obtain higher-level feature representations, it also easily results in the vanishing gradient issue, which makes the learning process much harder. To address the degradation problem, residual learning [10] has been introduced for deep networks. While residual learning has achieved huge success in the domain of computer vision, it has not appeared in the recommender system literature.\n\nThe basic idea of residual learning is to stack multiple convolutional layers together as a block and then employ a skip connection scheme that passes the previous layers's feature information to its posterior layer. The skip connection scheme allows to explicitly fit the residual mapping rather than the original identity mapping, which can maintain the input information and thus enlarge the propagated gradients. Formally, denoting the desired mapping as  $H(E)$ , we let the residual block fit another mapping of  $F(E) = H(E) - E$ . The desired mapping now is recast into  $F(E) + E$  by element-wise addition (assuming that  $F(E)$  and  $E$  are of the same dimension). As has been evidenced in [10], optimizing the residual mapping  $F(E)$  is much easier than the original, unreferenced mapping  $H(E)$ . Inspired by [11, 18], we introduce two residual modules in Fig. 3 (a) and (b).\n\nIn (a), we wrap each dilated convolutional layer by a residual block, while in (b) we wrap every two dilated layers by a different residual block. That is, with the design of block (b), the input layer and the second convolutional layer should be connected by skip connection (i.e., the blue lines in Fig. 2). Specifically, each block is made up of the normalization, activation (e.g., ReLU [21]), convolutional layers and a skip connection in a specific order. In this work we adopt the state-of-the-art layer normalization [1] before each activation layer, as it is well suited to sequence processing and online learning in contrast with batch normalization [16].\n\nRegarding the properties of the two residual networks, the residual block in (a) consists of 3 convolution filters: one dilated filter of size  $1 \\times 3$  and two regular filters of size  $1 \\times 1$ . The  $1 \\times 1$  filters are\n\nintroduced to change the size of  $C$  so as to reduce the parameters to be learned by the  $1 \\times 3$  kernel. The first  $1 \\times 1$  filter (close to input  $E$  in Fig. 3 (a)) is to change  $C$  from  $2k$  to  $k$ , while the second  $1 \\times 1$  filter does the opposite transformation in order to maintain the spatial dimensions for the next stacking operation. To show the effectiveness of the  $1 \\times 1$  filters in (a), we compute the number of parameters in both (a) and (b). For simplicity, we omit the activation and normalization layers. As we can see, the number of parameters for the  $1 \\times 3$  filter is  $1 \\times 3 \\times 2k \\times 2k = 12k^2$  (i.e., in (b)) without the  $1 \\times 1$  filters. While in (a), the number of parameters to be learned is  $1 \\times 1 \\times 2k \\times k + 1 \\times 3 \\times k \\times k + 1 \\times 1 \\times k \\times 2k = 7k^2$ . The residual mapping  $F(E, \\{W_i\\})$  in (a) and (b) is formulated as:\n\n$$\nF (E, \\{W _ {i} \\}) = \\left\\{ \\begin{array}{l l} W _ {3} (\\sigma (\\psi \\left(W _ {2} (\\sigma (\\psi \\left(W _ {1} (\\sigma (\\psi (E)))\\right))\\right)))) & \\text {F i g . 3 (a)} \\\\ \\sigma \\left(\\psi \\left(W _ {4} ^ {\\prime} (\\sigma (\\psi \\left(W _ {2} ^ {\\prime} (E)\\right))\\right)\\right) & \\text {F i g . 3 (b)} \\end{array} \\right. \\tag {5}\n$$\n\nwhere  $\\sigma$  and  $\\psi$  denote ReLU and layer-normalization,  $W_{1}$  and  $W_{3}$  denote the convolution weight function of standard  $1\\times 1$  convolutions, and  $W_{2}$ ,  $W_{2}^{\\prime}$  and  $W_{4}^{\\prime}$  denote the weight function of  $l$ -dilated convolution filter with size of  $1\\times 3$ . Note that bias terms are omitted for simplifying notations.\n\n3.3.1 Dropout-mask: To avoid the future information leakage problem, we propose a masking-based dropout trick for the 1D dilated convolution to prevent the network from seeing the future items. Specifically, when predicting  $p(x_{i}|x_{0:i - 1})$ , the convolution filters are not allowed to make use of the information from  $x_{i:t}$ . Fig. 4 shows several different ways to perform convolution. As shown, our dropout-masking operation can be implemented either by padding the input sequence in (d) or shifting the output sequence by a few timesteps in (e). The padding method in (e) is very likely to result in information loss in a sequence, particularly for short sequences. Hence in this work, we apply the padding strategy in (d) with the padding size of  $(f - 1)*l$ .\n\n# 3.4 Final Layer, Network Training and Generating\n\nAs mentioned, the matrix in the last layer of the convolution architecture (see Fig. 2), denoted by  $E^o$ , preserves the same dimensional size of the input  $E$ , i.e.,  $E^o \\in \\mathbb{R}^{t \\times 2k}$ . However, the output should be a matrix or tensor that contains probability distributions of all items in the output sequence  $x_{1:t}$ , where the probability distribution of  $x_t$  is the desired one that generates top- $N$  predictions. To do this, we can simply use one more convolutional layer on top of the last convolutional layer in Fig. 2 with filter of size  $1 \\times 1 \\times 2k \\times n$ , where  $n$  is the number of items. Following the procedure of one-dimensional transformation in Fig. 3 (c), we obtain the expected output matrix  $E^p \\in \\mathbb{R}^{t \\times n}$ , where each row vector after the softmax operation represents the categorical distribution over  $x_i$  ( $0 < i \\leq t$ ).\n\nThe aim of optimization is to maximize the log-likelihood of the training data w.r.t.  $\\theta$ . Clearly, maximizing  $\\log p(\\mathbf{x})$  is mathematically equivalent to minimizing the sum of the binary cross-entropy loss for each item in  $x_{1:t}$ . For practical recommender systems with tens of millions items, the negative sampling strategy can be applied to bypasses the generation of full softmax distributions, where the  $1 \\times 1$  convolutional layer is replaced by a fully-connected (FC) layer with weight matrix  $E^g \\in \\mathbb{R}^{2k \\times n}$ . For example, we can apply\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/a87fffcaea73c95b0b4754543a2e130de01d209a37a8e0a6a01937d20d77c486.jpg)  \n(a)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/40832b6e3eb39eb2dfd9e587f83709fb4eb406cef498d5dff0f675473f7c3d74.jpg)  \n(b)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/cf5330c96695e3a3ac889ff578cd0f9310526c97db89f773ccb55b866a4953a1.jpg)  \n(c)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/3dad53e43bb1b22352911c31821b833ccd7b9468c2bf8c67fae0efdcce069f9e.jpg)  \n(d)  \nFigure 4: The future item can only be determined by the past ones according to Eq. (1). (a) (d) and (e) show the correct convolution process, while (b) and (c) are wrong. E.g., in (d), items of  $\\{1,2,3,4\\}$  are masked when predicting 1, which can be technically implemented by padding.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/91ab01ec-84f2-4ed1-b8e3-b18613979143/fe58f2614e55571a094afee3f925f8c0a5a49b353ef47461dbae949b7c2627de.jpg)  \n(e)\n\neither the sampled softmax [17] or kernel based sampling [2]. The recommendation accuracy by these negative sampling strategies is nearly identical with the full softmax method with properly tuned sampling size.\n\nFor comparison purpose, we only predict the next one item in our evaluation, and then stop the generating process. Nevertheless, the model is able to generate a sequence of items simply by feeding the predicted one item (or sequence) into the network to predict the next one, and thus the prediction at the generating phrase is sequential. This matches most real-world recommendation scenarios, where the next action is followed when the current one has been observed. But at both training and evaluation phases, the conditional predictions for all timesteps can be made in parallel, because the complete sequence of input items  $\\mathbf{x}$  is already available.",
  "experiments": "# 4 EXPERIMENTS\n\nIn this section we detail our experiments, report results for several data sets, and compare our model (called NextItNet) with the well-known RNN-based model GRURec [15, 28] and the state-of-the-art CNN-based model Caser. Note that (1) since the main contributions in this paper do not focus on combining various features, we omit the comparison with content- or context-based sequential recommendation models, such as the 3D CNN recommender [30] and other RNN variants [9, 20, 25, 27]; (2) the GRURec baseline could be regarded as the state-of-the-art Improved GRURec [28] when dealing with the long-range session data sets because our main data augmentation technique for the two baseline models follows the same way in Improved GRURec.\n\n# 4.1 Datasets and Experiment Setup\n\n4.1.1 Datasets and Preprocessing. The first data set 'Yoochoosebuys' (YOO for short) is chosen from the RecSys Challenge  $2015^{3}$ . We use the buying dataset for evaluation. For preprocessing, we filter out sessions of length shorter than 3. Meanwhile, we find that in the processed Yoo data  $96\\%$  sessions have a length shorter than 10, and we simply remove the  $4\\%$  longer sessions and refer it as a short-range sequential data.\n\nThe remaining data sets are extracted from Last.fm<sup>4</sup>: one medium-size (MUSIC_M) and one large-scale (MUSIC_L) collection by randomly drawing 20,000 and 200,000 songs respectively. In the Last.fm data set, we observe that most users listen to music several hundred times a week, and some even listen to more than one hundred songs\n\nwithin a day. Hence, we are able to test our model in both short- and long-range sequences by cutting up these long-range listening sessions. In MUSIC_L, we define the maximum session length  $t$  as 5, 10, 20, 50 and 100, and then extract every  $t$  successive items as our input sequences. This is done by sliding a window of both size and stride of  $t$  over the whole data. We ignore sessions in which the time span between the last two items is longer than 2 hours. In this way, we create 5 data sets, referred to as RAW-SESSIONS. We randomly split these RAW-SESSIONS data into training (50%), validation (5%), and testing (45%) sets.\n\nAs mentioned before, the performance of  $\\text{Caser}$  and  $\\text{GRURec}$  is supposed to degrade significantly for long sequence inputs, such as when  $t = 20, 50$  and 100. For example, when setting  $t = 50$ ,  $\\text{Caser}$  and  $\\text{GRURec}$  will predict  $x_{49}$  by using  $x_{0:48}$ , but without explicitly modeling the item inter-dependencies between  $x_0$  and  $x_{48}$ . To remedy this defect, when  $t > 5$ , we follow the common approach [20, 28] by manually creating additional sessions from the training sets of  $\\text{RAW-SESSIONS}$  so that  $\\text{Caser}$  and  $\\text{GRURec}$  can leverage the full dependency to a large extent. Still setting  $t = 50$ , one training session will then produce 45 more sub-sessions by padding the beginning and removing the end indices, referred to as  $\\text{SUB-SESSIONS}$ . The example of the 45 sub-sessions are given as follows:  $\\{x_{-1}, x_0, x_1, \\ldots, x_{48}\\}$ ,  $\\{x_{-1}, x_{-1}, x_0, \\ldots, x_{47}\\}$ , ...,  $\\{x_{-1}, x_{-1}, x_{-1}, \\ldots, x_4\\}$ . Regarding  $\\text{MUSIC\\_M}$ , we only show the results when  $t = 5$ . We show the statistics of  $\\text{RAW-SESSIONS}$  & training data of  $\\text{SUB-SESSIONS}$  (i.e.,  $\\text{SUB-SESSIONS-T}$ ) in Table 1.\n\nAll models were trained on GPUs (TITAN V) using Tensorflow. The learning rates and batch sizes of baseline methods were manually set according to performance in validation sets. For all data sets, NextItNet used the learning rate of 0.001 and batch size of 32. Embedding size  $2k$  is set to 64 for all models without special mention. We report results with residual block (a) and full softmax. We have validated the performance of results block (b) separately. To further evaluate the effectiveness of the two residual blocks, we have also tested our model in another dataset, namely, Weishi<sup>5</sup>. The improvements are about two times compared with the same model without residual blocks.\n\n4.1.2 Evaluation Protocols. We reported the evaluated results by three popular top- $N$  metrics, namely MRR@ $N$  (Mean Reciprocal Rank) [15], HR@ $N$  (Hit Ratio) [13] and NDCG@ $N$  [34] (Normalized Discounted Cumulative Gain).  $N$  is set to 5 and 20 for comparison. We evaluate the prediction accuracy of the last (i.e., next) item of each sequence in the testing set, similarly to [14].\n\nTable 1: Session statistics of all data sets.  \n\n<table><tr><td>DATA</td><td>YOO</td><td>MUSIC_M5</td><td>MUSIC_L5</td><td>MUSIC_L10</td><td>MUSIC_L20</td><td>MUSIC_L50</td><td>MUSIC_L100</td></tr><tr><td>RAW-SESSIONS</td><td>0.14M</td><td>0.61M</td><td>2.14M</td><td>1.07M</td><td>0.53M</td><td>0.21M</td><td>0.11M</td></tr><tr><td>SUB-SESSIONS-T</td><td>0.07M</td><td>0.31M</td><td>1.07M</td><td>3.21M</td><td>4.28M</td><td>4.91M</td><td>5.10M</td></tr></table>\n\nMUSIC_M5 denotes MUSIC_M with maximum session size of 5. The same applies to MUSIC_L. 'M' denotes 1 million.\n\nTable 2: Accuracy comparison. The upper, middle and below tables are MRR@5, HR@5 and NDCG@5 respectively.  \n\n<table><tr><td>DATA</td><td>YOO</td><td>MUSIC_M5</td><td>MUSIC_L5</td><td>MUSIC_L10</td><td>MUSIC_L20</td><td>MUSIC_L50</td><td>MUSIC_L100</td></tr><tr><td>MostPop</td><td>0.0050</td><td>0.0024</td><td>0.0006</td><td>0.0007</td><td>0.0008</td><td>0.0007</td><td>0.0007</td></tr><tr><td>GRURec</td><td>0.1645</td><td>0.3019</td><td>0.2184</td><td>0.2124</td><td>0.2327</td><td>0.2067</td><td>0.2086</td></tr><tr><td>Caser</td><td>0.1523</td><td>0.2920</td><td>0.2207</td><td>0.2214</td><td>0.1947</td><td>0.2060</td><td>0.2080</td></tr><tr><td>NextItNet</td><td>0.1715</td><td>0.3133</td><td>0.2327</td><td>0.2596</td><td>0.2748</td><td>0.2735</td><td>0.2583</td></tr><tr><td>MostPop</td><td>0.0151</td><td>0.0054</td><td>0.0014</td><td>0.0016</td><td>0.0016</td><td>0.0016</td><td>0.0016</td></tr><tr><td>GRURec</td><td>0.2773</td><td>0.3610</td><td>0.2626</td><td>0.2660</td><td>0.2694</td><td>0.2589</td><td>0.2593</td></tr><tr><td>Caser</td><td>0.2389</td><td>0.3368</td><td>0.2443</td><td>0.2631</td><td>0.2433</td><td>0.2572</td><td>0.2588</td></tr><tr><td>NextItNet</td><td>0.2871</td><td>0.3754</td><td>0.2695</td><td>0.3014</td><td>0.3166</td><td>0.3218</td><td>0.3067</td></tr><tr><td>MostPop</td><td>0.0075</td><td>0.0031</td><td>0.0008</td><td>0.0009</td><td>0.0010</td><td>0.0009</td><td>0.0009</td></tr><tr><td>GRURec</td><td>0.1923</td><td>0.3166</td><td>0.2294</td><td>0.2258</td><td>0.2419</td><td>0.2197</td><td>0.2212</td></tr><tr><td>Caser</td><td>0.1738</td><td>0.3032</td><td>0.2267</td><td>0.2318</td><td>0.2068</td><td>0.2188</td><td>0.2207</td></tr><tr><td>NextItNet</td><td>0.2001</td><td>0.3288</td><td>0.2419</td><td>0.2700</td><td>0.2853</td><td>0.2855</td><td>0.2704</td></tr></table>\n\nMostPop returns the most popular item respectively. Regarding the setup of our model, we use two-hidden-layer convolution structure with dilation factor 1, 2, 4 for the first four data sets (i.e., YOO, MUSIC_M5, MUSIC_L5 and MUSIC_L10), while for the last three long-range sequence data sets, we use 1, 2, 4, 8, 1, 2, 4, 8, to obtain above results.\n\nTable 3: Accuracy comparison. The upper, middle and below tables are MRR@20, HR@20 and NDCG@20 respectively.  \n\n<table><tr><td>DATA</td><td>YOO</td><td>MUSIC_M5</td><td>MUSIC_L5</td><td>MUSIC_L10</td><td>MUSIC_L20</td><td>MUSIC_L50</td><td>MUSIC_L100</td></tr><tr><td>MostPop</td><td>0.0090</td><td>0.0036</td><td>0.0009</td><td>0.0010</td><td>0.0011</td><td>0.0011</td><td>0.0011</td></tr><tr><td>GRURec</td><td>0.1839</td><td>0.3103</td><td>0.2242</td><td>0.2203</td><td>0.2374</td><td>0.2151</td><td>0.2162</td></tr><tr><td>Caser</td><td>0.1660</td><td>0.2979</td><td>0.2234</td><td>0.2268</td><td>0.2017</td><td>0.2133</td><td>0.2153</td></tr><tr><td>NextItNet</td><td>0.1901</td><td>0.3223</td><td>0.2375</td><td>0.2669</td><td>0.2815</td><td>0.2794</td><td>0.2650</td></tr><tr><td>MostPop</td><td>0.0590</td><td>0.0180</td><td>0.0052</td><td>0.0053</td><td>0.0056</td><td>0.0056</td><td>0.0056</td></tr><tr><td>GRURec</td><td>0.4603</td><td>0.4435</td><td>0.3197</td><td>0.3434</td><td>0.3158</td><td>0.3406</td><td>0.3336</td></tr><tr><td>Caser</td><td>0.3714</td><td>0.3937</td><td>0.2703</td><td>0.3150</td><td>0.3110</td><td>0.3273</td><td>0.3298</td></tr><tr><td>NextItNet</td><td>0.4645</td><td>0.4626</td><td>0.3159</td><td>0.3709</td><td>0.3814</td><td>0.3789</td><td>0.3731</td></tr><tr><td>MostPop</td><td>0.0195</td><td>0.0066</td><td>0.0018</td><td>0.0019</td><td>0.0021</td><td>0.0020</td><td>0.0020</td></tr><tr><td>GRURec</td><td>0.2460</td><td>0.3405</td><td>0.2460</td><td>0.2481</td><td>0.2553</td><td>0.2433</td><td>0.2427</td></tr><tr><td>Caser</td><td>0.2122</td><td>0.3197</td><td>0.2342</td><td>0.2469</td><td>0.2265</td><td>0.2392</td><td>0.2412</td></tr><tr><td>NextItNet</td><td>0.2519</td><td>0.3542</td><td>0.2554</td><td>0.2904</td><td>0.3041</td><td>0.3021</td><td>0.2895</td></tr></table>\n\nTable 4: Effects of sub-session in terms of MRR@5. The upper, middle and below tables represent GRU, Caser and NextItNet respectively. \"10\", \"20\", \"50\" and \"100\" are the session lengths.  \n\n<table><tr><td>Sub-session</td><td>10</td><td>20</td><td>50</td><td>100</td></tr><tr><td>Without</td><td>0.1985</td><td>0.1645</td><td>0.1185</td><td>0.0746</td></tr><tr><td>With</td><td>0.2124</td><td>0.2327</td><td>0.2067</td><td>0.2086</td></tr><tr><td>Without</td><td>0.1571</td><td>0.1012</td><td>0.0216</td><td>0.0084</td></tr><tr><td>With</td><td>0.2214</td><td>0.1947</td><td>0.2060</td><td>0.2080</td></tr><tr><td>Without</td><td>0.2596</td><td>0.2748</td><td>0.2735</td><td>0.2583</td></tr></table>\n\nTable 5: Effects of the residual block in terms of MRR@5. \"Without\" means no skip connection. \"M5\", \"L5\", \"L10\" and \"L50\" denote MUSIC_M5, MUSIC_L5, MUSIC_L10 and MUSIC_L50 respectively.  \n\n<table><tr><td>DATA</td><td>M5</td><td>L5</td><td>L10</td><td>L50</td></tr><tr><td>Without</td><td>0.2968</td><td>0.2146</td><td>0.2292</td><td>0.2432</td></tr><tr><td>With</td><td>0.3300</td><td>0.2455</td><td>0.2645</td><td>0.2760</td></tr></table>\n\n# 4.2 Results Summary\n\nOverall performance results of all methods are illustrated in Table 2 and 3, which clearly show that the neural network models (i.e., Caser, GRURec and our model) obtain very promising accuracy in the top-Nsequential recommendation task. For example, in\n\nTable 6: Effects (MRR@5) of increasing embedding size. The upper and below tables are MUSIC_M5 and MUSIC_L100 respectively.  \n\n<table><tr><td>2k</td><td>16</td><td>32</td><td>64</td><td>128</td></tr><tr><td>GRURec</td><td>0.2786</td><td>0.2955</td><td>0.3019</td><td>0.3001</td></tr><tr><td>Caser</td><td>0.2855</td><td>0.2982</td><td>0.2979</td><td>0.2958</td></tr><tr><td>NextItNet</td><td>0.2793</td><td>0.3063</td><td>0.3133</td><td>0.3183</td></tr><tr><td>GRURec</td><td>0.1523</td><td>0.1826</td><td>0.2086</td><td>0.2043</td></tr><tr><td>Caser</td><td>0.0643</td><td>0.1129</td><td>0.2080</td><td>0.2339</td></tr><tr><td>NextItNet</td><td>0.1668</td><td>0.2289</td><td>0.2583</td><td>0.2520</td></tr></table>\n\nMUSIC_M5, the three neural models perform more than 120 times better on MRR@5 than MostPop, which is a widely used recommendation benchmark. The best MRR@20 result we have achieved by NextItNet is 0.3223 in this data set, which roughly means that the desired item is ranked on position 3 in average among the 20,000 candidate items. We then find that among these neural network based models, NextItNet largely outperforms Caser & GRURec. We believe there are several reasons contributing to the state-of-the-art results. First, as highlighted in Section 3.1, NextItNet takes full advantage of the complete sequential information. This can be easily verified in Table 4, where we observe that Caser & GRURec without subsession perform extremely worse with long sessions. In addition, even with sub-session Caser & GRURec still show significantly worse results than NextItNet because the separate optimization of each sub-session is clearly suboptimal compared with leveraging full sessions by NextItNet. Second, unlike Caser, NextItNet has no pooling layers, although it is also a CNN based model. As a result, NextItNet preserves the whole spatial resolution of the original embedding matrix  $E$  without any information lost. The third advantage is that NextItNet can support deeper layers by using residual learning, which better suits for modeling complicated relations and long-range dependencies. We have separately validated the performance of residual block in Fig. 3 (b) and showed the results in Table 5. It can be observed that the performance of NextItNet can be significantly improved by the residual block design. Table 6 shows the impact of embedding sizes.\n\nIn addition to the advantage of recommendation accuracy, we have also evaluated the efficiency of NextItNet in Table 7. First, it can be seen that NextItNet and Caser requires less training time than GRURec in all three data sets. The reason that CNN-based models can be trained much faster is due to the full parallel mechanism of convolutions. Clearly, the training speed advantage of CNN models are more preferred by modern parallel computing systems. Second, it shows that NextItNet achieves further improvements in training time compared with Caser. The faster training speed is mainly because NextItNet leverages the complete sequential information during training and then converges much faster by less training epochs. To better understand of the convergence behaviours, we have shown them in Fig. 5. As can be seen, our model with the same number of training sessions converges faster (a) and better (b, c, d) than Caser and GRURec. This confirms our claim in Section 3.1 since Caser and GRURec cannot make full use of the internal sequential information in the session.\n\nTable 7: Overall training time (mins).  \n\n<table><tr><td>Model</td><td>GRURec</td><td>Caser</td><td>NextItNet</td></tr><tr><td>MUSIC_L5</td><td>66</td><td>59</td><td>54</td></tr><tr><td>MUSIC_L20</td><td>282</td><td>191</td><td>76</td></tr><tr><td>MUSIC_L100</td><td>586</td><td>288</td><td>150</td></tr></table>",
  "hyperparameter": "Learning rate: 0.001; Batch size: 32; Embedding size (2k): 64 (tested with 16, 32, 64, 128); Convolutional filter width (f): 3; Dilation factors: [1, 2, 4] for short sequences (length ≤10), [1, 2, 4, 8, 1, 2, 4, 8] for long sequences (length ≥20); Number of inner channels (k): 32 (half of embedding size); Residual block type: Fig. 3(a) with 1×1 and 1×3 filters; Padding size for masked convolution: (f-1)×l where l is dilation factor; Maximum session lengths tested: 5, 10, 20, 50, 100"
}