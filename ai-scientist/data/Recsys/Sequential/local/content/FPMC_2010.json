{
  "id": "FPMC_2010",
  "paper_title": "Factorizing Personalized Markov Chains for Next-Basket Recommendation",
  "alias": "FPMC",
  "year": 2010,
  "domain": "Recsys",
  "task": "SequentialRecommendation",
  "idea": "FPMC (Factorizing Personalized Markov Chains) addresses next-basket recommendation by factorizing a personalized transition cube using a pairwise interaction model. The method combines two components: (1) a Matrix Factorization (MF) component that captures long-term user-item preferences, and (2) a Factorized Markov Chain (FMC) component that models sequential item-to-item transitions. This factorization approach overcomes the data sparsity problem in personalized Markov chains while maintaining both personalization and sequential dynamics. The model is optimized using Sequential BPR (S-BPR), a ranking-based objective that maximizes the probability of observed items being ranked higher than unobserved items.",
  "introduction": "# Introduction\nRecommender systems are critical for websites (e.g., e-commerce) to boost sales, click rates, and user satisfaction. Two dominant approaches are matrix factorization (MF) and Markov chains (MC):\n- MF learns users’ general tastes by factorizing user-item preference matrices, ignoring sequential behavior.\n- MC models sequential behavior via a global transition graph, predicting next actions based on recent user behaviors but lacking personalization.\n\nThis paper proposes **Factorized Personalized Markov Chains (FPMC)**, a method that unifies MF and MC to capture both sequential effects (MC strength) and long-term user tastes (MF strength). Key innovations:\n1. Personalized transition matrices: Each user has an individual transition matrix, forming a transition cube (user × item × item).\n2. Factorization of the transition cube: Addresses data sparsity (direct estimation of personalized transitions is unreliable) via a pairwise interaction model (special case of Tucker Decomposition), propagating information across similar users/items/transitions.\n3. Extension of Bayesian Personalized Ranking (BPR) to sequential basket data (S-BPR) for optimizing ranking performance.\n\nFPMC subsumes both MF and unpersonalized factorized MC (FMC) as special cases. Experiments on real-world e-commerce data show FPMC outperforms MF, MC, and FMC.",
  "method": "# Method\n## 1. Problem Formalization\n- **Users/Items**: \\( U = \\{u_1,...,u_{|U|}\\} \\), \\( I = \\{i_1,...,i_{|I|}\\} \\).\n- **Sequential Basket Data**: Each user \\( u \\) has a purchase history \\( B^u = (B_1^u,...,B_{t_u-1}^u) \\), where \\( B_t^u \\subseteq I \\) is the basket of items bought at time \\( t \\).\n- **Task**: Generate a personalized ranking of items for user \\( u \\)’s next basket \\( B_t^u \\), given \\( B_1^u,...,B_{t-1}^u \\).\n\n## 2. Personalized Markov Chains (PMC)\n### 2.1 Unpersonalized MC for Baskets\n- **Order 1 MC**: Predict next basket based only on the last basket: \\( p(B_t | B_{t-1}) \\).\n- **Transition Probability**: Modeled as item-level transitions \\( a_{l,i} = p(i \\in B_t | l \\in B_{t-1}) \\) (avoids exponential state space of baskets).\n- **Prediction for Baskets**: Average transitions from all items in the last basket:\n\\[ p(i \\in B_t | B_{t-1}) = \\frac{1}{|B_{t-1}|} \\sum_{l \\in B_{t-1}} a_{l,i} \\]\n\n### 2.2 Personalized MC Extension\n- **User-Specific Transitions**: \\( a_{u,l,i} = p(i \\in B_t^u | l \\in B_{t-1}^u) \\), forming a transition cube \\( A \\in [0,1]^{|U|×|I|×|I|} \\).\n- **Limitation**: Direct MLE estimation is unreliable due to data sparsity (most \\( a_{u,l,i} \\) have no observations).\n\n## 3. Factorization of Transition Cube\n### 3.1 Pairwise Interaction Model\nTo handle sparsity, factorize the transition cube using a pairwise interaction model (special case of Tucker Decomposition):\n\\[ \\hat{a}_{u,l,i} = \\langle v_u^{U,I}, v_i^{I,U} \\rangle + \\langle v_i^{I,L}, v_l^{L,I} \\rangle + \\langle v_u^{U,L}, v_l^{L,U} \\rangle \\]\n- \\( v_u^{U,I} \\in \\mathbb{R}^{k_{U,I}} \\): User feature vector for interaction with target items.\n- \\( v_i^{I,U} \\in \\mathbb{R}^{k_{U,I}} \\): Target item feature vector for interaction with users.\n- \\( v_i^{I,L} \\in \\mathbb{R}^{k_{I,L}} \\): Target item feature vector for interaction with last basket items.\n- \\( v_l^{L,I} \\in \\mathbb{R}^{k_{I,L}} \\): Last basket item feature vector for interaction with target items.\n- \\( v_u^{U,L} \\in \\mathbb{R}^{k_{U,L}} \\): User feature vector for interaction with last basket items (drops out for ranking optimization).\n- \\( k_{U,I}, k_{I,L} \\): Latent dimensions.\n\n### 3.2 FPMC Prediction\nFor user \\( u \\)’s next basket, the predicted score for item \\( i \\) is:\n\\[ \\hat{p}(i \\in B_t^u | B_{t-1}^u) = \\langle v_u^{U,I}, v_i^{I,U} \\rangle + \\frac{1}{|B_{t-1}^u|} \\sum_{l \\in B_{t-1}^u} \\langle v_i^{I,L}, v_l^{L,I} \\rangle \\]\n- First term: MF component (captures long-term user-item preferences).\n- Second term: FMC component (captures sequential transitions).\n\n## 4. Optimization with S-BPR\n### 4.1 Sequential BPR (S-BPR) Criterion\nOptimize for personalized ranking by maximizing the probability that observed items in \\( B_t^u \\) are ranked higher than unobserved items:\n\\[ \\max_\\Theta \\sum_{u} \\sum_{B_t^u \\in B^u} \\sum_{i \\in B_t^u} \\sum_{j \\notin B_t^u} \\ln \\sigma(\\hat{x}_{u,t,i} - \\hat{x}_{u,t,j}) - \\lambda_\\Theta \\|\\Theta\\|_F^2 \\]\n- \\( \\hat{x}_{u,t,i} = \\hat{p}(i \\in B_t^u | B_{t-1}^u) \\) (FPMC score).\n- \\( \\sigma(\\cdot) \\): Sigmoid function.\n- \\( \\Theta = \\{v^{U,I}, v^{I,U}, v^{I,L}, v^{L,I}\\} \\): Model parameters.\n- \\( \\lambda_\\Theta \\): Regularization coefficient.\n\n### 4.2 Stochastic Gradient Descent (SGD) Learning\n- **Bootstrap Sampling**: Draw quadruples \\( (u,t,i,j) \\) ( \\( i \\in B_t^u \\), \\( j \\notin B_t^u \\) ) to reduce computation.\n- **Gradients**: Update parameters using gradients of the S-BPR objective (see paper for detailed gradient formulas).\n- **Complexity**: \\( O(\\#it \\cdot (k_{U,I} + k_{I,L} \\cdot \\overline{|B|})) \\), where \\( \\#it \\) = iterations, \\( \\overline{|B|} \\) = average basket size.\n\n## 5. Special Cases\n- **MF**: Set \\( k_{I,L} = 0 \\) (only the first term remains).\n- **FMC**: Set \\( k_{U,I} = 0 \\) (only the second term remains).",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Dataset\nReal-world anonymized e-commerce data (online drug store), split into two subsets:\n| Dataset          | #Users | #Items | #Baskets | Avg. Basket Size | Avg. Baskets per User | #Triples (u,i,t) |\n|------------------|--------|--------|----------|------------------|-----------------------|------------------|\n| Sparse (10-core) | 71,602 | 7,180  | 233,476  | 11.3             | 3.2                   | 2,635,125        |\n| Dense Subset     | 10,000 | 1,002  | 90,655   | 9.2              | 9.0                   | 831,442          |\n- 10-core subset: Users/items with at least 10 interactions.\n- Dense subset: Top 10k users (most purchases) and top 1k items (most purchased).\n\n### 1.2 Baselines\n- Most Popular (MP): Rank items by global purchase frequency.\n- MC Dense: Unpersonalized MC with MLE-estimated transition matrix.\n- MF: Matrix factorization (special case of FPMC, \\( k_{I,L}=0 \\)).\n- FMC: Factorized unpersonalized MC (special case of FPMC, \\( k_{U,I}=0 \\)).\n\n### 1.3 Evaluation Protocol\n- **Split**: Last basket of each user as test set, rest as training set.\n- **Filter**: Exclude items already purchased by the user from test predictions (focus on new item discovery).\n- **Metrics**:\n  1. Half-Life Utility (HLU): Weighted utility of top-ranked items (half-life α=5).\n  2. F-Measure@Top5: Harmonic mean of average precision and recall for top-5 items.\n  3. AUC: Average area under ROC curve per user.\n- **Hyperparameters**: Latent dimensions \\( k_{U,I}=k_{I,L} \\in \\{8,16,32,64,128\\} \\); regularization and learning rate tuned via validation set.\n\n## 2. Main Results\n### 2.1 Key Findings\n- **All Methods Outperform MP**: Demonstrates the value of personalized/sequential modeling.\n- **Factorization Beats Dense MC**: FMC outperforms MC Dense on both datasets. Advantage is larger on sparse data (12% vs. 88% transition matrix density) due to better sparsity handling; even on dense data, factorization prevents overfitting.\n- **FPMC Dominates**: FPMC outperforms MF, FMC, and MC Dense on all metrics and datasets:\n  - Sparse data: FPMC’s HLU/AUC/F-Measure are 5–10% higher than MF/FMC.\n  - Dense data: FPMC maintains superiority, combining MF’s long-term taste capture and FMC’s sequential modeling.\n- **Dimensionality Impact**: Performance improves with latent dimensions up to 64–128, then plateaus (no overfitting due to regularization).\n\n### 2.2 Transition Matrix Properties\n| Dataset          | Missing Values | Non-Zero Entries | Zero Entries |\n|------------------|----------------|------------------|--------------|\n| Sparse (10-core) | 2.0%           | 12.1%            | 85.9%        |\n| Dense Subset     | 0.0%           | 88.6%            | 11.4%        |\n- MC Dense’s transition matrix is highly sparse on real-world data, justifying factorization.",
  "hyperparameter": "Latent dimensions: k_{U,I} = k_{I,L} ∈ {8, 16, 32, 64, 128}, with optimal performance at 64-128 before plateauing. Regularization coefficient λ_Θ and learning rate are tuned via validation set (specific values not provided in the paper). The model uses bootstrap sampling for stochastic gradient descent (SGD) learning. For evaluation, half-life α=5 is used for the HLU metric, and top-5 items are considered for F-Measure calculation."
}