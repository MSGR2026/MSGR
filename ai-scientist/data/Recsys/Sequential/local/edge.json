[
  {
    "source": "GRU4Rec_2015",
    "target": "GLINTRU_2025",
    "type": "in-domain",
    "similarities": "1. Both methods use GRU (Gated Recurrent Unit) as a core component for capturing sequential dependencies in user-item interaction sequences, leveraging the update and reset gate mechanisms to model temporal patterns.\n2. Both approaches employ an item embedding layer to project discrete item indices into dense vector representations before feeding them into the recurrent network.\n3. Both methods output item representations/embeddings that are used to compute prediction scores via dot product or similar similarity measures with candidate item embeddings.\n4. Both frameworks support standard recommendation loss functions (Cross-Entropy loss) for training the sequential recommendation task.\n5. Both methods apply dropout mechanisms for regularization - GRU4Rec uses embedding dropout while GLINT-RU inherits similar regularization strategies.\n6. The fundamental problem formulation is identical: given a user's historical item sequence, predict the next item the user will interact with by generating ranking scores for all candidate items.\n7. Both methods process variable-length sequences and extract the final hidden state representation for making predictions about the next item.",
    "differences": "1. **Dense Selective GRU Module**: GLINT-RU introduces temporal convolution layers before and after the GRU to aggregate local temporal features from adjacent items. These convolutions with kernel size k enable each GRU cell to access context from neighboring items rather than only preceding ones, creating a denser information flow through the recurrent pathway.\n2. **Selective Gate Mechanism**: GLINT-RU employs a selective gate that dynamically filters GRU hidden states based on convolved input features using SiLU activation. This gating function performs element-wise multiplication between projected hidden states and gate weights derived from the input, allowing adaptive information selection based on data characteristics.\n3. **Expert Mixing Block with Linear Attention**: GLINT-RU introduces a parallel architecture combining GRU-based temporal modeling with a linear attention mechanism. The linear attention uses ELU activation with L2 normalization instead of softmax to reduce computational complexity. Learnable mixing weights α₁ and α₂ (normalized via softmax) dynamically balance the contributions from the attention expert and GRU expert. A data-aware gate using GeLU activation further filters the mixed output.\n4. **Removal of Positional Embeddings**: Unlike transformer-based models, GLINT-RU removes explicit positional embedding layers, relying on GRU's recurrent mechanism to inherently encode positional information through accumulated hidden states. The paper argues that the GRU's sequential processing naturally captures positional representations within the hidden state evolution.\n5. **Gated MLP Block**: GLINT-RU replaces standard feed-forward networks with a gated MLP block that applies GeLU-based gating to filter information before final prediction. This gate multiplies the transformed features with a gated version of the same input, enhancing the model's capacity to capture complex user behavior patterns.\n6. **Channel Crossing Layer**: GLINT-RU introduces a linear projection layer Φ(H) between GRU output and the selective gate to transform hidden states into a latent space, which is not present in the original GRU4Rec architecture. This projection facilitates better information selection through dimensionality transformation.\n7. **Single-Layer Architecture**: GLINT-RU achieves competitive performance with only one layer of the expert mixing block, whereas GRU4Rec typically stacks multiple GRU layers (num_layers parameter). The design prioritizes efficiency through parallel expert computation rather than depth, with theoretical complexity O((2k+12)Nd²).\n8. **Prediction Score Computation**: While GRU4Rec directly uses dense layer output for scoring, GLINT-RU generates item representations through the gated MLP block and computes scores using softmax over the dot product of representations and embeddings, allowing for more expressive prediction.\n9. **Architectural Focus vs Training Techniques**: GRU4Rec's advanced techniques (sequence preprocessing, embedding dropout augmentation, pre-training/fine-tuning for temporal shifts, and teacher-student distillation) are not emphasized in GLINT-RU, which focuses primarily on architectural innovations through expert mixing and selective gating mechanisms.",
    "rank": "rank1"
  },
  {
    "source": "SASRec_2018",
    "target": "GLINTRU_2025",
    "type": "in-domain",
    "similarities": "1. Both papers address the sequential recommendation task, where the goal is to predict the next item a user will interact with based on their historical interaction sequence.\n2. Both methods use an item embedding layer to transform discrete item IDs into dense vector representations of dimension d.\n3. Both approaches employ a prediction layer that computes relevance scores via dot product between the learned sequence representation and item embeddings, using softmax for final prediction.\n4. Both methods use Cross-Entropy loss (or similar classification loss) as the training objective to optimize the model.\n5. Both architectures incorporate non-linear transformations through feed-forward networks (FFN/MLP) to capture complex feature interactions.\n6. Both methods handle variable-length sequences by padding shorter sequences to a fixed maximum length N.\n7. Both papers analyze computational complexity and emphasize efficiency, with complexity linear in sequence length for their core operations.\n8. Both methods aim to capture item dependencies and generate user preference representations from the interaction sequence without explicit user embeddings.",
    "differences": "1. **Positional Encoding**: SASRec uses learnable positional embeddings added to item embeddings to encode sequence order. GLINT-RU removes positional embeddings entirely, instead relying on GRU's inherent sequential processing to capture positional information through hidden state propagation.\n2. **Core Architecture**: SASRec uses stacked self-attention blocks (Transformer encoder) as the main sequence modeling component. GLINT-RU introduces an \"Expert Mixing Block\" that combines a Dense Selective GRU module with a Linear Attention mechanism in parallel, enabling both temporal and semantic feature extraction.\n3. **Attention Mechanism**: SASRec uses standard scaled dot-product self-attention with causal masking. GLINT-RU uses Linear Attention with L2 normalization and ELU activation to reduce complexity from O(N²d) to O(Nd²), improving efficiency for long sequences.\n4. **GRU Integration**: SASRec does not use any recurrent components. GLINT-RU introduces a Dense Selective GRU that applies temporal 1D convolution before GRU input to aggregate local context, uses a Selective Gate mechanism with SiLU activation to filter hidden states based on input, and applies another temporal convolution after GRU output.\n5. **Expert Mixing Mechanism**: GLINT-RU introduces learnable mixing parameters (α₁, α₂) with softmax normalization to dynamically weight the contributions of the Linear Attention expert and Dense Selective GRU expert. This parallel expert design allows the model to adaptively balance between global semantic modeling and local temporal modeling.\n6. **Gating Mechanisms**: GLINT-RU employs multiple gating mechanisms: (a) Selective Gate in GRU module using SiLU activation, (b) Data-aware gate after expert mixing using GeLU activation, (c) Gated MLP block using GeLU for final representation learning. SASRec relies on standard dropout and layer normalization without such gating strategies.\n7. **Feed-Forward Block**: SASRec uses a standard two-layer FFN with ReLU activation. GLINT-RU uses a Gated MLP block where the output is element-wise multiplied with a gated transformation of the input, providing more expressive nonlinear transformations.\n8. **Layer Stacking**: SASRec stacks multiple self-attention blocks (typically 2+ layers) with residual connections and layer normalization. GLINT-RU achieves competitive performance with only one layer of the expert mixing block, avoiding deep stacking and improving training efficiency.\n9. **Residual Connections**: SASRec explicitly uses residual connections (x + Dropout(g(LayerNorm(x)))) around each sub-layer to facilitate gradient flow in deep networks. GLINT-RU does not emphasize residual connections in the main architecture description.\n10. **Temporal Convolution**: GLINT-RU introduces two temporal 1D convolution layers with kernel size k in the Dense Selective GRU module to capture local temporal patterns from adjacent items. SASRec does not use any convolutional components, relying solely on attention mechanisms for pattern extraction.",
    "rank": "rank2"
  },
  {
    "source": "SASRec_2018",
    "target": "DIFF_2025",
    "type": "in-domain",
    "similarities": "1. Both methods are designed for sequential recommendation, taking a user's historical item interaction sequence as input to predict the next item.\n2. Both utilize the Transformer architecture with self-attention mechanisms as the core sequence encoder, including multi-head attention and feed-forward networks (FFN).\n3. Both employ embedding layers to convert item IDs into dense vector representations, with learnable embedding matrices.\n4. Both use causal (unidirectional) attention masking to ensure the model only attends to previous items when predicting the next item, preventing information leakage from future positions.\n5. Both apply layer normalization and dropout for training stabilization and regularization, following standard Transformer practices.\n6. Both use the final position's hidden representation as the user representation for next-item prediction.\n7. Both compute prediction scores via dot-product between the user representation and item embeddings, followed by softmax for probability distribution.\n8. Both support cross-entropy loss as the training objective for next-item prediction.\n9. Both handle variable-length sequences by padding shorter sequences and truncating longer ones to a maximum length.\n10. The stacking of multiple self-attention blocks (layers) is common to both, enabling hierarchical feature extraction from the sequence.",
    "differences": "1. **Side-Information Integration**: SASRec uses only item IDs, while DIFF incorporates multiple item attributes (e.g., brand, category) alongside item IDs. The paper suggests creating separate embedding layers for each attribute type and implementing a fusion mechanism to combine heterogeneous information sources effectively.\n2. **Frequency-based Noise Filtering**: DIFF introduces signal processing techniques using Discrete Fourier Transform to decompose embeddings into low-frequency (stable long-term patterns) and high-frequency (volatile short-term patterns) components. The filtered representation follows the form: filtered_E = E_low + β * E_high, where β is a learnable scaling parameter. This preprocessing step aims to reduce noise while preserving informative signals. Implementation would involve FFT operations on embedding sequences, frequency component separation, inverse FFT, and recombination with trainable weights.\n3. **Dual Fusion Strategy**: DIFF employs both early fusion (combining ID and attributes before encoding) and intermediate fusion (combining attention scores during encoding). The paper describes early fusion as concatenating or summing different embedding types to create a unified representation, while intermediate fusion operates at the attention score level by merging attention matrices from different information sources before applying softmax. SASRec uses only a single information stream.\n4. **Dual-Path Architecture**: According to the paper, DIFF maintains two parallel encoding paths: one focused on item ID sequences with attribute guidance (ID-centric), and another processing early-fused embeddings (attribute-enriched). The final representation is a weighted combination of both paths using a balancing parameter α. This design allows the model to capture both ID-specific patterns and attribute-enriched patterns simultaneously.\n5. **Decoupled Attention Computation**: DIFF computes separate query and key projections for item IDs and each attribute type. The attention scores from these multiple sources are then aggregated (via summation, concatenation, or gating depending on fusion_type) before softmax normalization. This differs from SASRec's unified attention mechanism operating on a single embedding stream.\n6. **Position Encoding Strategy**: Both methods use learnable position embeddings. However, DIFF's dual-path design suggests position information may be utilized differently in the ID-centric path versus the attribute-enriched path. Position embeddings could be processed through separate query/key projections to generate position-aware attention scores that are combined with content-based scores.\n7. **Representation Alignment Objective**: DIFF introduces an auxiliary alignment loss to ensure semantic consistency between item ID embeddings and attribute embeddings. The paper describes using contrastive learning or similarity matching between the two representation spaces. This multi-task objective (main recommendation loss + alignment loss weighted by λ) may help prevent representation collapse and improve feature learning.\n8. **Feature Embedding Pipeline**: For handling multiple attribute types, DIFF would require a feature embedding mechanism that can process categorical attributes (like categories, brands) into dense vectors. The paper suggests using separate embedding layers for each attribute and aggregating them via pooling or attention mechanisms before fusion with ID embeddings.\n9. **Trainable Frequency Control**: DIFF introduces learnable parameters to control the contribution of high-frequency components for each embedding type (item IDs and each attribute). This allows the model to adaptively determine the optimal noise filtering strength for different information sources during training.\n10. **Hyperparameter Configuration**: Key hyperparameters mentioned in the paper include: α (balance between dual paths), λ (alignment loss weight), β (frequency filtering strength), c (frequency cutoff threshold), and fusion_type (sum/concat/gate). These parameters control different aspects of the model's architecture and training dynamics."
  },
  {
    "source": "FDSA_2020",
    "target": "DIFF_2025",
    "type": "in-domain",
    "similarities": "1. Both methods address the sequential recommendation problem with side-information (item attributes/features), aiming to predict the next item based on user interaction history.\n2. Both methods use separate embedding layers for item IDs and item attributes/features, maintaining distinct embedding matrices for different types of information.\n3. Both methods employ Transformer-based self-attention mechanisms as the core sequence encoder, using multi-head attention with query, key, and value projections.\n4. Both methods apply position embeddings to inject positional information into the sequence representations.\n5. Both methods use feed-forward networks (FFN) after attention layers to introduce non-linearity and capture complex feature interactions.\n6. Both methods apply layer normalization and dropout for training stabilization and regularization.\n7. Both methods use a prediction layer that computes dot-product similarity between the final user representation and item embeddings for next-item prediction.\n8. Both methods support cross-entropy loss for training the recommendation task.\n9. Both methods process item ID sequences and attribute sequences through separate encoding paths before fusion, following a dual-stream architecture pattern.",
    "differences": "1. **Frequency-Domain Noise Filtering**: DIFF introduces frequency-based filtering using Discrete Fourier Transform to decompose embeddings into low-frequency and high-frequency components. The filtered representation follows: filtered_E = E_LFC + β * E_HFC, where β is a learnable scalar controlling the high-frequency contribution. FDSA does not employ frequency-domain processing, relying solely on standard time-domain Transformer operations.\n2. **Fusion Strategy Hierarchy**: DIFF employs both early fusion (combining item ID and attribute embeddings before encoding) and intermediate fusion (combining attention scores during encoding). FDSA uses late fusion, concatenating the final outputs of separate Transformer encoders and applying a linear projection. This difference reflects distinct design philosophies: DIFF's multi-stage fusion versus FDSA's post-encoding aggregation.\n3. **Attention Score Aggregation**: In DIFF's ID-centric path, attention scores computed from item IDs and attributes are aggregated before softmax normalization. FDSA fuses the output representations after attention computation via concatenation and linear transformation. This means DIFF performs score-level fusion while FDSA performs representation-level fusion.\n4. **Representation Alignment**: DIFF introduces an auxiliary alignment loss to ensure semantic consistency between item ID and attribute embedding spaces. The paper describes using similarity matching or contrastive objectives to align these representations. FDSA does not include such alignment mechanisms, using only the main recommendation loss.\n5. **Feature Aggregation Mechanism**: FDSA uses VanillaAttention (a weighted aggregation mechanism) to combine multiple feature embeddings with learned attention weights. DIFF uses simpler fusion functions (summation, concatenation, or gating controlled by fusion_type parameter) for combining attribute embeddings. These represent different approaches to multi-source information integration.\n6. **Dual-Path User Representation**: DIFF computes the final user representation as: R_u = α * R_v + (1-α) * R_va, where R_v comes from the ID-centric path and R_va from the attribute-enriched path, with α controlling the balance. FDSA concatenates encoder outputs and applies a linear layer without explicit path weighting.\n7. **Frequency Parameter Learning**: DIFF introduces learnable scalar parameters to control high-frequency component strength for each embedding type (item IDs and each attribute). This allows adaptive noise filtering where different information sources may require different filtering strengths based on their inherent noise characteristics.\n8. **Temperature-Scaled Alignment**: In DIFF's alignment loss, similarity scores are scaled by a learnable temperature parameter τ before applying softmax. This temperature scaling helps control the sharpness of the similarity distribution and can improve alignment quality. FDSA does not employ temperature-scaled similarity matching.\n9. **Multi-Task Objective**: DIFF's total loss combines the recommendation loss and alignment loss: L = L_rec + λ * L_align, where λ balances the two objectives. FDSA uses a single-task objective focused solely on recommendation accuracy. This multi-task design in DIFF aims to improve both prediction performance and representation quality simultaneously."
  },
  {
    "source": "SASRec_2018",
    "target": "FAME_2025",
    "type": "in-domain",
    "similarities": "1. Both methods are built upon the self-attentive sequential recommendation framework, using Transformer-based architecture to model user behavior sequences for next-item prediction.\n2. Both use item embedding layers to convert item IDs into dense vector representations, with learnable embedding matrices of dimension d.\n3. Both employ positional embeddings to inject position information into the sequence, as the self-attention mechanism itself is position-agnostic.\n4. Both utilize the scaled dot-product attention mechanism with queries, keys, and values, following the standard Transformer attention formulation.\n5. Both apply Layer Normalization and Dropout for regularization and training stability, using residual connections to facilitate gradient flow.\n6. Both use point-wise feed-forward networks (FFN) with ReLU activation to introduce non-linearity after the attention layer.\n7. Both generate the final prediction by computing dot product between the sequence representation and item embeddings.\n8. Both use cross-entropy loss as the training objective for next-item prediction.\n9. The target paper (FAME) explicitly states it is built upon SASRec and can be integrated into any attention-based recommendation model, maintaining the core SASRec structure.",
    "differences": "1. **Multi-Head Prediction Mechanism**: SASRec concatenates all attention heads and uses a single FFN for prediction, while FAME treats each attention head independently for prediction. Each head generates its own preference score using sub-embeddings of dimension d'=d/H, requiring implementation of separate prediction paths per head.\n2. **Head-Specific Item Sub-Embeddings**: FAME introduces learnable projection matrices W_f^(h) to transform item embeddings into head-specific sub-embeddings x_v^(h) for each head h. This requires adding H projection matrices of size d×d' and computing separate item representations per head.\n3. **Shared Head-Specific FFN**: FAME uses a shared FFN' across all heads but with reduced dimension d' instead of d. The FFN' parameters (W_1', W_2', b_1', b_2') operate on d'-dimensional inputs, requiring modification of the FFN layer dimensions.\n4. **Gate Mechanism for Head Aggregation**: FAME introduces a gate mechanism (Equation 10-11) with parameters W_g and b_g to compute importance weights for each head's prediction. The final preference score is a weighted sum of all heads' predictions, requiring implementation of softmax-based gating.\n5. **Mixture-of-Experts (MoE) Self-Attention**: FAME replaces the standard query generation with N expert networks per head. Each expert has its own query projection matrix W_Q(n)^(h), generating N different query vectors per item per head. This requires implementing N parallel query transformations.\n6. **Expert Router Network**: FAME adds a router network with parameters W_exp^(h) to dynamically weight the importance of each expert's output within each head. The router computes importance scores β via softmax over concatenated expert representations.\n7. **Integrated Item Representation**: The final item representation in each head is computed as a weighted sum of all experts' outputs (Equation 17), aggregating multiple preference perspectives within each facet.\n8. **Training Pipeline**: FAME employs a two-stage training approach: first pre-training the base SASRec model, then replacing the final layer's query matrix with MoE network and fine-tuning end-to-end. The new components (FFN', gate, router) are randomly initialized.\n9. **Architecture Modification Location**: FAME only modifies the final Transformer layer, keeping earlier layers unchanged. The MoE attention replaces only the query generation while retaining original key and value matrices.\n10. **Computational Structure**: While SASRec has a single attention pathway, FAME requires computing N×H attention patterns (N experts × H heads) in the final layer, significantly increasing the computational graph complexity for the last layer.",
    "rank": "rank1"
  },
  {
    "source": "SASRec_2018",
    "target": "BASRec_2025",
    "type": "in-domain",
    "similarities": "1. Both papers adopt the same sequential-recommendation backbone: an item-embedding matrix M∈ℝ^{|I|×d}, learnable position embeddings P∈ℝ^{n×d}, a stack of TransformerEncoder blocks (self-attention + FFN), LayerNorm & residual paths, and a final MF-style prediction layer r_{i,t}=F_t·M_i^T; the RecBole SASRec implementation (TransformerEncoder, item_embedding, position_embedding, LayerNorm, dropout, BCE/BPR loss) can be used as-is.\n2. Training objective is identical binary cross-entropy with negative sampling; the calculate_loss() routine in SASRec (Eq. 13 in BASRec) can be reused verbatim, including the sigmoid-based BCE on (H_u, E_u^+, E_u^-).\n3. Two-stage training strategy in BASRec re-uses the first stage of SASRec (warm-up epochs with original sequences only) before augmentation is turned on; the existing RecBole trainer loop needs only a flag to freeze/unfreeze augmentation.",
    "differences": "1. **Data Augmentation Framework**: BASRec introduces a comprehensive augmentation strategy operating at two levels. The paper describes single-sequence augmentation techniques that modify individual sequences through reordering operations or item substitution based on similarity. Additionally, cross-sequence augmentation performs mixup operations across different user sequences in the batch. These augmentation modules are applied in the embedding space rather than the input space, allowing for smooth interpolation between representations.\n2. **Sequence Manipulation Operations**: The paper proposes two primary sequence-level operations: (a) M-Reorder, which randomly shuffles a contiguous sub-sequence within the original sequence to disrupt local order patterns while maintaining overall context; (b) M-Substitute, which replaces selected items with semantically similar alternatives to introduce controlled noise. The substitution candidates are typically selected based on embedding similarity computed via cosine distance.\n3. **Embedding-Space Mixup**: BASRec applies mixup in the learned representation space following a Beta distribution for interpolation coefficients. The paper mentions two variants: item-wise mixup (operating along the sequence length dimension with coefficients Λ_I ∈ ℝ^{B×N}) and feature-wise mixup (operating along the feature dimension with Λ_F ∈ ℝ^{B×D}). The mixup formula follows: mixed_emb = λ * emb1 + (1-λ) * emb2, where λ ~ Beta(α, α).\n4. **Adaptive Loss Weighting Mechanism**: The paper introduces a weighting scheme for augmented samples to prevent over-regularization. The weight is computed based on the augmentation rate (proportion of sequence modified) and the mixup coefficient. The formula has the form: ω = normalize(1/(rate * λ)), mapping raw weights to a [0,1] range via min-max normalization. This ensures that heavily augmented samples (higher rate and stronger mixing) receive lower loss weights.\n5. **Cross-Batch Mixing Strategy**: For cross-sequence augmentation, the paper describes shuffling the batch and pairing sequences randomly. The mixup can be applied at different granularities: either uniformly across all positions (item-wise) or uniformly across all features (feature-wise), with the choice controlled by a strategy parameter (item_wise/feature_wise/both).\n6. **Two-Stage Training Protocol**: BASRec employs a curriculum learning approach with two distinct training stages. Stage 1 trains the base model on original sequences without augmentation to establish a stable foundation. Stage 2 introduces augmentation gradually. The transition is controlled by epoch counting (if current_epoch >= stage1_epochs). This staged approach helps prevent early training instability caused by aggressive augmentation.\n7. **Similarity-Based Item Selection**: For M-Substitute, the paper suggests computing item similarity to find appropriate substitution candidates. This could be implemented by: (a) computing pairwise cosine similarity between all item embeddings; (b) for each item, caching its top-k most similar items; (c) randomly selecting from these candidates during substitution. The similarity matrix can be computed once and reused (with gradient detachment to avoid backpropagation through the similarity computation).\n8. **Multi-Component Loss Architecture**: The total training objective combines three components: (a) main loss on original sequences (ensures basic recommendation quality); (b) single-sequence augmentation loss with adaptive weighting (improves robustness to local perturbations); (c) cross-sequence augmentation loss (enhances generalization across users). All three losses use the same base loss function (CE or BPR) but operate on different augmented views of the data.\n9. **Forward Pass Augmentation Control**: The paper suggests implementing augmentation as conditional branches within the forward() method, controlled by an 'augment' flag. During training, the method is called multiple times with different augmentation settings. During inference, augmentation is disabled to use only the original sequence. This design keeps augmentation logic within the model while allowing flexible control during training and testing.\n10. **Hyperparameter Sensitivity**: Key hyperparameters mentioned include: α (Beta distribution parameter, typical range 0.2-0.6), rate_min and rate_max (augmentation intensity bounds, e.g., 0.1-0.7), stage1_epochs (warm-up duration, e.g., 50 epochs), and mixup_strategy (item_wise/feature_wise/both). The paper emphasizes that these parameters may need dataset-specific tuning to balance augmentation benefits against noise introduction.",
    "rank": "rank1"
  },
  {
    "source": "FEARec_2021",
    "target": "BASRec_2025",
    "type": "in-domain",
    "similarities": "1. Both methods adopt the standard Transformer-based sequential recommendation architecture, utilizing item embeddings with positional encodings, layer normalization, and dropout for regularization.\n2. Both employ stacked encoder blocks with feed-forward networks (FFN) using GELU activation, residual connections, and layer normalization following standard Transformer design principles.\n3. Both methods use the same prediction interface: computing dot-product similarity between the sequence representation and item embeddings, supporting both single-item and full-ranking prediction scenarios.\n4. Both follow similar training protocols with two-stage strategies and utilize the same data processing pipeline for handling variable-length sequences with padding and masking.",
    "differences": "1. **Frequency-Domain Processing**: FEARec employs frequency-domain analysis with FFT-based operations for sequence decomposition, while BASRec focuses on standard self-attention mechanisms without frequency-domain transformations. This represents a fundamental difference in how temporal patterns are captured—frequency decomposition versus direct attention over time steps.\n2. **Data Augmentation Focus**: BASRec introduces comprehensive data augmentation strategies including sequence reordering and item substitution at both single-sequence and cross-sequence levels, with embedding-space mixup operations. FEARec does not employ such augmentation techniques, relying instead on frequency filtering for noise reduction.\n3. **Loss Function Design**: The papers adopt different loss functions suited to their respective objectives. BASRec's augmentation framework requires adaptive loss weighting to balance original and augmented samples, while FEARec uses standard loss functions without per-sample weighting schemes.\n4. **Architecture Simplicity**: BASRec maintains a relatively straightforward architecture based on standard TransformerEncoder without additional frequency analysis components. FEARec incorporates hybrid attention mechanisms that combine time-domain and frequency-domain information, requiring specialized attention modules.\n5. **Regularization Strategy**: FEARec employs frequency-domain regularization and contrastive learning objectives, while BASRec achieves regularization through data augmentation and adaptive loss weighting. These represent different philosophical approaches to preventing overfitting and improving generalization.",
    "rank": "rank2"
  },
  {
    "source": "SASRec_2018",
    "target": "TALE_2025",
    "type": "in-domain",
    "similarities": "1. Both model user-item interactions as sequences and predict the next item given a prefix; the SASRec code already provides the dataset parsing logic (`SequentialRecommender` base class) that outputs `item_seq` and `item_seq_len` tensors—exactly the tensors TALE needs to build source/target pairs.\n\n2. Both papers perform sequence truncation/padding to a fixed length `n`; SASRec’s embedding layer (`item_embedding` with `padding_idx=0`) and position tensor construction (`position_ids = torch.arange(...)`) can be reused to create TALE’s one-hot/multi-hot source matrix rows without rewriting the batching pipeline.\n\n3. Both use popularity-aware weighting: SASRec implicitly down-weights popular items via the cross-entropy loss over the whole item corpus; TALE explicitly forms a diagonal popularity matrix. The SASRec popularity tensor (`test_item_emb = self.item_embedding.weight`) can be re-purposed to pre-compute TALE’s trend-aware popularity counts per time-window offline.\n\n4. Training loop skeleton (Adam optimizer, negative sampling, GPU collation) and metric computation (`full_sort_predict` + ranking metrics) in the SASRec implementation are dataset-agnostic and can be kept for TALE after replacing the model forward pass with the closed-form matrix solver.",
    "differences": "1. TALE replaces the entire neural backbone (self-attention blocks, FFN, residual LN, dropout) with a closed-form ridge-regression solver that produces a dense item-item transition matrix `B̂`; a new module must wrap `torch.linalg.solve` or `lstsq` to compute Eq. (6) on GPU/CPU and store `B̂` as a parameter buffer—no transformer encoder is needed.\n\n2. TALE requires timestamp tensors `T^u` for every sequence; SASRec’s dataloader only yields item ids. A new data column `TIMESTAMP_SEQ` must be added to the dataset config and collate function so that each mini-batch provides aligned `(item_seq, timestamp_seq)` tensors for computing the time-interval weight matrix `W_time` online via Eq. (13).\n\n3. TALE introduces two new weight matrices (`W_time`, `W_trend`) that are not static dropout masks but data-dependent, sparse, and row-specific; implement two custom CUDA-friendly operators: (a) an exponential time-decay kernel with threshold `c` and learnable `τ_time`, and (b) a sliding-window popularity counter that outputs `P_trend` for each calendar window of `N` days—both absent in SASRec.\n\n4. TALE uses single-target augmentation (only the immediate next item) whereas SASRec trains on every position via teacher forcing. Create a new sampler that emits triples `(source_multi_hot, target_one_hot, delta_time)` instead of the current `(item_seq, pos_item)` pairs; the existing `SequentialRecommender` dataset class must be subclassed or re-configured to yield these structures.",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "NextItNet_2018",
    "type": "in-domain",
    "similarities": "1. Both models inherit from `SequentialRecommender` and adopt pairwise BPR loss (`BPRLoss()`), so the training loop, negative sampling and `calculate_loss()` skeleton can be reused almost verbatim; only the score computation inside `forward()` needs replacement.\n2. Item embedding tables (`nn.Embedding(n_items, embed_dim)`) and Xavier/uniform weight initialization patterns are identical—`self.item_embedding` in NextItNet can reuse the same init helper used for `IU_emb/IL_emb` in FPMC.\n3. Prediction interfaces (`predict()`, `full_sort_predict()`) follow the same pattern: extract final user/sequence representation, dot with candidate item embeddings, return scores—only the source of the representation tensor changes (FPMC: MF+FMC; NextItNet: dilated-CNN output).\n4. Both papers regularize embedding weights with L2; the `RegLoss()` utility and `reg_weight` scheduling in FPMC can be kept for NextItNet with no change.",
    "differences": "1. NextItNet requires a deep 1D dilated CNN pipeline (`ResidualBlock_b` stacks) that is completely absent in FPMC—new modules `ResidualBlock_b`, `conv_pad` with causal (left) padding, layer-norm, ReLU, and 1×1/1×3 convolutions must be written from scratch.\n2. Causal masking to prevent future-item leakage: FPMC has no mask; NextItNet needs padding of size `(kernel-1)*dilation` at every dilated layer—implement inside `conv_pad()` and ensure `forward()` preserves temporal causality.\n3. NextItNet outputs a dense sequence representation per position (teacher-forcing on `{x1…xt}`), hence uses `CrossEntropyLoss` with full-softmax or sampled-softmax; FPMC only scores a single next basket—extend loss choice in `calculate_loss()` to handle `CE` branch and sampled-softmx helper.\n4. Positional/order modeling: FPMC uses Markov order-1 plus user factor; NextItNet relies solely on dilated receptive fields—no positional embedding is added, but dilations list `[1,2,4,8]*block_num` and `kernel_size` hyper-parameters must be exposed in config and constructed dynamically.\n5. Receptive-field management: NextItNet needs `block_num`, `dilations` and residual shortcut logic; FPMC is shallow—add these new config keys and build the `Sequential(*rb)` stack in `__init__`.",
    "rank": "rank2"
  },
  {
    "source": "SASRec_2018",
    "target": "SINE_2021",
    "type": "in-domain",
    "similarities": "1. Both follow RecBole SequentialRecommender I/O: read ITEM_SEQ and ITEM_SEQ_LEN from interaction, output a [B, H] sequence representation, and compute scores via a single matrix multiplication with item_embedding.weight in full_sort_predict().\n2. Both rely on nn.Embedding(n_items, embed_dim, padding_idx=0) and standard xavier_normal_ / LayerNorm initialization; the embedding+LayerNorm setup and _init_weights() style are directly reusable.\n3. Both support CE/BPR-style training branches in calculate_loss() and use the same score template: seq_output * pos/neg item embeddings for BPR or seq_output @ item_embedding.T for CE.\n4. Both keep the prediction pipeline identical (predict for a single item, full_sort_predict for all items), so inference and evaluator integration are copy-paste compatible.",
    "differences": "1. SINE introduces a learnable prototype codebook C (nn.Embedding(L, D)) with prototype_size and interest_size, plus custom parameters w1–w4 and w_k_1/w_k_2 for sparse-interest extraction and top-K activation; SASRec has no external memory bank.\n2. SINE’s forward() is a multi-step concept-activation → intention-assignment → interest-aggregation pipeline with batched softmaxes and normalization, which is completely absent in SASRec’s TransformerEncoder path.\n3. SINE does not use positional embeddings or causal attention masks; the sequence signal is captured via prototype attention rather than masked self-attention.\n4. SINE adds an NLL loss option: it applies log_softmax to full logits and adds calculate_reg_loss() * reg_loss_ratio; SASRec typically uses CE/BPR without this covariance regularizer.",
    "rank": "rank1"
  },
  {
    "source": "Caser_2018",
    "target": "SINE_2021",
    "type": "in-domain",
    "similarities": "1. Both models use the same RecBole SequentialRecommender interface and interaction fields; calculate_loss/predict/full_sort_predict signatures can be aligned one-to-one.\n2. Both compute final scores by dot product between a sequence representation and item_embedding.weight, making full_sort_predict() identical in structure.\n3. Both initialize nn.Embedding weights with xavier_normal_ and keep LayerNorm parameters (bias=0, weight=1) for stable training.\n4. Both can train with CE loss over the full item vocabulary using logits = seq_output @ item_embedding.T.",
    "differences": "1. Caser’s encoder is CNN-based with horizontal/vertical convolutions and max-pooling; SINE replaces this with a prototype-driven sparse-interest module and has no convolution layers.\n2. SINE introduces concept activation via w1/w2, a prototype bank C, and a top-K selection (interest_size) that does not exist in Caser.\n3. SINE uses dual attention matrices (P_k|t and P_t|k) and normalization (ln2, ln4) to align item embeddings with prototypes; Caser has no such attention or normalization stack.\n4. SINE includes a covariance regularizer over the prototype matrix (calculate_reg_loss) and an optional NLL loss with log_softmax; Caser only applies standard CE/BPR without this additional term.\n5. Caser uses user_embedding in addition to item_embedding, while SINE is user-agnostic and only learns item/prototype parameters.",
    "rank": "rank2"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "SINE_2021",
    "type": "in-domain",
    "similarities": "1. Both operate on ITEM_SEQ/ITEM_SEQ_LEN and output a single sequence vector used for prediction; predict() and full_sort_predict() are identical in shape and scoring logic.\n2. Both use nn.Embedding with padding_idx=0 and xavier_normal_ initialization, and keep device handling consistent with self.device.\n3. Both support CE and BPR branches in calculate_loss() using the same pos/neg embedding dot-product pattern.\n4. Both are RecBole SequentialRecommender subclasses, so training loop, evaluator, and metrics integration remain unchanged.",
    "differences": "1. GRU4Rec encodes sequences with a recurrent GRU; SINE replaces the encoder with a prototype-based sparse-interest mechanism and removes any RNN dependence.\n2. SINE defines a learnable concept codebook C and multiple attention parameter tensors (w1–w4, w_k_1/w_k_2), plus top-K selection, which GRU4Rec does not have.\n3. SINE introduces a covariance regularizer on prototype embeddings and scales it by reg_loss_ratio; GRU4Rec has no equivalent regularization term.\n4. SINE optionally trains with NLL loss by applying log_softmax over full logits, which is not part of the GRU4Rec implementation.",
    "rank": "rank3"
  },
  {
    "source": "BERT4Rec_2019",
    "target": "FEARec_2021",
    "type": "in-domain",
    "similarities": "1. Both use Transformer-style encoder blocks with multi-head attention, FFN (dense → GELU → dense), LayerNorm, residuals, and position embeddings; initialization via normal_(mean=0, std=initializer_range) is the same.\n2. Both compute full-item logits with seq_output @ item_embedding.T and reuse predict/full_sort_predict patterns for evaluation.\n3. Both share CE/BPR loss entry points and the same SequentialRecommender interaction fields for training and inference.\n4. Both use LayerNorm + Dropout on input embeddings before feeding the encoder stack.",
    "differences": "1. BERT4Rec is bidirectional and uses masked language modeling with a [MASK] token and mask_ratio; FEARec is strictly left-to-right and uses a causal attention mask (torch.triu).\n2. BERT4Rec extends item_embedding to n_items+1 and adds output_bias for masked prediction; FEARec uses n_items without a mask token or output bias.\n3. FEARec replaces vanilla attention with HybridAttention (FFT-based autocorrelation + time-domain attention) and FEAEncoder blocks, which BERT4Rec does not have.\n4. FEARec adds contrastive losses using sem_aug sequences (same_item_index) and optional frequency-domain regularization when fredom is enabled; BERT4Rec has only the masked prediction loss.",
    "rank": "rank1"
  },
  {
    "source": "SASRec_2018",
    "target": "FEARec_2021",
    "type": "in-domain",
    "similarities": "1. Both are unidirectional sequential models with position embeddings, LayerNorm + Dropout preprocessing, and final representation gathered at item_seq_len-1.\n2. Both generate a causal attention mask to prevent future-item leakage and feed it into the encoder.\n3. Both compute logits with seq_output @ item_embedding.T and support CE/BPR loss branches.\n4. Both keep the same RecBole evaluation interface (predict/full_sort_predict) with a dense [B, n_items] score matrix.",
    "differences": "1. FEARec replaces the TransformerEncoder with FEAEncoder built on HybridAttention (FFT-based frequency sampling + time-domain attention), which is not present in SASRec.\n2. FEARec adds contrastive learning: it builds sem_aug via same_item_index, injects sem_aug/sem_aug_lengths into interaction, and computes NCE losses with tau, lmd, lmd_sem.\n3. FEARec optionally adds frequency-domain regularization when fredom is enabled; SASRec has no frequency branch.\n4. FEARec exposes additional frequency-related hyper-parameters (global_ratio, dual_domain, topk_factor, fredom_type) beyond SASRec’s standard transformer config.",
    "rank": "rank2"
  },
  {
    "source": "Caser_2018",
    "target": "FEARec_2021",
    "type": "in-domain",
    "similarities": "1. Both use item embeddings with padding_idx=0 and a full-sort prediction head (seq_output @ item_embedding.T).\n2. Both can train with CE or BPR loss in calculate_loss() and share the same SequentialRecommender training loop.\n3. Both apply dropout and normalization to stabilize training and use similar initialization patterns.",
    "differences": "1. Caser is CNN-based and uses user_embedding; FEARec is user-agnostic and uses a Transformer-like FEAEncoder with HybridAttention.\n2. FEARec adds position embeddings and a causal attention mask (torch.triu), while Caser does not use positional encoding or attention masks.\n3. FEARec introduces contrastive learning (sem_aug + NCE) and optional frequency regularization when fredom is enabled; Caser has no such loss paths.\n4. FEARec depends on extra frequency/contrastive hyper-parameters (global_ratio, dual_domain, lmd, lmd_sem, tau), which are irrelevant to Caser.",
    "rank": "rank3"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "FEARec_2021",
    "type": "in-domain",
    "similarities": "1. Both rely on ITEM_SEQ/ITEM_SEQ_LEN inputs and produce a single sequence vector for scoring; full_sort_predict is the same matrix multiplication against item_embedding.weight.\n2. Both support CE/BPR losses using the standard RecBole loss factory and share the same predict() signature.\n3. Both use embedding + dropout + LayerNorm preprocessing before sequence encoding.",
    "differences": "1. FEARec replaces the GRU encoder with FEAEncoder (HybridAttention + FFN) and requires a causal attention mask; the GRU path is removed entirely.\n2. FEARec computes same_item_index to build sem_aug sequences in calculate_loss(), and applies contrastive NCE losses weighted by lmd/lmd_sem; GRU4Rec has no augmented views or NCE losses.\n3. FEARec optionally adds FFT-based frequency regularization (fredom/fredom_type) and time-delay aggregation, which are absent in GRU4Rec.\n4. FEARec introduces several frequency-related hyper-parameters (global_ratio, dual_domain, topk_factor) and requires the HybridAttention module implementation.",
    "rank": "rank4"
  },
  {
    "source": "SRGNN_2019",
    "target": "GCSAN_2021",
    "type": "in-domain",
    "similarities": "1. Both build a session graph from item_seq using alias_inputs and a normalized adjacency tensor A; the _get_slice() logic for constructing A_in/A_out and alias_inputs is directly reusable.\n2. Both use a gated GNN cell with GRU-style reset/update gates (linear_edge_in/out, w_ih/w_hh) to propagate node embeddings.\n3. Both gather the last valid position via gather_indexes(item_seq_len-1) and score by dot product with item_embedding.weight.\n4. Both support CE/BPR loss branches with the same pos/neg score computation; GCSAN then adds EmbLoss regularization.",
    "differences": "1. GCSAN adds a TransformerEncoder stack after GNN (n_layers, n_heads, inner_size, attn_dropout_prob), which SRGNN does not have.\n2. GCSAN mixes the Transformer output at and the GNN output ht with a learnable weight (weight parameter), while SRGNN uses only a GNN-based session representation.\n3. GCSAN explicitly adds EmbLoss regularization on item embeddings with reg_weight; SRGNN relies mainly on dropout.\n4. GCSAN requires an attention mask via get_attention_mask() for self-attention, which is not used in SRGNN.",
    "rank": "rank1"
  },
  {
    "source": "STAMP_2018",
    "target": "GCSAN_2021",
    "type": "in-domain",
    "similarities": "1. Both use item_embedding with padding_idx=0 and compute full_sort_predict() as seq_output @ item_embedding.T.\n2. Both output a single session vector and follow the same predict()/full_sort_predict() interface in RecBole.\n3. Both can use CE loss over the full item set in calculate_loss().",
    "differences": "1. GCSAN requires session-graph construction (_get_slice) with alias_inputs and adjacency A; STAMP has no graph preprocessing.\n2. GCSAN uses a gated GNN to update node embeddings, then applies a TransformerEncoder; STAMP uses attention pooling over RNN-free embeddings.\n3. GCSAN fuses self-attention output with last-click GNN state using weight, which is absent in STAMP.\n4. GCSAN adds EmbLoss regularization (reg_weight) and attention mask generation; STAMP does not include these components.",
    "rank": "rank2"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "GCSAN_2021",
    "type": "in-domain",
    "similarities": "1. Both consume ITEM_SEQ/ITEM_SEQ_LEN and output a single sequence vector for scoring; predict() and full_sort_predict() are identical in shape and scoring.\n2. Both support CE/BPR loss branches with the same pos/neg dot-product computation and loss_fct usage.\n3. Both rely on item_embedding with padding_idx=0 and use xavier_normal_ initialization for embeddings.",
    "differences": "1. GCSAN replaces the GRU encoder with a session-graph GNN plus TransformerEncoder stack, requiring _get_slice() and GNNCell implementations.\n2. GCSAN introduces alias_inputs-based indexing to map sequence positions to unique graph nodes, which GRU4Rec does not need.\n3. GCSAN adds a learnable fusion weight between self-attention output and last GNN state, while GRU4Rec only uses the final GRU hidden state.\n4. GCSAN adds EmbLoss regularization via reg_weight, not present in GRU4Rec.",
    "rank": "rank3"
  },
  {
    "source": "FPMC_2010",
    "target": "GCSAN_2021",
    "type": "in-domain",
    "similarities": "1. Both use item_embedding tables with padding_idx=0 and compute scores through vector dot-products against item_embedding.weight.\n2. Both can be trained with BPR loss using the same pos/neg score template in calculate_loss().\n3. Both use full_sort_predict() with a single matrix multiplication for efficient inference.",
    "differences": "1. GCSAN requires building session graphs and normalized adjacency matrices in _get_slice(), which is absent in FPMC’s Markov factorization.\n2. GCSAN adds a GNN with GRU-style gates and a TransformerEncoder block; FPMC has no graph propagation or attention layers.\n3. GCSAN fuses GNN and self-attention representations with a learnable weight parameter, whereas FPMC directly scores MF+FMC terms.\n4. GCSAN adds EmbLoss regularization and attention masking for self-attention; FPMC lacks these components.",
    "rank": "rank4"
  },
  {
    "source": "SASRec_2018",
    "target": "HGN_2019",
    "type": "in-domain",
    "similarities": "1. Both models adopt learnable item embeddings (nn.Embedding) to map item IDs → ℝᵈ; the source’s self.item_embedding can be reused with different parameter naming (source: item_embedding, target: item_embedding + item_embedding_for_prediction).\n2. Both use a position-agnostic sequence encoder that outputs a single user/sequence vector of size [B, d]; reuse SASRec’s gather_indexes() utility to extract the final hidden state after pooling.\n3. Training pipeline is identical: BPR or CE loss with Adam, negative sampling, full_sort_predict() for fast evaluation—copy SASRec’s calculate_loss() scaffold and only replace the forward() call.\n4. Prediction head is a simple vector-inner-product layer: score = seq_output · item_emb^T; SASRec’s predict() & full_sort_predict() can be kept verbatim once seq_output is produced.",
    "differences": "1. HGN needs a **user embedding table** (nn.Embedding(n_user, d)) that SASRec lacks; add this to __init__ and pass user IDs through the whole forward path.\n2. HGN introduces two novel gating modules (feature & instance) that have no analogue in SASRec; implement feature_gating() and instance_gating() with separate Linear layers w1-w4 and a learnable bias b, then chain them before pooling.\n3. HGN keeps **two distinct item embedding matrices**—one for input sequence (E) and one for prediction (Q)—whereas SASRec shares M; clone the embedding layer under a second name item_embedding_for_prediction and freeze/disable weight tying.\n4. HGN aggregates the sequence by **weighted average pooling** (or max) using instance-gate scores, not by taking the last hidden state; replace SASRec’s gather_indexes() step with pool_type-controlled pooling that uses the gate scores returned by instance_gating().\n5. Regularisation: HGN adds explicit L2 on all gating weights and embeddings; extend SASRec’s loss with an extra reg_loss() term that accumulates ||w1||…||w4||, ||u||, ||e||, ||q||.",
    "rank": "rank1"
  },
  {
    "source": "NextItNet_2018",
    "target": "HGN_2019",
    "type": "in-domain",
    "similarities": "1. Both adopt embedding lookup for items (NextItNet: `self.item_embedding` → HGN: `self.item_embedding` + `self.item_embedding_for_prediction`)—the tensor shape `[batch, seq, d]` and nn.Embedding initialization can be reused.\n2. Both output a single user/state vector per sequence and score all items with an inner-product (`torch.matmul(seq_output, all_item_emb.T)`); the final `full_sort_predict` routine is almost drop-in.\n3. Support optional BPR or Cross-Entropy loss; the loss factory pattern (`if loss_type=='BPR': BPRLoss() else: CE`) and negative sampling plumbing can be copied.\n4. Regularization on embeddings and dense weights is computed the same way (`torch.norm(p=2)` summation) and added to the total loss; the helper `reg_loss()` structure can be adapted.\n5. Dilated-causal padding mask (`conv_pad` with left-only padding) guarantees no future leakage; the same padding math `(kernel-1)*dilation` and `ZeroPad2d` can be reused if HGN later experiments with masked convolutions.",
    "differences": "1. HGN requires user embeddings (`self.user_embedding`) and user-dependent gating—completely absent in NextItNet; new modules `feature_gating()` and `instance_gating()` must be written.\n2. HGN replaces the stacked dilated residual CNN with simple MLP gates (`w1`, `w2`, `w3`, `w4`) plus pooling; all `ResidualBlock_*` classes and dilated convolution utilities can be removed.\n3. HGN explicitly models instance-level item-item product (`torch.sum(seq_item_embedding, dim=1)`); an extra summation branch has to be added inside `forward()`.\n4. HGN prediction layer adds three terms (`user_emb + gated_short + item_item`) instead of a single CNN output; the final projection Linear layer in NextItNet is unnecessary—output vector is directly used.\n5. HGN needs two separate item embedding tables (input & output) to support the asymmetric item-item product; the second table `item_embedding_for_prediction` and its initialization must be introduced.",
    "rank": "rank2"
  },
  {
    "source": "FPMC_2010",
    "target": "HRM_2020",
    "type": "in-domain",
    "similarities": "1. Both model user-item interactions via embedding dot-product: FPMC's MF term (v_u·v_i) is identical to HRM's final score (v^Hybrid·v^I); reuse the same embedding matrices and BPR loss from FPMC implementation\n2. Both capture sequential transition by aggregating the last-basket items: FPMC averages item embeddings in the last basket; HRM applies avg/max pooling over the last H items—reuse the item-embedding lookup and pooling logic from FPMC's 'fmc' branch\n3. Both use pairwise BPR loss with negative sampling; the loss function class BPRLoss and mini-batch sampling loop in FPMC can be copied verbatim\n4. Both train with SGD and Xavier-normal embedding initialization; keep FPMC's _init_weights() and dropout wrapper",
    "differences": "1. HRM introduces a two-level pooling hierarchy: layer-1 pools item embeddings of the last H positions, layer-2 pools user embedding with the layer-1 result—requires new forward() path with configurable pooling_type_layer_1 & pooling_type_layer_2 (max/avg) not present in FPMC\n2. HRM uses high-order history (H>1) instead of FPMC's single last-basket; implement slicing logic high_order_item_embedding = seq_item_embedding[:, -self.high_order:, :] and length-aware masking\n3. HRM concatenates user and transaction vectors before layer-2 pooling, creating a 2×embedding_size tensor; add torch.cat([user, transaction], dim=1) and handle variable-length padding via inverse_seq_item()\n4. HRM supports both BPR and Cross-Entropy loss while FPMC only implements BPR; add CE branch in calculate_loss() with full softmax over |I| items, requiring full_sort_predict to return logits instead of pairwise scores",
    "rank": "rank1"
  },
  {
    "source": "SASRec_2018",
    "target": "LightSANs_2020",
    "type": "in-domain",
    "similarities": "1. Both use stacked self-attention blocks with identical sub-layer ordering: multi-head self-attention → add&norm → feed-forward → add&norm; the `TransformerEncoder` class in SASRec can be directly extended by inserting the low-rank pooling modules before the scaled-dot-product call.\n2. Identical embedding & prediction pipeline: item-embedding table, learnable position embedding, gather-last-hidden-state for next-item score; SASRec’s `embedding_layer()`, `gather_indexes()` and `item_embedding.weight` for final logits can be reused verbatim.\n3. Same training routine: binary cross-entropy with one negative sampling per position, Adam optimiser, dropout + LayerNorm for regularisation; the `calculate_loss()` and `full_sort_predict()` methods only need the new sequence representation returned by LightTransformerEncoder.\n4. Causal (left-to-right) attention mask is kept; SASRec’s `get_attention_mask()` routine that produces the lower-triangular boolean mask is still valid and should be forwarded into the new LightMultiHeadAttention module.\n5. Residual around every sub-layer and pre-LN structure (LayerNorm before SA/FF) is preserved; the residual wrapper and LayerNorm initialisation in SASRec can be copied to `LightTransformerLayer`.",
    "differences": "1. LightSANs replaces dense Q,K,V projections with low-rank decomposed attention: an `ItemToInterestAggregation` pooler compresses sequence length L → k latent interests before SDPA; this module (θ parameter + einsum) must be implemented from scratch.\n2. Position encoding is decoupled: a separate absolute-position branch (`pos_q_linear`, `pos_k_linear`) is added and its attention scores are merged with content scores inside `LightMultiHeadAttention.forward()`; SASRec has no such dual-branch design.\n3. Attention mask application changes: the mask is now applied on the reduced k×k interest-interest matrix instead of the original L×L matrix; the mask tensor has to be re-shaped to [B, n_heads, k, k] before softmax.\n4. Transformer block constructor signature is extended with `k_interests` and `seq_len` to build the pooling layers; the config parser and model init in LightSANs must expose and pass these new hyper-parameters.\n5. No explicit feature-based branch (category, brand, text) is present in SASRec; if one wants to reproduce the full FDSA ablation in the LightSANs paper, an additional feature embedding lookup, vanilla attention layer and a second LightTransformerEncoder need to be added and concatenated before the prediction head.",
    "rank": "rank1"
  },
  {
    "source": "BERT4Rec_2019",
    "target": "LightSANs_2020",
    "type": "in-domain",
    "similarities": "1. Both employ Transformer-style self-attention blocks with multi-head attention, layer-norm, residual connections and position-wise feed-forward networks (code reuse: TransformerEncoder class, FeedForward module, LayerNorm, dropout patterns).\n2. Shared embedding layer pattern: item_embedding + position_embedding summed, followed by LayerNorm and dropout (code reuse: embedding_layer() method, item_embedding & position_embedding nn.Embedding tables).\n3. Identical training pipeline: sequence truncation/padding, gather_indexes() to fetch the last valid hidden state, BPR or Cross-Entropy loss computed on that state (code reuse: calculate_loss(), predict(), full_sort_predict() skeletons and loss_fct switch).\n4. Same initialization strategy: normal_(std=initializer_range) for Linear & Embedding, constant for LayerNorm bias (code reuse: _init_weights() can be copied verbatim).",
    "differences": "1. LightSANs replaces dense Q,K,V projections with low-rank decomposed attention: ItemToInterestAggregation pools seq-len → k_interests latent factors before attention; implement new LightMultiHeadAttention module and plug it into LightTransformerLayer.\n2. LightSANs introduces decoupled absolute position encoding: separate pos_q_linear / pos_k_linear nets and pos_scaling factor; add these branches inside LightMultiHeadAttention.forward().\n3. LightSANs removes the bidirectional [mask] Cloze objective—training is purely left-to-right next-item prediction; drop mask_token, mask_ratio, reconstruct_test_data() and masked_index logic from BERT4Rec.\n4. LightSANs uses a distinct transformer encoder stack (LightTransformerEncoder) that expects pos_emb as an explicit second argument; adapt the forward() call in LightSANs class to pass position_embedding into trm_encoder.",
    "rank": "rank2"
  },
  {
    "source": "NARM_2018",
    "target": "LightSANs_2020",
    "type": "in-domain",
    "similarities": "1. Both models adopt the same sequential-recommendation interface: receive a padded variable-length item-id sequence and its true length, return the final hidden state that is fed to a dot-product scorer with the full item-embedding matrix; therefore the whole data-loader, batch collation, padding-mask generation and the final score computation (`calculate_loss`, `predict`, `full_sort_predict`) can be copied almost verbatim from NARM.\n2. Both use learnable item embeddings (`nn.Embedding(n_items, hidden_size, padding_idx=0)`) and the identical parameter-sharing item-embedding table for encoding input items and scoring candidate items; the initialisation strategy (`xavier_normal_` for embeddings and linears) and the optional BPR / cross-entropy losses are already implemented in NARM and can be reused.\n3. The training loop, BPTT truncation, learning-rate schedule, dropout placement and layer-norm eps choice are identical; the `LightSANs` class can therefore inherit the same `SequentialRecommender` base and reuse the training harness provided by RecBole.",
    "differences": "1. NARM’s core is a **bi-linear attention over GRU hidden states**, whereas LightSANs replaces the entire recurrent encoder with **stacked Transformer layers**; you must newly implement `LightTransformerEncoder`, `LightTransformerLayer`, `LightMultiHeadAttention`, `FeedForward` and the accompanying residual + layer-norm blocks—none of these classes exist in NARM.\n2. LightSANs introduces **low-rank decomposed self-attention** (`ItemToInterestAggregation` that pools sequence into `k_interests` latent interests before attention) and **decoupled absolute position encoding** (`pos_q_linear`, `pos_k_linear`); these two modules are completely absent in NARM and need fresh code.\n3. NARM uses a single **global + local session vector** concatenation (`c_t = [c_global; c_local]`) while LightSANs outputs a whole sequence of contextualised vectors and only slices the last valid position; the forward pass logic must be rewritten to feed the whole sequence through the Transformer stack and call `gather_indexes` on the last layer output.\n4. LightSANs exposes new hyper-parameters (`n_layers`, `n_heads`, `k_interests`, `inner_size`, `attn_dropout_prob`, `hidden_act`, `layer_norm_eps`) that have no counterpart in NARM; the config parser and constructor must be extended to handle them.",
    "rank": "rank4"
  },
  {
    "source": "FPMC_2010",
    "target": "LightSANs_2020",
    "type": "in-domain",
    "similarities": "1. Both models adopt the pairwise BPR ranking loss (BPRLoss in RecBole) and share the identical train/neg-item sampling loop—LightSANs can directly reuse FPMC’s calculate_loss() skeleton by only replacing the score-computation call.\n2. Item and user embeddings are stored in nn.Embedding tables initialized with xavier_normal_/normal_; the parameter initialization block and weight decay regularization code in FPMC.__init__/_init_weights can be copied verbatim.\n3. Both expose the same RecBole API—forward(item_seq, item_seq_len), predict() and full_sort_predict()—so the outer training/evaluation harness (DataLoader, trainer, early-stop) requires zero changes.",
    "differences": "1. FPMC encodes order with a single last-item embedding (LI_emb) while LightSANs needs a learnable position_embedding table and a sinusoid/pe absolute position matrix; add an Embedding(max_seq_length, hidden_size) layer and the pos_emb generation routine.\n2. LightSANs replaces FPMC’s MF+FMC dot-product scorer with a multi-layer LightTransformerEncoder that contains (a) ItemToInterestAggregation low-rank pooling, (b) LightMultiHeadAttention with decoupled content & position biases, and (c) feed-forward & LayerNorm blocks—entire new modules not present in FPMC.\n3. FPMC outputs a scalar score per (user,last_item, candidate) triple; LightSANs first produces a sequence representation via transformer layers, then applies a final dot-product with the candidate item embedding—requires gathering the last valid hidden state (gather_indexes) and a last-step projection in full_sort_predict().\n4. LightSANs supports both BPR and Cross-Entropy loss via a config flag, so the loss branch in calculate_loss() must be extended with a CE path that computes logits over the whole item vocabulary, whereas FPMC only implemented BPR.",
    "rank": "rank5"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "NARM_2018",
    "type": "in-domain",
    "similarities": "1. GRU-based sequence encoder: both models embed the click sequence via nn.Embedding, feed the embeddings into a GRU (hidden_size, num_layers, batch_first=True) and extract the last valid hidden state with gather_indexes; GRU4Rec's self.gru_layers can be directly renamed to self.gru and reused.\n2. Training pipeline & loss: both support BPR and cross-entropy; the entire calculate_loss(), predict() and full_sort_predict() routines—including negative-item sampling for BPR and the matrix-multiplication logits = seq_output @ item_emb.T for CE—can be copied verbatim.\n3. Dropout & initialization: embedding dropout (self.emb_dropout) and Xavier normal init for embeddings are identical; the dropout layer can be kept and only the probability list extended.\n4. Bi-linear prediction head: GRU4Rec's final dense (H→E) is mathematically the same shape as NARM's self.b (2H→E) up to input width; the Linear layer construction and the subsequent dot-product scoring are already in the source code.",
    "differences": "1. Dual-encoder attention module: NARM needs a parallel local encoder—Linear layers a_1, a_2, v_t to compute item-level attention αtj = v^T σ(A1 ht + A2 hj) and produce c_local = Σ αtj hj; this attention subgraph is absent in GRU4Rec and must be implemented from scratch.\n2. Session-representation fusion: NARM concatenates c_global (ht) and c_local into a 2H vector and projects it with self.b; GRU4Rec only keeps the single ht, so the concatenation logic and the 2H→E Linear layer are new.\n3. Attention mask & padding handling: NARM applies a mask = item_seq.gt(0).unsqueeze(2) to zero-out padding positions before softmax; this masking code does not exist in GRU4Rec and needs to be added to avoid attending to padded time-steps.\n4. Dropout scheduling: NARM uses a two-element dropout_probs list (emb_dropout, ct_dropout) applied at different places; GRU4Rec has only one dropout prob, so the config parser and forward pass must be extended to accept and apply the second dropout after c_t is built.",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "NARM_2018",
    "type": "in-domain",
    "similarities": "1. **Pairwise BPR loss and sampling strategy**: Both models support BPR loss (`BPRLoss()`) and sample one positive/one negative per training instance; the `calculate_loss()` pattern (pos_score, neg_score, `self.loss_fct(pos, neg)`) can be lifted from FPMC to NARM without change.\n2. **Embedding table management**: `nn.Embedding(n_items, embed_dim, padding_idx=0)` is already used in FPMC for `IU_emb`, `LI_emb`, etc.; the same initialization (`xavier_normal_`) and lookup pattern (`self.item_embedding(item_seq)`) is directly reusable for NARM’s item embeddings.\n3. **Last-item extraction utility**: FPMC’s `torch.gather(item_seq, dim=1, index=item_seq_len-1)` to obtain the last click is identical to NARM’s `gather_indexes(gru_out, item_seq_len-1)`; the helper can be copied verbatim.\n4. **Bilinear scoring head**: FPMC’s `mf + fmc` is a special case of a bilinear decoder; NARM’s final `torch.matmul(seq_output, self.item_embedding.weight.t())` in `full_sort_predict` follows the same memory-efficient matrix multiplication pattern and can reuse the same CUDA buffer strategy.",
    "differences": "1. **Sequential encoder replacement**: FPMC has no recurrent layer; NARM needs a multi-layer `nn.GRU(batch_first=True)` followed by dropout—an entirely new component that must be inserted before attention.\n2. **Attention mechanism**: FPMC uses a uniform average over the last basket; NARM requires learnable attention with `a_1`, `a_2`, `v_t` linear layers and a masked softmax to compute `α`—these layers and the ensuing weighted sum `c_local` do not exist in FPMC.\n3. **Dual-session representation & fusion**: NARM concatenates `c_global` (last GRU state) and `c_local` (attention pooled) into a 2*hidden vector; FPMC only adds two scalars. A new `torch.cat([c_global, c_local], 1)` and a `self.b` projection layer must be implemented.\n4. **Training mode flexibility**: NARM supports both BPR and cross-entropy (`loss_type=='CE'`) with full-label softmax, requiring a `CrossEntropyLoss` path and logits of shape `[B, n_items]`; FPMC only implements BPR—this second training branch is completely new code.",
    "rank": "rank3"
  },
  {
    "source": "NARM_2018",
    "target": "RepeatNet_2019",
    "type": "in-domain",
    "similarities": "1. **GRU-based session encoder** – both use a single-layer GRU (batch_first=True) to encode item sequences; NARM’s `gru()` call and output tensor `gru_out` can be reused verbatim for RepeatNet’s `all_memory`.\n2. **Attention-based context vector** – NARM’s local encoder (`a_1`, `a_2`, `v_t`, Eq. 8) and RepeatNet’s repeat/explore decoders (Eq. 7 & 9) share the same tanh(Wh_t + Uh_τ) → v^T scoring pattern; the PyTorch modules `a_1`, `a_2`, `v_t` can be duplicated and renamed to `Wr`, `Ur`, `Vr` / `We`, `Ue`, `Ve`.\n3. **Last-hidden-state extraction** – `gather_indexes(gru_out, item_seq_len-1)` is identical in both codes; the helper function can be copied without change.\n4. **Bilinear decoder idea** – NARM’s final `seq_output = self.b(c_t)` (Eq. 10) is a linear projection into item-embedding space; RepeatNet’s explore decoder uses the same pattern (`matrix_for_explore` maps 2*hidden → n_items) so the same initialization (`xavier_normal_`) and dropout placement apply.\n5. **Cross-entropy loss with ignore_index=0** – NARM’s `nn.CrossEntropyLoss()` and RepeatNet’s `F.nll_loss(..., ignore_index=0)` both operate on the final logits; the data-loader collation and masking of padded items (timeline_mask) are identical.",
    "differences": "1. **Dual-decoder architecture** – RepeatNet adds a completely new *repeat* branch (CopyNet-style scatter-add over seen items) and an *explore* branch with explicit exclusion mask (−∞ for items in session); NARM has only one decoder.  New classes `Repeat_Recommendation_Decoder` and `Explore_Recommendation_Decoder` must be written.\n2. **Repeat/explore gating network** – a lightweight attention + softmax classifier (`Repeat_Explore_Mechanism`) that outputs P(r|IS) and P(e|IS) is absent in NARM; its weights `Wre`, `Ure`, `Vre`, `Wcre` need fresh implementation.\n3. **Joint loss with mode supervision** – RepeatNet optionally adds `L_mode` (Eq. 13) that requires on-the-fly labeling of each target as repeat/explore (`repeat_explore_loss()`); NARM’s loss is pure cross-entropy.  A new training step and label-generation loop must be coded.\n4. **Scatter-add operation for repeat probabilities** – the repeat decoder uses `scatter_add_` to aggregate attention scores over multiple occurrences of the same item id; NARM never performs index-based aggregation, so this CUDA-compatible scatter logic is entirely new.\n5. **Log-softmax + NLLLoss vs. raw softmax + CrossEntropy** – RepeatNet returns log-probs for numerical stability and uses `F.nll_loss`, whereas NARM outputs raw logits to `CrossEntropyLoss`; the final layer activation and loss wrapper must be adjusted accordingly.",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "RepeatNet_2019",
    "type": "in-domain",
    "similarities": "1. Both models use pairwise embeddings (user-item & item-item) that can be inherited from FPMC's `UI_emb`, `IU_emb`, `LI_emb`, `IL_emb`; RepeatNet only needs to drop the user branch and keep item matrices.\n2. The BPR-style ranking objective in FPMC (`BPRLoss` between pos/neg items) can be reused for RepeatNet’s explore-mode branch—just swap the score computation from `mf+fmc` to the GRU-attention output.\n3. FPMC’s mini-batch sampling loop (`calculate_loss`) already produces `(seq, pos, neg)` triples; the same `Pairwise` dataloader can feed RepeatNet after removing the user column.\n4. Xavier normal initialization used in FPMC (`_init_weights`) is identical to RepeatNet’s requirement; no change needed.\n5. The `full_sort_predict` routine that materializes all-item scores with a single matrix multiply (`all_iu_emb.transpose`) can be copied for RepeatNet’s explore decoder logits before the final softmax.",
    "differences": "1. RepeatNet needs a full GRU encoder (`nn.GRU`) plus attention layers (`Repeat_Explore_Mechanism`, `Repeat_Recommendation_Decoder`, `Explore_Recommendation_Decoder`)—none exist in FPMC; these must be written from scratch.\n2. RepeatNet outputs a *mixture* probability over two modes (repeat vs explore) computed by a context vector `c_I_S^re`; FPMC has no gating network, so the `softmax(W_{re}^c c_{I_S}^{re})` branch is entirely new.\n3. Repeat decoder uses a CopyNet-style scatter-add (`scatter_add_`) to aggregate attention mass onto items that appeared in the session; FPMC’s code has no such dynamic copying operation.\n4. Position-dependent masking (`timeline_mask`, `mask=item_seq==0`) and infinite logit suppression (`masked_fill_` with `-1e9`) are required for variable-length sessions; FPMC assumes fixed last-item input and lacks masking utilities.\n5. Joint-training loss (`L_mode`) that supervises the repeat/explore gate with a binary target derived from `pos_item in item_seq` is absent in FPMC; a new auxiliary loss method (`repeat_explore_loss`) has to be implemented.",
    "rank": "rank3"
  },
  {
    "source": "TransRec_2017",
    "target": "SASRec_2018",
    "type": "in-domain",
    "similarities": "1. Both inherit from SequentialRecommender base class and use identical dataset interface (user_num, item_seq, item_seq_len, pos_item_id, neg_item_id), so the data-loading and batching logic in TransRec can be reused verbatim.\n2. Item embedding table (self.item_embedding = nn.Embedding(n_items, hidden_size, padding_idx=0)) is created with the same padding strategy; the embedding weight tensor can be initialized and regularized exactly as in TransRec.\n3. BPR pairwise ranking loss is already implemented (BPRLoss() in TransRec); SASRec can keep the same sampling loop (one neg_item per pos_item) and the identical loss computation pattern: bpr_loss(pos_score, neg_score).\n4. The gather_last_items utility that extracts the embedding at position item_seq_len-1 is identical to SASRec’s gather_indexes; the same slicing code can be copied.\n5. full_sort_predict() performs a single matrix multiplication between user hidden state and the full item embedding table—this brute-force top-k scoring routine is already written in TransRec and can be reused without change.",
    "differences": "1. TransRec has no position embedding; SASRec needs an additional nn.Embedding(max_seq_length, hidden_size) and the logic to create position_ids = torch.arange(seq_len).unsqueeze(0).expand_as(item_seq). This entire module is absent in TransRec and must be added.\n2. TransRec uses a shallow translation layer (user_emb + T + last_item_emb); SASRec replaces this with a stack of TransformerEncoder blocks (multi-head self-attention + FFN + residual + layer-norm). A full TransformerEncoder class with causal masking, scaled-dot attention, and feed-forward sub-layers needs to be implemented from scratch.\n3. Causal mask construction (upper-triangular boolean mask to prevent attending to future positions) is not present in TransRec; SASRec must build an extended_attention_mask inside forward() before calling the encoder.\n4. TransRec’s regularization (EmbLoss + RegLoss on T) is user/translation-specific; SASRec requires dropout inside every transformer block and LayerNorm parameters that need their own initialization (_init_weights for LayerNorm gamma/beta) and have no counterpart in TransRec.\n5. TransRec fixes L2-distance scoring; SASRec supports both BPR and CrossEntropy loss modes. A new branch in calculate_loss() and an extra CE loss import are required, plus the logits = seq_output @ item_embedding.weight.T matrix for full vocab CE.",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "SASRec_2018",
    "type": "in-domain",
    "similarities": "1. Both models adopt the pairwise-ranking BPR loss (BPRLoss in RecBole) and the same negative-sampling mini-batch training loop; the `calculate_loss` routine in FPMC can be reused almost verbatim—only the forward-pass changes from `self.forward(user, item_seq, item_seq_len, pos_items)` to `self.forward(item_seq, item_seq_len)` and the returned vector is dotted with item embeddings instead of being a scalar score.\n2. Item and user embedding tables (`nn.Embedding`) plus Xavier/Normal initialization and L2 regularization are identical; the FPMC code’s `_init_weights` and embedding registry pattern can be copied for SASRec’s `item_embedding` and optional `position_embedding`.\n3. Sequence truncation/padding, length tensor (`item_seq_len`), and the `gather_indexes` helper used to fetch the last valid hidden state are already present in FPMC’s `full_sort_predict`; the same preprocessing pipeline and data-loader configuration can be reused for SASRec.\n4. The prediction & evaluation interface (`predict`, `full_sort_predict`) returns a score vector via a single matrix multiplication `seq_output @ item_emb.T`; the FPMC implementation already batches this efficiently and can be kept as-is.",
    "differences": "1. FPMC has no position awareness; SASRec requires a learnable `position_embedding` table and the addition `item_emb + position_emb`—this entire positional sub-module must be newly written.\n2. FPMC’s core is a shallow MF+FMC dot product; SASRec replaces this with a multi-layer TransformerEncoder (self-attention + FFN + residual + LayerNorm). A full `TransformerEncoder` class with causal attention masking, multi-head attention, and feed-forward blocks has to be implemented from scratch or imported—none of these layers exist in FPMC.\n3. FPMC uses the user ID embedding as an explicit preference vector; SASRec is user-agnostic and must build an implicit user representation via self-attention over the item sequence—consequently the `user` tensor input must be removed and all batching logic updated.\n4. SASRec supports both BPR and Cross-Entropy losses (`CE` branch) and needs a final LayerNorm & dropout on the summed embeddings; these training paths and extra regularization hooks are absent in FPMC and need to be added.\n5. Attention requires a lower-triangular mask to prevent future peeking; FPMC has no masking concept, so a `get_attention_mask` routine that builds `[B, 1, n, n]` boolean masks must be newly coded.",
    "rank": "rank2"
  },
  {
    "source": "HRM_2020",
    "target": "SHAN_2020",
    "type": "in-domain",
    "similarities": "1. Both models adopt a hierarchical two-layer architecture to fuse user and item representations: HRM's forward() already concatenates user embedding with pooled item embeddings (torch.cat([user_embedding.unsqueeze(dim=1), high_order_item_embedding.unsqueeze(dim=1)], dim=1)) — the same tensor shape (batch_size, 2, embedding_size) that SHAN needs before its second attention layer, so the concatenation code can be reused.\n2. Both use learnable user/item embedding tables (nn.Embedding) with identical initialization (xavier_normal_/normal_) and padding_idx=0; the embedding lookup lines in HRM (self.item_embedding(seq_item) and self.user_embedding(user)) can be copied verbatim.\n3. Both optimize a ranking objective with negative sampling: HRM already implements BPRLoss and the BPR branch in calculate_loss() (pos_score/neg_score computation via element-wise mul + sum) — SHAN reuses the same BPRLoss class and identical pairwise score computation, so the entire BPR loss block and reg_loss() pattern can be transplanted.\n4. Both generate the final user vector in exactly the same dimension (batch_size, embedding_size) and compute logits by matrix-multiplying it with the full item embedding matrix (torch.matmul(seq_output, test_items_emb.transpose(0, 1))) — the full_sort_predict() routine in HRM can be reused without change.",
    "differences": "1. SHAN replaces HRM's fixed pooling (avg/max) with learnable attention pools; two new modules long_term_attention_based_pooling_layer() and long_and_short_term_attention_based_pooling_layer() must be written — they contain extra Linear+ReLU, softmax over user-guided scores, and weighted-sum aggregation that HRM does not have.\n2. SHAN needs an inverse-item-sequence tensor (INVERSE_ITEM_SEQ) to place recent items at the end; HRM already contains an inverse_seq_item() helper that reverses the sequence — reuse this helper but feed its output as INVERSE_ITEM_SEQ instead of the original seq_item.\n3. SHAN introduces separate regularization weights for embedding matrices and attention MLPs (Θ_uv vs Θ_a); the reg_loss() method must be extended to split reg_weight into λ_uv and λ_a and apply L2 to long_w/long_short_w weights explicitly.\n4. SHAN uses a short_item_length hyper-parameter to slice only the last-k items for short-term attention; HRM’s high_order parameter serves the same purpose but is applied via tensor slicing inside forward() — adapt HRM’s slicing logic (seq_item_embedding[:, -self.high_order:, :]) to create short_item_embedding in SHAN.",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "SHAN_2020",
    "type": "in-domain",
    "similarities": "1. Both adopt pairwise BPR loss with identical negative-sampling strategy: FPMC’s `BPRLoss()` and `calculate_loss()` (pos_score vs neg_score) can be reused verbatim; only the score-computation module `forward()` needs replacement.\n2. Embedding look-up & regularization pattern: `nn.Embedding` tables for users/items, Xavier/Normal init, and L2 on embeddings (`reg_loss` in SHAN) mirror FPMC’s `_init_weights` and built-in weight-decay; the same `recbole` abstract class and `InputType.PAIRWISE` pipeline are inherited.\n3. Session split & last-item indexing: FPCM already extracts the last interaction with `item_seq_len-1` and `torch.gather`; the same slicing utility can feed SHAN’s short-term set (`short_item_length` tail) and long-term set (remaining prefix) without rewriting data-loader code.",
    "differences": "1. Replace FPMC’s factorized MF+FMC score (simple dot-products) with a **two-level attention pooling** implemented in `long_term_attention_based_pooling_layer` and `long_and_short_term_attention_based_pooling_layer`; these modules contain trainable `Linear+ReLU` transformations and softmax-over-time—completely new code not present in FPMC.\n2. Add **user-aware context vectors** inside attention: the user embedding is multiplied with transformed item/α vectors (`torch.matmul(..., user_embedding.unsqueeze(2))`) to generate personalized weights; FPMC has no such context term.\n3. Introduce **masking & variable-length pooling**: padding masks (`mask = seq_item.eq(0)`) and `-1e9` logits masking in both attention layers are required; FPCM code has no mask handling because it only uses the last item.\n4. Incorporate **explicit regularization on attention weights** (`Θ_a = {W1,W2}`) via separate coefficients `λ_a` in `reg_loss`, whereas FPCM only regularizes embeddings; implement this extra term and its gradient updates.\n5. Support **CE loss branch** beside BPR; add the corresponding `CrossEntropyLoss` path in `calculate_loss` and `full_sort_predict` for completeness.",
    "rank": "rank2"
  },
  {
    "source": "STAMP_2018",
    "target": "SRGNN_2019",
    "type": "in-domain",
    "similarities": "1. Both models embed items into the same d-dimensional space and reuse the identical `nn.Embedding` layer; STAMP’s `self.item_embedding` can be copied verbatim.\n2. Session representation is built by fusing “local” last-click embedding (STAMP’s `last_inputs` / SR-GNN’s `ht`) with a “global” context vector (STAMP’s `ma` vs SR-GNN’s `a`); the same attention-style fusion pattern (learn α, weighted sum, concat, linear) appears in both—STAMP’s `count_alpha`/`mlp_a`/`mlp_b` code can be refactored into SR-GNN’s `linear_one`/`linear_two`/`linear_three` blocks with only tensor-shape renaming.\n3. Final score computation and loss are identical: inner-product `seq_output · item_emb` followed by `CrossEntropyLoss` or `BPRLoss`; STAMP’s `calculate_loss`/`full_sort_predict` routines can be reused without change.\n4. Both papers zero-pad variable-length sessions and use the same `gather_indexes` helper to fetch the last valid hidden state; this utility can be lifted directly.",
    "differences": "1. SR-GNN requires building a directed session graph and its normalized adjacency matrix `A` (size `[B, L, 2L]`); STAMP has no graph structure—implement `_get_slice` to compute `A`, `alias_inputs`, and `items` tensors from raw sequences.\n2. Node embeddings are updated via a **gated GNN** (GRU-like propagation Eq.1-5) instead of simple averaging; create a new `GNN` module with `GNNCell` that performs `linear_edge_in`/`out`, gate computations, and `step`-times propagation—completely absent in STAMP.\n3. SR-GNN uses **two separate linear transformations** (`linear_edge_in`, `linear_edge_out`) to handle incoming/outgoing edges; these layers and their bias terms (`b_iah`, `b_ioh`) must be added to the codebase.\n4. Session representation fusion in SR-GNN concatenates `[a; ht]` and compresses with `linear_transform`, whereas STAMP uses element-wise product `hs * ht`; replace STAMP’s final `seq_output = hs * ht` with `self.linear_transform(torch.cat([a, ht], dim=1))`.",
    "rank": "rank1"
  },
  {
    "source": "NARM_2018",
    "target": "SRGNN_2019",
    "type": "in-domain",
    "similarities": "1. Both models adopt an encoder-decoder pipeline that first maps a variable-length session into a single d-dimensional vector and then score every candidate item with a dot-product: reuse NARM’s `seq_output` tensor and the final `torch.matmul(seq_output, test_items_emb.transpose(0, 1))` in `full_sort_predict`.\n2. They fuse “global” vs. “local” session signals with the identical soft-attention mechanism `α = softmax(qᵀσ(W₁·last + W₂·each_step))`; NARM’s `a_1`, `a_2`, `v_t` Linear layers and the alpha-computation block can be copied almost verbatim—only the input changes from GRU hidden states to GNN node states.\n3. Training objective and mini-batch loop are identical—CrossEntropy over the whole catalogue or BPR with in-batch negatives; reuse NARM’s `calculate_loss`, `BPRLoss` branch and the `loss_fct` interface.\n4. Both codes already build on RecBole’s `SequentialRecommender`, so `item_seq`, `item_seq_len`, `ITEM_SEQ`, `POS_ITEM_ID`, etc. are handled the same way—no dataset I/O rewrite needed.",
    "differences": "1. SR-GNN needs a session→graph converter that builds the weighted adjacency `A ∈ ℝ^{B×L×2L}` (in- & out-edges) and the alias table that maps raw item id to node index; NARM has no such `_get_slice` routine—implement it from scratch.\n2. Sequence encoder is replaced by a stack of gated-GNN cells (`GNNCell`) that perform neighbourhood aggregation; GRU weights (`w_ih`, `w_hh`, `b_ih`, `b_hh`) plus the extra `linear_edge_in/out` layers must be created—no counterpart in NARM.\n3. Node embeddings are updated for `step > 1` iterations before pooling; add a loop around `GNNCell` and ensure parameter sharing across steps—NARM’s single GRU pass is insufficient.\n4. Attention pooling now runs over the *unique* node list instead of the full padded sequence; masking logic and index-gather (`torch.gather(hidden, dim=1, index=alias_inputs)`) are new—adapt NARM’s mask trick but apply it to the compressed node tensor.",
    "rank": "rank2"
  },
  {
    "source": "FPMC_2010",
    "target": "SRGNN_2019",
    "type": "in-domain",
    "similarities": "1. Both models factorize item-item transition patterns: FPMC via pairwise dot-products ⟨v_i^{IL}, v_l^{LI}⟩ and SR-GNN via learned node embeddings updated by gated-GNN; the source’s `IL_emb` & `LI_emb` tables map 1-to-1 to SR-GNN’s `item_embedding` table, so the same embedding allocation & Xavier init in FPMC (`_init_weights`) can be reused for SR-GNN’s node embeddings.\n2. Session-level preference is summarized from the last action: FPMC pools the single last item (`item_last_click`) while SR-GNN uses the last node vector `ht` as local embedding; the source’s `gather_indexes` call can be copied to extract the final hidden state in SR-GNN.\n3. Ranking scores are produced by an inner product between a session vector and candidate item vectors: FPMC’s `score = mf + fmc` reduces to `session_vec · item_vec`, identical to SR-GNN’s `seq_output · test_item_emb`; the `full_sort_predict` routine that concatenates MF+FMC logits can be reused wholesale for SR-GNN’s final score matrix multiplication.\n4. Both support BPR pairwise loss; the `BPRLoss` module and triplet sampling logic (`pos_score`, `neg_score`) in FPMC’s `calculate_loss` can be dropped into SR-GNN without change when `loss_type == BPR`.",
    "differences": "1. SR-GNN requires building a directed session graph and its sparse connection matrix `A` (size `[B, L, 2L]`)—this graph construction step in `_get_slice` (numpy pre-processing + CUDA transfer) is completely absent in FPMC and must be implemented from scratch.\n2. Gated-GNN propagation (`GNNCell` with GRU-style update/reset gates) is the core of SR-GNN; FPMC has no recurrent unit, so the entire `GNN` class, `w_ih`, `w_hh` parameters, and multi-step propagation loop are new.\n3. SR-GNN uses an attention mechanism (`linear_one`, `linear_two`, `linear_three`) to fuse local (`ht`) and global (weighted sum over all nodes) session embeddings; FPMC simply adds MF and FMC scores—this soft-attention fusion module must be newly coded.\n4. SR-GNN optionally supports cross-entropy (`loss_type == 'CE'`) over all items, needing a full-item logits matrix and `nn.CrossEntropyLoss`; FPMC only implements BPR, so the CE branch and corresponding `full_sort_predict` softmax path are additional.",
    "rank": "rank4"
  },
  {
    "source": "NARM_2018",
    "target": "STAMP_2018",
    "type": "in-domain",
    "similarities": "1. Both models use item embedding layers (nn.Embedding) with identical initialization (normal_ with std 0.002) - the STAMP embedding layer can directly reuse NARM's embedding initialization code\n2. Both employ identical loss computation pipelines: support BPR and Cross-Entropy losses via config['loss_type'], with identical matrix multiplication-based score calculation (torch.matmul(seq_output, test_item_emb.transpose(0, 1)))\n3. Both share identical data preprocessing and batch handling: use gather_indexes to extract last valid item from padded sequences, identical interaction dictionary keys (ITEM_SEQ, ITEM_SEQ_LEN, POS_ITEM_ID)\n4. Both use identical prediction interfaces: predict() for single item scoring and full_sort_predict() for all-item ranking with torch.matmul-based efficient computation\n5. Both models process variable-length sessions with padding masks and sequence length tensors, enabling direct reuse of NARM's batch collation and mask generation utilities",
    "differences": "1. STAMP removes all RNN/GRU components - must implement new count_alpha() attention function using three parallel Linear layers (w1,w2,w3) plus bias term b_a, which has no equivalent in NARM's GRU-based architecture\n2. STAMP introduces novel trilinear attention mechanism: requires implementing Equation 7's attention computation (sigmoid(W1*xi + W2*xt + W3*ms + ba)) then applying softmax over time dimension - completely absent in NARM's GRU+attention hybrid\n3. STAMP uses session-level average pooling (ms = torch.sum(org_memory)/seq_len) as long-term memory - must implement this pooling operation which replaces NARM's GRU-encoded sequential behavior\n4. STAMP employs element-wise multiplication (hs * ht) between long-term and short-term interest vectors - requires implementing this fusion mechanism instead of NARM's concatenation-based fusion [c_global; c_local]\n5. STAMP needs two separate MLPs (mlp_a and mlp_b) with tanh activation for processing long-term vs short-term memories - must add these MLP layers that have no counterpart in NARM's bilinear decoding scheme",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "STAMP_2018",
    "type": "in-domain",
    "similarities": "1. Both models adopt a two-component fusion architecture that can reuse FPMC’s parallel `mf + fmc` pattern: STAMP’s `hs * ht` (hadamard product) can be implemented by re-purposing the same broadcast-add pipeline used in FPMC’s `score = mf + fmc`—only change the element-wise operation from add to multiply and ensure both tensors are `[B, d]` before fusion.\n2. Item embedding table and pairwise BPR loss are identical: FPMC’s `self.IU_emb`/`self.IL_emb` and `BPRLoss()` can be copied verbatim; STAMP only needs one `nn.Embedding` instead of three, so delete the redundant tables and keep the same `calculate_loss` skeleton that samples `pos_items`/`neg_items` and calls `BPRLoss()`.\n3. Last-click extraction logic is the same: reuse `item_last_click = torch.gather(item_seq, dim=1, index=item_seq_len-1)` from FPMC’s forward to obtain STAMP’s `last_inputs` tensor—no indexing code needs to be rewritten.\n4. Full-batch inference pattern (`full_sort_predict`) is identical: reuse the `torch.matmul(user_repr, all_item_emb.t())` broadcasting strategy; only replace `user_repr` with STAMP’s `seq_output` (`hs * ht`) and use the single `item_embedding.weight` table.",
    "differences": "1. Attention mechanism is entirely new: implement `count_alpha()` with three linear projections (`w1,w2,w3`) plus `sigmoid` and a `softmax` over the time dimension; this module has no counterpart in FPMC.\n2. Session-level average pooling `ms` must be added: compute `ms = item_seq_emb.sum(1) / item_seq_len.unsqueeze(1).float()`—FPMC never aggregates the whole sequence, only the last position.\n3. Two MLP towers (`mlp_a`, `mlp_b`) with `tanh` activation are required; FPMC has no hidden layers—add these `nn.Linear(d,d)` modules and apply them to `ma` and `last_inputs` respectively.\n4. Position-independent modeling: unlike FPMC that keeps user embeddings (`UI_emb`), STAMP is session-only; delete all user-related embeddings and forward arguments to fit the new interface that receives only `item_seq` and `item_seq_len`.\n5. Optional cross-entropy loss: STAMP supports both BPR and CE; add a branch in `calculate_loss` that computes `nn.CrossEntropyLoss()` over the full logits matrix when `loss_type=='CE'`—FPMC implementation lacks this path.",
    "rank": "rank3"
  },
  {
    "source": "BERT4Rec_2019",
    "target": "S3Rec_2020",
    "type": "in-domain",
    "similarities": "1. Bidirectional Transformer encoder backbone: S3Rec reuses BERT4Rec's `TransformerEncoder` (multi-head self-attention + FFN + residual/LN) and the same embedding composition (item + learnable position embeddings). The `forward()` routine can be copied almost verbatim, only adding a `bidirectional` flag to switch the causal mask on/off.\n\n2. Cloze-style masked-item prediction (MIP): both papers sample positions with probability `mask_ratio`, replace them with a special `[mask]` token (`self.mask_token = self.n_items`), and train a two-layer head (`output_ffn + GELU + LN`) to recover the original item ID via cross-entropy. BERT4Rec's `calculate_loss()` (CE branch) and `multi_hot_embed` gathering logic can be reused for S3Rec's MIP objective.\n\n3. Shared item vocabulary & embedding matrix: `item_embedding` is used for both input and output layers; the final prediction layer reuses the same weight tensor (`self.item_embedding.weight`) plus an optional output bias. The parameter initialization (`_init_weights`) and embedding shape conventions (`n_items+1` for mask) are identical.\n\n4. Position-aware input construction: both stack item and position embeddings, apply dropout, layer-norm, and feed the result into the Transformer stack. The data collation utilities (`reconstruct_test_data`, padding/truncation to `max_seq_length`) can be recycled for S3Rec's fine-tuning stage.",
    "differences": "1. Four self-supervised pre-training losses vs. single Cloze loss: implement `AAP`, `MAP`, `SP` in addition to `MIP`. Each loss needs its own projection head (`aap_norm`, `map_norm`, `sp_norm`) and a contrastive InfoNCE/BCE formulation with in-batch negatives. These heads and loss computations are absent in BERT4Rec.\n\n2. Attribute-aware embeddings and inputs: introduce `feature_embedding` table for item attributes, construct multi-hot feature targets (`associated_features`), and write a new `reconstruct_pretrain_data()` routine that yields (masked_item_sequence, pos_items, neg_items, masked_segment_sequence, pos_segment, neg_segment, features) tensors. BERT4Rec has no attribute vocabulary or feature loading logic.\n\n3. Two-stage training pipeline: add `train_stage` flag (`pretrain` vs `finetune`) and a checkpoint loader (`torch.load(self.pre_model_path)`) to warm-start the fine-tuning stage. During pre-training the model is bidirectional; during fine-tuning it switches to causal (unidirectional) attention via `bidirectional=False` in `get_attention_mask`. BERT4Rec is always bidirectional.\n\n4. Segment-level contrastive objective (`SP`): sample random contiguous subsequences, mask them out, and predict the original segment embedding against negatives. Requires new sampling logic (`_neg_sample`, segment padding helpers) and a context/segment representation aggregator (last-position hidden state of the masked segment sequence). None of these exist in BERT4Rec.\n\n5. Loss re-weighting and joint optimization: combine four losses with tunable weights (`aap_weight`, `mip_weight`, `map_weight`, `sp_weight`) into a single `pretrain_loss`. Implement a custom BCE-with-logits reduction that applies sequence-length masks to ignore padded positions. BERT4Rec only optimizes the Cloze cross-entropy.",
    "rank": "rank2"
  },
  {
    "source": "SASRec_2018",
    "target": "S3Rec_2020",
    "type": "in-domain",
    "similarities": "1. **Transformer-based encoder blocks** – Both papers stack identical self-attention + FFN blocks; the source `TransformerEncoder` class and its `forward()` pipeline (masking, multi-head attention, residual+LayerNorm, FFN) can be reused verbatim for the target.\n2. **Item & position embedding lookup** – The source `item_embedding` and `position_embedding` tables plus the additive fusion (`input_emb = item_emb + position_embedding`) are exactly the same in the target; only an extra `feature_embedding` table needs to be added.\n3. **Training/fine-tune losses** – Source already implements BPR and cross-entropy in `calculate_loss()`; the target re-uses the same loss functions during its fine-tune stage, so the corresponding code blocks can be copied with no change.",
    "differences": "1. **Two-stage training (pre-train → fine-tune)** – Source has a single supervised stage; target requires a new `pretrain()` method that optimizes four self-supervised losses (AAP/MIP/MAP/SP) with dedicated projection heads (`aap_norm`, `mip_norm`, etc.) and a weighted sum scheduler—none exist in the source.\n2. **Bidirectional attention mask during pre-training** – Source always applies a causal mask; target needs a `bidirectional=True` flag in `get_attention_mask()` to create the BERT-style fully-visible mask for pre-training, then switches back to causal for fine-tuning.\n3. **Data-reconstruction pipeline** – Source feeds raw sequences directly; target must implement `reconstruct_pretrain_data()` that randomly masks items/segments, samples negatives, builds feature-label tensors, and returns seven tensors per batch—this entire function is new.\n4. **Attribute & segment modeling** – Source has no attribute or segment embeddings; target adds `feature_embedding`, four contrastive scoring functions (`_associated_attribute_prediction`, `_masked_item_prediction`, …), and a segment-level scorer (`_segment_prediction`)—all require fresh implementation.",
    "rank": "rank3"
  },
  {
    "source": "HGN_2019",
    "target": "S3Rec_2020",
    "type": "in-domain",
    "similarities": "1. Both use learnable embedding matrices (item_embedding, user_embedding) that can be initialized with the same normal_(0,1/embedding_size) strategy; HGN’s Embedding init code block can be copied into S3Rec’s _init_weights.\n2. Both adopt BPR pairwise ranking loss with the identical BPRLoss() implementation; the negative sampling loop in HGN’s calculate_loss can be reused for S3Rec’s fine-tune stage.\n3. Both compute the final score via simple dot-product between sequence representation and candidate item embedding (torch.sum(seq_output*item_emb,dim=-1)); the predict() and full_sort_predict() helper functions are structurally identical and can be ported verbatim.\n4. Both use position-aware sequence encoding: HGN implicitly orders items via the sequential input tensor, S3Rec adds explicit position_embedding—re-use HGN’s tensor shape handling (batch_size, seq_len, hidden_size) to feed the Transformer encoder.",
    "differences": "1. S3Rec needs a full TransformerEncoder (multi-head self-attention + FFN) whereas HGN only has light-weight linear gating layers; create a new TransformerEncoder class with n_layers, n_heads, hidden_size, inner_size configs—no analogue in HGN.\n2. S3Rec introduces four self-supervised pre-training losses (AAP, MIP, MAP, SP) that require extra heads (aap_norm, mip_norm, …) and a complicated reconstruct_pretrain_data() routine with random masking & segment cropping—none of these exist in HGN.\n3. S3Rec switches between bidirectional (pre-train) and causal (fine-tune) attention masks via the bidirectional flag; implement get_attention_mask() that returns lower-triangular mask when False—HGN has no attention mask at all.\n4. S3Rec handles item attributes through an additional feature_embedding table and multi-hot feature tensors; add feature ingestion pipeline and extend data loader to yield associated_features—HGN is purely item-id based.",
    "rank": "rank4"
  },
  {
    "source": "SASRec_2018",
    "target": "SASRecCPR_2021",
    "type": "in-domain",
    "similarities": "1. **Identical Transformer backbone**: SASRecCPR reuses the full SASRec architecture—item/position embeddings, causal self-attention blocks, layer-norm, dropout, residual paths and the TransformerEncoder stack—so every line in SASRec’s `__init__()` and `forward()` up to `trm_encoder()` can be copied verbatim.\n2. **Same training pipeline**: batching, sequence truncation/padding, teacher-forcing with the last hidden state, Adam optimizer and GPU-ready parallel attention are inherited unchanged; only the head that produces logits is replaced.\n3. **Shared item embedding matrix**: both models keep a single `nn.Embedding(self.n_items, self.hidden_size)` used for input and output layers, so the embedding lookup code and weight initialization routine remain identical.\n4. **Causal masking & left-to-next-item objective**: the attention-mask construction, label shifting and ignore-index logic for padding are reused without modification; the new method only changes how logits are computed from the final hidden vector.",
    "differences": "1. **Softmax-CPR multi-facet head**: implement `n_facet_all` linear projectors (`project_arr`) that map the final hidden state to several vectors (context, pointer, reranker, vocab) and a partition-aware logit-combiner; this entire module is absent in SASRec.\n2. **Context/pointer logits for seen items**: add scatter-based operations that compute dot-products only for items present in the current sequence (`logit_hidden_context`, `logit_hidden_pointer`) and merge them via `scatter_add_`; requires bookkeeping of `item_seq` inside the loss.\n3. **Reranker partitions with top-k candidates**: introduce `reranker_CAN_NUM` lists and `torch.topk()` calls to select promising items, then re-score them with extra hidden states; implement `scatter_` logic to overwrite or add these refined logits.\n4. **Mixture-of-logits aggregator**: implement dynamic (`weight_facet_decoder`) or static (`weight_global`) gating to combine the facet logits into a final probability tensor and apply `NLLLoss` on the log-probability; SASRec’s simple `CrossEntropyLoss` on raw logits is insufficient.\n5. **Multi-input hidden states (Mi)**: optionally concatenate hidden states from several top layers and apply an MLP (`MLP_linear`, `gelu`) before projection; requires new config keys (`n_facet_hidden`, `n_facet_window`, `n_facet_MLP`) and corresponding tensor slicing.",
    "rank": "rank1"
  },
  {
    "source": "RepeatNet_2019",
    "target": "SASRecCPR_2021",
    "type": "in-domain",
    "similarities": "1. Both models explicitly handle the “copy-vs-explore” decision for next-item prediction: RepeatNet uses a GRU-based binary gate P(r|I_S) vs P(e|I_S) while SASRecCPR partitions the vocabulary into “context” (copy) and “vocabulary” (explore) sets; the Repeat_Explore_Mechanism module (lines 178-208) can be reused almost verbatim—just replace the GRU final hidden state h_t with the Transformer last-layer hidden state to produce the same 2-way soft gate.\n2. Both compute separate logits for items that occur in the input session vs those that do not: RepeatNet’s Repeat_Recommendation_Decoder (Eq 7-8 and lines 211-250) scatters attention scores over the input positions and aggregates by item-id; SASRecCPR’s “context” facet does an identical scatter-add (lines 410-418) using the same index-add pattern—so the scatter-add kernel and the mask-logic to zero-out non-context items can be copied.\n3. Both finally merge the two streams via a probabilistic mixture: RepeatNet does pred = π_repeat ⊙ P_repeat + π_explore ⊙ P_explore (line 157), SASRecCPR merges facet logits either by addition or learned weighting (lines 475-485); the same mixture-of-experts helper (lines 160-165) can be reused—just change the tensor names.\n4. Training objective is negative log-likelihood with the same ignore_index=0 padding trick; the NLL-loss call in RepeatNet (line 152) is identical to SASRecCPR (line 490) and can be reused without change.",
    "differences": "1. Sequence encoder changes from GRU to Transformer: SASRecCPR needs a full Transformer stack (self-attention + FFN + LN) with positional embeddings; RepeatNet’s single nn.GRU cell must be replaced by the TransformerEncoder module (lines 95-102) plus causal attention mask generation (lines 270-275) which is completely absent in RepeatNet.\n2. Multi-facet projection layers: SASRecCPR introduces n_facet_all separate linear projectors (line 80 project_arr) to map the hidden state into different semantic sub-spaces (context, reranker-k1, reranker-k2, vocabulary); RepeatNet has no such parameter group—this whole block (lines 305-340) must be newly implemented.\n3. Dynamic candidate retrieval for reranker partitions: SASRecCPR performs a top-k search (lines 380-390) on-the-fly to decide which items enter P(k1), P(k2), etc.; RepeatNet never does any candidate retrieval, so the efficient top-k gather/scatter logic (torch.topk + scatter_) has to be written from scratch.\n4. Item-embedding based pointer component: SASRecCPR optionally adds a pointer term f^T_{i_n,P} f_{x,i_n,L} (lines 420-430) that averages the hidden states of the same item in the sequence—RepeatNet only uses attention scores, so the “averaged linear projection then dot-product” pointer branch is new code.\n5. Multiple-input-hidden-states (Mi) expansion: SASRecCPR concatenates hidden states from several top layers and optionally feeds them through an MLP (lines 290-310); RepeatNet uses a single last GRU state—this hidden-state stacking and the MLP_linear layer (line 78) do not exist in the source repo and must be added.",
    "rank": "rank3"
  },
  {
    "source": "SASRec_2018",
    "target": "BERT4Rec_2019",
    "type": "in-domain",
    "similarities": "1. Transformer Encoder Stack – both rely on the same `TransformerEncoder` module (`recbole.model.layers.TransformerEncoder`) that already bundles multi-head self-attention + FFN + residual + LN + dropout; SASRec code can be reused verbatim, only the attention-mask generator needs to be switched from causal to bidirectional.\n2. Embedding Layer Pattern – item-embedding table + learnable position-embedding table added together, followed by `LayerNorm` and `dropout`; the SASRec `__init__` block and `_init_weights` can be copied, only the vocab size increases by 1 for the `[mask]` token.\n3. Position-wise FFN – same architecture (`Linear → activation → Linear`) with identical `inner_size`, `hidden_act`, `layer_norm_eps` hyper-parameters; the SASRec feed-forward sub-layer is already compatible and can remain unchanged.\n4. Stabilisation Gadgets – residual connection around each sub-layer, pre-LayerNorm, dropout on attention & hidden states; all already implemented in the shared `TransformerEncoder` and SASRec training recipe can be retained.\n5. Prediction Head Interface – both models expose a final sequence representation `[B L H]` and gather hidden vectors at specific positions; SASRec’s `gather_indexes` utility and the `full_sort_predict`/`predict` pattern can be reused, only the gathering index changes from `item_seq_len-1` to masked positions.\n6. Negative-Sampling & Optimiser – both use Adam, sampled softmax (Cross-Entropy) or BPR, and support GPU mini-batches; the SASRec `calculate_loss` skeleton (including negative-item sampling and target masking) can be adapted with minimal edits.",
    "differences": "1. Causal → Bidirectional Attention Mask – SASRec forces a triangular mask to prevent peeking at future tokens; BERT4Rec needs a fully-visible mask (`bidirectional=True`) that the existing `get_attention_mask` helper already supports but SASRec never calls, so a new code path must be added.\n2. Cloze / MLM Objective – SASRec trains every position to predict the next item; BERT4Rec randomly masks `ρ·L` positions and predicts only them. Requires new data loader logic to inject `[mask]` token ID (`n_items`), produce `masked_item_seq`, `masked_index`, `pos_items` tensors, and a `multi_hot_embed` helper to pull out only masked hidden states for loss computation.\n3. Additional Output FFN – BERT4Rec stacks an extra two-layer FFN (`output_ffn → GELU → output_ln`) plus an output bias vector on top of the last transformer layer; this small sub-network is absent in SASRec and must be implemented from scratch.\n4. Training/Inference Mismatch Handling – at test time BERT4Rec appends a `[mask]` token to the true sequence length and predicts that position; SASRec simply uses the last real position. A new method `reconstruct_test_data` is needed to shift the sequence and overwrite the last index with the mask token.\n5. Mask-aware Loss Computation – unlike SASRec that computes loss on every non-pad position, BERT4Rec computes loss only on masked indices; the existing `calculate_loss` must be rewritten to gather masked hidden states via `torch.bmm(multi_hot, seq_output)` and to apply the extra output bias.\n6. Activation & Expansion Factor – BERT4Rec uses GELU and expands the FFN to `4d` hidden units whereas SASRec keeps it `d→d`; the config file must change `inner_size=4*hidden_size` and `hidden_act=gelu` and the extra output FFN must follow the same expansion.",
    "rank": "rank1"
  },
  {
    "source": "Caser_2018",
    "target": "BERT4Rec_2019",
    "type": "in-domain",
    "similarities": "1. Both models use item embedding look-up + positional encoding: Caser’s `item_embedding` (nn.Embedding) and learned position-free order via CNN receptive field can be reused—BERT4Rec only needs to add `position_embedding` (nn.Embedding) and sum it with item embeddings before feeding the Transformer.\n2. Training & loss plumbing is identical: Caser already supports `loss_type=='CE'` (CrossEntropyLoss) and negative-sampling-ready `full_sort_predict`/`predict` routines; these two methods can be copied verbatim—only the input sequence construction (masked vs sliding-window) changes.\n3. Sequence truncation & padding handling: Caser limits `max_seq_length` to 10 to avoid CNN size errors; the same truncation logic and `SequentialRecommender` base-class padding utilities can be reused for BERT4Rec’s `max_seq_length` cap and `[mask]` token appending.\n4. Embedding initialization & regularization recipe: Caser’s `_init_weights` (normal_ 0-mean, 1/√d std) and `RegLoss` + dropout on fully-connected layers can be adopted directly for BERT4Rec’s item/position embeddings and output FFN.",
    "differences": "1. CNN → Transformer stack: Caser’s horizontal/vertical convolutions + max-pool must be replaced by a multi-layer TransformerEncoder (multi-head self-attention + FFN + LN + residual); a new `TransformerEncoder` class and `attention_mask` generator (bidirectional) need to be implemented.\n2. Cloze / masked-LM objective: Caser uses sliding-window next-item prediction; BERT4Rec needs a random `[mask]` insertion stage—add `mask_ratio`, `mask_token` id, `reconstruct_test_data` that appends `[mask]` at `item_seq_len-1`, and a `multi_hot_embed` gatherer to compute loss only on masked positions.\n3. Positional embeddings: Caser has none; BERT4Rec requires learnable `position_embedding` (size `max_seq_length × hidden_size`) and its addition to item embeddings—this layer is absent in Caser and must be created.\n4. Bidirectional attention mask & final-layer indexing: Caser needs no mask; BERT4Rec must build an all-ones attention mask (`bidirectional=True`) and at test time extract the hidden state at the last `[mask]` position via `gather_indexes`—new utility functions not present in Caser.\n5. Output projection head: Caser ends with `fc2` → logits over items; BERT4Rec adds an extra two-layer FFN (`output_ffn`, GELU, `output_ln`) plus a separate `output_bias` vector before the final softmax—this small head must be coded from scratch.",
    "rank": "rank2"
  },
  {
    "source": "FPMC_2010",
    "target": "BERT4Rec_2019",
    "type": "in-domain",
    "similarities": "1. Both models use pairwise BPR loss (source code: `BPRLoss()` in FPMC → reuse `loss_type='BPR'` branch in BERT4Rec’s `calculate_loss`).\n2. Both embed users/items into latent space (FPMC: `UI_emb`, `IU_emb`, `LI_emb`, `IL_emb` → BERT4Rec: `item_embedding`, `position_embedding`); reuse embedding initialization (`xavier_normal_`/`normal_`) and padding-idx handling.\n3. Both compute a single score per (user, seq, candidate) triad in `forward` and support `predict`/`full_sort_predict`; reuse the RecBole base-class interface and batch-gather pattern (`torch.gather` in FPMC → `gather_indexes` in BERT4Rec).\n4. Both models share the embedding matrix between input and output (FPMC implicitly via `IU_emb`/`IL_emb` → BERT4Rec explicitly `item_embedding.weight` used in logits); reuse the tied-embedding design to reduce parameters.",
    "differences": "1. FPMC is a shallow, first-order Markov model (only last-item transition) while BERT4Rec needs a deep bidirectional Transformer encoder (`TransformerEncoder` with `n_layers`, `n_heads`, `inner_size`, residual+LN, dropout); implement full multi-head self-attention, position-wise FFN, and attention mask generation (`get_attention_mask` with `bidirectional=True`).\n2. FPMC uses no positional encoding; BERT4Rec requires learnable `position_embedding` and explicit positional IDs—add `position_embedding` module and fuse it with item embeddings (`input_emb = item_emb + position_embedding`).\n3. Training paradigm differs: FPMC uses next-item prediction with BPR triples; BERT4Rec uses Cloze/masked-item prediction—implement `mask_ratio` sampling, `[MASK]` token (index `n_items`), `reconstruct_test_data`, `multi_hot_embed` to gather only masked positions, and support both BPR & CE losses over masked logits.\n4. FPMC computes a simple dot-product score (`mf + fmc`); BERT4Rec needs an extra output stack (`output_ffn`, GELU, `output_ln`, bias) to project Transformer outputs into item space—implement this two-layer output head and the final softmax layer for CE loss.",
    "rank": "rank5"
  },
  {
    "source": "DIN_2017",
    "target": "DIEN_2018",
    "type": "in-domain",
    "similarities": "1. Embedding & MLP backbone: both papers adopt the same high-dimensional sparse→low-dimensional dense embedding layer and the final MLP tower; DIN’s ContextSeqEmbLayer and MLPLayers can be reused verbatim in DIEN—only the input tensor to the MLP changes.\n2. Feature group taxonomy: User Profile / User Behavior / Ad / Context fields and their one-hot/multi-hot encoding are identical; the vocabulary building, field2type dispatch and dataset.get_user/item_feature() routines in DIN need no modification.\n3. Attention-based target-item gating: DIN’s SequenceAttLayer (queries=target item, keys=behavior sequence) is exactly the attention unit required in DIEN’s interest-evolving layer; the same mask_mat, att_list and activation='Sigmoid' can be kept—just call it twice (once for vanilla attention, once for AUGRU scaling).\n4. Negative log-likelihood loss & Dice activation: DIN’s BCEWithLogitsLoss and Dice-equipped MLPLayers are reused; DIEN only adds an auxiliary-loss term that is computed outside the main loss line.\n5. Training tricks: mini-batch aware reg (not shown in code but described) and Dice are already implemented; DIEN does not introduce new regularisers, so the same optimizer loop applies.",
    "differences": "1. Dual-GRU sequential cells: DIN has no recurrent layer—DIEN needs two new modules (InterestExtractorNetwork with standard GRU + InterestEvolvingLayer with AIGRU/AGRU/AUGRU cells); implement AUGRUCell/AUGRUCell and DynamicRNN that accepts PackedSequence and attention scores—none of these exist in DIN.\n2. Auxiliary-loss sampler: DIN only uses click labels; DIEN needs a negative-item sequence (neg_item_seq) per sample and an auxiliary MLP (auxiliary_net) to compute L_aux over T−1 time steps—requires new data-loader logic and a parallel sampling function.\n3. Attention-GRU fusion: DIN applies attention only for pooling; DIEN must embed attention scores inside GRU gates (AUGRU). Implement the gate-wise modulation (update_gate = att_score * u_t) and integrate it into the packed RNN loop—this is a completely new forward path.\n4. Two-stage flow: DIN’s forward() returns a single prediction tensor; DIEN returns (pred, aux_loss) and the loss becomes L_target + α·L_aux. Training step must be extended to aggregate both losses and backward jointly—needs modification of calculate_loss() and trainer loop, plus a new hyper-parameter α.",
    "rank": "rank1"
  },
  {
    "source": "FPMC_2010",
    "target": "Caser_2018",
    "type": "in-domain",
    "similarities": "1. Both models use pairwise BPR loss for implicit feedback ranking; the source's BPRLoss() class and loss_fct(pos_score, neg_score) wrapper can be reused verbatim in Caser for its BPR branch, saving implementation and testing time.\n2. Both embed users and items into the same latent space; the source’s UI_emb, IU_emb Embedding tables and their xavier_normal_ initialization can be copied to Caser’s user_embedding and item_embedding layers, ensuring identical latent-factor semantics.\n3. Both compute a single “sequence score” per user-item pair and expose predict() / full_sort_predict() interfaces; the tensor slicing, gather() for the last item, and batched matrix-multiply patterns in FPMC forward() can be adapted for Caser’s final score = torch.matmul(seq_output, all_item_emb.T) without redesigning the batching logic.\n4. Both use negative sampling (3 negatives per positive) and L2 regularization controlled by λ (reg_weight); the source’s regularization loops over named_parameters() ending in “weight” can be transplanted into Caser’s reg_loss() to avoid re-implementing weight decay.",
    "differences": "1. Caser needs 2-D convolution modules (nn.Conv2d) for horizontal & vertical filters plus max-pool1d—none exist in FPMC—so a new Conv2d stack with kernel sizes (h, emb_dim) and (max_len, 1) must be written and shape-checked (4-D input: B×1×L×d).\n2. Caser requires an additional fully-connected encoder (fc1, fc2) with dropout and ReLU that FPMC lacks; new Linear layers, dropout(p=0.5) and activation plumbing must be added between conv outputs and the final score layer.\n3. Caser supports both BPR and cross-entropy (CE) losses while FPMC only implements BPR; a new CE branch computing logits = seq_output @ item_emb.T and calling nn.CrossEntropyLoss() against pos_items labels has to be implemented and guarded by loss_type flag.\n4. Caser slides a window of L items to produce a d-dimensional sequence embedding z, whereas FPMC only uses the single last item; the whole forward() logic must be rewritten to embed the entire item_seq tensor, apply convolutions, pool, concatenate with user_emb, and project to z—no reuse of FPMC’s single-item gather() pattern is possible.",
    "rank": "rank3"
  },
  {
    "source": "SASRec_2018",
    "target": "DIN_2017",
    "type": "in-domain",
    "similarities": "1. Both employ learnable attention layers that compute query-key similarity scores followed by weighted sum over values; SASRec's SequenceAttLayer (with softmax_stag=False) can be reused almost verbatim as DIN's local activation unit—only the input construction (concatenating target-item embedding, behaviour embedding and their element-wise product) needs to be added before the MLP.\n2. Embedding layers are structurally identical: nn.Embedding tables map sparse IDs to dense vectors of configurable size; SASRec's item_embedding and position_embedding initialisation (_init_weights with xavier_normal_) can be copied for DIN's goods/cate/shop embeddings.\n3. Training loop skeleton is the same: mini-batch negative sampling, Binary-Cross-Entropy loss (BCEWithLogitsLoss), Adam optimiser, and GPU-friendly batched forward pass; SASRec's calculate_loss & predict methods can be kept, only replacing the sequence-output slicing with DIN's attention-weighted user embedding.\n4. Feature concatenation pattern (user_emb, target_item_emb, user_emb * target_item_emb) is already present in SASRec's BPR contrastive term (pos_score = seq_output * pos_emb); the same torch.cat & element-wise product snippet can be reused in DIN's din_in tensor construction.\n5. Dropout, LayerNorm andDice activation are already implemented in SASRec's TransformerEncoder and MLPLayers; DIN can directly import these modules for its MLP tower without rewriting.",
    "differences": "1. SASRec is autoregressive (causal self-attention mask forbids looking into the future), whereas DIN is non-causal and must attend behaviours to a target ad without length-dependent masking; remove the subsequent-item mask logic in SASRec's get_attention_mask and instead supply a behaviour-length mask to SequenceAttLayer.\n2. DIN needs explicit target-item (ad) features at inference time; SASRec only consumes a single sequence. Add a next_items argument in forward() and extend the dataset collate_fn to yield (user, seq, seq_len, target_ad) quadruplets.\n3. DIN pools multi-hot categorical features (visited_cate_ids, visited_goods_ids) before attention; SASRec uses single-item IDs. Implement a multi-hot embedding lookup followed by sum/average pooling inside ContextSeqEmbLayer, or reuse RecBole's FeatureType.FLOAT_SEQ handling.\n4. DIN introduces Mini-batch Aware Regularisation and Dice activation; SASRec uses standard LayerNorm+Dropout+ReLU. Port MBA-L2 by storing per-feature frequency tables and scaling the weight-decay term by αmj/nj in the optimiser step; Dice is already available in MLPLayers but must be set as the default activation.\n5. Evaluation metric switches from HR/NDCG (ranking) to AUC (CTR); replace SASRec's full_sort_predict (scores over all items) with a point-wise sigmoid output and compute AUC using sklearn.metrics.auc or RecBole's built-in evaluator.",
    "rank": "rank5"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "FPMC_2010",
    "type": "in-domain",
    "similarities": "1. Both models use pairwise BPR loss (BPRLoss() in RecBole) – the source’s calculate_loss() with pos_score/neg_score can be reused verbatim; only the forward() signature changes from (item_seq, item_seq_len) to (user, item_seq, item_seq_len, next_item).\n2. Item embedding tables (nn.Embedding(self.n_items, embedding_size)) and Xavier-normal initialization (_init_weights) are identical; the source’s item_embedding can be split into IU_emb, IL_emb, LI_emb in the target.\n3. full_sort_predict() pattern—matmul(seq_output, all_item_emb.T)—is reused: FPMC simply replaces seq_output by (user_emb @ all_iu_emb.T) + (last_item_emb @ all_il_emb.T); the batched matrix-multiplication infrastructure stays the same.\n4. Data-loader interface in RecBole (ITEM_SEQ, POS_ITEM_ID, NEG_ITEM_ID) is unchanged; no new dataset collation is needed—only USER_ID field is additionally fed.\n5. Training loop with negative sampling and SGD/Adam optimizer is identical; the source’s training scripts and batch-generation utilities can be kept.",
    "differences": "1. FPMC is not an RNN—remove the entire GRU layer, hidden_state logic, and BPTT window; instead implement the MF + FMC score in forward() with four embedding tables (UI, IU, IL, LI) that do not share weights.\n2. User embedding table (UI_emb) must be added—GRU4Rec has no user parameters; user IDs need to be loaded from the dataset and fed through the data loader (USER_ID field).\n3. Last-click extraction differs: GRU4Rec uses gather_indexes(gru_output, item_seq_len-1) to obtain final hidden state, whereas FPMC uses torch.gather(item_seq, ..., item_seq_len-1) to obtain the last item ID and then looks up LI_emb; implement a small utility to fetch the last non-padded item index.\n4. Prediction equation is additive (MF + FMC) not sequential—no temporal encoding, dropout, or layer norm is required; remove embedding_dropout and the dense projection layer after GRU.\n5. Loss normalization: FPMC paper uses per-user S-BPR with λΘ‖Θ‖²; ensure the RecBole config sets the regularization weight (reg_weight) and that the optimizer receives grouped parameters for UI/IU/IL/LI embeddings so weight decay is applied correctly—this is absent in the GRU4Rec implementation.",
    "rank": "rank1"
  },
  {
    "source": "SASRec_2018",
    "target": "FPMC_2010",
    "type": "in-domain",
    "similarities": "1. Both models learn item embeddings; reuse SASRec's `item_embedding` matrix but create two separate copies (IU_emb, IL_emb) to implement FPMC's asymmetric item factors.\n2. Both adopt BPR pairwise ranking; reuse the `BPRLoss` class and the `calculate_loss` sampling pattern (positive vs. negative items) verbatim.\n3. Both generate a single vector per user-sequence and score all candidate items in one matrix multiplication; the final `full_sort_predict` matmul (`seq_repr @ item_emb.T`) can be transferred with minimal reshaping.\n4. Position-independent sequence representation: SASRec already extracts the last hidden state (`gather_indexes(output, item_seq_len-1)`); the same slicing logic gives FPMC the “last-click” embedding without extra code.\n5. Xavier normal initialization for embeddings is already implemented; apply it to the four FPMC embedding tables (UI_emb, IU_emb, LI_emb, IL_emb).",
    "differences": "1. FPMC needs explicit user embeddings (UI_emb) – a new `nn.Embedding(self.n_users, embedding_size)` table that SASRec does not possess; add user look-up in forward.\n2. FPMC decomposes preference into MF + FMC terms requiring two separate inner products per candidate; implement distinct matmuls (`user*target` and `last_item*target`) and add them—SASRec uses a single self-attention output.\n3. FPMC only considers the very last interaction (order-1 Markov) while SASRec consumes the whole sequence via attention; replace the TransformerEncoder stack with a simple look-up of the last non-padded item.\n4. No position or attention modules are needed; remove `position_embedding`, `trm_encoder`, `LayerNorm`, dropout, and causal-mask creation entirely.\n5. Training data loader must expose `USER_ID` tensor; extend SASRec’s pairwise sampler to yield quadruples (user, seq, len, pos, neg) instead of (seq, len, pos, neg).",
    "rank": "rank4"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "SASRecCPR_2021",
    "type": "in-domain",
    "similarities": "1. **Embedding-based item representation**: Both models map discrete item IDs to dense embeddings; GRU4Rec's `nn.Embedding(self.n_items, self.embedding_size)` can be reused verbatim for SASRecCPR's `self.item_embedding` layer.\n2. **Sequence-to-vector encoder pattern**: Both forward() return a single vector per sample (`seq_output = gather_indexes(gru_output, item_seq_len-1)` vs. `stacked_hidden_emb_arr = stacked_hidden_emb_arr_raw[:,-1,:]`); GRU4Rec's `gather_indexes` utility can be directly imported.\n3. **Cross-entropy training with full-item vocabulary**: `calculate_loss` in both papers ends with `logits = linear(seq_output, item_embedding.weight.T)` and `nn.CrossEntropyLoss`; the CE branch in GRU4Rec can serve as the skeleton for SASRecCPR before the Softmax-CPR logits are merged.\n4. **BPTT through padded mini-batches**: Both implementations assume variable-length sequences packed in a single tensor (`item_seq`, `item_seq_len`) and apply mask/gather on the last valid position; the data-loader and batch collation logic from GRU4Rec work unchanged.\n5. **Xavier/Normal weight init & dropout regularization**: `_init_weights` in GRU4Rec already handles `nn.Embedding` and `nn.Linear`; the same method can initialize every new projection layer added by Softmax-CPR.\n6. **Real-time full-sort prediction**: `full_sort_predict` pattern (compute score for entire catalog via single mat-mult) is identical; only the tensor that feeds the final mat-mult changes from `seq_output` to `prediction_prob`.",
    "differences": "1. **Transformer encoder stack vs. GRU cell**: SASRecCPR needs a full multi-head self-attention Transformer (`TransformerEncoder` with `n_layers`, `n_heads`, `inner_size`, residual+LN); GRU4Rec has none of these—must implement from scratch or import a library module.\n2. **Learnable absolute position embedding**: GRU4Rec has no position signal; SASRecCPR requires `nn.Embedding(max_seq_length, hidden_size)` added to item embeddings—new parameter block and corresponding initialization.\n3. **Attention mask construction**: Transformer needs a 3-D attention mask preventing leakage (`extended_attention_mask = get_attention_mask(item_seq)`); GRU4Rec relies on recurrent hidden state so no mask is built—`get_attention_mask()` is a new method.\n4. **Softmax-CPR multi-facet projection layers**: Requires `n_facet_all` separate linear projectors (`project_arr`), MLP gating (`MLP_linear`), reranker top-k indexing (`scatter_` / `scatter_add_`), and context-pointer scoring—none exist in GRU4Rec; roughly 5× more parameters in output layer.\n5. **Dynamic partition-based logits assembly**: SASRecCPR replaces the single `nn.Linear(hidden_size, n_items)` with conditional logit branches (context, reranker-k1, k2, k3, vocab); the final probability is a mixture—requires new `calculate_loss_prob` logic and `NLLLoss` over merged probabilities instead of plain `CrossEntropyLoss`.\n6. **Post-removal flag & inference mode handling**: `post_remove_context` option zeros probabilities of already-seen items at inference; needs additional `scatter_` calls in `predict` and `full_sort_predict`—no equivalent in GRU4Rec.",
    "rank": "rank1"
  },
  {
    "source": "NARM_2018",
    "target": "SASRecCPR_2021",
    "type": "in-domain",
    "similarities": "1. Both models adopt an encoder-decoder paradigm where a sequence encoder (GRU vs Transformer) produces a hidden state that is fed to a decoder that scores every candidate item via dot-product with item embeddings; the source code's `item_embedding` table and final scoring `torch.matmul(seq_output, test_item_emb.transpose(0,1))` can be reused verbatim.\n2. They share the same training pipeline—mini-batch gradient descent with cross-entropy loss on the next-item prediction task; the source's `calculate_loss()` method (CE branch) already performs `nn.CrossEntropyLoss(logits, pos_items)` and can be adopted after the target model produces its logits.\n3. Both support full-item ranking at inference: the source's `full_sort_predict()` that returns `torch.matmul(seq_output, item_embedding.weight.t())` is exactly the interface the target needs once its own `seq_output` tensor is ready.\n4. Dropout, layer-norm and embedding initialization routines (`xavier_normal_`, `_init_weights`) are identical in spirit and can be copied from the source with no change.",
    "differences": "1. Core encoder: source uses a **single GRU** (`self.gru`) while the target requires a **multi-layer Transformer** (`TransformerEncoder` with self-attention, position embeddings, feed-forward blocks); the whole `forward()` of the source must be replaced by a transformer stack plus sinusoidal position embeddings.\n2. Attention paradigm: source computes **item-level attention over GRU hidden states** (`αtj = v^T σ(A1 ht + A2 hj)`) to build `c_local`, whereas the target relies on **multi-head self-attention inside the transformer**; the source’s attention matrices `a_1, a_2, v_t` and the manual weighted-sum loop disappear—no analogue exists in the target.\n3. Softmax layer: source applies a **single dot-product followed by vanilla softmax**, while the target implements **Softmax-CPR**—multiple projected hidden facets (`project_arr`), context/pointer logits for seen items, reranker logits for top-k unseen items, and a partitioning/merging logic; this entire block (200+ lines in `calculate_loss_prob`) is completely new and must be written from scratch.\n4. Sequence-length handling: source uses `gather_indexes(gru_out, item_seq_len-1)` to pick the last real hidden state, whereas the target takes the **last transformer layer output at the last position** (`stacked_hidden_emb_arr = ...[:, -1, :]`)—a one-line change but conceptually different indexing logic.\n5. Position encoding: source has **no explicit position signal** (GRU order is implicit), while the target needs **learned absolute position embeddings** (`self.position_embedding`) added to item embeddings before transformer layers; this submodule is absent in the source.",
    "rank": "rank2"
  },
  {
    "source": "Caser_2018",
    "target": "SASRecCPR_2021",
    "type": "in-domain",
    "similarities": "1. Both models use item embedding look-up and a final dot-product / linear layer to score every candidate item; Caser’s `self.item_embedding` and final `torch.matmul(seq_output, test_item_emb.transpose(0, 1))` can be reused almost verbatim in SASRecCPR.\n2. Training objective is negative-log-likelihood (cross-entropy) with sampled negatives; Caser’s `nn.CrossEntropyLoss()` loop and mini-batch negative sampling logic can be adopted directly for SASRecCPR’s `calculate_loss_prob()`.\n3. User and item IDs are fed through `nn.Embedding` layers; Caser’s embedding initialization (`normal_` with std=1/d) and padding-idx handling can be copied for SASRecCPR’s `item_embedding` and optional `user_embedding`.\n4. Sequence truncation / padding and attention mask generation are already implemented in Caser’s `forward()`; the same`item_seq` tensor pre-processing and mask utilities can be reused for SASRecCPR’s transformer encoder input.\n5. Regularization (L2 on embeddings & dense weights, dropout before FC) is identical; Caser’s `RegLoss` module and dropout probability schedule can be transferred to SASRecCPR.",
    "differences": "1. Caser uses horizontal/vertical CNNs plus an FC stack to produce a single hidden vector z, whereas SASRecCPR requires a multi-layer Transformer encoder with self-attention; the entire CNN section (`conv_v`, `conv_h`, `fc1`, `fc2`) must be replaced by `TransformerEncoder` from RecBole and accompanying position embeddings.\n2. SASRecCPR needs multiple linear projection heads (`project_arr`, `project_emb`, `weight_facet_decoder`) to generate facet-specific logits (context, pointer, reranker, etc.); Caser has no such multi-head output and these modules must be written from scratch.\n3. SASRecCPR implements dynamic partitioning & scatter-based logit merging (`scatter_` / `scatter_add_`) to handle in-sequence versus out-of-sequence items; Caser’s plain softmax output layer has no partitioning logic, so the entire scatter-merge pipeline is new code.\n4. Position encoding: Caser has no explicit positional signal, while SASRecCPR requires learnable `position_embedding` added to item embeddings; a new `position_embedding` table and corresponding lookup code must be added.\n5. Loss computation: Caser returns a scalar CE loss, but SASRecCPR returns per-sample loss (`nn.NLLLoss(reduction='none')`) and combines facet probabilities before logging; the loss wrapper in `calculate_loss_prob()` is a new component not present in Caser.",
    "rank": "rank4"
  },
  {
    "source": "NextItNet_2018",
    "target": "SASRecCPR_2021",
    "type": "in-domain",
    "similarities": "1. Both models adopt a generative, left-to-next-item paradigm: NextItNet predicts all positions in parallel at training time (Eq. 2) but keeps only the last-step prediction for evaluation; SASRecCPR likewise computes a full sequence of hidden states and finally uses the last position to produce logits. The source code’s trick of slicing [:, -1, :] before the final Linear layer can be reused verbatim.\n2. Item and positional embeddings are summed and Layer-Normalized before being fed to the deep stack (NextItNet § 3.2.1 & LN inside residual blocks; SASRecCPR forward()). The embedding table initialization (normal_(0,initializer_range)) and the LN weight=1, bias=0 lines can be copied.\n3. Both papers rely on parameter-efficient 1×1 convolutions/linear projections to map hidden states → item space (NextItNet § 3.4 last 1×1 conv; SASRecCPR project_arr list). The source’s final_layer Linear can be generalized into a ModuleList of project_arr.\n4. Training objective is cross-entropy over the whole catalogue with optional negative sampling; the loss computation pattern logits = h @ E.weight.t(); loss = CE(logits, pos) is identical and can be reused.",
    "differences": "1. NextItNet uses stacked 1-D dilated CNNs with causal padding to capture long-range dependencies, while SASRecCPR replaces the entire convolutional trunk by a Transformer encoder (multi-head self-attention + FFN). A new TransformerEncoder module (n_layers, n_heads, inner_size, dropout) must be implemented; the dilated residual_blocks Sequential is discarded.\n2. Causal masking is implicit in NextItNet’s left padding (conv_pad with ZeroPad2d((k-1)*dilation, 0, 0, 0)), whereas SASRec needs an explicit 2-D attention mask (get_attention_mask) that forbids positions > current. This mask construction is not present in the source.\n3. SASRecCPR introduces Softmax-CPR: multiple facet-specific hidden states (n_facet_all) obtained by extra linear projections, plus pointer-style logits for in-sequence items and reranker logits for top-k candidates. None of these project_arr, context partition scatter/scatter_add operations, nor the candidate top-k extraction loops exist in NextItNet and must be written from scratch.\n4. Position encoding in NextItNet is learned implicitly by the order-sensitive convolution; SASRec adds absolute position_embedding lookup that has to be created and summed to item embeddings.\n5. NextItNet predicts probabilities for every timestep in parallel and can reuse them; SASRecCPR computes probabilities only at the last position (seq_len=1 after slicing) and needs extra logic to handle dynamic weighting (dynamic/static/max_logits modes) and partition merging—none of which appear in the convolutional source.",
    "rank": "rank5"
  },
  {
    "source": "GRU4Rec_2015",
    "target": "TransRec_2017",
    "type": "in-domain",
    "similarities": "1. Both models inherit from SequentialRecommender base class and follow the same RecBole interface (forward, calculate_loss, predict, full_sort_predict), so the overall training/evaluation loop and data-loading pipeline can be reused verbatim.\n2. Item embedding table (nn.Embedding(self.n_items, embedding_size)) and the final dot-product or distance-based scoring against all candidate items (full_sort_predict) are structurally identical—only the transformation applied to the sequence representation changes.\n3. Negative-sampling pairwise training with BPRLoss is already implemented in GRU4Rec; the same sampler and mini-batch construction can be kept for TransRec’s S-BPR objective, merely swapping the score computation from cosine/logits to βj − L2-distance.\n4. Parameter initialization utilities (xavier_normal_initialization) and embedding regularization losses (EmbLoss, RegLoss) are shared; TransRec’s user embeddings, bias terms and global T vector can be registered and initialized with the same helper.",
    "differences": "1. TransRec replaces the entire GRU encoder with a *translation* operation: seq_output = user_emb + T + last_item_emb; hence the GRU layers, dropout, and dense projection in GRU4Rec must be removed and substituted by a simple addition module plus user-ID lookup.\n2. A user embedding matrix (self.user_embedding) and a global translation vector (self.T) are new parameters absent in GRU4Rec; these need to be created and updated with every batch.\n3. Scoring changes from inner-product to Euclidean distance plus bias: implement _l2_distance (or an L1 variant) and add βj term; the existing matmul-based full-sort will be replaced by pairwise distance broadcasting as shown in the provided TransRec code.\n4. Item embeddings must be L2-normalized to the unit ball after every gradient step; this explicit normalize() call is missing in GRU4Rec and must be inserted in TransRec’s training loop.\n5. Data augmentation tricks from GRU4Rec (sequence preprocessing, embedding dropout, privileged distillation) are not used in TransRec; they can be disabled or kept optional, but the TransRec paper does not require them.",
    "rank": "rank1"
  },
  {
    "source": "SASRec_2018",
    "target": "TransRec_2017",
    "type": "in-domain",
    "similarities": "1. Both models inherit from SequentialRecommender base class and share the same PyTorch training framework; the pairwise BPR loss (BPRLoss) and embedding regularization (EmbLoss/RegLoss) utilities can be directly reused from SASRec.\n2. Sequence truncation, padding, and negative-sampling data-handling logic inside calculate_loss() and predict() can be copied verbatim; only the internal representation changes from Transformer hidden state to translated vector.\n3. The full_sort_predict() routine—batch expansion of candidate item embeddings and score broadcasting—has identical tensor shapes and GPU memory layout, so the same vectorized implementation applies after replacing the final dot-product by −L2-distance + bias.",
    "differences": "1. SASRec is attention-based while TransRec is metric-based; remove the entire TransformerEncoder stack and instead implement a single parameter T (global translation) plus user_embedding lookup to form γ_i + T + t_u.\n2. Position embeddings are mandatory in SASRec but absent in TransRec; delete position_embedding and its addition in forward().\n3. TransRec adds a learnable popularity bias β_j per item; introduce an extra nn.Embedding(self.n_items, 1) and fold β_j into the score computation (β_j − ‖seq_output − item_emb‖₂).\n4. Output scoring changes from simple inner-product to Euclidean distance; replace torch.matmul(seq_output, item_emb.T) with −torch.cdist(seq_output, item_emb, p=2) + bias and ensure unit-norm constraint on item embeddings via post-update normalization (not needed in SASRec).",
    "rank": "rank3"
  },
  {
    "source": "NARM_2018",
    "target": "TransRec_2017",
    "type": "in-domain",
    "similarities": "1. Both models treat the last hidden item embedding as the key signal for next-item prediction—NARM’s `gather_indexes(gru_out, item_seq_len-1)` can be reused as `gather_last_items(item_seq, item_seq_len-1)` in TransRec to extract the previous-item vector.\n2. Both adopt a pairwise ranking objective with negative sampling; NARM’s `BPRLoss` implementation (lines 108-111) can be plugged into TransRec’s `bpr_loss` without change.\n3. Both use a shared item embedding matrix for encoding and scoring; the `item_embedding` module and `full_sort_predict` logic (matrix multiply `seq_output @ item_embedding.weight.T`) are identical and can be copied verbatim.\n4. Both apply L2-distance-based scoring in the final layer; NARM’s implicit Euclidean dot-product in `calculate_loss` (line 104) mirrors TransRec’s explicit `_l2_distance` so the distance computation can be refactored but keeps the same tensor ops.",
    "differences": "1. TransRec introduces a user-specific translation vector `t_u` and a global translation `T` that are **absent** in NARM; these must be created as `nn.Embedding(self.n_users, self.embedding_size)` and `nn.Parameter(torch.zeros(self.embedding_size))` respectively.\n2. TransRec replaces the GRU encoder with a simple additive translation `prev_item + user + T`; the entire GRU block and attention mechanism in NARM (`gru`, `a_1`, `a_2`, `v_t`, mask handling) must be removed and substituted by the parameter-free translation sum.\n3. TransRec adds an explicit popularity bias `β_j` via `self.bias` embedding; this additional scoring term is not present in NARM and needs to be incorporated in both `calculate_loss` and prediction paths.\n4. TransRec constrains item embeddings to the unit sphere by manual re-normalization after each gradient step; NARM does not enforce this constraint, so a post-update hook or optimizer step callback must be newly implemented.",
    "rank": "rank4"
  },
  {
    "source": "BERT4Rec_2019",
    "target": "TransRec_2017",
    "type": "in-domain",
    "similarities": "1. Both model sequences of item IDs per user and share the same PyTorch abstract base class SequentialRecommender, so dataloader, batching, padding/truncation logic and evaluation helpers (predict, full_sort_predict) can be reused verbatim.\n2. Both adopt a pairwise BPR-style loss with negative sampling; the BPRLoss module, neg-item sampling pipeline and pairwise training loop from BERT4Rec can be reused for TransRec with no change.\n3. Item and user embedding tables (nn.Embedding) plus optional bias terms are created the same way; initialisation utilities (xavier_normal_initialization) and L2-regularisation helpers (EmbLoss, RegLoss) are already present and can be kept.\n4. The pattern of extracting the “last” item of each sequence via gather_indexes/gather_last_items is identical; this helper can be copied directly.\n5. Distance-based scoring in TransRec (β − ‖·‖₂) is a simple linear layer + negative L2 distance; the output_bias nn.Embedding and torch.sqrt(torch.sum((x-y)**2,…)) already exist in BERT4Rec’s metric learning path and can be reused.",
    "differences": "1. BERT4Rec is deep and bidirectional Transformer-based while TransRec is a shallow, order-insensitive translation metric model; the entire TransformerEncoder stack, multi-head attention, position embeddings, layer-norm, dropout, GELU feed-forward, masking logic and Cloze objective must be removed.\n2. TransRec needs a single global translation vector T (nn.Parameter) and a user-specific offset t_u (nn.Embedding) which are absent in BERT4Rec; these must be newly created and initialised to zero/unit vectors.\n3. BERT4Rec predicts masked positions anywhere in the sequence using a masked-language-model head; TransRec only scores the immediate next item via β − distance, so the two-layer output projection (output_ffn + GELU + output_ln) and masked-index gathering code can be discarded.\n4. Positional embeddings are essential in BERT4Rec but meaningless in TransRec; the position_embedding table and its addition to input_emb must be removed.\n5. Training data generation: BERT4Rec produces multiple masked sequences per original sequence; TransRec needs only one training triple (prev_item, user, next_item, neg_item) per sequence—masking and subsequence sampling logic must be replaced by simple last-item extraction.",
    "rank": "rank5"
  }
]