[
  {
    "source": "DualGNN_2021",
    "target": "BM3_2023",
    "type": "in-domain",
    "similarities": "1. Both employ simplified graph neural network architectures for user-item interaction modeling - DualGNN removes self-loops and feature transformations similar to LightGCN, while BM3 directly uses LightGCN; DualGNN's implementation uses symmetric normalization 1/√(|Ni||Nu|) for edge weights, which is equivalent to the D^(-1/2)AD^(-1/2) normalization in BM3\n2. Both utilize multimodal feature projection to align different modalities into a shared latent space - DualGNN uses linear transformations Wm and bias bm for each modality, while BM3 uses similar MLP-based projections fm; DualGNN's implementation shows explicit visual and textual feature handling with separate GCN modules\n3. Both incorporate regularization mechanisms to prevent overfitting - DualGNN uses explicit regularization weights and dropout (drop_rate=0.1 in implementation), while BM3 employs dropout-based contrastive learning; DualGNN's code shows sophisticated dropout strategies with separate visual/textual node dropping\n4. Both frameworks process visual and textual modalities specifically for multimedia recommendation, with similar feature embedding initialization strategies using pretrained features",
    "differences": "1. Learning paradigms differ fundamentally - DualGNN uses supervised learning with BPR loss for direct preference modeling, while BM3 employs self-supervised contrastive learning with cosine similarity-based losses (1 - cosine_similarity) to bootstrap representations without negative sampling\n2. Graph modeling approaches are distinct - DualGNN constructs separate single-modal bipartite graphs and a user co-occurrence graph with Top-K sampling strategy for multi-modal aggregation, while BM3 uses a single unified user-item interaction graph with LightGCN propagation and residual connections for items\n3. Multimodal fusion strategies differ significantly - DualGNN uses learnable attention weights (αu,v, αu,a, αu,t) with multiple fusion methods (concatenation, sum, maximum), while BM3 treats modalities as separate contrastive views and fuses them through contrastive alignment rather than explicit fusion\n4. Data augmentation and view generation methods are completely different - DualGNN uses sophisticated node dropping strategies with separate visual/textual item masking (dropv_node_idx, dropt_node_idx), while BM3 uses simple Bernoulli dropout on latent embeddings with stop-gradient mechanisms for target network updates\n5. Computational complexity varies - DualGNN requires multiple graph constructions and propagations (user-item bipartite graphs per modality plus user co-occurrence graph), while BM3 uses a single graph with more efficient contrastive learning that eliminates the need for negative sampling"
  },
  {
    "source": "LATTICE_2021",
    "target": "BM3_2023",
    "type": "in-domain",
    "similarities": "1. Both methods employ multimodal feature transformation using linear projection layers to map visual and textual features into a unified embedding space of dimension d, with LATTICE using Wm ∈ R^(d' × dm) and BM3 using similar linear transformations for feature alignment.\n2. Both utilize graph neural network architectures for learning item representations, with LATTICE implementing graph convolution on learned item-item graphs and BM3 employing LightGCN for user-item interaction modeling; LATTICE's implementation includes normalization techniques like D^(-1/2)AD^(-1/2) for adjacency matrices similar to BM3's normalized adjacency computation.\n3. Both papers incorporate residual connections in their final embedding computation - LATTICE uses skip connections (λS̃^m + (1-λ)Ã^m) to combine initial and learned graph structures, while BM3 adds residual connections to item embeddings (H_i + H_i^0) to prevent over-smoothing.\n4. Both methods handle multimodal fusion through learnable parameters - LATTICE uses modal_weight parameters with softmax normalization for adaptive modality weighting, while BM3 implicitly fuses modalities through joint contrastive learning objectives.\n5. Both implementations use Xavier initialization for embedding parameters and include regularization mechanisms, with LATTICE's code implementing L2 regularization and BM3 using EmbLoss for parameter regularization.",
    "differences": "1. Learning paradigm: LATTICE follows supervised learning with BPR loss for recommendation, while BM3 adopts self-supervised contrastive learning using cosine similarity without requiring negative sampling, fundamentally changing the optimization objective from ranking-based to representation alignment.\n2. Graph modeling approach: LATTICE constructs and learns item-item similarity graphs using k-NN sparsification on multimodal features, dynamically updating graph structures through transformed features, whereas BM3 focuses solely on user-item bipartite graphs without item-item relationship modeling.\n3. Data augmentation strategy: LATTICE performs graph structure learning and modification as implicit augmentation, while BM3 explicitly uses Bernoulli dropout on latent embeddings (h·Bernoulli(p)) as a lightweight augmentation technique, avoiding computationally expensive graph augmentations.\n4. Multimodal integration: LATTICE learns separate modality-specific graphs and fuses them through adaptive weighting mechanisms, while BM3 treats multimodal features as additional views for contrastive learning without explicit graph-level fusion.\n5. Computational complexity: LATTICE requires k-NN computation and graph structure learning for each modality with O(N²) similarity computation, while BM3 uses simple dropout-based augmentation with O(N) complexity, making it more scalable for large datasets.\n6. Training strategy: LATTICE uses traditional collaborative filtering with multimodal enhancement, while BM3 employs a teacher-student framework with stop-gradient mechanisms and online-target network updates for stable contrastive learning."
  },
  {
    "source": "GRCN_2020",
    "target": "LATTICE_2021",
    "type": "in-domain",
    "similarities": "1. Both inject multimodal content into graph-based recommendation by adjusting the graph used for message passing (LATTICE builds item–item graphs; GRCN reweights user–item edges), then apply LightGCN-style linear propagation.\n2. Both train end-to-end with BPR and negative sampling, using inner-product scoring without auxiliary self-supervised losses.\n3. Both use simple modality encoders (linear projections) and learn modality importance weights when fusing multimodal signals.",
    "differences": "1. Graph locus and learning: LATTICE learns a global item–item kNN graph from content (with feature transforms and a skip connection to the initial graph), while GRCN keeps the user–item graph and refines each edge weight via a user-conditioned prototypical network with neighbor routing.\n2. Modality fusion granularity: LATTICE fuses modality-specific graphs with global scalar weights (softmax over modalities), whereas GRCN fuses per-edge multimodal affinities using per-user and per-item base vectors and a max operator, yielding personalized, direction-specific weights.\n3. Use of content in representations: LATTICE uses content only to shape propagation (propagating item ID embeddings and adding them as a residual to a downstream CF item embedding), while GRCN also concatenates distilled modality features into the final user/item representations for scoring.\n4. System integration: LATTICE is a plug-and-play item-side module that augments arbitrary CF backbones, whereas GRCN is a monolithic bipartite GCN that performs graph refinement, propagation, and prediction within one model."
  },
  {
    "source": "MMGCN_2019",
    "target": "LATTICE_2021",
    "type": "in-domain",
    "similarities": "1. Both construct modality-aware graphs and apply GNN-style message passing to capture high-order relationships.\n2. Both maintain per-modality representation paths and fuse modalities to form final user/item representations for scoring.\n3. Both optimize a pairwise BPR objective on implicit feedback with inner-product scoring.",
    "differences": "1. Graph formulation/learning: LATTICE learns content-driven item–item kNN graphs per modality (refined via trainable feature transforms with a skip connection), whereas MMGCN uses fixed interaction-based user–item bipartite graphs per modality.\n2. Representation scope: LATTICE only updates item embeddings via item–item propagation and relies on an external CF backbone for users, while MMGCN jointly learns modal-specific user and item embeddings within the GNN to model per-user modality preferences.\n3. Use of multimodal features: LATTICE uses modalities solely to induce/fuse graph structure and propagates over item ID embeddings; MMGCN uses modality features as node inputs, aggregates them along the interaction graph, and links modalities via an ID-embedding bridge.\n4. Fusion and architecture: LATTICE fuses modalities at the graph level with learnable weights and performs LightGCN-style linear propagation; MMGCN performs modality-specific nonlinear aggregation/combination and fuses at the representation level by summation."
  },
  {
    "source": "VBPR_2016",
    "target": "SLMRec_2022",
    "type": "in-domain",
    "similarities": "1. Both combine collaborative signals with item content embeddings to mitigate cold-start.\n2. Both learn user/item embeddings trained with sampled negatives on implicit feedback in a pairwise-ranking spirit.\n3. Both project raw content features into a learned latent space before combining with ID-based representations.",
    "differences": "1. Objective/training: VBPR uses supervised BPR loss only; SLMRec replaces BPR with InfoNCE for the main task and adds auxiliary SSL losses (feature dropout/masking and cross-modal alignment).\n2. Architecture: VBPR is shallow MF with a linear visual embedding and no graph; SLMRec uses a LightGCN encoder per modality plus projection heads and multi-task training.\n3. Modality handling/fusion: VBPR uses only visual features added to ID factors; SLMRec models four modalities separately, aligns them in ID-anchored spaces, and contrasts both intra-modal augmented views and inter-modal pairs.\n4. Data augmentation and negatives: VBPR has no data augmentation and uses standard negative sampling per user; SLMRec creates multiple augmented views via factor- and modality-level masking and uses in-batch InfoNCE denominators for both main and SSL."
  },
  {
    "source": "MMGCN_2019",
    "target": "SLMRec_2022",
    "type": "in-domain",
    "similarities": "1. Both build per-modality user–item graphs and learn modality-specific user/item representations via graph-based message passing.\n2. Both coordinate modalities through an ID embedding space, using projections to make modal features comparable.\n3. Both train on implicit feedback with pairwise comparisons of positive vs. sampled negative items.",
    "differences": "1. Learning objective: SLMRec adopts InfoNCE for the main recommendation loss and adds auxiliary SSL losses, whereas MMGCN uses only BPR without any self-supervision.\n2. Training paradigm: SLMRec relies on view generation via masking (feature dropout and whole-modality masking) and contrastive learning with in-batch negatives; MMGCN has no augmentation and samples (u,i,i') triplets for BPR.\n3. Architecture: MMGCN designs explicit aggregation and combination layers per modality (with mean/max neighbor pooling and ID-coordinated fusion), while SLMRec uses a LightGCN backbone per modality plus projection heads solely for cross-modal alignment in hierarchical spaces.\n4. Fusion philosophy: MMGCN deterministically fuses modal signals in the forward pass (layer-wise combination and final summation), whereas SLMRec enforces cross-modal consistency via contrastive alignment to ID (including staged id·v → id·v·a → id·v·a·t alignment) and leaves fusion largely to the encoder and contrastive objectives."
  },
  {
    "source": "LATTICE_2021",
    "target": "FREEDOM_2023",
    "type": "in-domain",
    "similarities": "1. **Item-Item Graph Construction via kNN**: Both papers construct modality-aware item-item graphs using cosine similarity on raw multimodal features (visual and textual), followed by kNN sparsification to retain only top-k similar neighbors. The similarity computation follows the same formula: S_ij^m = (e_i^m)^T e_j^m / (||e_i^m|| ||e_j^m||).\n\n2. **Multimodal Feature Integration**: Both methods aggregate visual and textual modality graphs using weighted combination. LATTICE uses learnable modal_weight parameters with softmax normalization (self.modal_weight = nn.Parameter(torch.Tensor([0.5, 0.5]))), while FREEDOM uses a hyperparameter α_v for visual modality and α_t = 1 - α_v for textual modality.\n\n3. **Graph Normalization Strategy**: Both apply symmetric normalization to adjacency matrices: Ã = D^(-1/2) A D^(-1/2), where D is the diagonal degree matrix. The implementation in LATTICE's code uses compute_normalized_laplacian function, which FREEDOM also adopts.\n\n4. **LightGCN-style Graph Convolution**: Both employ simplified graph convolution without feature transformation or nonlinear activation. The propagation follows h_i^l = Σ_j A_ij h_j^(l-1), aggregating neighbor information through normalized adjacency matrices.\n\n5. **Dual Graph Architecture**: Both models utilize two types of graphs - an item-item graph constructed from multimodal features and a user-item bipartite graph from interactions. The final embeddings integrate information from both graph structures.\n\n6. **Feature Transformation Layers**: Both use linear transformation layers to project raw multimodal features into a common embedding space (image_trs and text_trs in code), enabling effective fusion across modalities.",
    "differences": "1. **Item-Item Graph Learning Strategy**: LATTICE dynamically learns latent graph structures during training by transforming features (W_m e_i^m + b_m) and recomputing adjacency matrices each epoch, then combining with initial graphs via skip connection (λS̃^m + (1-λ)Ã^m). In contrast, FREEDOM freezes the item-item graph by pre-computing it once before training and keeping it fixed, eliminating O(N²d_m) computational cost per epoch. LATTICE's code shows build_item_graph flag controls whether to rebuild graphs.\n\n2. **Graph Edge Representation**: LATTICE maintains weighted adjacency matrices where edge values represent continuous similarity scores between items. FREEDOM discretizes edges to binary values (0 or 1) after kNN selection (Ŝ_ij^m ∈ {0,1}), simplifying the graph structure and reducing memory overhead.\n\n3. **User-Item Graph Processing**: LATTICE uses the original user-item interaction graph throughout training without modification. FREEDOM introduces degree-sensitive edge pruning that stochastically drops edges based on node degree (p_k = 1/(√ω_i √ω_j)), sampling edges from multinomial distribution to create a denoised graph A_ρ in each epoch. The implementation shows degree_ratio parameter controls dropout proportion.\n\n4. **Training Efficiency and Stability**: LATTICE's dynamic graph learning requires rebuilding similarity matrices and adjacency structures every epoch (when build_item_graph=True), making training computationally expensive. FREEDOM's frozen item-item graph approach significantly reduces training time, while the iterative edge pruning on user-item graph prevents over-smoothing and overfitting.\n\n5. **Graph Construction Details**: LATTICE stores both image_original_adj and text_original_adj separately, then dynamically computes weighted combinations during forward pass. FREEDOM pre-computes a single fused mm_adj matrix (mm_image_weight * image_adj + (1-mm_image_weight) * text_adj) and saves it to disk (mm_adj_freedomdsp_{k}_{weight}.pt), loading it directly in subsequent runs.\n\n6. **Edge Sampling Mechanism**: LATTICE's code shows no edge dropout mechanism on either graph. FREEDOM implements sophisticated edge sampling: for user-item graph, it uses torch.multinomial with degree-based probabilities, creating masked_adj that changes per epoch; for item-item graph, it remains static. The dropout parameter (default >0) controls the pruning ratio.\n\n7. **Layer Configuration**: LATTICE uses n_ui_layers for user-item graph convolution and implicitly handles item-item graph layers. FREEDOM explicitly separates n_mm_layers (L_ii) for multimodal item-item graph and n_ui_layers (L_ui) for user-item graph, providing finer-grained control over information propagation depth in each graph type."
  },
  {
    "source": "SLMRec_2022",
    "target": "FREEDOM_2023",
    "type": "in-domain",
    "similarities": "1. Both employ LightGCN-style graph convolution for information propagation on user-item bipartite graphs, using normalized adjacency matrices and multi-layer aggregation without feature transformation or nonlinear activation. The SLMRec implementation uses `torch.sparse.mm` for efficient sparse matrix multiplication.\n2. Both construct item-item similarity graphs using multimodal features (visual and textual) to capture latent item relationships beyond collaborative signals. SLMRec builds separate modality-specific graphs while FREEDOM fuses them into a unified frozen graph.\n3. Both utilize multimodal features (visual, textual, and optionally acoustic) through embedding transformation layers to project raw features into a common latent space. SLMRec implementation uses `nn.Linear` layers (`v_dense`, `t_dense`) to transform features to the ID embedding dimension.\n4. Both adopt multi-task learning frameworks where the main recommendation task (BPR loss) is combined with auxiliary tasks. SLMRec uses self-supervised learning tasks while FREEDOM implicitly regularizes through graph structure optimization.\n5. Both normalize adjacency matrices using the symmetric normalization $D^{-1/2}AD^{-1/2}$ to ensure stable gradient propagation. SLMRec implementation pre-computes this as `self.norm_adj` and stores it as a sparse tensor on device.\n6. Both employ hyperparameter-controlled fusion of different modalities, with SLMRec using task-specific weights and FREEDOM using `mm_image_weight` (α_v) to balance visual and textual contributions in graph construction.",
    "differences": "1. Learning paradigm: SLMRec employs explicit self-supervised learning with three auxiliary tasks (Feature Dropout, Feature Masking, Fine-and-Coarse alignment) using InfoNCE loss to maximize mutual information between augmented views. FREEDOM focuses on graph structure optimization through freezing item-item graphs and denoising user-item graphs, without explicit contrastive learning objectives.\n2. Graph construction strategy: SLMRec dynamically computes separate graph convolutions for each modality during training, requiring O(N²d_m) complexity per epoch for multiple modality-specific graphs. FREEDOM pre-computes and freezes a single unified item-item graph (stored as `mm_adj_freedomdsp_*.pt`) before training, eliminating this computational burden entirely during optimization.\n3. Graph denoising approach: SLMRec applies feature-level dropout uniformly across all dimensions with probability `dropout_rate` to create augmented views for contrastive learning. FREEDOM implements degree-sensitive edge pruning on the user-item graph using multinomial sampling with probabilities p_k = 1/(√ω_i√ω_j), where high-degree nodes have edges more likely pruned to alleviate over-smoothing.\n4. Item-item graph representation: SLMRec uses weighted similarity matrices with continuous affinity values for each modality-specific graph. FREEDOM employs k-NN sparsification (k=10) to create binary adjacency matrices (0/1 values) from top-k similar items, significantly reducing memory footprint and computational cost.\n5. Multimodal fusion mechanism: SLMRec performs late fusion after separate graph propagations for each modality, using configurable fusion modes (concatenation, addition, etc.) via `mm_fusion()` method. FREEDOM performs early fusion by aggregating modality-specific item-item graphs with learnable weights before propagation, creating a single unified structure.\n6. Training efficiency: SLMRec's implementation shows O(LMN²d_m) complexity per epoch due to dynamic graph construction and multiple SSL loss computations (FAC requires six projection heads: `g_i_iv`, `g_v_iv`, `g_iv_iva`, `g_a_iva`, `g_iva_ivat`, `g_t_ivat`). FREEDOM achieves O(LME) complexity by freezing item-item graphs and only performing degree-based edge sampling on the user-item graph with E edges.\n7. Auxiliary task design: SLMRec implements three distinct SSL tasks with different granularities (factor-level FD, modality-level FM, and hierarchical FAC with multi-space alignment). FREEDOM has no explicit auxiliary tasks but implicitly regularizes through structural graph modifications applied iteratively each epoch.\n8. Inference strategy: SLMRec uses the same augmented graphs during both training and inference. FREEDOM distinguishes training (using pruned graph `masked_adj`) from inference (using original full graph `norm_adj`), as implemented in the `pre_epoch_processing()` method that conditionally applies edge pruning."
  },
  {
    "source": "LATTICE_2021",
    "target": "DRAGON_2023",
    "type": "in-domain",
    "similarities": "1. **Multimodal Graph Construction**: Both papers construct modality-specific item graphs using k-NN based on cosine similarity of multimodal features (visual and textual). LATTICE's implementation builds k-NN graphs with sparsification (top-k neighbors) and symmetric normalization, which DRAGON adopts for its homogeneous item graph construction.\n\n2. **Feature Transformation**: Both employ linear transformation layers to project raw multimodal features into a common embedding space. LATTICE uses `image_trs` and `text_trs` (Linear layers) to transform features to `feat_embed_dim`, which DRAGON directly adopts with the same architecture and naming convention in its implementation.\n\n3. **Graph Neural Network Backbone**: Both leverage LightGCN-style message passing for user-item bipartite graph convolution. LATTICE's implementation can use LightGCN as the CF backbone (controlled by `cf_model` parameter), and DRAGON explicitly adopts LightGCN's symmetric normalization formula for heterogeneous graph propagation.\n\n4. **Multimodal Fusion Strategy**: Both papers use learnable weighted aggregation to fuse multiple modalities. LATTICE uses `modal_weight` parameter with softmax normalization, while DRAGON employs `weight_i` and `weight_u` parameters for item and user representations respectively, both initialized with softmax.\n\n5. **Graph Adjacency Caching**: Both implementations cache computed adjacency matrices to disk (LATTICE saves `image_adj_{k}.pt` and `text_adj_{k}.pt`; DRAGON saves `mm_adj_{k}.pt`) to avoid redundant computation across training runs, demonstrating practical engineering optimization.\n\n6. **Embedding Initialization**: Both use pretrained multimodal features with `nn.Embedding.from_pretrained()` and set `freeze=False` to allow fine-tuning during training, enabling adaptive feature learning.",
    "differences": "1. **Graph Modeling Scope**: LATTICE focuses exclusively on item-item relationships through modality-aware latent structure learning, constructing only item graphs from multimodal features. DRAGON extends this with dual graph modeling: heterogeneous user-item bipartite graphs (one per modality) AND homogeneous graphs (user co-occurrence graph based on commonly interacted items, and item semantic graph), capturing both dyadic and internal relations.\n\n2. **Dynamic vs Static Graph Learning**: LATTICE employs dynamic graph structure learning with skip connections (Eq. 5: `A^m = λS̃^m + (1-λ)Ã^m`), where it learns high-level feature-based graphs and combines them with initial graphs using learnable coefficient λ. DRAGON pre-establishes and freezes homogeneous graphs to maintain initial co-occurrence and semantic relations, only learning on heterogeneous graphs dynamically.\n\n3. **User Representation Learning**: LATTICE learns user embeddings purely from user-item interactions through CF methods (LightGCN/NGCF). DRAGON introduces explicit user-user relationship modeling via the co-occurrence graph with attention-weighted aggregation (Eq. 4), where edge weights are computed using softmax over commonly interacted item counts, enabling direct user preference pattern capture.\n\n4. **Multimodal Fusion Granularity**: LATTICE performs global fusion with a single `modal_weight` parameter (2D tensor for v and t) applied uniformly across all items. DRAGON implements fine-grained, instance-specific fusion with per-user (`weight_u`) and per-item (`weight_i`) learnable weights (shape: [num_user/num_item, 2, 1]), allowing personalized and item-specific multimodal importance.\n\n5. **Data Augmentation Strategy**: LATTICE does not employ explicit data augmentation. DRAGON implements node dropout augmentation specifically for contrastive learning, randomly dropping nodes from visual and textual modality graphs (`dropv_node_idx`, `dropt_node_idx`) with configurable `drop_rate` (0.1), creating augmented views `edge_index_dropv` and `edge_index_dropt` for self-supervised learning.\n\n6. **Item Graph Integration**: LATTICE learns separate latent structures for each modality and fuses them into a final unified item graph for downstream CF. DRAGON constructs a single fused homogeneous item graph (`mm_adj`) by weighted combination of visual and textual similarity graphs (`mm_image_weight * image_adj + (1-mm_image_weight) * text_adj`), used consistently across all layers.\n\n7. **Architecture Complexity**: LATTICE implements a simpler pipeline: modality-aware graph learning → multi-layer GCN on item graphs → downstream CF (LightGCN/NGCF with configurable layers). DRAGON has a more complex four-component architecture: (1) heterogeneous graph learning per modality, (2) multimodal representation fusion module with MLPs (`MLP_v`, `MLP_t`), (3) homogeneous graph learning for users and items, (4) final prediction module, requiring coordination across multiple graph types.\n\n8. **Computational Efficiency**: LATTICE's implementation includes NGCF option with separate `GC_Linear_list` and `Bi_Linear_list` for each layer, increasing parameters. DRAGON adopts only LightGCN (no feature transformation, no nonlinear activation) for heterogeneous graphs, reducing computation, but adds overhead from dual homogeneous graph propagation and per-instance fusion weights.\n\n9. **Loss Function Design**: LATTICE uses standard BPR loss with L2 regularization on embeddings. DRAGON (based on typical contrastive multimodal architectures) likely incorporates additional contrastive losses between augmented views (dropv vs dropt) alongside BPR, though not explicitly shown in the provided code snippet, aligning with its data augmentation strategy."
  },
  {
    "source": "SLMRec_2022",
    "target": "DRAGON_2023",
    "type": "in-domain",
    "similarities": "1. Both employ LightGCN-style graph convolution for user-item interaction modeling, using symmetric normalization (1/√|Nu|√|Ni|) and layer-wise aggregation without feature transformation or nonlinear activation. The SLMRec implementation uses torch.sparse.mm for efficient sparse graph operations and applies layer averaging via torch.mean(embs, dim=1).\n2. Both construct modality-specific user-item bipartite graphs where each modality maintains the same graph structure but different node features, enabling separate propagation for visual, textual (and acoustic in SLMRec) modalities to learn uni-modal representations.\n3. Both utilize multimodal fusion strategies to combine representations from different modalities. SLMRec's implementation supports multiple fusion modes (concat, add, weighted) through the mm_fusion() method, with fusion applied after graph convolution and before final prediction.\n4. Both frameworks adopt multi-task learning paradigms where the main recommendation task (BPR loss) is combined with auxiliary tasks. SLMRec implements this through a joint loss L = Lmain + α*Lssl, with configurable weighting parameter α.\n5. Both initialize item embeddings with pre-trained multimodal features (visual and textual) and use linear projection layers to transform features to the target embedding dimension. SLMRec's implementation uses nn.Linear for v_dense and t_dense transformations with Xavier initialization.",
    "differences": "1. Learning objectives differ fundamentally: SLMRec employs self-supervised learning with three SSL tasks (Feature Dropout, Feature Masking, Fine-And-Coarse alignment) using InfoNCE loss to maximize mutual information between augmented views, while DRAGON focuses on dual representation learning combining heterogeneous and homogeneous graphs without explicit SSL auxiliary tasks.\n2. Graph modeling approaches are substantially different: SLMRec uses only user-item bipartite graphs for each modality, whereas DRAGON introduces two additional homogeneous graphs - a user co-occurrence graph (top-k users with most common interactions using attention-weighted aggregation) and an item semantic similarity graph (k-NN based on feature similarity with frozen structure).\n3. Multimodal fusion mechanisms differ: SLMRec uses simple fusion operations (concatenation/addition) after graph convolution with optional dropout for robustness, while DRAGON employs a more sophisticated approach with separate MLP transformations (MLP_v, MLP_t) for each modality and learnable fusion weights (weight_u, weight_i) that are softmax-normalized to balance modality contributions.\n4. Data augmentation strategies are distinct: SLMRec implements stochastic augmentation with feature dropout (masking factors with probability p) and feature masking (dropping entire modalities) to create contrastive pairs, while DRAGON uses deterministic node dropping (drop_rate=0.1) on specific items for modality-specific graphs (dropv_node_idx, dropt_node_idx) without contrastive learning.\n5. Computational complexity differs: SLMRec requires computing SSL losses across multiple augmented views (2-3 forward passes per batch depending on SSL task), while DRAGON has higher memory overhead from maintaining three separate graph structures (user-item, user-user, item-item) but potentially fewer forward passes, trading off computation for memory."
  },
  {
    "source": "BM3_2023",
    "target": "MGCN_2023",
    "type": "in-domain",
    "similarities": "1. Both papers leverage Graph Convolutional Networks (GCN) for collaborative filtering on user-item interaction graphs. BM3 uses LightGCN with residual connections (mean aggregation across layers), while MGCN employs similar symmetric normalized Laplacian propagation (D^(-1/2)AD^(-1/2)) for capturing collaborative signals.\n\n2. Both methods incorporate multimodal features (visual and textual) through learnable linear transformations to project raw features into a shared embedding space. BM3 uses single-layer MLPs (W_m, b_m) for projection, and MGCN similarly employs linear transformations (image_trs, text_trs) to align modality dimensions with ID embeddings.\n\n3. Both frameworks maintain separate user and item ID embeddings initialized with Xavier uniform distribution, which are enhanced through graph propagation. The implementation code shows both use nn.Embedding for user_embedding and item_id_embedding with identical initialization strategies.\n\n4. Both papers address the multimodal recommendation problem by combining collaborative filtering signals from interaction graphs with content-based signals from item modalities, aiming to improve recommendation accuracy through complementary information fusion.\n\n5. Both methods implement regularization mechanisms: BM3 uses explicit EmbLoss for regularization (reg_weight), while MGCN includes reg_weight parameter for controlling embedding regularization, preventing overfitting during training.",
    "differences": "1. Learning paradigm: BM3 adopts a self-supervised learning approach with bootstrap contrastive learning (no negative sampling), using stop-gradient and predictor networks to prevent collapse. MGCN uses supervised learning with BPR loss and optional contrastive loss (cl_loss), requiring explicit negative sampling during training.\n\n2. Multimodal fusion strategy: BM3 treats modalities independently with separate contrastive losses (loss_t, loss_v, loss_tv, loss_vt) using cosine similarity between online-target pairs. MGCN introduces a behavior-guided purifier with gating mechanisms (gate_v, gate_t) that filter modality noise using ID embeddings as guidance (E_id ⊙ σ(W_2·E_m)), and a behavior-aware fuser with attention-based adaptive fusion.\n\n3. Graph topology: BM3 operates solely on the user-item bipartite graph for collaborative filtering. MGCN constructs a multi-view architecture with both user-item graphs and item-item affinity graphs (built via KNN on modality similarities with topk=knn_k), capturing semantically correlative signals through item-item propagation on separate visual and textual graphs.\n\n4. Data augmentation: BM3 uses simple dropout-based augmentation (Bernoulli dropout with ratio p) applied to latent embeddings after encoding, avoiding graph structure augmentation. MGCN does not employ explicit augmentation but relies on KNN graph sparsification for noise reduction and multi-view propagation for feature enrichment.\n\n5. Computational complexity: BM3's implementation uses a single user-item graph with L layers (n_layers) and performs dropout-based augmentation, resulting in O(|E|·d·L) complexity. MGCN constructs and propagates over three graphs (user-item, image-item, text-item) with KNN sparsification, requiring additional graph construction (build_sim, build_knn_normalized_graph) and multi-view propagation, leading to higher preprocessing cost but potentially sparser item-item graphs.\n\n6. Feature enhancement mechanism: BM3 adds residual connections only to item embeddings (H_i = READOUT(...) + H_i^0) to combat over-smoothing. MGCN implements modality-specific gating for preference-aware filtering and uses query_common attention mechanism for adaptive modality weight learning, providing more fine-grained control over multimodal information integration.\n\n7. Training objectives: BM3 optimizes multiple contrastive terms (ID-to-modality, modality-to-modality) with a shared predictor network across all views. MGCN combines recommendation loss (BPR or BCE) with optional contrastive loss (cl_loss) and regularization, using separate gate networks for visual and textual modalities without shared prediction heads."
  },
  {
    "source": "SLMRec_2022",
    "target": "MGCN_2023",
    "type": "in-domain",
    "similarities": "1. **Graph-based collaborative filtering foundation**: Both papers employ LightGCN-style graph convolution on user-item bipartite graphs to capture collaborative signals. The propagation follows the same paradigm: all_emb = sparse_mm(norm_adj, all_emb) with layer-wise aggregation. SLMRec's implementation uses mean aggregation (torch.mean(embs, dim=1)) across layers, which is consistent with standard LightGCN practice.\n\n2. **Multimodal feature integration architecture**: Both methods transform raw visual and textual features into the same embedding space as ID embeddings using linear projections (SLMRec: v_dense/t_dense layers; MGCN: image_trs/text_trs layers). Both then propagate these modal features through graph structures to enrich representations.\n\n3. **Modality-specific graph construction**: Both papers construct separate graphs for different modalities. SLMRec propagates each modality through the user-item graph independently, while MGCN additionally builds item-item KNN graphs based on modality similarity. SLMRec's implementation shows it computes separate embeddings (i_emb, v_emb, t_emb) through the same graph structure.\n\n4. **Multi-view fusion strategy**: Both employ fusion mechanisms to combine ID and multimodal embeddings. SLMRec's code shows mm_fusion methods (concatenation or addition based on config) to merge [i_emb_u, v_emb_u, t_emb_u] for users and items. MGCN uses attention-based fusion with learnable gates.\n\n5. **BPR loss for main recommendation task**: Both use Bayesian Personalized Ranking loss as the primary objective for the recommendation task, computing scores via dot product between user and item embeddings.",
    "differences": "1. **Self-supervised learning paradigm**: SLMRec's core contribution is SSL with three tasks (Feature Dropout, Feature Masking, Fine-and-Coarse alignment). The implementation shows dropout-based augmentation (self.dropout on graph propagation) and InfoNCE losses for contrastive learning. MGCN focuses on behavior-guided purification and does not employ SSL tasks, instead using a contrastive loss (cl_loss) for regularization.\n\n2. **Modality noise handling**: MGCN introduces a behavior-guided purifier with gating mechanisms (gate_v, gate_t) that filter modality features using ID embeddings as guidance: E_purified = E_id ⊙ σ(W*E_modal). SLMRec does not explicitly model or filter modality noise; its implementation directly uses transformed features without purification gates.\n\n3. **Item-item graph modeling**: MGCN constructs explicit KNN-based item-item affinity graphs for each modality (image_adj, text_adj) with sparsification (topk=knn_k) and performs separate graph convolutions on these graphs. SLMRec only uses the user-item bipartite graph for all modalities, as evident from compute_graph function that always uses self.norm_adj (the user-item Laplacian).\n\n4. **Multi-granularity alignment strategy**: SLMRec's FAC task implements hierarchical spatial transformations (g_i_iv, g_v_iv, g_iv_iva, etc.) to align modalities in different granularity spaces (S_id·v, S_id·v·a, S_id·v·a·t). The implementation shows multiple projection layers for cross-modal alignment. MGCN does not employ such hierarchical alignment, instead using direct attention-based fusion.\n\n5. **Feature augmentation techniques**: SLMRec's implementation includes explicit augmentation methods: dropout on node features (self.dropout with configurable dropout_rate) and modality masking (randomly dropping entire modalities). MGCN does not use feature augmentation; its approach relies on graph structure and gating for feature refinement.\n\n6. **Training objective complexity**: SLMRec uses multi-task learning with weighted combination of BPR loss and SSL losses (L = L_main + α*L_ssl), where SSL includes multiple InfoNCE terms for different tasks. The code shows separate ssl_criterion and complex loss computation for FAC/FD/FM tasks. MGCN's loss is simpler: BPR loss + reg_weight*regularization + cl_loss*contrastive_term.\n\n7. **Computational efficiency**: SLMRec's implementation shows it computes embeddings for all modalities through the same graph (multiple forward passes through compute_graph), which is computationally expensive. MGCN optimizes by pre-computing and caching KNN graphs (image_adj_file, text_adj_file saved to disk) and uses sparse operations, improving efficiency.\n\n8. **User preference modeling**: MGCN explicitly models user modality preferences through separate gating networks (gate_image_prefer, gate_text_prefer) and attention mechanisms (query_common with softmax). SLMRec's implementation shows it aggregates user features by averaging interacted items' embeddings without explicit preference modeling beyond the graph structure."
  },
  {
    "source": "M2GRL_2020",
    "target": "MGCN_2023",
    "type": "in-domain",
    "similarities": null,
    "differences": null
  },
  {
    "source": "BM3_2023",
    "target": "LGMRec_2024",
    "type": "in-domain",
    "similarities": "1. Both adopt LightGCN-style graph propagation for collaborative filtering on user-item interaction graphs, using the simplified formula H^(l+1) = D^(-1/2)AD^(-1/2)H^l without feature transformation or non-linear activation layers. The BM3 implementation includes residual connections (H_i = READOUT(...) + H_i^0) for item embeddings to mitigate over-smoothing, which is also employed in LGMRec.\n2. Both project multimodal features (vision and text) into a unified embedding space using linear transformations before graph propagation. BM3 uses h_m = e_m*W_m + b_m (MLP with bias), while LGMRec uses ẽ_i^m = e_i^m·W_m (linear projection). The BM3 implementation freezes pre-trained modal features (freeze=False allows fine-tuning) and applies Xavier initialization to transformation matrices.\n3. Both employ layer combination strategies to aggregate multi-layer graph embeddings. BM3 uses mean pooling (READOUT function) across all layers [H^0, H^1, ..., H^L], and LGMRec adopts the same LAYERCOMB mean function for embedding integration, enabling effective capture of multi-hop neighborhood information.\n4. Both leverage dropout-based augmentation for contrastive learning. BM3 applies dropout to target embeddings (u_target = F.dropout(u_target, self.dropout)) with stop-gradient to prevent collapse, while LGMRec uses dropout (self.drop) with keep_rate parameter for embedding perturbation in its contrastive loss component.\n5. Both construct normalized adjacency matrices using the same symmetrically normalized Laplacian D^(-1/2)AD^(-1/2). The BM3 implementation includes epsilon (1e-7) in degree calculation to avoid division by zero, and both convert sparse scipy matrices to PyTorch sparse tensors for efficient GPU computation.",
    "differences": "1. Graph modeling approach: BM3 operates on a single user-item bipartite graph with ID embeddings and modal features propagated separately, while LGMRec introduces dual-level graph learning - local graph embedding (LGE) on user-item graphs and global hypergraph embedding (GHE) that captures higher-order dependencies through learnable hyperedges (v_hyper, t_hyper parameters) across different modality spaces.\n2. User modal feature initialization: BM3 does not explicitly initialize user modal features and relies on graph propagation from item features, whereas LGMRec explicitly initializes user modal embeddings by aggregating neighbor item modal features (ẽ_u^m = 1/|N_u| Σ ẽ_i^m), ensuring decoupled updates between ID embeddings and modal signals to avoid coupling collaborative and modal information.\n3. Contrastive learning strategy: BM3 employs a bootstrapped self-supervised approach with online-target network architecture, using cosine similarity loss (1 - cosine_similarity) between predicted and target embeddings with stop-gradient on targets. LGMRec uses a different contrastive framework that likely involves InfoNCE-style loss with temperature parameter (tau=0.2) and Gumbel-softmax for hyperedge assignment, focusing on local-global consistency.\n4. Hypergraph construction: LGMRec uniquely introduces hypergraph learning through Gumbel-softmax-based hyperedge assignment (F.gumbel_softmax(iv_hyper, tau, dim=1)) for both users and items across modalities, enabling capture of complex many-to-many relationships. BM3 lacks this global hypergraph structure and relies solely on pairwise user-item edges.\n5. Fusion mechanism: BM3 uses a shared predictor (nn.Linear) to align online embeddings with target embeddings across ID and modal spaces, while LGMRec employs an alpha-weighted fusion (self.alpha parameter) to combine local graph embeddings (CGE + MGE) with global hypergraph embeddings (GHE), providing explicit control over local-global trade-offs.\n6. Computational complexity: BM3 requires separate forward passes for online and target networks with stop-gradient operations, plus predictor transformations for all embeddings. LGMRec's hypergraph construction adds computational overhead through Gumbel-softmax operations and additional hypergraph propagation layers (n_hyper_layer), but eliminates the online-target dual network requirement, potentially offering different efficiency trade-offs depending on hypergraph size."
  },
  {
    "source": "SLMRec_2022",
    "target": "LGMRec_2024",
    "type": "in-domain",
    "similarities": "1. **Graph-based collaborative filtering foundation**: Both papers employ LightGCN-style graph convolution on user-item interaction graphs to capture collaborative signals. The source implementation uses simplified GCN propagation: `torch.sparse.mm(g_droped, all_emb)` with layer-wise aggregation via mean pooling, which LGMRec inherits with the same `(D^(-1/2) A D^(-1/2)) E^l` formulation.\n\n2. **Multimodal feature integration architecture**: Both methods process visual and textual modalities through separate channels before fusion. The source code implements modal-specific dense projections (`self.v_dense`, `self.t_dense`) to transform pre-trained features into unified embedding space, matching LGMRec's TRANSFORM function with learnable weight matrices `W_m`.\n\n3. **Multi-task learning framework**: Both adopt multi-task strategies combining recommendation (main task) with auxiliary self-supervised tasks. The source implementation shows explicit loss combination: `L = Lmain + α*Lssl`, where α balances recommendation and SSL objectives, similar to LGMRec's weighted fusion of collaborative, modal, and contrastive losses.\n\n4. **Layer-wise embedding aggregation**: Both papers aggregate multi-layer graph embeddings. The source code implements `torch.stack(embs, dim=1)` followed by `torch.mean(embs, dim=1)` for layer combination, which LGMRec adopts in its LAYERCOMB function for both CGE and MGE modules.\n\n5. **Separate modality-specific graph propagation**: Both methods independently propagate information on modality-specific graphs. The source implementation creates separate embeddings for each modality (`self.i_emb`, `self.v_emb`, `self.t_emb`) through independent graph convolutions, matching LGMRec's MGE module design that maintains decoupled modality representations.",
    "differences": "1. **User modal feature initialization strategy**: SLMRec reuses user ID embeddings for all modality-specific graphs (source code: `compute_graph(users_emb, self.v_dense_emb)`), causing coupling between collaborative and modal signals. LGMRec introduces decoupled initialization by aggregating neighbor item modal features: `ẽ_u^m = (1/|N_u|) Σ ẽ_i^m`, ensuring independent updates of ID and modal embeddings.\n\n2. **Global structure modeling**: SLMRec focuses solely on local user-item graph topology through standard GNN propagation. LGMRec introduces a novel Global Hypergraph Embedding (GHE) module that captures high-order item correlations across modalities using hypergraph neural networks with learnable hyperedge assignments via Gumbel-Softmax: `F.gumbel_softmax(torch.mm(modal_feat, hyper_matrix), tau)`.\n\n3. **Self-supervised learning paradigm**: SLMRec employs three SSL tasks (Feature Dropout, Feature Masking, Fine-and-Coarse alignment) with InfoNCE loss for intra-modal and cross-modal contrastive learning. The source code implements dropout-based augmentation (`self.dropout(torch.sparse.mm(...))`) and spatial transformation with projection heads (`self.g_i_iv`, `self.g_v_iv`). LGMRec uses a simpler contrastive learning approach focusing on local-global consistency between LGE and GHE representations.\n\n4. **Modal fusion granularity**: SLMRec's FAC task implements hierarchical multi-granularity space alignment (S_id·v, S_id·v·a, S_id·v·a·t) with cascaded projection functions for progressive modal alignment. LGMRec employs a unified fusion strategy combining collaborative and modality embeddings at the same semantic level: `E_final = FUSION(E_lge^id, E_lge^v, E_lge^t, E_ghe^v, E_ghe^t)`.\n\n5. **Computational architecture**: SLMRec processes all modalities through the same interaction graph structure with different input features, resulting in O(L·|E|·d) complexity per modality. LGMRec adds hypergraph computation with O(K·|I|·H·d) complexity for GHE module, where K is hypergraph layers and H is hyperedge number, introducing additional overhead but capturing richer global dependencies.\n\n6. **Feature augmentation mechanisms**: The source code shows SLMRec implements explicit feature dropout with configurable rate (`self.dropout_rate`) and feature masking by randomly dropping entire modalities during training. LGMRec does not employ such augmentation strategies, relying instead on the structural differences between local graphs and global hypergraphs to provide diverse views for contrastive learning."
  },
  {
    "source": "LATTICE_2021",
    "target": "DA-MRS_2024",
    "type": "in-domain",
    "similarities": "1. Both construct modality-specific item-item semantic graphs from multimodal features (visual and textual) using cosine similarity and k-NN sparsification to capture item relationships.\n2. Both employ normalized graph adjacency matrices for stable gradient propagation during training, using symmetric normalization D^(-1/2)AD^(-1/2).\n3. Both integrate multiple modality-specific graphs through learnable fusion mechanisms - LATTICE uses softmax-normalized modal weights, while DA-MRS extends this concept with alignment strategies.\n4. Both leverage user-item interaction graphs alongside item-item semantic graphs for collaborative filtering, combining content-based and collaborative signals.\n5. Both support flexible CF backbones (e.g., LightGCN, NGCF) for user-item graph propagation, treating the item graph construction as a modular preprocessing component.\n6. LATTICE's implementation pre-computes and caches item-item adjacency matrices (image_adj_{k}.pt, text_adj_{k}.pt) for efficiency; DA-MRS adopts similar caching strategy for item_graph_dict.",
    "differences": "1. Graph construction philosophy: LATTICE learns dynamic latent structures by transforming raw features through trainable linear layers (W_m, b_m) and combines initial k-NN graphs with learned graphs via skip connections (λS̃^m + (1-λ)Ã^m). DA-MRS focuses on denoising through cross-modal consistency - it prunes links where S^m_ij < mean(S^m) and enforces S^m_ij=0 if ∃m', S^m'_ij=0, avoiding false positives from noisy modalities.\n2. Item-item graph types: LATTICE constructs only semantic graphs from content features. DA-MRS introduces an additional Item-item Behavior Graph (IIB-Graph) built from user co-occurrence patterns (S^c_ij), capturing dynamic behavioral relationships that complement static semantic graphs.\n3. Training objectives: LATTICE uses standard BPR loss with L2 regularization. DA-MRS proposes denoising user feedback through pseudo-label generation (prob1+prob2+prob3+prob3) and adds KL divergence loss (kl_weight) plus neighbor consistency loss (neighbor_weight) for cross-modal alignment.\n4. Multimodal alignment: LATTICE performs simple weighted aggregation of modality-specific embeddings using learnable modal_weight parameters. DA-MRS explicitly aligns multimodal content with user feedback through two mechanisms: user preference-guided alignment and graded item relation alignment.\n5. Feature transformation: LATTICE learns high-level features via linear transformations (feat_embed_dim dimension) and dynamically updates graph structures during training (build_item_graph flag). DA-MRS uses frozen pre-trained embeddings with simple linear projections (image_trs, text_trs) to embedding_dim, avoiding overfitting.\n6. Computational strategy: LATTICE rebuilds item graphs each epoch when build_item_graph=True, adapting to learned representations. DA-MRS pre-computes all graphs offline (get_knn_adj_mat, get_session_adj) with fixed structures, prioritizing efficiency over adaptivity.\n7. Sparsification criteria: LATTICE applies uniform top-k selection per item. DA-MRS uses adaptive thresholding - if item_num ≤ knn_k, it keeps all valid neighbors; otherwise top-k, preventing over-pruning for items with few semantic connections."
  },
  {
    "source": "BM3_2023",
    "target": "DA-MRS_2024",
    "type": "in-domain",
    "similarities": "1. Both employ LightGCN-style graph convolutional propagation for user-item interaction modeling. BM3's implementation uses the simplified propagation formula H^(l+1) = D^(-1/2)AD^(-1/2)H^l with mean aggregation across layers and residual connections on item embeddings (H_i = READOUT(...) + H_i^0). DA-MRS adopts the same backbone CF architecture.\n\n2. Both utilize multi-modal features (visual and textual) and transform them into a shared embedding space. BM3 projects features using linear transformations (h_m = e_m W_m + b_m) implemented as nn.Linear layers (image_trs, text_trs). DA-MRS uses identical linear projection layers (image_trs, text_trs) with the same initialization strategy (xavier_uniform).\n\n3. Both construct normalized adjacency matrices for graph propagation. BM3's implementation uses symmetric normalization D^(-1/2)AD^(-1/2) via sparse tensor operations, computing degree matrix from interaction data. DA-MRS employs the same normalization strategy in compute_normalized_laplacian() for both user-item and item-item graphs.\n\n4. Both leverage item-item semantic relationships derived from multi-modal content. BM3 implicitly captures these through contrastive learning between modality-specific features, while DA-MRS explicitly constructs item-item semantic graphs using cosine similarity on pre-trained embeddings.\n\n5. Both use dropout-based augmentation strategies. BM3's implementation applies F.dropout() to target embeddings (u_target, i_target, t_feat_target, v_feat_target) with stop-gradient to prevent trivial solutions. DA-MRS likely employs similar dropout mechanisms during training.",
    "differences": "1. Learning paradigm: BM3 adopts self-supervised contrastive learning without negative sampling, using a predictor network with cosine similarity loss (1 - cosine_similarity) between online and target views. DA-MRS uses supervised BPR loss with explicit positive/negative sampling, incorporating KL-divergence regularization (kl_weight) for pseudo-label alignment.\n\n2. Graph construction strategy: BM3 relies solely on the user-item bipartite graph for structure learning, applying dropout augmentation on learned embeddings. DA-MRS constructs multiple explicit item-item graphs: (a) modality-specific semantic graphs (image_adj, text_adj) using k-NN with cross-modal consistency filtering (mask_v, mask_t), (b) behavior-based co-occurrence graph (session_adj) from item_graph_dict with weighted edges.\n\n3. Multi-modal fusion approach: BM3 uses separate contrastive losses for each modality (loss_t, loss_v, loss_tv, loss_vt) with a shared predictor, treating modalities independently. DA-MRS performs graph-based fusion by propagating features through modality-specific adjacency matrices (image_adj, text_adj) and combines them via neighbor_weight parameter.\n\n4. Denoising mechanisms: BM3 implements implicit denoising through stop-gradient on target networks and dropout augmentation. DA-MRS introduces explicit denoising: (a) cross-modal consistency filtering (removing S_ij^m if S_ij^m' = 0 in other modalities), (b) average similarity thresholding (S_ij^m < S̄^m), (c) pseudo-label generation via label_prediction() and generate_pesudo_labels() for feedback denoising.\n\n5. Computational complexity: BM3's implementation is more lightweight, using only one user-item graph with O(|E|·d·L) complexity for L-layer propagation. DA-MRS maintains multiple item-item graphs (3 graphs: visual, textual, behavioral) requiring O(|I|²·k) for k-NN construction and additional O(|I|·k·d·L) for multi-graph propagation, significantly increasing memory and computation costs.\n\n6. Training objectives: BM3 optimizes cl_weight * (contrastive_losses) + reg_weight * (embedding_regularization) with EmbLoss for L2 regularization. DA-MRS combines lambda_coeff * (BPR_loss) + kl_weight * (KL_divergence) + neighbor_weight * (alignment_loss), incorporating three distinct loss components for different learning signals.\n\n7. Feature preprocessing: BM3 sets freeze=False for embedding layers, allowing fine-tuning of pre-trained features during training. DA-MRS sets freeze=True for both image_embedding and text_embedding, keeping pre-trained features fixed and only learning projection weights."
  },
  {
    "source": "FREEDOM_2023",
    "target": "SMORE_2025",
    "type": "in-domain",
    "similarities": "1. Both methods construct item-item graphs based on multimodal feature similarity using cosine similarity computation between item pairs, as shown in FREEDOM's Eq.(1) and SMORE's Eq.(8).\n2. Both methods apply kNN-based graph sparsification to retain only top-K similar edges for each item, converting dense similarity matrices into sparse graphs (FREEDOM Eq.(2), SMORE Eq.(9)).\n3. Both methods normalize the item-item adjacency matrices using symmetric normalization with degree matrices (D^{-1/2} * A * D^{-1/2}) to stabilize graph convolution operations.\n4. Both methods use LightGCN-style message passing on the user-item bipartite graph without learnable transformation matrices, aggregating multi-hop neighbor information (FREEDOM Eq.(7), SMORE Eq.(15)).\n5. Both methods employ MLP layers to project raw multimodal features into a shared latent space with the same dimensionality as ID embeddings (FREEDOM Eq.(9), SMORE Eq.(1)).\n6. Both methods use BPR loss as the primary optimization objective for ranking-based recommendation (FREEDOM Eq.(10), SMORE Eq.(24)).\n7. Both methods combine ID embeddings with multimodal-enhanced representations for final user/item embeddings through additive fusion.\n8. Both methods handle visual and textual modalities separately before fusion, maintaining modality-specific processing pipelines.",
    "differences": "1. **Modality Fusion Strategy**: FREEDOM uses a simple weighted sum to fuse modality-specific item-item graphs (Eq.3 with hyperparameter α_v), while SMORE introduces Spectrum Modality Fusion using FFT to convert features to frequency domain, apply learnable filters for denoising, and perform point-wise multiplication for fusion (Eq.2-7). For implementation, SMORE requires: (a) FFT transformation of projected features, (b) complex-valued learnable filter weights W_{2,m}^c, (c) point-wise product fusion in frequency domain, (d) IDFT to convert back.\n2. **Graph Structure**: FREEDOM freezes a single fused item-item graph before training, while SMORE maintains separate modality-specific graphs AND a fusion graph constructed via max-pooling across modalities (Eq.11). Implementation requires maintaining multiple sparse adjacency matrices and computing max operation across modality graphs.\n3. **Denoising Approach**: FREEDOM applies degree-sensitive edge pruning on the user-item graph during training (sampling edges inversely proportional to node degrees), while SMORE performs denoising in frequency domain through learnable spectral filters. SMORE's approach requires implementing complex-valued neural network components.\n4. **Feature Gating Mechanism**: SMORE introduces behavioral-guided gating (Eq.12) that modulates multimodal features using ID embeddings through sigmoid gates before graph propagation. This requires element-wise multiplication of ID embeddings with sigmoid-transformed modal features.\n5. **Modality-Aware Preference Module**: SMORE adds an attention mechanism (Eq.17-18) to learn modality-specific weights using fusion features, and explicit preference gates (Eq.19-20) derived from behavioral signals. Implementation needs: attention computation over fusion features, separate gate networks for uni-modal and fusion preferences.\n6. **Contrastive Learning**: SMORE incorporates InfoNCE contrastive loss (Eq.22) to align behavioral embeddings with modality-side information, which FREEDOM lacks. Implementation requires computing contrastive loss between ID embeddings and aggregated modal features with temperature scaling.\n7. **User Modality Features**: FREEDOM only propagates item features on item-item graph, while SMORE explicitly computes user modality features through weighted-sum aggregation from interacted items (Eq.14). This requires additional aggregation layer for user modal representations.\n8. **Graph Convolution Depth**: FREEDOM uses configurable multiple layers (n_mm_layers) on item-item graph, while SMORE explicitly uses shallow (single) layer on item-item graphs to avoid high-order noise, with deeper propagation only on user-item graph.\n9. **Loss Function**: FREEDOM's loss includes BPR on both ID embeddings and projected modal features (Eq.10 with λ weighting), while SMORE uses BPR + contrastive loss + L2 regularization (Eq.25) without explicit modal feature reconstruction loss."
  },
  {
    "source": "MGCN_2023",
    "target": "SMORE_2025",
    "type": "in-domain",
    "similarities": "1. Both methods adopt a multi-view graph learning paradigm that combines User-Item View (behavioral/collaborative signals) and Item-Item View (semantic/modality-specific signals) for multimodal recommendation.\n2. Both use LightGCN-style message propagation on the user-item interaction graph with symmetric normalization (D^{-1/2}AD^{-1/2}) and layer-wise aggregation via mean pooling to capture high-order collaborative signals.\n3. Both construct item-item affinity graphs based on cosine similarity of raw modality features and apply KNN sparsification to retain top-K edges, followed by symmetric normalization (D^{-1/2}SD^{-1/2}).\n4. Both employ a behavior-guided gating mechanism to filter preference-relevant modality features: E_id ⊙ σ(W·H + b), using ID embeddings to guide modality feature purification.\n5. Both use shallow (single-layer) graph convolution on item-item modality graphs to avoid over-smoothing and noise amplification from deep propagation.\n6. Both aggregate user modality features by weighted-sum aggregation from interacted items: h_u = Σ(1/√|N_u||N_i|)·h_i.\n7. Both use attention mechanisms to extract modality-shared or weighted features: softmax(q^T·tanh(W·H + b)) pattern for computing attention weights.\n8. Both employ InfoNCE contrastive loss to maximize mutual information between behavioral embeddings and multimodal side embeddings for users and items.\n9. Both use BPR loss as the primary optimization objective combined with contrastive auxiliary loss and L2 regularization.\n10. Both produce final user/item representations by adding behavioral embeddings and multimodal side embeddings: e = e_id + e_mul.\n11. Both use inner product for prediction: ŷ = e_u^T · e_i.",
    "differences": "1. **Modality Fusion Approach**: MGCN separates modality-shared and modality-specific features via subtraction (E_specific = E_m - E_shared) and fuses them with preference gates. SMORE introduces Spectrum Modality Fusion using FFT to convert features to frequency domain, applies learnable complex filters for denoising, and performs point-wise product fusion (equivalent to circular convolution) before IFFT conversion back - this is a novel frequency-domain fusion approach requiring implementation of FFT/IFFT operations with complex-valued learnable filters W^c ∈ C^{n×d}.\n2. **Fusion Graph Construction**: MGCN only has modal-specific item-item graphs. SMORE additionally constructs a Fusion Graph using max-pooling strategy across modality graphs: S^f_{a,b} = max(S^m_a, S^{m'}_b) to capture cross-modality complementary features, requiring separate propagation on this fusion graph.\n3. **Feature Propagation Targets**: MGCN propagates behavior-guided modal features on item-item graphs. SMORE propagates both denoised uni-modal features (Ḧ_{i,m}) AND fused features (Ḧ_{i,f}) through their respective graphs, requiring dual propagation paths.\n4. **Modality Preference Extraction**: MGCN uses behavioral embeddings directly through gate functions to get preferences. SMORE uses fusion embeddings (H̄_f) to compute attention weights (α_m) for weighting uni-modal features, then extracts BOTH uni-modal preferences (Q_m) AND fusion preferences (Q_f) from behavioral embeddings - requiring separate preference gates for each.\n5. **Final Side Feature Computation**: MGCN: E_mul = E_s + (1/|M|)Σ(Ẽ_m ⊙ P_m). SMORE: H_s = (1/|M|)(Σ(H*_m ⊙ Q_m)) + (H_f ⊙ Q_f), explicitly incorporating fusion preference-weighted fusion features as an additive term.\n6. **Denoising Mechanism**: MGCN relies on behavior-guided gating for implicit denoising. SMORE implements explicit frequency-domain filtering with learnable complex weights that act as frequency selectors to suppress noise-related information.\n7. **Attention Weight Computation**: MGCN uses shared attention parameters across modalities for extracting common features. SMORE uses modality-specific attention parameters (W_{5,m}, b_{5,m}, p_m) and fusion embeddings as the query source rather than the modality features themselves.\n8. **Implementation Complexity**: SMORE requires: (a) FFT/IFFT operations on modality features, (b) complex-valued neural network components for filters, (c) separate processing pipeline for fusion features throughout the model, (d) max-pooling graph construction, (e) dual contrastive signals from both uni-modal and fusion paths."
  },
  {
    "source": "LATTICE_2021",
    "target": "PGL_2025",
    "type": "in-domain",
    "similarities": "1. Both methods operate in the same domain (Multimedia Recommendation) and address the same task of leveraging multimodal features (visual and textual) to improve recommendation performance.\n2. Both methods construct item-item similarity graphs based on multimodal features using cosine similarity and kNN sparsification to capture item relationships.\n3. Both methods apply Laplacian normalization (symmetric normalization) to the constructed adjacency matrices for stable graph convolution operations.\n4. Both methods use message passing/graph convolution on item-item graphs to propagate and aggregate information, capturing high-order item relationships.\n5. Both methods fuse multiple modality-specific graphs through weighted combination to obtain a unified item-item relationship graph.\n6. Both methods combine the learned item representations from multimodal graphs with collaborative filtering embeddings (user-item interaction graph) for final prediction.\n7. Both methods use BPR (Bayesian Personalized Ranking) loss as the primary optimization objective for pairwise ranking.\n8. Both methods compute final prediction scores using inner product between user and item embeddings.\n9. Both methods use ID embeddings for users and transform/use multimodal features for items in their respective graph learning components.",
    "differences": "1. **Embedding Initialization**: LATTICE uses separate ID embeddings for items in the CF component, while PGL maps item content features through MLP and concatenates them as item raw representations, arguing content features are more informative for model convergence.\n2. **Principal Graph Learning (Novel Component)**: PGL introduces principal subgraph extraction from the user-item interaction graph, which LATTICE does not have. This includes two extraction operators:\n   - Global-Aware Extraction: Uses randomized SVD decomposition followed by truncated reconstruction with ratio γ to identify principal subgraphs with highest information content.\n   - Local-Aware Extraction: Uses multinomial sampling based on node degree (edges connected to lower-exposure nodes have higher sampling probability), with sampling ratio p (typically 0.3).\n3. **Training vs Inference Strategy**: PGL performs message passing on the principal subgraph during training but uses the complete graph during inference to avoid subgraph fragmentation issues. LATTICE uses the same graph structure for both training and inference.\n4. **Latent Graph Learning Simplification**: PGL pre-constructs latent item-item graphs based on raw modality features and keeps them fixed, avoiding the dynamic graph structure learning in LATTICE. LATTICE learns latent structures by transforming features through linear layers and combining learned graphs with initial graphs via skip connection (λ coefficient).\n5. **Feature Transformation**: LATTICE transforms raw modality features to high-level features using linear transformation (W_m, b_m) for dynamic graph learning. PGL simplifies this by using pre-computed similarity matrices without learnable transformation during latent graph construction.\n6. **Self-Supervised Learning**: PGL incorporates an auxiliary self-supervised task using feature masking and InfoNCE loss to enhance representation distinguishability. LATTICE does not include self-supervised learning components.\n7. **Loss Function**: PGL uses combined loss: L = L_BPR + λ_SSL * L_SSL, while LATTICE only uses BPR loss with embedding regularization.\n8. **Final Representation Combination**: PGL explicitly combines representations from principal graph learning and latent graph learning: ê_u = e_u^principal + e_u^latent. LATTICE adds normalized item graph embeddings to CF item embeddings.\n9. **Graph Convolution Target**: PGL applies principal graph learning on the user-item bipartite graph (extracting subgraphs from interaction matrix), while LATTICE focuses graph convolution primarily on item-item graphs constructed from multimodal features.\n10. **Modality Weight Learning**: LATTICE uses learnable modal weights with softmax normalization. PGL mentions weighted addition of modality matrices but the specific mechanism for weight learning is not detailed in the same way."
  },
  {
    "source": "FREEDOM_2023",
    "target": "PGL_2025",
    "type": "in-domain",
    "similarities": "1. Both methods construct item-item graphs based on multimodal feature similarity using kNN sparsification (k=10) and Laplacian normalization, pre-computing these graphs before training to avoid computational overhead during training.\n2. Both employ LightGCN-style message passing on user-item bipartite graphs without learnable transformation matrices, using simple neighborhood aggregation.\n3. Both use degree-sensitive edge sampling for user-item graph processing - FREEDOM uses probability p_k = 1/sqrt(w_i*w_j) for pruning, and PGL uses the same weighting scheme 1/sqrt(|N_u|*|N_i|) for local-aware extraction.\n4. Both combine representations from item-item graph and user-item graph for final item embeddings, using additive fusion.\n5. Both adopt BPR loss as the primary optimization objective for recommendation.\n6. Both use MLP layers to project raw multimodal features (visual and textual) into a shared embedding space with ID embeddings.\n7. Both methods differentiate between training and inference phases - using processed/sampled graphs during training but original complete graphs during inference.\n8. Both weight different modality item-item graphs with hyperparameters (alpha_v in FREEDOM, similar weighting in PGL) to obtain the final fused item-item similarity matrix.",
    "differences": "1. **Embedding Initialization**: FREEDOM uses separate user ID embeddings and item ID embeddings as raw representations, while PGL concatenates projected multimodal content features to form item raw representations, arguing content features are more informative for convergence.\n2. **User-Item Graph Processing Strategy**: FREEDOM applies degree-sensitive edge pruning (dropout) that randomly removes edges based on degree probability in each epoch. PGL offers two extraction methods: (a) Global-aware extraction using randomized SVD decomposition followed by truncated reconstruction with eigenvalue retention ratio γ and sparsification threshold ε; (b) Local-aware extraction using multinomial sampling with retention ratio p (typically 0.3).\n3. **SVD-based Principal Graph Learning**: PGL introduces a novel global-aware extraction using ApproxSVD(A, d) decomposition, truncating eigenvalues to retain top γd and bottom (1-γ)d eigenvalues, then reconstructing and sparsifying. This is entirely absent in FREEDOM.\n4. **Message Passing Formula**: PGL uses a generalized form H(F(A))x where F is the subgraph extraction function and H is the message passing function. FREEDOM directly applies sparse matrix multiplication on the processed adjacency matrix.\n5. **Self-Supervised Learning**: PGL incorporates an auxiliary SSL task using feature masking and InfoNCE loss to enhance representation distinguishability (L = L_BPR + λ_SSL * L_SSL). FREEDOM only uses BPR loss with additional modality-specific BPR terms.\n6. **Modality Feature Usage in Loss**: FREEDOM includes modality-specific BPR losses (mf_v_loss, mf_t_loss) weighted by λ in the training objective. PGL does not explicitly include separate modality losses but relies on SSL for auxiliary supervision.\n7. **Readout Function**: FREEDOM uses mean pooling across all GCN layers for user-item graph embeddings (following LightGCN). PGL's paper does not explicitly specify but implies direct output from message passing.\n8. **Implementation of Graph Normalization**: For the reconstructed principal graph in PGL, additional sparsification with threshold ε (typically 1e-3) is applied after SVD reconstruction, which is not present in FREEDOM's processing pipeline.\n9. **Training Graph vs Inference Graph**: While both differ between training and inference, PGL explicitly extracts a 'principal subgraph' concept for training to capture local structural features, whereas FREEDOM frames it as 'denoising' through edge dropout."
  },
  {
    "source": "BM3_2023",
    "target": "CM3_2025",
    "type": "in-domain",
    "similarities": "1. Both methods utilize LightGCN as the backbone for graph-based representation learning on the user-item interaction graph, employing the same simplified graph convolution without feature transformation and non-linear activation layers.\n2. Both methods project high-dimensional multimodal features (visual and textual) into a lower-dimensional latent space using linear transformations or MLPs, ensuring multimodal features share the same embedding dimension as ID embeddings.\n3. Both methods use a READOUT function (mean in BM3, sum in CM3) to aggregate representations across multiple GCN layers to obtain final user and item embeddings.\n4. Both methods incorporate residual connections for item embeddings, adding initial item representations to the aggregated GCN outputs to mitigate over-smoothing issues.\n5. Both methods compute recommendation scores using inner products between user and item representations for top-K recommendation.\n6. Both methods handle two modalities (vision and text) with modality-specific projection layers, and the architecture can be extended to more modalities.\n7. Both methods operate on the same type of datasets (Amazon Baby, Sports, Electronics) with similar preprocessing and evaluation protocols.",
    "differences": "1. **Multimodal Fusion Strategy**: BM3 does not explicitly fuse multimodal features but aligns them separately with ID embeddings through contrastive losses. CM3 introduces Spherical Bézier Multimodal Fusion using De Casteljau's algorithm to iteratively interpolate multimodal features on a hyperspherical manifold, creating mixed features that lie on the unit hypersphere.\n2. **Feature Concatenation**: CM3 concatenates individual modality features with the mixed multimodal features to form item representations (Equation 4), while BM3 keeps modality features separate and uses them only in contrastive alignment losses.\n3. **Loss Function Design**: BM3 uses a self-supervised contrastive learning framework with three losses: graph reconstruction loss (cosine similarity between user-item pairs), inter-modality alignment loss, and intra-modality masked loss, all based on dropout-based augmentation and stop-gradient. CM3 uses alignment loss (L2 distance for positive pairs) and uniformity losses, with a novel calibrated uniformity loss that differentiates item relationships based on similarity scores.\n4. **Uniformity Loss Innovation**: CM3 introduces calibrated uniformity loss (Equation 10) that amplifies repulsion between dissimilar items by a factor of e^{2t(1-φ)}, where φ is the pre-computed similarity score. This replaces the standard uniformity loss for items while keeping standard uniformity for users. BM3 has no such uniformity constraint.\n5. **User Preference Mining and Architectural Pattern**: CM3's Section 3.4.2 describes partitioning user embeddings into |M|+1 segments corresponding to different modalities and using a learnable weight matrix W₃ ∈ ℝ^{|U|×(|M|+1)×1} to compute differentiated user preferences (Equation 7). When combined with Equation 4's specification that item representations have dimension ℓ = (|M|+1)d and the requirement to reshape user embeddings to [|U|, |M|+1, d] for element-wise weighting, this suggests a three-path parallel architecture rather than BM3's unified embedding approach. The paper's method description, supported by the architectural diagram, indicates that each modality (visual, textual) plus the mixed modality maintains separate representation pathways. One natural implementation strategy to achieve the described 'partition' effect while satisfying the dimensional requirements would be to initialize independent learnable representations for each modality pathway (e.g., three separate parameter tensors of shape [n_users, d] corresponding to visual, textual, and mixed user preferences), then concatenate them with their respective item features before feeding into the GCN. This differs fundamentally from BM3's single user_embedding approach, as the 'partition' in CM3 appears to reflect an architectural design where modality-specific preferences exist independently from initialization rather than being extracted from a monolithic embedding post-training.\n6. **Item-Item Graph**: CM3 constructs and performs graph convolutions on an item-item graph S based on multimodal features for higher-order item relationships (Section 3.4.3), while BM3 only uses the user-item interaction graph.\n7. **Contrastive View Generation**: BM3 generates contrastive views through dropout-based augmentation with Bernoulli masking and uses a predictor MLP with stop-gradient on target views. CM3 does not use such augmentation-based contrastive learning.\n8. **Projection Architecture**: BM3 uses single-layer linear projections for multimodal features, while CM3 uses two-layer DNNs with non-linear activation (Leaky_relu) for feature projection (Equation 1).\n9. **GCN Input Dimension and Concatenation Pattern**: BM3 maintains separate embeddings of dimension d for ID and each modality. CM3's Section 3.4.1 specifies that the GCN operates on concatenated features with total dimension ℓ = (|M|+1)d. This dimensional specification, combined with the requirement that user embeddings must be reshapable to [|U|, |M|+1, d] for the preference weighting operation in Section 3.4.2, implies the GCN input should be constructed by horizontally concatenating |M|+1 separate d-dimensional pathways (one for each modality plus the mixed modality). In practice, this might be implemented by concatenating three [n_nodes, d] tensors along dimension 1 to form a [n_nodes, 3d] GCN input when |M|=2, where each pathway consists of the corresponding user preference representation concatenated with its matched item features (e.g., visual user preference with visual item features, textual user preference with textual item features, mixed user preference with mixed features). After GCN propagation, the user portion can be reshaped from [n_users, 3d] to [n_users, 3, d] to apply the learned modality weights W₃ before final fusion.\n10. **Regularization**: BM3 explicitly includes L2 regularization on online embeddings in the loss function. CM3's regularization is implicit through the uniformity losses that encourage uniform distribution on the hypersphere."
  },
  {
    "source": "FREEDOM_2023",
    "target": "CM3_2025",
    "type": "in-domain",
    "similarities": "1. Both methods utilize a dual-graph learning paradigm, incorporating both user-item bipartite graphs and item-item graphs constructed from multimodal features to capture collaborative signals and item semantic relationships.\n2. Both methods employ LightGCN-style graph convolutions for information propagation on the user-item graph, using normalized adjacency matrices and layer-wise aggregation with a READOUT function (mean/sum) to derive final representations.\n3. Both methods construct item-item graphs based on multimodal feature similarities using k-NN sparsification, where top-k similar items are connected based on cosine similarity of their multimodal embeddings.\n4. Both methods use Deep Neural Networks (MLPs/DNNs) to project high-dimensional raw multimodal features (visual and textual) into lower-dimensional latent spaces that align with the ID embedding dimension.\n5. Both methods combine representations from multiple graphs - the user-item graph provides collaborative filtering signals while the item-item graph provides multimodal semantic information, with final item representations aggregating information from both sources.\n6. Both methods consider visual and textual modalities (M = {v, t}) and use pre-trained feature extractors to obtain initial multimodal features.\n7. Both methods use inner product (dot product) between user and item representations to compute preference scores for top-K recommendation.",
    "differences": "1. Loss Function Design: FREEDOM uses BPR loss with auxiliary multimodal reconstruction losses, while CM3 adopts alignment and uniformity losses from contrastive learning. CM3 introduces a novel 'calibrated uniformity loss' that differentiates item relationships based on their multimodal similarity scores, amplifying repulsion between dissimilar items by factor e^{2t(1-φ)}.\n2. Multimodal Fusion Strategy: FREEDOM uses a simple weighted sum (α_v * visual + α_t * textual) to combine modality-specific item-item graphs. CM3 introduces 'Spherical Bézier Multimodal Fusion' using De Casteljau's algorithm for iterative spherical interpolation on hyperspherical manifold, enabling infinite multimodal fusion while preserving geometric properties.\n3. User-Item Graph Processing: FREEDOM implements degree-sensitive edge pruning (denoising) by sampling edges based on inverse degree probability during training. CM3 does not mention any graph denoising or edge dropout mechanism on the user-item graph.\n4. Item-Item Graph Usage: FREEDOM freezes the item-item graph before training (pre-computed and fixed), eliminating O(N²d_m) computational cost during training. CM3 does not explicitly mention freezing the item-item graph, suggesting it may be dynamically computed or updated.\n5. User Preference Modeling and Architectural Pattern: CM3 introduces a learnable weight matrix W₃ ∈ ℝ^{|U|×(|M|+1)×1} to model user-specific preferences across different modalities (Section 3.4.2). The paper describes partitioning user embeddings into |M|+1 segments and applying element-wise weighting, which when combined with the dimensional requirement ℓ = (|M|+1)d from Equation 4 and the need to reshape to [|U|, |M|+1, d], suggests a multi-path representation structure where each modality pathway maintains independent learnable parameters. This contrasts with FREEDOM's approach of not explicitly differentiating user preferences across modalities. The method description indicates that implementing this 'partition' effect may naturally involve initializing separate parameter tensors for each modality's user preference (visual, textual, and mixed), which are then concatenated with their respective item features before graph learning, rather than learning a unified embedding that is later split.\n6. Feature Concatenation and GCN Input Construction: CM3 concatenates all modality features plus the mixed feature (|M|+1 vectors) to form item representations before graph learning, with the paper specifying that GCN input has dimension ℓ = (|M|+1)d in Section 3.4.1. This dimensional specification, combined with the architectural requirements from Section 3.4.2, implies that the GCN may operate on horizontally concatenated pathways (e.g., [visual_path, textual_path, mixed_path] forming [n_nodes, 3d] when |M|=2), where each pathway pairs the corresponding modality's user preference with its item features. FREEDOM keeps modality features separate throughout graph learning and only combines item-item graph output with user-item graph output at the final stage, without this multi-path concatenation pattern.\n7. Representation Space: CM3 explicitly constrains representations on a unit hypersphere for alignment and uniformity optimization. FREEDOM does not impose such geometric constraints on the embedding space.\n8. Graph Convolution on Item-Item Graph: FREEDOM uses the item-item graph output directly as the multimodal view representation (h̃_i = h̃_i^{L_ii}). CM3 adds a residual connection between the final layer representation and initial representation (X̂ = X̄^{L_ii} + X̃^0).\n9. Similarity Score Computation for Calibrated Loss: CM3 pre-computes clamped similarity scores s(ī, ī') between items using mixed multimodal features for the calibrated uniformity loss, which is a unique component not present in FREEDOM."
  },
  {
    "source": "MMGCN_2019",
    "target": "MVGAE_2021",
    "type": "in-domain",
    "similarities": "1. Both build modality-specific GCN-based encoders on the user–item bipartite graph to propagate multi-hop neighborhood signals.\n2. Both start from pre-extracted item content features and trainable user embeddings per modality to learn representations.\n3. Both fuse multi-modal signals into unified user/item embeddings before prediction.\n4. Both score user–item pairs via inner-product similarity.",
    "differences": "1. Learning objective: MVGAE optimizes a VAE-style ELBO to reconstruct the adjacency with KL regularization, whereas MMGCN uses BPR pairwise ranking with negative sampling.\n2. Representation fusion: MVGAE models modality-specific Gaussian posteriors and fuses them via Product-of-Experts (precision/uncertainty-weighted) then hierarchically with a collaborative latent (naturally handling missing modalities), while MMGCN learns deterministic vectors coordinated by an ID embedding and sums them across modalities.\n3. Architectural emphasis: MVGAE includes a separate collaborative GCN encoder and hierarchical PoE fusion to explicitly separate feature-based and collaborative factors, while MMGCN relies on per-modality aggregation (mean/max) and combination (concat/element-wise) layers without a dedicated collaborative encoder.\n4. Training/inference mechanics: MVGAE samples via reparameterization during training and may apply a sigmoid to embeddings before inner-product for sparsity robustness, whereas MMGCN trains deterministically with triplet sampling under BPR."
  },
  {
    "source": "VBPR_2016",
    "target": "MVGAE_2021",
    "type": "in-domain",
    "similarities": "1. Both combine collaborative signals with item content to improve recommendations, particularly for cold-start items.\n2. Both score user–item affinity via an inner product in a learned latent space.\n3. Both train from implicit feedback without explicit labels (self-supervision from interactions).",
    "differences": "1. Learning objective: MVGAE is generative (maximizes a VAE-style ELBO to reconstruct the adjacency with KL regularization), whereas VBPR is discriminative (optimizes pairwise BPR ranking with negative sampling).\n2. Architecture: MVGAE uses modality-specific GCN encoders and hierarchical product-of-experts fusion with reparameterized Gaussian latents; VBPR is a shallow MF model augmenting factors with a linear visual embedding and no uncertainty or message passing.\n3. Multimodal handling: MVGAE supports arbitrary modalities and missing-modality robustness via PoE (precision-weighted fusion); VBPR uses a single visual modality (fixed CNN features) fused additively.\n4. Collaborative modeling: MVGAE explicitly captures high-order graph connectivity with a dedicated collaborative encoder; VBPR models only first-order interactions via dot-products without graph propagation."
  },
  {
    "source": "MMGCN_2019",
    "target": "DualGNN_2021",
    "type": "in-domain",
    "similarities": "1. Both build modality-specific user–item bipartite graphs and learn modal-specific user/item representations via message passing.\n2. Both use pooling-style neighborhood aggregation and stacked layers to encode higher-order collaborative signals.\n3. Both rely on pre-extracted visual/acoustic/text item features with trainable user representations per modality.\n4. Both fuse multi-modal representations to score user–item pairs by similarity for recommendation.",
    "differences": "1. MMGCN applies feature transformations and nonlinearities in both aggregation and combination layers and bridges modalities with a user-ID embedding, whereas DualGNN uses LightGCN-style propagation (no self-loop, no transform/activation) for single-modal graphs.\n2. MMGCN interleaves cross-modal combination within each layer and then sums modalities, while DualGNN first learns per-modality embeddings and then performs user-level multimodal fusion via attentive concat/sum/max with per-user weights.\n3. DualGNN adds a separate user–user co-occurrence graph (Top-K sampling with mean/softmax weighting) to learn personalized fusion patterns; MMGCN relies solely on modality-specific user–item graphs.\n4. MMGCN models and fuses modal-specific item representations symmetrically, whereas DualGNN omits an item-side multimodal fusion module, assuming items share a unified fusion pattern."
  },
  {
    "source": "VBPR_2016",
    "target": "DualGNN_2021",
    "type": "in-domain",
    "similarities": "1. Both augment collaborative signals with item content features and rank by user–item similarity using learned representations.\n2. Both avoid heavy end-to-end content encoders, relying on shallow linear fusion (VBPR linear visual embedding; DualGNN LightGCN-style propagation and simple attentive fusion).\n3. Both learn user-specific content preferences (VBPR user visual preference vector; DualGNN per-user modality weights).",
    "differences": "1. Learning objective/training: VBPR explicitly optimizes a pairwise BPR ranking loss with negative sampling; DualGNN centers learning on message passing over graphs (modality-specific user–item graphs and a user co-occurrence graph) with top-K neighbor sampling.\n2. Representation granularity: VBPR uses a single linear projection of CNN visual features combined with MF factors; DualGNN learns per-modality embeddings, then constructs and refines a user-specific multi-modal vector via attentive concat/sum/max and co-occurrence aggregation.\n3. Architecture: VBPR is a shallow bilinear MF model; DualGNN is a dual-GNN design (LightGCN-style per-modality propagation plus user–user GCN) that captures higher-order collaborative and user co-occurrence signals.\n4. Multimodal usage: VBPR uses only visual features to tackle cold-start; DualGNN fuses visual, acoustic, and textual modalities and models user-specific fusion patterns (no symmetric fusion module on the item side)."
  }
]
