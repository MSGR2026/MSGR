{
  "id": "LGMRec_2024",
  "paper_title": "LGMRec: Local and Global Graph Learning for Multimodal Recommendation",
  "alias": "LGMRec",
  "year": 2024,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# Methodology\n\nIn this section, we first formulate the problem of multimodal recommendation and present the overall framework of our LGMRec, and then introduce each component in detail.\n\n# Problem Statement and Overview\n\nWe set the user set as $\\mathcal { U } = \\{ \\boldsymbol { u } \\}$ and item set as $\\mathcal { T } = \\{ i \\}$ . The ID embeddings of each user $u \\in \\mathcal { U }$ and item $i \\in \\mathcal { T }$ are denoted as $\\mathbf { e } _ { u } \\in \\mathbb { R } ^ { d }$ and $\\mathbf { e } _ { i } \\in \\mathbb { R } ^ { d }$ , respectively, where $d$ is the embedding dimension. The user-item interactions can be represented as a matrix $\\mathbf { R } \\in \\mathbb { R } ^ { | \\mathcal { U } | \\times | \\mathcal { T } | }$ , in which the element $r _ { u , i } = 1$ if user $u$ interacts with item $i$ , and $r _ { u , i } = 0$ otherwise. Based on interaction matrix $\\mathbf { R }$ , we can construct the user-item interaction graph $\\mathcal { G } = \\{ \\mathcal { U } \\cup \\mathcal { I } , \\mathcal { E } \\}$ , where $\\mathcal { E }$ is edge set build on observed interactions, i.e., a nonzero $r _ { u , i }$ corresponds to an edge between user $u$ and item $i$ on the graph $\\mathcal { G }$ . Further, we incorporate item multimodal contents and denote the original modality feature of item $i$ generated from pre-trained models as $\\mathbf { e } _ { i } ^ { m } \\in \\mathbb { R } ^ { d _ { m } }$ under modality $m \\in$ $\\mathcal { M }$ , where $\\mathcal { M }$ is the set of modalities and $d _ { m }$ denotes the dimension of modal features. In this work, we consider two mainstream modalities, vision $v$ and text $t$ , i.e., $\\mathcal { M } = \\{ v , t \\}$ . Given the above settings, the multimodal recommendation aims to learn a prediction function to forecast the score $\\hat { r } _ { u , i }$ of an item $i$ adopted by a user $u$ via joint modeling user behaviors and multimodal contents. Formally,\n\n$$\n\\hat { r } _ { u , i } = \\mathrm { P R E D I C T I O N } \\left( \\mathbf { R } , \\mathbf { E } ^ { i d } , \\{ \\mathbf { E } _ { i } ^ { m } \\} _ { m \\in \\mathcal { M } } \\right)\n$$\n\nwhere PREDICTION $( \\cdot )$ is the prediction function, $\\begin{array} { r l } { \\mathbf { E } ^ { i d } } & { { } = } \\end{array}$ [eu1 , . . . , eu|U| , ei1 , . . . , ei|I| ] ∈ R(|U |+|I|)×d denotes the ID embedding matrix by stacking all the ID embeddings of users and items, $\\mathbf { E } _ { i } ^ { m } \\equiv [ \\mathbf { e } _ { i _ { 1 } } ^ { m } , \\ldots , \\mathbf { e } _ { i _ { | \\mathcal { T } | } } ^ { m } ] \\in \\mathbb { R } ^ { | \\mathcal { T } | \\times d _ { m } }$ is the item modal feature matrix under modality $m$ .\n\nOverview. As illustrated in Figure 2, the framework of LGMRec consists of three major components: (i) Local graph embedding (LGE) module, which adopts GNN to capture collaborative-related and modality-related user local interests on user-item interaction graph with ID embeddings and modal features, respectively; (ii) Global hypergraph embedding (GHE) module, which learns the global user and item representations by capturing the global hypergraph structure dependencies from different item modal feature spaces; and (iii) Fusion and prediction module, which fuses both local and global embeddings to predict final user preference scores for items.\n\n# Local Graph Embedding (LGE) Module\n\nThe LGE module is designed to independently learn the collaborative-related and modality-related user and item representations with local topology structure for avoiding unstable updates of user embeddings and promoting decoupled user interest learning.\n\nCollaborative Graph Embedding (CGE) We first capture the high-order connectivity via the message propagation on the user-item interaction graph with ID embeddings. In particular, the collaborative graph propagation function CGPROG $( \\cdot )$ in the $( l + 1 )$ -th layer can be formatted as,\n\n$$\n\\mathbf { E } ^ { l + 1 } = \\mathbf { C } \\mathbf { G } \\mathbf { P } \\mathbf { R } \\mathbf { O } \\mathbf { G } ( \\mathbf { E } ^ { l } ) = \\left( \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\mathbf { A } \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\right) \\mathbf { E } ^ { l } ,\n$$\n\nwhere $\\mathbf { C G P R O G } ( \\cdot )$ function inherits the lightweight form of the simplified graph convolutional network (Chen et al. 2020; He et al. 2020), $\\mathbf { A } \\in \\mathbb { R } ^ { ( | \\mathcal { U } | + | \\mathcal { T } | ) \\times ( | \\mathcal { U } | + | \\mathcal { T } | ) }$ is the adjacency matrix constructed from interaction matrix $\\mathbf { R }$ , and $\\mathbf { D }$ is the diagonal matrix of $\\mathbf { A }$ . Each diagonal element $\\mathbf { D } _ { j , j }$ in $\\mathbf { D }$ denotes the number of nonzero entries in the $j$ -th row vector of matrix A. The initial embeddings matrix is set as ${ \\bf E } ^ { 0 } = { \\bf E } ^ { i d }$ . Then, we adopt the layer combination (He et al. 2020) to integrate all embeddings from hidden layers,\n\n$$\n\\mathbf { E } _ { l g e } ^ { i d } = \\mathrm { L A Y E R C O M B } \\left( \\mathbf { E } ^ { 0 } , \\mathbf { E } ^ { 1 } , \\mathbf { E } ^ { 2 } , \\ldots , \\mathbf { E } ^ { L } \\right) ,\n$$\n\nwhere $\\mathbf { E } _ { l g e } ^ { i d } \\in \\mathbb { R } ^ { ( | \\mathcal { U } | + | \\mathcal { T } | ) \\times d }$ is collaborative-related embeddings of users and items with local neighborhood information. We use the mean function to achieve LAYERCOMB $( \\cdot )$ for embedding integration.\n\nModality Graph Embedding (MGE) Considering the semantic differences between modalities, we further independently infer the modality-related embeddings of users and items on the interaction graphs with modal features. The original modal features of items are usually generated from different pre-trained models, e.g., ResNet (He et al. 2016), BERT (Kenton and Toutanova 2019), they have different dimensions in different feature spaces. We require the projection of high-dimensional modal feature ${ \\bf e } _ { i } ^ { m }$ of each item into a unified embedding space $\\mathbb { R } ^ { d }$ as,\n\n$$\n\\widetilde { \\mathbf { e } } _ { i } ^ { m } = \\mathrm { T R A N S F O R M } ( \\mathbf { e } _ { i } ^ { m } ) = \\mathbf { e } _ { i } ^ { m } \\cdot \\mathbf { W } _ { m } ,\n$$\n\nwhere $\\widetilde { \\mathbf { e } } _ { i } ^ { m }$ is item $i$ ’s transformed modal feature, TRANS $\\mathsf { \\mathrm { 7 O R M } } ( \\cdot )$ is a projection function parameterized by a transformation matrix $\\mathbf { W } _ { m } \\ \\in \\mathbb { R } ^ { d _ { m } \\times \\hat { d } }$ . Due to the difficulty in obtaining user modal information, existing methods often reuse user ID embedding as input for modality-specific graphs, resulting in coupling of collaborative and modal signals. Different from them, we initialize the user modal features by aggregating item modal features,\n\n$$\n\\widetilde { \\mathbf { e } } _ { u } ^ { m } = \\frac { 1 } { | \\mathcal { N } _ { u } | } \\sum _ { i \\in \\mathcal { N } _ { u } } \\widetilde { \\mathbf { e } } _ { i } ^ { m } ,\n$$\n\nwhere $\\mathcal { N } _ { u }$ denotes the neighbor set of user $u \\in \\mathcal { U }$ on useritem interaction graph $\\mathcal { G }$ . This operation ensures the separate updates of $\\mathrm { I D }$ embedding and modal features. Thereafter, we can construct the modal feature matrix $\\begin{array} { l l } { \\widetilde { \\mathbf { E } } ^ { m } } & { = } \\end{array}$ $[ \\widetilde { \\mathbf { e } } _ { u _ { 1 } } ^ { m } , \\hdots , \\widetilde { \\mathbf { e } } _ { u _ { | M | } } ^ { m } , \\widetilde { \\mathbf { e } } _ { i _ { 1 } } ^ { m } , \\hdots , \\widetilde { \\mathbf { e } } _ { i _ { | T | } } ^ { m } ] \\in \\mathbb { R } ^ { ( | U | + | \\mathcal { Z } | ) \\times d }$ eas initial input $\\widetilde { \\mathbf { E } } ^ { m , 0 }$ to learn modality-related embeddings via implementing a light graph propagation function MGPROG $( \\cdot )$ ,\n\n$$\n\\widetilde { \\mathbf { E } } ^ { m , k + 1 } = \\mathbf { M } \\mathbf { G } \\mathbf { P } \\mathbf { R } \\mathbf { O } \\mathbf { G } ( \\widetilde { \\mathbf { E } } ^ { m , k } ) = \\left( \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\mathbf { A } \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\right) \\widetilde { \\mathbf { E } } ^ { m , k } .\n$$\n\nHere, we choose high-order modal embeddings $\\widetilde { \\mathbf { E } } ^ { m , K }$ in the last $K$ -th layer as the modality-related embeddings (i.e., $\\mathbf { E } _ { l g e } ^ { m } = \\widetilde { \\mathbf { E } } ^ { m , K } )$ with local modal information.\n\n# Global Hypergraph Embedding (GHE) Module\n\nThe GHE module is designed to capture the modality-aware global representations of users and items against sparse and noisy user behaviors.\n\nHypergraph Dependency Constructing Explicit attribute information of item modalities is often unavailable, especially for visual modalities. Hence, we define learnable implicit attribute vectors $\\{ \\mathbf { v } _ { a } ^ { m } \\} _ { a = 1 } ^ { A }$ $( \\mathbf { v } _ { a } ^ { m } \\in \\mathbb { R } ^ { d _ { m } } )$ as hyperedge embeddings under modality $m$ to adaptively learn the dependencies between implicit attributes and items/users , where $A$ is the number of hyperedges. Specifically, We obtain hypergraph dependency matrices in low-dimensional embedding space by,\n\n$$\n\\mathbf H _ { i } ^ { m } = \\mathbf E _ { i } ^ { m } \\cdot \\mathbf V ^ { m \\top } , \\quad \\mathbf H _ { u } ^ { m } = \\mathbf A _ { u } \\cdot \\mathbf H _ { i } ^ { m \\top } ,\n$$\n\nwhere $\\mathbf { H } _ { i } ^ { m } ~ \\in ~ \\mathbb { R } ^ { | \\mathcal { T } | \\times A }$ and $\\mathbf { H } _ { u } ^ { m } ~ \\in ~ \\mathbb { R } ^ { | \\mathcal { U } | \\times A }$ are the itemhyperedge and user-hyperedge dependency matrices, respectively. $\\mathbf { E } _ { i } ^ { m }$ is the raw item modal feature matrix, ${ \\mathbf { V } } ^ { m } =$ $[ \\mathbf { v } _ { 1 } ^ { m } , \\ldots , \\mathbf { v } _ { A } ^ { m } ] \\ \\in \\ \\mathbb { R } ^ { A \\times d _ { m } }$ is the hyperedge vector matrix, and $\\mathbf { A } _ { u } \\in \\mathbb { R } ^ { | \\mathcal { U } | \\times | \\mathcal { T } | }$ is the user-related adjacency matrix extracted from A. Intuitively, items with similar modal features are more likely to be connected to the same hyperedge. The user-hyperedge dependencies are indirectly derived through the user-item interactions, which implies the user behavior intention, i.e., the more frequently users interact with items under a certain attribute, the more they may prefer the attribute.\n\nTo further avoid the negative impact of meaningless relationships, we employ the Gumbel-Softmax reparameterization (Jang, Gu, and Poole 2017) to ensure that an item is attached to only one hyperedge as much as possible,\n\n$$\n\\widetilde { \\mathbf { h } } _ { i , * } ^ { m } = \\mathrm { S O F T M A X } \\left( \\frac { \\log \\delta - \\log ( 1 - \\delta ) + \\mathbf { h } _ { i , * } ^ { m } } { \\tau } \\right) ,\n$$\n\nwhere $\\mathbf { h } _ { i , * } ^ { m } \\in \\mathbb { R } ^ { A }$ is the $i$ -th row vector of $\\mathbf { H } _ { i } ^ { m }$ that reflects the relations between item $i$ and all hyperedges. $\\delta \\in \\mathbb { R } ^ { A }$ is a noise vector, where each value $\\delta _ { j } \\sim \\mathrm { U n i f o r m } ( 0 , 1 )$ , and $\\tau$ is a temperature hyperparameter. Afterwards, we can get the augmented item-attribute hypergraph dependency matrix $\\widetilde { \\mathbf { H } } _ { i } ^ { m }$ . By performing similar operations on $\\mathbf { H } _ { u } ^ { m }$ , we can obtain the augmented user-attribute relation matrix $\\widetilde { \\mathbf { H } } _ { u } ^ { m }$ .\n\nHypergraph Message Passing By taking the attribute hyperedge as an intermediate hub, we achieve hypergraph message passing to deliver global information to users and items without being limited by hop distances. Formally,\n\n$$\n\\mathbf { E } _ { i } ^ { m , h + 1 } = \\operatorname { D R o P } ( \\widetilde { \\mathbf { H } } _ { i } ^ { m } ) \\cdot \\operatorname { D R o P } ( \\widetilde { \\mathbf { H } } _ { i } ^ { m \\top } ) \\cdot \\mathbf { E } _ { i } ^ { m , h } ,\n$$\n\nwhere $\\mathbf { E } _ { i } ^ { m , h }$ is the global embedding matrix of items in the $h$ -th hypergraph layer, andon. We take collaborative $\\operatorname { D R O P } ( { \\mathord { \\cdot } } )$ denotes ng matrix out func-of items $\\mathbf { E } _ { i , l g e } ^ { i d ^ { \\ast } }$\n\nas the initial global embedding matrix when $h = 0$ . Further, we can calculate the global user embedding matrix as,\n\n$$\n\\mathbf E _ { u } ^ { m , h + 1 } = \\operatorname { D R o P } ( \\widetilde { \\mathbf H } _ { u } ^ { m } ) \\cdot \\operatorname { D R o P } ( \\widetilde { \\mathbf H } _ { i } ^ { m \\top } ) \\cdot \\mathbf E _ { i } ^ { m , h } .\n$$\n\nApparently, the hypergraph passing explicitly enables global information transfer by taking the item collaborative embedding and modality-aware hypergraph dependencies as input. Then, we can obtain the global embeddings matrix ${ \\bf E } _ { g h e }$ by aggregating global embeddings from all modalities,\n\n$$\n{ \\bf E } _ { g h e } = \\sum _ { m \\in \\mathcal { M } } { \\bf E } ^ { m , H } , \\quad { \\bf E } ^ { m , H } = [ { \\bf E } _ { u } ^ { m , H } , { \\bf E } _ { i } ^ { m , H } ] ,\n$$\n\nwhere $\\mathbf { E } _ { u } ^ { m , H } \\in \\mathbb { R } ^ { | \\mathcal { U } | \\times d }$ and $\\mathbf { E } _ { i } ^ { m , H } \\in \\mathbb { R } ^ { | \\mathcal { T } | \\times d }$ are global embedding matrices of user $u$ and item $i$ obtained in the $H$ -th hypergraph layer under modality $m$ , respectively.\n\nTo further achieve the robust fusion of global embeddings among different modalities, we develop cross-modal hypergraph contrastive learning to distill the self-supervision signals for global interest consistency. Specifically, we take the global embeddings of users acquired in different modalities as positive pairs and different users as negative pairs, and then employ the InfoNCE (Gutmann and Hyvarinen 2010) ¨ to formally define user-side hypergraph contrastive loss as,\n\n$$\n\\mathcal { L } _ { \\mathrm { H C L } } ^ { u } = \\sum _ { u \\in \\mathcal { U } } - \\log \\frac { \\exp ( s ( \\mathbf { E } _ { u } ^ { v , H } , \\mathbf { E } _ { u } ^ { t , H } ) / \\tau ) } { \\sum _ { u ^ { \\prime } \\in \\mathcal { U } } \\exp ( s ( \\mathbf { E } _ { u } ^ { v , H } , \\mathbf { E } _ { u ^ { \\prime } } ^ { t , H } ) / \\tau ) } ,\n$$\n\nwhere $s ( \\cdot )$ is the cosine function, and $\\tau$ is the temperature factor, generally set to 0.2. Note here we only consider visual and textual modalities, i.e., $m \\in \\{ v , t \\}$ . Similarly, we can define item-side cross-modal contrastive loss $\\mathcal { L } _ { \\mathrm { H C L } } ^ { i }$ .\n\n# Fusion and Prediction\n\nWe acquire the final representations by fusing their two types of local emb $\\mathbf { E } ^ { \\ast }$ of uings emsand $\\mathbf { E } _ { l g e } ^ { i d } , \\mathbf { E } _ { l g e } ^ { m }$ global embeddings ,\n\n$$\n\\mathbf { E } ^ { * } = \\mathbf { E } _ { l g e } ^ { i d } + \\sum _ { m \\in \\mathcal { M } } \\mathrm { N o R M } \\bigl ( \\mathbf { E } _ { l g e } ^ { m } \\bigr ) + \\alpha \\cdot \\mathrm { N o R M } \\bigl ( \\mathbf { E } _ { g h e } \\bigr ) ,\n$$\n\nwhere $\\operatorname { N o R M } ( \\cdot )$ is a normalization function to alleviate the value scale difference among embeddings, $\\alpha$ is an adjustable factor to control the integration of global embeddings.\n\nWe then use inner product to calculate the preference score The B $\\hat { r } _ { u , i }$ of user sian pers $u$ towards item alized ranking $i$ , i.e., (BPR) $\\hat { r } _ { u , i } = \\mathbf { e } _ { u } ^ { * } \\cdot \\mathbf { e } _ { i } ^ { * \\top }$ · e∗i ⊤ .. 2012) is employed to optimize model parameters,\n\n$$\n\\mathcal { L } _ { \\mathrm { B P R } } = - \\sum _ { ( u , i ^ { + } , i ^ { - } ) \\in \\mathcal { R } } \\ln \\sigma \\left( \\hat { r } _ { u , i ^ { + } } - \\hat { r } _ { u , i ^ { - } } \\right) + \\lambda _ { 1 } \\| \\Theta \\| _ { 2 } ^ { 2 } ,\n$$\n\nwhere $\\mathcal { R } \\ = \\ \\{ ( u , i ^ { + } , i ^ { - } ) | ( u , i ^ { + } ) \\in \\ \\mathcal { G } , ( u , i ^ { - } ) \\ \\notin \\ \\mathcal { G } \\}$ is a set of triples for training, $\\sigma ( \\cdot )$ is the sigmoid function, and $\\lambda _ { 1 }$ and $\\Theta$ represent the regularization coefficient and model parameters, respectively.\n\nFinally, we integrate hypergraph contrastive loss with the BPR (Rendle et al. 2012) loss into a unified objective as,\n\n$$\n\\mathcal { L } = \\mathcal { L } _ { \\mathrm { B P R } } + \\lambda _ { 2 } \\cdot ( \\mathcal { L } _ { \\mathrm { H C L } } ^ { u } + \\mathcal { L } _ { \\mathrm { H C L } } ^ { i } )\n$$\n\nwhere $\\lambda _ { 2 }$ is a hyperparameter for loss term weighting. We minimize the joint objective $\\mathcal { L }$ by using Adam optimizer (Kingma and Ba 2014). The weight-decay regularization term is applied over model parameters $\\Theta$ .\n\nTable 1: Statistics of the three evaluation datasets   \n\n<table><tr><td>Dataset</td><td>#User</td><td>#Item</td><td>#Interaction</td><td>Sparsity</td></tr><tr><td>Baby</td><td>19,445</td><td>7,050</td><td>160,792</td><td>99.883%</td></tr><tr><td>Sports</td><td>35,598</td><td>18,357</td><td>296,337</td><td>99.955%</td></tr><tr><td>Clothing</td><td>39,387</td><td>23.033</td><td>278,677</td><td>99.969%</td></tr></table>",
  "experiments": "",
  "hyperparameter": "- L: Number of collaborative graph propagation layers. Typical values: {1,2,3,4}\n- K: Number of modality graph embedding layers. Typical values: {1,2,3,4}\n- H: Number of hypergraph embedding layers. Typical values: {1,2,3,4}\n- A: Number of hyperedges. Typical values: {1,2,4,8,16,32,64,128,256}\n- α: Adjustable factor for fusing global embeddings. Typical values: {0.1,0.2,...,1.0}\n- ρ: Dropout ratio for hypergraph dependencies. Typical values: {0.1,0.2,...,1.0}\n- λ2: Weight for hypergraph contrastive loss. Typical values: {1e-6,1e-5,...,0.1}"
}
