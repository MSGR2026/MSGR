{
  "id": "BM3_2023",
  "paper_title": "Bootstrap Latent Representations for Multi-modal Recommendation",
  "alias": "BM3",
  "year": 2023,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 3BOOTSTRAPPED MULTI-MODAL MODEL\n\nIn this section,we elaborate on our bootstrapped multi-modal model, which encompasses three components as illustrated in Fig.1: a) multi-modal latent space convertor, b) contrastive view generator, and c) multi-modal contrastive loss.\n\n# 3.1 Multi-modal Latent Space Convertor\n\nLet $\\mathbf { e } _ { u } , \\mathbf { e } _ { i } \\in \\mathbb { R } ^ { d }$ denote the input ID embeddings of the user $u \\in \\mathcal { U }$ and the item $i \\in \\mathcal { I }$ ,where $d$ is the embedding dimension,and $\\mathcal { U } , \\mathcal { I }$ are the sets of users and items,respectively.Their cardinal numbers are set as $| \\mathcal { U } |$ and $| { \\cal T } |$ ,respectively.We denote the modality-specific features obtained from the pre-trained model as $\\mathbf { e } _ { m } \\in \\mathbb { R } ^ { d _ { m } }$ ,where $m \\in { \\mathcal { M } }$ denotes a specific modality from the full set of modalities $\\mathcal { M }$ ,and $d _ { m }$ denotes the dimension of the features.The cardinal number of $\\mathcal { M }$ is denoted by $| { \\cal M } |$ .In this paper, we consider two modalities:vision u and text $t$ .However, themodelcanbe easily extended to scenarios with more than two modalities.As the multi-modal feature spaces are different from each other,we first convert the multi-modal features and ID embeddings into the same latent space.\n\n3.1.1Multi-modal Features.The features of an item obtained from differentmodalities are ofdifferent dimensionsandin different feature spaces.For a multi-modal feature vector $\\mathbf { e } _ { m }$ ,we first project it into a latent low dimension using a projection function $f _ { m }$ based on Multi-Layer Perceptron (MLP). Then, we have:\n\n$$\n\\mathbf { h } _ { m } = \\mathbf { e } _ { m } \\mathbf { W } _ { m } + \\mathbf { b } _ { m } ,\n$$\n\nwhere $\\mathbf { W } _ { m } \\in \\mathbb { R } ^ { d _ { m } \\times d } , \\mathbf { b } _ { m } \\in \\mathbb { R } ^ { d }$ denote the linear transformation matrix and bias in the MLP of $f _ { m }$ .In this way,each uni-modal latent representation $\\mathbf { h } _ { m }$ shares the same space with ID embeddings.\n\n3.1.2ID Embeddings.Previous work [39] has verified the crucial role of ID embeddings in multi-modal recommendation.Although the ID embeddings of users and items can be directly initialized within the latent space, they do not encode any structural information about the user-item interaction graph.Inspired by the recent success of applying GCN for recommendation, we use a backbone network of LightGCN[1o] with residual connection to encode the structure of the user-item interaction graph.\n\nSuppose $\\mathcal { G } = ( \\mathcal { V } , \\mathcal { E } )$ bea given graph with node set $\\mathcal { V } = \\mathcal { U } \\cup \\mathcal { I }$ and edge set $\\varepsilon$ .The number of nodes is denoted by $\\lvert \\mathcal { V } \\rvert$ ,and the number of edges is denoted by $| \\mathcal E |$ .Theadjacency matrixis denoted by $\\mathbf { A } \\in \\mathbb { R } ^ { | \\mathcal { V } | \\times | \\mathcal { V } | }$ ,and the diagonal degree matrix is denoted by D. In $\\mathcal { G }$ ,the edges describe observed user-item interactions.If a user has interactions with an item,we build an edge between the user node and the item node. Moreover, we use $\\mathbf { H } ^ { l } \\in \\mathbb { R } ^ { | \\mathcal { V } | \\times d }$ to denote the ID embeddings at the $l$ -th layer by stacking all the embeddings of users and items at layer $l$ Specifically, the initial ID embeddings $\\mathbf { H } ^ { 0 }$ is a collection of embeddings $\\mathbf { e } _ { u }$ and $\\mathbf { e } _ { i }$ from all users and items. A typical feed forward propagation GCN[14] to calculate the hidden ID embedding $\\mathbf { H } ^ { l + 1 }$ at layer $l + 1$ is recursively conducted as:\n\n$$\n\\mathbf { H } ^ { l + 1 } = \\sigma \\left( \\hat { \\mathbf { A } } \\mathbf { H } ^ { l } \\mathbf { W } ^ { l } \\right) ,\n$$\n\nwhere $\\sigma ( \\cdot )$ is a non-linear function,e.g., the ReLu function, $\\hat { \\bf A } =$ $\\hat { \\mathbf { D } } ^ { - 1 / 2 } ( \\mathbf { A } + \\mathbf { I } ) \\hat { \\mathbf { D } } ^ { - 1 / 2 }$ is the re-normalization of the adjacency matrix A,and $\\hat { \\bf D }$ is the diagonal degree matrix of $\\mathbf { A } + \\mathbf { I } .$ For node classification, the last layer of a GCNis used to predict the label of a node viaa softmaxclassifier.\n\nOn top of the vanilla GCN,LightGCN simplifes its structure by removing the feature transformation $\\mathbf { W } ^ { l }$ and non-linear activation $\\sigma ( \\cdot )$ layers for recommendation.As they found these two layers impose adverse effects on recommendation performance. The simplified graph convolutional layer in LightGCN is defined as:\n\n$$\n\\mathbf { H } ^ { l + 1 } = ( \\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - 1 / 2 } ) \\mathbf { H } ^ { l } ,\n$$\n\nwhere the node embeddings of the $( l + 1 )$ -th hidden layer are only linearly aggregated from the $l$ -th layer with a transition matrix $\\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - \\bar { 1 } / 2 }$ . The transition matrix is exactly the weighted adjacency matrix mentioned above.\n\nWe use a readout function to aggregate all representations in hidden layers for user and item final representations.However,\n\n![](images/a1956f287c63dd63acd25ad939cdc64ec7e5429b5dd770d423caeee81e19c8f6.jpg)  \nFigure 1: The structure overview of the proposed BM3.Projections $f _ { v }$ and $f _ { t }$ ,as well as predictor $f _ { p }$ ,are all one-layer MLPs.The parameters of predictor $f _ { p }$ are shared in the Contrastive View Generator (bottom left) for ID embeddings and multi-modal latent representations.\n\nGCNs may suffer from the over-smoothing problem [2,16,18]. Follwing LATTICE [39],we also add a residual connection [2,14] to the item initial embeddings ${ \\bf H } _ { i } ^ { 0 }$ to obtain the final representations of items.That is:\n\n$$\n\\begin{array} { l } { { \\displaystyle { \\bf H } _ { \\bf u } = \\mathrm { R E A D O U T } ( { \\bf H } _ { u } ^ { 0 } , { \\bf H } _ { u } ^ { 1 } , { \\bf H } _ { u } ^ { 2 } , . . . , { \\bf H } _ { u } ^ { L } ) } ; } \\\\ { ~ } \\\\ { { \\displaystyle { \\bf H } _ { \\bf i } = \\mathrm { R E A D O U T } ( { \\bf H } _ { i } ^ { 0 } , { \\bf H } _ { i } ^ { 1 } , { \\bf H } _ { i } ^ { 2 } , . . . , { \\bf H } _ { i } ^ { L } ) + { \\bf H } _ { i } ^ { 0 } } , } \\end{array}\n$$\n\nwhere the READouT function can be any differentiable function.We use the default mean function of LightGCN for its final ID embedding updating.\n\nWith the multi-modal latent space convertor, we can obtain three types of latent embeddings:user ID embeddings,item ID embeddings,and uni-modal item embeddings. In the following section, we illustrate the design of losses in BM3 for efficient parameter optimization without negative samples.\n\n# 3.2Multi-modal Contrastive Loss\n\nPrevious studies on SSL use the stop-gradient strategy to prevent the model from resulting in a trivial constant solution [5,7].Besides, they use online and target networks to make the model parameters learn in a teacher-student manner [29].BM3 simplifies the current SSL paradigm [5,7] by postponing the data augmentation after the encoding of the online network.We first illustrate the data augmentation in BM3.\n\n3.2.1Contrastive View Generator. Prior studies [15,30,36] use graph augmentations to generate two alternate views of the original graph for self-supervised learning.Input features are encoded through both graphs to generate the contrastive views.To reduce the computational complexity and the memory cost, BM3 removes the requirement of graph augmentations with a simple latent embedding dropout technique that is analogous to node dropout [28]. The contrastive latent embedding $\\dot { \\mathbf { h } }$ of h under a dropout ratio $\\mathcal { P }$ is\n\ncalculated as:\n\n$$\n\\dot { \\mathbf { h } } = \\mathbf { h } \\cdot \\mathrm { B e r n o u l l i } ( \\boldsymbol { p } ) .\n$$\n\nFollowing [5,7], we also place stop-gradient on the contrastive view $\\dot { \\mathbf { h } }$ .Whilst we feed the original embedding $\\mathbf { h }$ into a predictor of MLP.\n\n$$\n\\begin{array} { r } { \\tilde { \\mathbf { h } } = \\mathbf { h } \\mathbf { W } _ { p } + \\mathbf { b } _ { p } , } \\end{array}\n$$\n\nwhere $\\mathbf { W } _ { \\mathcal { P } } \\in \\mathbb { R } ^ { d \\times d } , \\mathbf { b } _ { \\mathcal { P } } \\in \\mathbb { R } ^ { d }$ denote the linear transformation matrix and bias in the predictor function $f _ { p }$\n\n3.2.2Graph Reconstruction Loss.BM3 takes a positive user-item pair $( u , i )$ as input.With the generated contrastive view $( \\dot { \\mathbf { h } } _ { u } , \\dot { \\mathbf { h } } _ { i } )$ of the online representations $( \\tilde { \\mathbf { h } } _ { u } , \\tilde { \\mathbf { h } } _ { i } )$ ,we define a symmetrized loss function as the negative cosine similarity between $( \\tilde { \\mathbf { h } } _ { u } , \\dot { \\mathbf { h } } _ { i } )$ and $( \\dot { \\mathbf { h } } _ { u } , \\tilde { \\mathbf { h } } _ { i } )$ ：\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { r e c } = C ( \\tilde { \\mathbf { h } } _ { u } , \\dot { \\mathbf { h } } _ { i } ) + C ( \\dot { \\mathbf { h } } _ { u } , \\tilde { \\mathbf { h } } _ { i } ) . } \\end{array}\n$$\n\nFunction $C ( \\cdot , \\cdot )$ in the above equation is defined as:\n\n$$\nC ( \\mathbf { h } _ { u } , \\mathbf { h } _ { i } ) = - \\frac { \\mathbf { h } _ { u } ^ { T } \\mathbf { h } _ { i } } { | | \\mathbf { h } _ { u } | | _ { 2 } | | \\mathbf { h } _ { i } | | _ { 2 } } ,\n$$\n\nwhere $| | \\cdot | | _ { 2 }$ is $\\ell _ { 2 }$ -norm.The total loss is averaged over alluser-item pairs.The intuition behind this is that we intend to maximize the prediction of the positively perturbed item i given a user $u$ ,and vice versa.The minimized possible value for this loss is $- 1$\n\nFinally,we stop gradient on the target network and force the backpropagation of loss over the online network only.We follow the stop gradient (sg) operator as in [5,7],and implement the operator by updating Eq. (7) as:\n\n$$\n\\mathcal { L } _ { r e c } = C ( \\tilde { \\mathbf { h } } _ { u } , s g ( \\dot { \\mathbf { h } } _ { i } ) ) + C ( s g ( \\dot { \\mathbf { h } } _ { u } ) , \\tilde { \\mathbf { h } } _ { i } ) .\n$$\n\nWith the stop gradient operator, the target network receives no gradient from $( \\dot { \\mathbf { h } } _ { u } , \\dot { \\mathbf { h } } _ { i } )$\n\nTable 1: Comparison of computational complexity on graph-based multi-modal methods.   \n\n<table><tr><td>Component</td><td>MMGCN</td><td>GRCN</td><td>DualGNN</td><td>LATTICE</td><td>BM3</td></tr><tr><td>Graph Convolution</td><td>0(|M|X)</td><td>0((|M| +1)X)</td><td>0(M|X + kd|ul)</td><td>0(x） O(πβ+∑I丨²dm</td><td>0(X)</td></tr><tr><td>Feature Transform</td><td>o(∑ II(dm+d)dh) m∈M</td><td>(∑ IIIdmd)</td><td>0(∑ I|(dm+d)dh)</td><td>mEM +k|I|log(II))</td><td>(∑IIdmd)</td></tr><tr><td>BPR/CL Losses</td><td>0(2dB)</td><td>m∈M 0((2 +|M)dB)</td><td>mEM 0((2 + |M|)dB)</td><td>0(2dB)</td><td>m∈M 0((2 +2|MI)dB)</td></tr></table>\n\nTo fit the page, we set $\\overline { { X = 2 L | \\mathcal { E } | d / B } }$ and $\\overline { { d _ { h } } }$ denotes the dimension of the hidden layer in a two-layer MLP.\n\n3.2.3Inter-modality Feature Alignment Loss.In addition,we furtheralign the multi-modal features of items with their target ID embeddings.The alignment encourages the $\\mathrm { I D }$ embeddings close to each other on items with similar multi-modal features.For each uni-modal latent embedding $\\mathbf { h } _ { m }$ of an item $i$ ,the contrastive view generator outputs its contrastive pair as $( \\tilde { \\mathbf { h } } _ { m } ^ { i } , \\dot { \\mathbf { h } } _ { m } ^ { i } )$ . We use the negative cosine similarity to perform the alignment between $\\dot { \\mathbf { h } } _ { i }$ and $\\tilde { \\mathbf { h } } _ { m } ^ { i }$\n\n$$\n\\mathcal { L } _ { a l i g n } = C ( \\tilde { \\mathbf { h } } _ { m } ^ { i } , \\dot { \\mathbf { h } } _ { i } ) .\n$$\n\n3.2.4Intra-modality Feature Masked Loss.Finally, BM3 uses intramodlity feature masked loss to further encourage the learning of predictor with sparse representations of latent embeddings. Sparse is verified scale efficient in large transformers [11,25].We randomly mask out a subset of the latent embedding $\\mathbf { h } _ { m }$ by dropout with the contrastive view generator and denote the sparse embedding as $\\dot { \\mathbf { h } } _ { m } ^ { i }$ The intra-modality feature masked loss is defined as:\n\n$$\n\\mathcal { L } _ { m a s k } = C ( \\tilde { \\mathbf { h } } _ { m } ^ { i } , \\dot { \\mathbf { h } } _ { m } ^ { i } ) .\n$$\n\nAdditionally,we add regularization penalty on the online embeddings (i.e., $\\mathbf { h } _ { u }$ and $\\mathbf { h } _ { i }$ ). Our final loss function is:\n\n$$\n\\mathcal { L } = \\mathcal { L } _ { r e c } + \\mathcal { L } _ { a l i g n } + \\mathcal { L } _ { m a s k } + \\lambda \\cdot ( | | \\mathbf { h } _ { u } | | _ { 2 } ^ { 2 } + | | \\mathbf { h } _ { i } | | _ { 2 } ^ { 2 } ) .\n$$\n\n# 3.3 Top $K$ Recommendation\n\nTo generate item recommendations for a user, we first predict the interaction scores between the user and candidate items.Then, we rank candidate items based on the predicted interaction scores in descending order,and choose $K$ top-ranked items as recommendations to the user. Classical CF methods recommend top- $K$ items by ranking scores of the inner product of a user embedding with all candidate item embeddings.As our MMCL can learn a good predictor on user and item latent embeddings,we use the embeddings transformed by the predictor $f _ { p }$ for inner product. That is:\n\n$$\ns ( \\mathbf { h } _ { u } , \\mathbf { h } _ { i } ) = \\tilde { \\mathbf { h } } _ { u } \\cdot \\tilde { \\mathbf { h } } _ { i } ^ { T } .\n$$\n\nA high score suggests that the user prefers the item.\n\n# 3.4 Computational Complexity\n\nThe computational cost of BM3 mainly occurs in linear propagation of the normalized adjacency matrix $\\hat { \\bf A }$ The analytical complexity of LightGCN and BM3 are in the same magnitude with $O ( 2 L | \\mathcal { E } | d / B )$ on graph convolutional operation, where $L$ is the number of LightGCN layers,and $B$ is the training batch size.However, BM3 has additional costs on multi-modal feature projection and prediction. The projection cost is $O ( \\sum _ { m \\in \\mathcal { M } } | \\mathcal { I } | d _ { m } d )$ on all modalities. The contrastive loss cost is $O ( ( 2 + 2 | M | ) d B )$ .The total computational cost for BM3 is $\\begin{array} { r } { O ( 2 L | \\mathcal { E } | d / B + \\sum _ { m \\in \\mathcal { M } } | \\mathcal { I } | d _ { m } d + ( 2 + 2 | \\mathcal { M } | ) d B ) } \\end{array}$ . We summarize the computational complexity of the graph-based multi-modal methods in Table 1.Both MMGCN and DualGNN use a two-layer MLP for multi-modal feature projection. On the contrary, LATTICE constructs an item-item graph from the multi-modal features.It costs $O ( | \\boldsymbol { \\cal { I } } | ^ { 2 } d _ { m } )$ to build the similarity matrix between items, $O ( | \\boldsymbol { \\cal { I } } | ^ { 3 } )$ to normalize the matrix,and $O ( k | \\tau | \\log ( | \\tau | ) )$ to retrieve top $k$ most similar items for each item.\n\nTable 2: Statistics of the experimental datasets.   \n\n<table><tr><td>Datasets</td><td>#Users</td><td>#Items</td><td>#Interactions</td><td>Sparsity</td></tr><tr><td>Baby</td><td>19,445</td><td>7,050</td><td>160,792</td><td>99.88%</td></tr><tr><td>Sports</td><td>35,598</td><td>18,357</td><td>296,337</td><td>99.95%</td></tr><tr><td>Electronics</td><td>192,403</td><td>63,001</td><td>1,689,188</td><td>99.99%</td></tr></table>",
  "experiments": "",
  "hyperparameter": "- `τ_ssl`: The temperature parameter for contrastive learning, controlling the scale of similarity in the contrastive loss. Typical values: {0.1, 0.2, 0.5, 1.0}.\n- `α`: Balances the contribution of the self-supervised learning task with the main recommendation task. Typical values: {0.01, 0.05, 0.1, 0.5, 1.0}.\n- `λ`: Regularization coefficient for L2 regularization. Typical values: {0.0001, 0.001, 0.01, 0.1}.\n- `P`: The dropout ratio for generating contrastive views of embeddings. Typical values: {0.3, 0.5}.\n- `L`: The number of layers in the graph convolution network (LightGCN backbone). Typical values: {1, 2}.\n- `d`: The dimensionality of latent embeddings. Fixed value: 64.\n- `K`: The number of negative samples per user-item pair, used for contrastive learning in some settings. Typical values: {5, 10, 20}."
}
