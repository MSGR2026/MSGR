{
  "id": "MMGCN_2019",
  "paper_title": "MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video",
  "alias": "MMGCN",
  "year": 2019,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video\n\nYinwei Wei Shandong University weiyinwei@hotmail.com\n\nXiang Wang§ National University of Singapore xiangwang@u.nus.edu\n\nLiqiang Nie§ Shandong University nieliqiang@gmail.com\n\nXiangnan He University of Science and Technology of China xiangnanhe@gmail.com\n\nRichang Hong Hefei University of Technology hongrc@hfut.edu.cn\n\nTat-Seng Chua National University of Singapore chuats@comp.nus.edu.sg\n\n# ABSTRACT\n\nPersonalized recommendation plays a central role in many online content sharing platforms. To provide quality micro-video recommendation service, it is of crucial importance to consider the interactions between users and items (i.e., micro-videos) as well as the item contents from various modalities (e.g., visual, acoustic, and textual). Existing works on multimedia recommendation largely exploit multi-modal contents to enrich item representations, while less effort is made to leverage information interchange between users and items to enhance user representations and further capture user’s fine-grained preferences on different modalities.\n\nIn this paper, we propose to exploit user-item interactions to guide the representation learning in each modality, and further personalized micro-video recommendation. We design a Multimodal Graph Convolution Network (MMGCN) framework built upon the message-passing idea of graph neural networks, which can yield modal-specific representations of users and micro-videos to better capture user preferences. Specifically, we construct a useritem bipartite graph in each modality, and enrich the representation of each node with the topological structure and features of its neighbors. Through extensive experiments on three publicly available datasets, Tiktok, Kwai, and MovieLens, we demonstrate that our proposed model is able to significantly outperform stateof-the-art multi-modal recommendation methods.\n\n# CCS CONCEPTS\n\n• Information systems $\\longrightarrow$ Recommender systems; Personalization; Multimedia and multimodal retrieval; $\\bullet$ Computing methodologies $\\longrightarrow$ Neural networks.\n\n# KEYWORDS\n\nGraph Convolution Network, Multi-modal Recommendation, Micro-video Understanding\n\n# ACM Reference Format:\n\nYinwei Wei, Xiang Wang§, Liqiang $\\mathrm { N i e ^ { \\ S } }$ , Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video. In Processdings of the 27th ACM Int’l Conf. on Multimedia (MM‘19), Oct. 21–25, 2019, Nice, France. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3343031. 3351034\n\n# 1 INTRODUCTION\n\nPersonalized recommendation has become a core component in many online content sharing services, spanning from image, blog to music recommendation. Recent success of micro-video sharing platforms, such as Tiktok and Kwai, bring increasing attentions to micro-video recommendation. Distinct from these item contents (e.g., image, music) that are solely from a single modality, microvideos contain rich multimedia information — frames, sound tracks, and descriptions — that involve multiple modalities of visual, acoustic, and textual ones [24, 25, 28].\n\nIncorporating such multi-modal information into historical interactions between users and micro-videos help establish an indepth understanding of user preferences:\n\n• There is a semantic gap between different modalities. Take Figure 1 as an example, while having visually similar frames, micro-videos $i _ { 1 }$ and $i _ { 2 }$ have dissimilar textural representations i idue to different topic words. In such cases, ignoring such modality difference would mislead the modeling of item representations. • A user may have different tastes on modalities of a micro-video. For example, a user is attracted by the frames, but may turn out to be disappointed with its poor sound tracks. Multiple modalities, hence, have varying contributions to user preferences. • Different modalities serve as different channels to explore user interests. In Figure 1, if user $u _ { 1 }$ cares more about frames, $i _ { 2 }$ is umore suitable to be recommended; whereas, $u _ { 1 }$ might click $i _ { 3 }$ idue to interest in textural descriptions.\n\nTherefore, it is of crucial importance to distinguish and consider modal-specific user preferences.\n\nHowever, existing works on multimedia recommendation [8, 17] mainly treat multi-modal information as a whole and incorporate them into a collaborative filtering (CF) framework, while lacking the modeling of modal-specific user preferences. Specifically, multimodal features of each item are unified as a single representation, reflecting their content similarity; thereafter, such representations are incorporated with user and item representations derived from CF framework, such as MF [30]. For instance, VBPR [17] leverages visual features to enrich ID embeddings of items; ACF [8] employs the attention mechanism on a user’s history to encode two-level personal tastes on historical items and item contents into user representations. Such signals can be summarized as the paths connecting the target user and item based on historical interactions [38, 41]. For example, given two paths $p _ { 1 } : = u _ { 1 } $ $i _ { 1 }  u _ { 2 }  i _ { 2 }$ and $p _ { 2 } : = u _ { 1 }  i _ { 1 }  u _ { 2 }  i _ { 3 }$ ; this would suggest ithat $i _ { 2 }$ and $i _ { 3 }$ p u i uare likely to be of interest to $u _ { 1 }$ . However, we argue i i uthat these signals are not sufficient to draw such conclusion. The key reason is that they ignore the differences and user preferences among modalities.\n\n![](images/8b5ed431dc200a3444b499bef263aa038d237085306068d1972a2fbb728f9697.jpg)  \nFigure 1: An illustration of modal-specific user preferences.\n\nTo address the limitations, we focus on information interchange between users and items in multiple modalities. Inspired by the recent success of graph convolution networks (GCNs) [14, 22], we use the information-propagation mechanism to encode high-order connectivity between users and micro-videos in each modality, so as to capture user preference on modal-specific contents. Towards this end, we propose a Multi-modal Graph Convolution Network (MMGCN). Specifically, we construct a user-item bipartite graph on each modality. Intuitively, the historical behaviors of users reflect personal interests; meanwhile, the user groups can also profile items [38, 41]. Hence, in each modality (e.g., visual), we aggregate signals from the corresponding contents (e.g., frames) of interacted items and incorporate them into user representations; meanwhile, we boost the representation of an item with its user group. By performing such aggregation and combination operators recursively, we can enforce the user and item representations to capture the signals from multi-hop neighbors, such that a user’s modal-specific preference is represented well in his/her representation. Ultimately, the prediction of an unseen interaction can be calculated as similarities between the user and micro-video representations. We validate our framework over three publicly accessible datasets — Tiktok, Kwai, and Movielens. Experimental results show that our model can yield promising performance. Furthermore, we visualize user preference on different modalities, which clearly shows the differences in modal-specific preferences by different users.\n\nThe main contributions of this work are threefold:\n\n• We explore how information interchange on various modalities reflects user preferences and affects recommendation performance.\n\n• We develop a new method MMGCN, which employs information propagation on the modality-aware bipartite user-item graph, to obtain better user representations based on item content information.   \n• We perform extensive experiments on three public datasets to demonstrate that our proposed model outperforms several stateof-the-art recommendation methods. In addition, we released our codes, parameters, and the baselines to facilitate further researchers by others1.\n\n# 2 MODEL FRAMEWORK\n\nIn this section, we elaborate our framework. As illustrated in Figure 2, our framework consists of three components — aggregation layers, combination layers, and prediction layer. By stacking multiple aggregation and combination layers, we encode the information interchange of users and items into the representation learning in each modality. Lastly, we fuse multimodal representations to predict the interaction between each user and each micro-video in the prediction layer. In what follows, we detail each component.\n\n# 2.1 Modality-aware User-Item Graphs\n\nInstead of unifying multi-modal information, we treat each modality individually. Particularly, we have historical interactions (e.g., view, browse, or click) between users and micro-videos. Here we represent the interaction data as a bipartite user-item graph $\\mathcal { G } = \\{ ( u , i ) | u \\in \\mathcal { U } , i \\in \\mathcal { I } \\}$ , where $\\boldsymbol { \\mathcal U }$ and $\\boldsymbol { \\mathcal { T } }$ separately denote the u, i u , iuser and micro-video sets. An edge $y _ { u i } = 1$ indicates an observed interaction between user $u$ yand micro-video $i$ ; otherwise $y _ { u i } = 0$ .\n\nBeyond the interactions, we have multiple modalities for each micro-video — visual, acoustic, and textual features. For simplicity, we use $m \\in \\mathcal { M } = \\{ v , a , t \\}$ as the modality indicator, where , , and $t$ m v, a, t v arepresent the visual, acoustic, and textual modalities, respectively. tTo accurately capture the users’ preferences on a particular modality $m$ , we split the bipartite graph $\\mathcal { G } _ { m }$ from $\\mathcal { G }$ by keeping only the mfeatures for modality $m$ .\n\n# 2.2 Aggregation Layer\n\nIntuitively, we can utilize the interaction data to enrich the representations of users and items. To be more specific, historical interactions of a user can describe user’s interest and capture the behavior similarity with other users. Meanwhile, the user group of a micro-video can provide complementary data to its multi-modal contents. We hence incorporate the information interchange into the representation learning.\n\nInspired by the message-passing mechanism of GCN, for a user (or micro-video) node in the bipartite graph $\\mathcal { G } _ { m }$ , we employ an aggregation function $f ( \\cdot )$ to quantify the influence (i.e., the representation being propagated) from its neighbors and output a representation as follows:\n\n$$\n\\mathbf { h } _ { m } = f ( N _ { u } ) ,\n$$\n\nwhere $N _ { u } = \\{ j | ( u , j ) \\in G _ { m } \\}$ denotes the neighbors of user $u$ , i.e., j u, jinteracted micro-videos. We implement $f ( \\cdot )$ via:\n\n![](images/e95c3b78d8babbf30079ba2e7c0f4a8dac87eb67d27c6088151dd4608ae5e4cf.jpg)  \nFigure 2: Schematic illustration of our proposed MMGCN model. It constructs the user-microvideo bipartite graph for each modality to capture the modal-specific user preference for the personalized recommendation of micro-video.\n\n• Mean Aggregation employs the average pooling operation on the modal-specific features, and applies a nonlinear transformation, as follows:\n\n$$\nf _ { \\mathrm { a v g } } ( N _ { u } ) = \\mathrm { L e a k y R e L U } \\Big ( \\frac { 1 } { | N _ { u } | } \\sum _ { j \\in N _ { u } } { \\mathbf { W } } _ { 1 , m } { \\mathbf { j } } _ { m } \\Big ) ,\n$$\n\nwhere $\\mathbf { j } _ { m } \\in \\mathbb { R } ^ { d _ { m } }$ is the $d _ { m }$ -dimension representation of microvideo $j$ in modality $m$ d; $\\mathbf { W } _ { 1 , m } ~ \\in ~ \\mathbb { R } ^ { d _ { m } ^ { \\prime } \\times d _ { m } }$ is the trainable j m ,transformation matrix to distill useful knowledge, where $d _ { m } ^ { \\prime }$ dis the transformation size; and we select LeakyReLU(·) as the nonlinear activation function [38, 41]. Such aggregation method assumes that different neighbors would have the same contributions to the representation of user $u$ , namely, user $u$ is influenced equally by his/her neighbors.\n\n• Max Aggregation leverages the max pooling operation to perform dimension-aware feature selection, as follows:\n\n$$\nf _ { \\operatorname* { m a x } } ( N _ { u } ) = \\mathrm { L e a k y R e L U } \\Bigl ( \\operatorname* { m a x } _ { j \\in { N _ { u } } } { \\bf W } _ { 1 , m } { \\bf j } _ { m } \\Bigr ) ,\n$$\n\nwhere each dimension of $\\mathbf { h } _ { m }$ is set as the max-num of the corresponding neighbor values. As such, different neighbors have varying contributions to the output representations.\n\nHence, the aggregation layer is capable of encoding the structural information and distribution of neighbors into the representation of the ego user; analogously, we can update the representations for item nodes.\n\n# 2.3 Combination Layer\n\nWhile containing the information being propagated from the neighbors, such representations forgo user ’s own feature and uthe interaction among different modalities. However, existing GNN efforts (e.g., GCN [22], GraphSage [14], GAT [33]) only consider homogeneous features from one data source. Hence, directly applying their combination operations fails to capture the interactions between different modalities.\n\nIn this section, we present a new combination layer, which integrates the structural information $\\mathbf { h } _ { m }$ , the intrinsic information $\\mathbf { u } _ { m }$ , and the modality connection $\\mathbf { u } _ { i d }$ into a unified representation, which is formulated as:\n\n$$\n{ \\bf u } _ { m } ^ { ( 1 ) } = g ( { \\bf h } _ { m } , { \\bf u } _ { m } , { \\bf u } _ { i d } ) ,\n$$\n\nwhere $\\mathbf { u } _ { m } \\in \\mathbb { R } ^ { d _ { m } }$ is the representation of user $u$ in modality $m$ and ${ \\bf u } _ { i d } \\in \\mathbb { R } ^ { d }$ is the $d$ u m-dimension embedding of user ID, remaining dinvariant and serves as the connection across modalities.\n\nInspired by prior work [3] on multi-modal representation, we first apply the idea of coordinated fashion, namely, separately projecting $\\mathbf { u } _ { m } , \\forall m \\in \\mathcal { M }$ into the latent space that is the same as $\\mathbf { u } _ { i d }$ :\n\n$$\n\\hat { \\mathbf { u } } _ { m } = \\mathrm { L e a k y R e L U } ( \\mathbf { W } _ { 2 , m } \\mathbf { u } _ { m } ) + \\mathbf { u } _ { i d } ,\n$$\n\nwhere $\\mathbf { W } _ { 2 , m } \\in \\mathbb { R } ^ { d \\times d _ { m } }$ is the trainable weight matrix to transfer $\\mathbf { u } _ { m }$ ,into the ID embedding space. As such, the representations from different modalities are comparable in the same hyperplane. Meanwhile, the ID embedding $\\mathbf { u } _ { i d }$ essentially bridges the gap between modal-specific representations, and propagates information across modalities during the gradient back-propagation process. In this work, we implement the combination function $g ( \\cdot )$ via the following two methods:\n\n• Concatenation Combination which concatenates the two representations, using a nonlinear transformation:\n\n$$\n\\begin{array} { r } { g _ { \\mathrm { c o } } ( \\mathbf { h } _ { m } , \\mathbf { u } _ { m } , \\mathbf { u } _ { i d } ) = \\mathrm { L e a k y R e L U } \\Big ( \\mathbf { W } _ { 3 , m } ( \\mathbf { h } _ { m } | | \\hat { \\mathbf { u } } _ { m } ) \\Big ) , } \\end{array}\n$$\n\nwhere $| |$ is the concatenation operation, and ${ \\bf W } _ { 3 , m }$ ∈ $\\mathbb { R } ^ { d _ { m } ^ { \\prime } \\times ( d _ { m } ^ { \\prime } + d ) }$ is the trainable model parameters.\n\n• Element-wise Combination that considers the element-wise feature interaction between two representations:\n\n$$\ng _ { \\mathrm { e l e } } ( \\mathbf { h } _ { m } , \\mathbf { u } _ { m } , \\mathbf { u } _ { i d } ) = \\mathrm { L e a k y R e L U } \\Big ( \\mathbf { W } _ { 3 , m } \\mathbf { h } _ { m } + \\hat { \\mathbf { u } } _ { m } \\Big ) ,\n$$\n\nwhere $\\mathbf { W } _ { 3 , m } \\in \\mathbb { R } ^ { d \\times d _ { m } ^ { \\prime } }$ denotes a weight matrix to transfer the ,current representations into the common space. In the elementwise combination, the interactions between two representations\n\nare taken into consideration, while two representations are assumed to be independent in Concatenation Combination.\n\n# 2.4 Model Prediction\n\nBy stacking more aggregation and combination layers, we explore the higher-order connectivity inherent in the user-item graphs. As such, we can gather the information propagated from the $l \\cdot$ hop neighbors in modality $m$ l, mimicking the exploration process mof users. Formally, the representation from $l$ -hop neighbors of user $u$ and the output of $l$ l-th multi-modal combination layer are recursively formulated as:\n\n$$\n\\mathbf { h } _ { m } ^ { ( l ) } = f ( N _ { u } ) \\quad \\mathrm { a n d } \\quad \\mathbf { u } _ { m } ^ { ( l ) } = g ( \\mathbf { h } _ { m } ^ { ( l ) } , \\mathbf { u } _ { m } ^ { ( l - 1 ) } , \\mathbf { u } _ { i d } ) ,\n$$\n\nwhere u lm $\\mathbf { u } _ { m } ^ { ( l - 1 ) }$ is the representation generated from the previous layer, memorizing the information from its $( l - 1 )$ -hop neighbors. $\\mathbf { u } _ { m } ^ { ( 0 ) }$ is set as $\\mathbf { u } _ { m }$ lat the initial iteration. Wherein, user $u$ is associated with trainable vectors $\\mathbf { u } _ { m } , \\forall m \\in \\mathcal { M }$ , which are randomly initialized; whereas, item $i$ is associated with the pre-extracted features $\\mathbf { i } _ { m } , \\forall m \\in M$ i. As a result, $\\mathbf { u } _ { m } ^ { ( l - 1 ) }$ characterizes the user preferences , mon item features in modality $m$ , and considers the influence of mmodality interactions that reflect the underlying relationships between modalities.\n\nAfter stacking $L$ single-modal aggregation and multi-modal Lcombination layers, we obtain the final representations for user $u$ and micro-video  via the linear combination of multi-modal representations, as:\n\n$$\n\\mathbf { u } ^ { * } = \\sum _ { m \\in \\mathcal { M } } \\mathbf { u } _ { m } ^ { ( L ) } \\quad \\mathrm { a n d } \\quad \\mathbf { i } ^ { * } = \\sum _ { m \\in \\mathcal { M } } \\mathbf { i } _ { m } ^ { ( L ) }\n$$\n\n# 2.5 Optimization\n\nTo predict the interaction between the users and micro-videos, we fuse their modal-specific representations and apply Bayesian Personalized Ranking (BPR) [30], which is a well-known pairwise ranking optimization framework, as the learning model. In particular, we model a triplet of one user and two micro-videos, in which one of the micro-videos is observed and the other one is not, formally as,\n\n$$\n\\mathcal { R } = \\{ ( u , i , i ^ { \\prime } ) | ( u , i ) \\in \\mathcal { G } , ( u , i ^ { \\prime } ) \\not \\in \\mathcal { G } \\} ,\n$$\n\nwhere $N ( u )$ consists of all micro-videos associated with $u$ , and $\\mathcal { R }$ is a set of triples for training. Further, it is assumed that the user prefers the observed micro-video rather than the unobserved one. Hence, the objective function can be formulated as,\n\n$$\n\\mathcal { L } = \\sum _ { ( u , i , i ^ { \\prime } ) \\in \\mathcal { R } } - \\ln \\mu ( \\boldsymbol { \\mathsf { u } } ^ { * } ^ { \\top } \\boldsymbol { \\mathsf { i } } ^ { * } - \\boldsymbol { \\mathsf { u } } ^ { * } ^ { \\top } \\boldsymbol { \\mathsf { i } } ^ { * } ) + \\lambda \\left\\| \\boldsymbol { \\Theta } \\right\\| _ { 2 } ^ { 2 } ,\n$$\n\nwhere $\\mu ( \\cdot )$ is the sigmoid function; $\\lambda$ and $\\Theta$ represent the µ λ Θregularization weight and the parameters of the model, respectively.",
  "experiments": "",
  "hyperparameter": "- `embedding_dim` (latent feature dimension): Controls the size of user ID embeddings and modality-specific latent vectors; typically searched in {32, 64, 128}, with 64 often used for reporting.\n- `num_layers` (GCN_depth): Number of stacked aggregation+combination layers per modality, which determines how many hops of neighborhood information are propagated; usually chosen from {1, 2, 3}, with 2 layers giving the best trade-off between expressiveness and over-smoothing.\n- `batch_size`: Number of training triplets per mini-batch when optimizing the BPR loss; searched in {128, 256, 512}.\n- `learning_rate`: Step size for SGD optimization of all network parameters; tuned over {0.0001, 0.0005, 0.001, 0.005, 0.01}.\n- `lambda_reg` (L2_regularizer): Weight of L2 regularization on model parameters in the BPR objective, controlling overfitting; searched in {0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1}.\n- `aggregation_type`: Choice of neighbor aggregation function in each modality-specific GCN (e.g., `mean` vs `max` pooling), affecting how neighbor features are summarized.\n- `combination_type`: Choice of combination layer to fuse structural embedding, modality features, and ID embedding (`concat` + MLP vs element-wise linear combination), which changes how cross-modal interactions are modeled."
}
