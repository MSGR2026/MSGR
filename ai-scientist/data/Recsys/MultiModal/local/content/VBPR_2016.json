{
  "id": "VBPR_2016",
  "paper_title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback",
  "alias": "VBPR",
  "year": 2016,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# Evaluation Methodology\n\nWe split our data into training/validation/test sets by selecting for each user $u$ a random item to be used for validation $\\nu _ { u }$ and another for testing $\\mathcal { T } _ { u }$ . All remaining data is used for training. The predicted ranking is evaluated on $\\mathcal { T } _ { u }$ with the widely used metric AUC (Area Under the ROC curve):\n\n$$\nA U C = \\frac { 1 } { | \\mathcal { U } | } \\sum _ { u } \\frac { 1 } { | E ( u ) | } \\sum _ { ( i , j ) \\in E ( u ) } \\delta ( \\widehat { x } _ { u , i } > \\widehat { x } _ { u , j } )\n$$\n\nwhere the set of evaluation pairs for user $u$ is defined as\n\n$$\nE ( u ) = \\{ ( i , j ) | ( u , i ) \\in \\mathcal { T } _ { u } \\land ( u , j ) \\notin ( \\mathcal { P } _ { u } \\cup \\mathcal { V } _ { u } \\cup \\mathcal { T } _ { u } ) \\} ,\n$$\n\nand $\\delta ( b )$ is an indicator function that returns 1 iff $b$ is true . In all cases we report the performance on the test set $\\tau$ for the hyperparameters that led to the best performance on the validation set $\\nu$ .\n\n# Baselines\n\nMatrix Factorization (MF) methods are known to have stateof-the-art performance for implicit feedback datasets. Since there are no comparable visual-aware MF methods, we mainly compare against state-of-the-art MF models, in addition to a recently proposed content-based method.\n\nTable 3: AUC on the test set $\\tau$ (#factors $= 2 0$ ). The best performing method on each dataset is boldfaced.   \n\n<table><tr><td>Dataset</td><td> Setting</td><td>(a) RAND</td><td>(b) MP</td><td>（c） IBR</td><td>(d) MM-MF</td><td>(e) BPR-MF</td><td>(f) VBPR</td><td colspan=\"2\">improvement f vs.best fvs.e</td></tr><tr><td> Amazon Women</td><td>All Items Cold Start</td><td>0.4997 0.5031</td><td>0.5772 0.3159</td><td>0.7163 0.6673</td><td>0.7127 0.5489</td><td>0.7020 0.5281</td><td>0.7834 0.6813</td><td>9.4% 2.1%</td><td>11.6% 29.0%</td></tr><tr><td>Amazon Men</td><td>All Items Cold Start</td><td>0.4992 0.4986</td><td>0.5726 0.3214</td><td>0.7185 0.6787</td><td>0.7179 0.5666</td><td>0.7100 0.5512</td><td>0.7841 0.6898</td><td>9.1% 1.6%</td><td>10.4% 25.1%</td></tr><tr><td>Amazon Phones</td><td>All Items Cold Start</td><td>0.5063 0.5014</td><td>0.7163 0.3393</td><td>0.7397 0.6319</td><td>0.7956 0.5570</td><td>0.7918 0.5346</td><td>0.8052 0.6056</td><td>1.2% -4.2%</td><td>1.7% 13.3%</td></tr><tr><td>Tradesy.com</td><td>All Items Cold Start</td><td>0.5003 0.4972</td><td>0.5085 0.3721</td><td>N/A N/A</td><td>0.6097 0.5172</td><td>0.6198 0.5241</td><td>0.7829 0.7594</td><td>26.3% 44.9%</td><td>26.3% 44.9%</td></tr></table>\n\n• Random (RAND): This baseline ranks items randomly for all users.\n\n• Most Popular (MP): This baseline ranks items according to their popularity and is non-personalized.\n\n• MM-MF: A pairwise MF model from Gantner et al. (2011), which is optimized for a hinge ranking loss on $x _ { u i j }$ and trained using SGA as in BPR-MF.\n\n• BPR-MF: This pairwise method was introduced by Rendle et al. (2009) and is the state-of-the-art of personalized ranking for implicit feedback datasets.\n\nWe also include a ‘content-based’ baseline for comparison against another method which makes use of visual data, though which differs in terms of problem setting and data (it does not make use of feedback but rather graphs encoding relationships between items as input):\n\n• Image-based Recommendation (IBR): Introduced by McAuley et al. (2015), it learns a visual space and retrieves stylistically similar items to a query image. Prediction is then performed by nearest-neighbor search in the learned visual space.\n\nThough significantly different from the pairwise methods considered above, for comparison we also compared against a point-wise method, WRMF (Hu, Koren, and Volinsky, 2008).\n\nMost baselines are from MyMediaLite (Gantner et al., 2011). For fair comparison, we use the same total number of dimensions for all MF based methods. In our model, visual and non-visual dimensions are fixed to a fifty-fifty split for simplicity, though further tuning may yield better performance. All experiments were performed on a standard desktop machine with 4 physical cores and 32GB main memory. Reproducibility. All hyperparameters are tuned to perform the best on the validation set. On Amazon, regularization hyperparamter $\\lambda _ { \\Theta } = 1 0$ works the best for BPR-MF, MM-MF and VBPR in most cases. While on Tradesy.com, $\\lambda _ { \\Theta } = 0 . 1$ is set for BPR-MF and VBPR and $\\lambda _ { \\Theta } = 1$ for MM-MF. $\\lambda _ { \\mathbf { E } }$ is always set to 0 for VBPR. For IBR, the rank of the Mahalanobis transform is set to 100, which is reported to perform very well on Amazon data. All of our code and datasets shall be made available at publication time so that our experimental evaluation is completely reproducible.\n\n![](images/becdddfd3680cf25bfc3539463d6d60e08f11748cf5cd773990512115fcf34e4.jpg)  \nFigure 2: AUC with varying dimensions.\n\n![](images/573ba7b7443f203d77ef9a6ecf7754164e256946ce4763e83ce7fb5f77ebd658.jpg)  \nFigure 3: AUC with training iterations (#factor $\\scriptstyle = 2 0$ ).\n\n# Performance\n\nResults in terms of the average AUC on different datasets are shown in Table 3 (all with 20 total factors). For each dataset, we report the average AUC on the full test set $\\tau$ (denoted by ‘All Items’), as well as a subset of $\\tau$ which only consists of items that had fewer than five positive feedback instances in the training set (i.e., cold start). These cold start items account for around $60 \\%$ of the test set for the two Amazon datasets, and $80 \\%$ for Tradesy.com; this means for such sparse real-world datasets, a model must address the inherent cold start nature of the problem and recommend items accurately in order to achieve acceptable performance. The main findings from Table 3 are summarized as follows:\n\n1. Building on top of BPR-MF, VBPR on average improves on BPR-MF by over $12 \\%$ for all items, and more than $28 \\%$ for cold start. This demonstrates the significant benefits of incorporating CNN features into our ranking task. 2. As expected, IBR outperforms BPR-MF & MM-MF in cold start settings where pure MF methods have trouble learning meaningful factors. Moreover, IBR loses to MF methods for warm start since it is not trained on historical user feedback.\n\n![](images/f1d51b148fc442eb17548089a3d01853d5a31da939cd7296aa41fe12480eeb84.jpg)  \nFigure 4: 2-D visualization (with t-SNE (?)) of the 10-D visual space learned from Amazon Women. All images are from the test set. For clarity, the space is discretized into a grid and for each grid cell one image is randomly selected among overlapping instances.\n\n3. By combining the strengths of both MF and content-based methods, VBPR outperforms all baselines in most cases.\n\n4. Our method exhibits particularly large improvements on Tradesy.com, since it is an inherently cold start dataset due to the ‘one-off’ nature of trades.\n\n5. Visual features show greater benefits on clothing than cellphone datasets. Presumably this is because visual factors play a smaller (though still significant) role when selecting cellphones as compared to clothing.\n\n6. Popularity-based methods are particularly ineffective here, as cold items are inherently ‘unpopular’.\n\nFinally, we found that pairwise methods indeed outperform point-wise methods (WRMF in our case) on our datasets, consistent with our analysis in Related Work. We found that on average, VBPR beats WRMF by $1 4 . 3 \\%$ for all items and $2 0 . 3 \\%$ for cold start items.\n\nSensitivity. As shown in Figure 2, MM-MF, BPR-MF, and VBPR perform better as the number of factors increases, which demonstrates the ability of pairwise methods to avoid overfitting. Results for other Amazon categories are similar and suppressed for brevity.\n\nTraining Efficiency. In Figure 3 we demonstrate the AUC (on the test set) with increasing training iterations. Generally speaking, our proposed model takes longer to converge than MM-MF and BPR-MF, though still requires only around 3.5 hours to train to convergence on our largest dataset (Women’s Clothing).\n\n# Visualizing Visual Space\n\nVBPR maps items to a low-dimensional ‘visual space,’ such that items with similar styles (in terms of how users evaluate them) are mapped to nearby locations. We visualize this space (for Women’s Clothing) in Figure 4. We make the following two observations: (1) although our visual features are extracted from a CNN pre-trained on a different dataset, by using the embedding we are nevertheless able to learn a ‘visual’ transition (loosely) across different subcategories, which confirms the expressive power of the extracted features; and (2) VBPR not only helps learn the hidden taxonomy, but also more importantly discovers the most relevant underlying visual dimensions and maps items and users into the uncovered space.",
  "experiments": "",
  "hyperparameter": "- embedding_dim (D): Dimension of the trainable ID embeddings for all users and items in the interaction graph. It controls the capacity of the collaborative representation space; in experiments it is fixed to 64 for all models for fair comparison.\n- distilled_feature_dim (D'): Dimension of the modality-specific content space after projecting raw visual, acoustic, and textual features through a linear layer and nonlinearity. It controls how rich the content-side representation is; typically chosen comparable to the ID embedding size and tuned on validation. :contentReference[oaicite:1]{index=1}\n- num_gcn_layers (L): Number of stacked graph convolutional layers used to propagate messages over the refined user–item graph. It controls how many hops of neighbors contribute to each embedding; the paper mainly uses L = 2 and shows that deeper stacks may over-smooth representations. :contentReference[oaicite:2]{index=2}\n- num_routing_iterations (T): Number of routing iterations in the prototypical network that updates a user’s preference prototype from her interacted items. Larger T lets the prototype move further toward content-consistent neighbors but risks oversmoothing; the ablation varies T (e.g., 1–3) and tunes it per dataset. :contentReference[oaicite:3]{index=3}\n- learning_rate: Step size of the Adam optimizer used to train all parameters (embeddings, projection matrices, base vectors). Searched over {0.0001, 0.001, 0.01, 0.1, 1} and selected based on validation Recall@10. :contentReference[oaicite:4]{index=4}\n- reg_weight_lambda: L2 regularization coefficient in the BPR loss, applied to model parameters to prevent overfitting on sparse implicit data. Tuned in {0.00001, 0.0001, 0.001, 0.01, 0.1} on the validation set. :contentReference[oaicite:5]{index=5}\n- early_stopping_patience: Number of epochs without improvement in validation Recall@10 before training is stopped. Set to 20 consecutive epochs to avoid overfitting while keeping training efficient. :contentReference[oaicite:6]{index=6}"
}
