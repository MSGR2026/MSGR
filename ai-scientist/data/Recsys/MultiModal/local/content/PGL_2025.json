{
  "id": "PGL_2025",
  "paper_title": "Mind Individual Information! Principal Graph Learning for Multimedia Recommendation",
  "alias": "PGL",
  "year": 2025,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# PGL\n\nTo better capture local structural features,we propose the principal graph learning framework for multimedia recommendation. It first extracts the principal subgraph from the complete user-item interaction graph,and then employs message passing on the principal subgraph.The framework of PGL is illustrated in Figure 4.\n\n# Embedding\n\nAs with many works (Zhou and Shen 2023; Yu et al. 2023c), we map the user unique hot ID representations to the user ID embeddings as the user raw representations.For the items, we map the content features using MLP to a low dimensional representation and concatenate them to obtain the item raw representations.Because the item content features are more informative than the item ID representations, it is more conducive to the model convergence.\n\n# Principal Graph Learning\n\nTo better mine local structural features, we propose the principal user-item graph learning. Specifically, during model training，we first extract the principal subgraph from the complete graph structure.We then perform message passing on this principal subgraph.This approach not only mines richer individual information,but also captures cooccurrence patterns through multi-hop message passing. During model inference, we employ the message passing mechanism on the original complete graph instead, avoiding the problem of insufficient global information utilization caused by subgraph fragmentation.\n\nFormally, the message passing of $x$ is represented as:\n\n$$\n\\tilde { x } = H ( F ( \\tilde { \\mathbf { A } } ) ) x .\n$$\n\n![](images/216d4a68e13db886fe0d9d00d329b431ef17c81e992cc70a0efc6e6dbe5709c0.jpg)  \nFigure 4: Comparison of (b) the proposed framework PGL and (a) exsiting framework LATTICE.\n\nFor subgraph extraction $F ( \\cdot )$ ， we design two operators: global-aware and local-aware subgraph extraction. The idea is to extract the most informative subgraph from the global/local perspective.\n\n· Global-Aware Extraction Truncated reconstruction $\\tilde { \\mathbf { A } }$ identifies the principal subgraphs with the highest information content, guided by the global structural signal. Specifically, we first decompose $\\tilde { \\mathbf { A } }$ using Singular Value Decomposition (SVD) (Rangarajan 2Oo1). However, performing the exact SVD on $\\tilde { \\mathbf { A } }$ is computationally expensive,making it impractical in real-world scenarios.Instead,we adopt the randomized SVD algorithm proposed by (Halko,Martinsson,and Tropp 2O11). It approximates the range of the input matrix with a low-rank orthonormal matrix,and performs SVD on this smaller matrix. It is formally represented as:\n\n$$\n\\dot { \\mathbf { U } } , \\dot { \\mathbf { A } } , \\dot { \\mathbf { U } } ^ { \\top } = \\operatorname { A p p r o x } { \\mathbf { S } } \\mathbf { V } \\mathbf { D } ( \\tilde { \\mathbf { A } } , d ) ,\n$$\n\nwhere $\\dot { \\pmb { \\Lambda } }$ is eigenvalues matrix, $\\dot { \\mathbf { U } }$ is corresponding eigenvector matrix, and $d$ is the rank of decomposed matrix.It is equal to the dimension of user/item raw representations.\n\nNext,we employ the truncated reconstruction to obtain principal subgraph $\\dot { \\bf A }$ ：\n\n$$\n\\begin{array} { r } { \\ddot { { \\mathbf U } } , \\ddot { { \\mathbf A } } , \\dot { { \\mathbf U } } ^ { \\top } = \\operatorname { T r u n c a t i o n } ( \\dot { { \\mathbf U } } , \\dot { { \\mathbf A } } , \\dot { { \\mathbf U } } ^ { \\top } , \\gamma ) , } \\\\ { \\dot { { \\mathbf A } } = \\ddot { { \\mathbf U } } \\ddot { { \\mathbf A } } \\ddot { { \\mathbf U } } ^ { \\top } , \\qquad } \\\\ { \\ddot { { \\mathbf A } } = { \\mathrm { S p a r s i f i c a t i o n } ( \\dot { { \\mathbf A } } , \\epsilon ) } , } \\end{array}\n$$\n\nwhere $\\gamma$ is the truncation ratio within the range $( 0 , 1 )$ The truncation function retains the eigenvalues belonging to the range before or after $\\gamma d$ ，resulting in $\\ddot { \\textbf { A } } =$ $\\mathrm { D i a g } ( \\lambda _ { 1 } , \\cdot \\cdot \\cdot , \\lambda _ { \\gamma d } , 0 , \\cdot \\cdot \\cdot , 0 , \\lambda _ { ( 1 - \\gamma ) d } , \\cdot \\cdot \\cdot , \\lambda _ { d } )$ . The sparsification function sets elements with reconstruction values less than the threshold $\\epsilon$ (typically $1 e ^ { - 3 }$ ）to O, ensuring the obtained principal graph $\\dot { \\bf A }$ remains sparse.\n\n· Local-Aware Extraction The expressive principal subgraph can also be extracted from the complete graph by leveraging local exposure information. The extraction process is formalized as:\n\n$$\n\\ddot { \\mathbf { A } } = \\mathrm { E x t r a c t i o n } ( \\tilde { \\mathbf { A } } , p ) .\n$$\n\nHere, the extraction function is a multinomial samplig procedure, where $p$ denotes the percentage of original edges. Experiments finds that most of the time, $p = 0 . 3$ achieves an optimal result. The probability of sampling an edge connecting user $u$ and item $i$ is $1 / \\sqrt { | \\mathcal { N } _ { u } | } \\sqrt { | \\mathcal { N } _ { j } | }$ . This weighting scheme is motivated on the intuition that edges connected to nodes with lower exposure tend to be more reliable and informative (Wu et al. 2021).\n\n# Latent Graph Learning\n\nFollowing (Zhang et al. 2021; Zhou and Shen 2023; Yu et al. 2023c), we also conduct message passing on the latent itemitems to capture collaborative signals under modality information. Specifically, we first pre-construct latent item-item graphs based on similarity of item raw modality features $e _ { i , m }$ . It avoids the learning of latent graph structures in the classical LATTICE. Then, we use $k \\mathbf { N N }$ sparsization $( k { = } 1 0 )$ and Laplacian regularization to obtain the normalized matrix $\\mathbf { S } _ { m }$ . Besides, each modality item-item matrices are weighted added to obtain the final item-item similarity matrix S.Here, we still use the message passing mechanism of Equation 8.\n\n# Prediction\n\nTo make recommendations for user $u$ ,we predict the interaction scores between the user $u$ and all candidate items, and choose $K$ top-ranked items as recommendations to the user $u$ .The interaction score is calculated as:\n\n$$\n\\begin{array} { r } { \\begin{array} { c } { f _ { \\mathrm { p r e d i c t } } ( u , i ) = \\hat { \\mathbf { e } } _ { u } ^ { \\intercal } \\hat { \\mathbf { e } } _ { i } , } \\\\ { \\hat { \\mathbf { e } } _ { u } = \\mathbf { e } _ { u } ^ { \\mathrm { p r i n c i p a l } } + \\mathbf { e } _ { u } ^ { \\mathrm { l a t e n t } } , \\hat { \\mathbf { e } } _ { i } = \\mathbf { e } _ { i } ^ { \\mathrm { p r i n c i p a l } } + \\mathbf { e } _ { i } ^ { \\mathrm { l a t e n t } } , } \\end{array} } \\end{array}\n$$\n\nwhere $\\mathbf { e } _ { i } ^ { \\mathrm { { p r i n c i p a l } } }$ and $\\mathbf { e } _ { i } ^ { \\mathrm { { l a t e n t } } }$ represent the output of principal graph learning and latent graph learning correspondingly. A\n\n<table><tr><td>Datasets</td><td colspan=\"4\">Baby</td><td colspan=\"4\">Sports</td><td colspan=\"4\">Clothing</td></tr><tr><td></td><td>Methods|R@10 R@20 N@10 N@20|</td><td></td><td></td><td></td><td></td><td></td><td></td><td>R@10 R@20 N@10 N@20|</td><td>|R@10 R@20 N@10 N@20</td><td></td><td></td><td></td></tr><tr><td>MF-BPR(UAI&#x27;09)</td><td>10.0357</td><td></td><td></td><td>0.0575 0.0192 0.0249</td><td>0.0432</td><td>20.0653 0.0241</td><td></td><td>0.0298</td><td>0.0187</td><td>0.0279 0.0103</td><td></td><td>0.0126</td></tr><tr><td>LightGCN (SIGIR&#x27;20)</td><td>0.0479</td><td>0.0754 0.0257</td><td></td><td>0.0328</td><td>0.0569</td><td></td><td>0.0864 0.0311</td><td>0.0387</td><td>0.0340</td><td>0.0526 0.0188</td><td></td><td>0.0236</td></tr><tr><td>DirectAU (KDD&#x27;22)</td><td>0.0460</td><td>）0.0672 0.0263</td><td></td><td>30.0318</td><td>0.0630</td><td>）0.0958</td><td>30.0351</td><td>0.0436</td><td>0.0468</td><td>0.0683 0.0257</td><td></td><td>0.0311</td></tr><tr><td>LayerGCN (ICDE&#x27;23)</td><td>0.0529</td><td>0.0820</td><td>0.0281</td><td>0.0355</td><td>0.0594</td><td></td><td>0.0916 0.0323</td><td>0.0406</td><td>0.0371</td><td>0.0566</td><td>0.0200</td><td>0.0247</td></tr><tr><td>VBPR (AAAI&#x27;16)</td><td>0.0423</td><td>0.0663</td><td>0.0223</td><td>0.0284</td><td>0.0558</td><td></td><td>0.0856 0.0307</td><td>0.0384</td><td>0.0281</td><td>0.0415</td><td>0.0158</td><td>0.0192</td></tr><tr><td>MMGCN (MM&#x27;19)</td><td></td><td>0.0378 0.0615 0.0200</td><td></td><td>0.0261</td><td>0.0370</td><td>）0.06050.0193</td><td></td><td>0.0254</td><td>0.0218</td><td>0.0345 0.0110</td><td></td><td>0.0142</td></tr><tr><td>DualGNN (TMM&#x27;21)</td><td>0.0448</td><td>：0.0716</td><td>50.0240</td><td>0.0309</td><td>0.0568</td><td>0.0859</td><td>0.0310</td><td>0.0385</td><td>0.0454</td><td>0.0683</td><td>0.0241</td><td>0.0299</td></tr><tr><td>LATTICE (MM&#x27;21)</td><td>0.0547</td><td>0.0850</td><td>0.0292</td><td>0.0370</td><td>0.0620</td><td>0.0953</td><td>0.0335</td><td>0.0421</td><td>0.0492</td><td>0.0733</td><td>0.0268</td><td>0.0330</td></tr><tr><td>SLMRec (TMM&#x27;22)</td><td>0.0529</td><td>0.0775</td><td>50.0290</td><td>0.0353</td><td>0.0663</td><td>0.0990</td><td>0.0365</td><td>0.0450</td><td>0.0452</td><td>0.0675</td><td>0.0247</td><td>0.0303</td></tr><tr><td>MICRO (TKDE&#x27;22)</td><td>0.0584</td><td>0.0929</td><td>0.0318</td><td>0.0407</td><td>0.0679</td><td>0.1050</td><td>0.0367</td><td>0.0463</td><td>0.0521</td><td>0.0772</td><td>0.0283</td><td>0.0347</td></tr><tr><td>BM3 (WWW&#x27;23)</td><td>0.0564</td><td>0.0883</td><td>0.0301</td><td>0.0383</td><td>0.0656</td><td>0.0980</td><td>0.0355</td><td>0.0438</td><td>0.0422</td><td>0.0621</td><td>0.0231</td><td>0.0281</td></tr><tr><td>MMSSL (WWW&#x27;23)</td><td>0.0613</td><td>0.0971</td><td></td><td>0.0326 0.0420</td><td>0.0673</td><td>0.1013</td><td>0.0380</td><td>0.0474</td><td>0.0531</td><td>0.0797</td><td>0.0291</td><td>0.0359</td></tr><tr><td>FREEDOM (MM&#x27;23)</td><td>0.0627</td><td>0.0992</td><td>0.0330</td><td>0.0424</td><td>0.0717</td><td>0.1089</td><td>0.0385</td><td>0.0481</td><td>0.0629</td><td>0.0941</td><td>0.0341</td><td>0.0420</td></tr><tr><td>MGCN (MM&#x27;23)</td><td>0.0620</td><td>）0.0964</td><td>0.0339</td><td>0.0427</td><td>0.0729</td><td></td><td>0.1106 0.0397</td><td>0.0496</td><td>0.0641</td><td>0.0945</td><td>0.0347</td><td>0.0428</td></tr><tr><td>LGMRec (AAAI&#x27;24)</td><td>0.0644</td><td>0.1002</td><td>0.0349</td><td>0.0440</td><td></td><td>0.0720 0.1068</td><td>30.0390</td><td>）0.0480</td><td>0.0555</td><td>0.0828</td><td>0.0302</td><td>0.0371</td></tr><tr><td>PGL w/ global (Ours)</td><td>0.0663</td><td>0.1040 0.0351</td><td></td><td>0.0448</td><td>0.0760</td><td>0.1144</td><td>0.0410</td><td>0.0509</td><td>0.0690</td><td>0.1014 0.0369</td><td></td><td>0.0451</td></tr><tr><td>PGL w/ local (Ours)</td><td>0.0676 (</td><td>0.1022</td><td></td><td>0.0360 0.0449</td><td></td><td></td><td>0.0789 0.1174 0.0428 0.0528</td><td></td><td></td><td>0.0712 0.1034 0.0385 0.0467</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 1: Performance comparison of baselines and our method in terms of Recall $@ \\mathrm { K }$ $( \\mathbb { R } ^ { \\ @ K ) }$ ，and NDCG@K $( \\mathrm { N } @ K )$ . The best result is in boldface and the second best is underlined.\n\nhigh score suggests that the user $u$ is more likely to click the item $i$\n\n# Optimization\n\nDuring the phase of model training,we adopt the Bayesian Personalized Ranking(BPR） loss $\\mathcal { L } _ { \\mathrm { B P R } }$ as the basic optimization task,which assumes that users prefer historically interacted items over unclicked ones.And it is combined with auxiliary self-supervised tasks to jointly update the representations of users and items:\n\n$$\n\\begin{array} { r } { \\mathcal { L } = \\mathcal { L } _ { \\mathrm { B P R } } + \\lambda _ { \\mathrm { S S L } } \\mathcal { L } _ { \\mathrm { S S L } } , } \\end{array}\n$$\n\nwhere $\\lambda _ { \\mathrm { S S L } }$ is the hyperparameter to control the intensity of the self-supervised auxiliary task.\n\nThe self-supervised task is used to further enhance the distinguishability of representations. Specifically, it first masks certain node representations through feature masking,and then maximizes the consistency of features after two random masks through in batch InfoNCE:\n\n$$\n\\begin{array} { r l } & { { \\hat { \\bf e } } _ { u } ^ { \\prime } , { \\hat { \\bf e } } _ { u } ^ { \\prime \\prime } = { \\bf M a s k } ( { \\hat { \\bf e } } _ { u } , \\rho ) , } \\\\ & { { \\hat { \\bf e } } _ { i } ^ { \\prime } , { \\hat { \\bf e } } _ { i } ^ { \\prime \\prime } = { \\bf M a s k } ( { \\hat { \\bf e } } _ { i } , \\rho ) , } \\\\ & { \\quad \\mathcal { L } _ { \\mathrm { S S L } } = \\displaystyle \\sum _ { u \\in \\mathcal { U } } - \\log \\frac { ( { \\hat { \\bf e } } _ { u } ^ { \\prime } \\cdot { \\hat { \\bf e } } _ { u } ^ { \\prime \\prime } / \\tau ) } { \\sum _ { v \\in \\mathcal { U } } \\exp ( { \\hat { \\bf e } } _ { v } ^ { \\prime } \\cdot { \\hat { \\bf e } } _ { v } ^ { \\prime } / \\tau ) } } \\\\ & { \\quad \\quad \\quad + \\displaystyle \\sum _ { i \\in \\mathcal { L } } - \\log \\frac { \\exp ( { \\hat { \\bf e } } _ { i } ^ { \\prime } \\cdot { \\hat { \\bf e } } _ { i } ^ { \\prime \\prime } / \\tau ) } { \\sum _ { j \\in \\mathcal { L } } \\exp ( { \\hat { \\bf e } } _ { j } ^ { \\prime } \\cdot { \\hat { \\bf e } } _ { i } ^ { \\prime \\prime } / \\tau ) } , } \\end{array}\n$$\n\nwhere $\\rho$ controls the feature masking ratio and $\\tau$ is the temperature hyper-parameter of the softmax function.",
  "experiments": "",
  "hyperparameter": "- γ (truncation ratio): Controls the truncation in global-aware extraction. Typical value: 0.25, range: (0,1)\n- p (sampling ratio): Percentage of original edges in local-aware extraction. Typical value: 0.3\n- λ_SSL (weight for SSL): Controls the intensity of the self-supervised loss. Searched in [0.005, 0.01, 0.05, 0.1, 0.5]\n- ρ (feature masking ratio): Controls the masking in self-supervised task. Searched in [0.05, 0.1, 0.2, 0.3, 0.4]\n- τ (temperature): Temperature in the softmax for self-supervised task. Typical value: 0.2"
}
