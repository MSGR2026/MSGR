{
  "id": "SLMRec_2022",
  "paper_title": "Self-supervised Learning for Multimedia Recommendation",
  "alias": "SLMRec",
  "year": 2022,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# III. METHODOLOGY\n\nWe present a multi-task learning framework for multimedia recommendation, which fuses various modal information into user and item representations and distills the multi-modal patterns. Fig. 2 illustrates the model architecture of SLMRec. Here we consider four modalities (i.e., visual, acoustic, textual, and ID embeddings) serving as the basic input of the model and design three SSL tasks, which can be classified as modal-agnostic tasks and the modal-specific task. Especially, we leverage a multi-task strategy to improve the recommendation, where the graph-based recommender serves as the main task (see Section II) and SSL serves as the auxiliary task.\n\n# A. Self-Supervised Learning\n\nDifferent from supervised learning, SSL is used to distill potential information in unlabelled data. Inspired by the Masked Modeling in BERT [13] and Two-Tower [17], we treat each modality as an independent feature and design two modalagnostic tasks (i.e., FD, FM). On the other hand, we consider modality specificity and relations between modalities to design a modal-specific task (i.e., FAC).\n\n1) Modal-Agnostic Task: We process features through a generic scheme (e.g., masking) to generate multiple views for a node, which can be formulated as:\n\n$$\nZ _ { m } ^ { \\prime } = H ( t ^ { \\prime } ( E _ { m } ) , \\mathcal { G } _ { m } ) , Z _ { m } ^ { \\prime \\prime } = H ( t ^ { \\prime \\prime } ( E _ { m } ) , \\mathcal { G } _ { m } ) , t , t ^ { \\prime } t ^ { \\prime \\prime } \\sim \\mathcal { T } ,\n$$\n\n![](images/4e46f6a8654394f3654c6e921265fb3a4eeff857bb4c6f46339b26a2c54383c0.jpg)  \nFig. 2. The diagram depicts the main components under the multi-task strategy. The first layer represents the flow of the main task, and the next layers represent our self-supervised tasks, depicting modal-agnostic tasks and the modal-specific task, respectively.\n\nwhere the mono-modal features augmented by two stochastic transformations (i.e., $t ^ { \\prime }$ and $t ^ { \\prime \\prime }$ ), from the same family of augmentations $\\mathcal { T }$ , are fed into the graph encoder to produce the final representations $Z _ { m } ^ { \\prime }$ and $Z _ { m } ^ { \\prime \\prime }$ . After that, for every item in each modality, we construct a positive pair defined as $( z _ { i , m } ^ { \\prime } , z _ { i , m } ^ { \\prime \\prime } )$ and negative pairs defined as $( z _ { i , m } ^ { \\prime } , z _ { j , m } ^ { \\prime \\prime } )$ for $i \\neq j$ . Especially, the modal-agnostic transformations $t$ =, which create two augmented examples by masking part of the information, can be formulated as:\n\n$$\nt ( E ) = M \\odot E ,\n$$\n\nwhere $M$ and $E$ denote the masking matrix and item features, respectively. The inner production $\\odot$ of them produce the augmented example. In terms of the granularities of the masking part, we apply two masking patterns as our transformations.\n\n- Feature Dropout: With a certain probability, we dropout every factor of the modal and build multiple broken views of features for contrastive learning to improve model robustness. Feature Masking: We stochastically mask one modality of items and generate two subsets of modalities. In this work, we encourage maximizing the mutual information between modalities.\n\nHence, we define the SSL loss on modality-agnostic tasks within the augmented embeddings:\n\n$$\n\\mathcal { L } _ { s s l } = - \\log \\frac { \\exp ( \\sin ( z _ { i } ^ { \\prime } , z _ { i } ^ { \\prime \\prime } ) / \\tau _ { s s l } ) } { \\sum _ { j } ^ { N } \\exp ( \\sin ( z _ { i } ^ { \\prime } , z _ { j } ^ { \\prime \\prime } ) / \\tau _ { s s l } ) } ,\n$$\n\nwhere $\\mathrm { s i m } ( \\cdot )$ measures the similarity between the two vectors, ( )and cosine similarity function is applied to that; $\\tau$ is a tunable hyperparameter of temperature and $N$ denotes the number of examples in a batch.\n\nNote that we design factor and modality granularity masking, where we call factor granularity masking as FD and modality granularity masking as FM. We provide a more specific explanation in Fig. 2. For four modalities of a given item, FD fades the node color to present the dropout process of all modalities while FM discards one node to present the masking process of a specific modality. In terms of granularity, FM further uncovers the relationship between modalities, as different modalities can be compared to maximize the mutual information that alleviates the inconsistency in modal fusion. However, conventional masking treats each modal independently and does not respect modal specificity. Next, we consider the modal-specific tasks to enforce modalities’ consistency.\n\n2) Modal-Specific Task: We generally process modalities in separate channels and fuse them through naive combinations (e.g., concatenation, and addition). As we mentioned in the previous Section III-A1, intra-modal masking facilitates modal fusion but does not respect modal specificity. Hence, we hope to (1) distinguish each modality’s specificity, and (2) conduct comparisons for each modality with all other modalities as much as possible for improving multi-modal recommendation. To address the above issues, we consider the treatment of different granularity spaces in multi-modal versatile network [18] to construct modal alignments based on spatial transformations.\n\nBefore alignment is performed, we encode original modality features by graph-based encoder the same as the main task. Given the mono-modal representations of items, we apply the projection head as the spatial transformation which can be defined as:\n\n$$\nz _ { m  s ^ { \\prime } } = g _ { m  s ^ { \\prime } } ( z _ { m } ) , z _ { s ^ { \\prime }  s ^ { \\prime \\prime } } = g _ { s ^ { \\prime }  s ^ { \\prime \\prime } } ( z _ { m  s ^ { \\prime } } ) ,\n$$\n\nwhere $m , s ^ { \\prime }$ , and $s ^ { \\prime \\prime }$ are different spaces, especially, $m$ denotes the space $S _ { m }$ of each modality here. $g _ { m  s ^ { \\prime } }$ and $g _ { s ^ { \\prime }  s ^ { \\prime \\prime } }$ are simple projections to obtain the embedding $z _ { m  s ^ { \\prime } }$ and $z _ { s ^ { \\prime }  s ^ { \\prime \\prime } }$ , respectively.\n\nConsidering the granularity and maneuverability of each modality, we take ID as the dominant embedding and directly align the visual, acoustic, and textual embeddings with ID embeddings in different navigable spaces respectively, which is illustrated in Fig. 2.\n\nFollowing this idea, we propose a multi-layer granularity space corresponding to the different modalities: in the first level of granularity space $( S _ { i d \\cdot v } )$ the visual embeddings and the ID embeddings are compared, while in the second level of granularity space $( S _ { i d \\cdot v \\cdot a } )$ the acoustic embeddings are compared with the ID embeddings and the visual embeddings, and the textual embeddings compared with the other modal embeddings in the last level of granularity space $( S _ { i d \\cdot v \\cdot a \\cdot t } )$ . Note that the acoustic embeddings and visual embeddings are not compared directly. However, with the help of spatial transformation strategy that allows the visual embeddings to be projected into space $S _ { i d \\cdot v \\cdot a }$ by $g _ { i d \\cdot v  i d \\cdot v \\cdot a } \\circ g _ { v  i d \\cdot v }$ and acoustic embedding and visual embedding to be indirectly aligned based on the alignments with ID embeddings.\n\nAlgorithm 1: Multi-task Learning of SLMRec-FAC.   \n\n<table><tr><td colspan=\"2\">data:user-item interactions and multi-modal features hyperparameters:T,Tssl,α,λ initialization; while not converge do foreach epoch do Perform Eq.1 to encode modalities foreachbatch do Evaluate Lmain according to Eq. 13 Perform Eq.1O for spatial transformations Evaluate Lssl according to Eq. 11 Evaluate L according to Eq.14 Update parameters end end end</td></tr></table>\n\nHence, we define multiple InfoNCE losses [19] of direct alignments and sum them up, which can be formulated as:\n\n$$\n\\begin{array} { r l } & { \\mathcal { L } _ { s s l } = - \\displaystyle \\sum _ { m \\in \\mathcal { M } \\propto \\{ i d \\} } \\log } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\frac { \\displaystyle \\exp ( \\sin ( z _ { i , i d \\to s _ { m } } , z _ { i , m  s _ { m } } ) / \\tau _ { s s l } ) } { \\displaystyle \\sum _ { j \\in [ N ] } \\exp ( \\sin ( z _ { i , i d  s _ { m } } , z _ { j , m  s _ { m } } ) / \\tau _ { s s l } ) } , } \\end{array}\n$$\n\nwhere $s _ { m }$ denotes the space for contrastive learning between the modality $m$ and $\\mathrm { I D }$ (e.g., $s _ { a }$ means the space $S _ { i d \\cdot v \\cdot a }$ ).\n\n# B. Multi-Task Strategy\n\nTo improve the recommendation result, we leverage the multitask strategy to tackle sparse supervision signal problems and improve representation learning.\n\n1) Contrastive Pairwise Learning: We sample one observed interaction as $( u , i )$ , and then randomly sample $k$ items, which ( )have not been interacted with $u ( e . g . , \\ j _ { 1 } , j _ { 2 } , . . . , j _ { k } )$ . Thus, we establish the positive and negative pairs, which can be formulated as:\n\n$$\n( u , i ) , ( u , j _ { 1 } ) , ( u , j _ { 2 } ) , . . . , ( u , j _ { k } ) .\n$$\n\nHere, we use LightGCN [1] as the backbone of the graph-based encoder for independently producing representations in terms of modalities. We abandon the classic BPR loss [3] as the main objective loss. Instead, we choose InfoNCE to maximize the agreement of positive pairs and minimize that of negative pairs, which can be defined as:\n\n$$\n\\mathcal { L } _ { m a i n } = - \\log \\frac { \\exp { ( \\sin ( z _ { u } ^ { \\top } z _ { i } ) / \\tau ) } } { \\sum _ { j \\in N _ { u } } \\exp { ( \\sin ( z _ { u } ^ { \\top } z _ { j } ) / \\tau ) } } ,\n$$\n\nTABLE I THE STATISTICS OF THE EVALUATION DATASET. V, A, AND T DENOTE THE FEATURE DIMENSIONS OF VISUAL, ACOUSTIC, AND TEXTUAL MODALITIES, RESPECTIVELY   \n\n<table><tr><td>Dataset</td><td>Inter.#</td><td>User.#</td><td>Item.#</td><td>V</td><td>A</td><td>T</td></tr><tr><td>Tiktok</td><td>564,365</td><td>36.638</td><td>71,494</td><td>256</td><td>128</td><td>128</td></tr><tr><td>Kwai</td><td>235,918</td><td>7,010</td><td>80.986</td><td>2,048</td><td>，</td><td>1</td></tr><tr><td>Movielens</td><td>1,184,023</td><td>55,485</td><td>5,986</td><td>2,048</td><td>128</td><td>100</td></tr></table>\n\n2) Optimization: We simultaneously optimize the model by joint the main task and SSL task as followed:\n\n$$\n\\mathcal { L } = \\mathcal { L } _ { m a i n } + \\alpha \\mathcal { L } _ { s s l } + \\lambda \\| \\Theta \\| _ { 2 } ^ { 2 } ,\n$$\n\nwhere $\\alpha$ and $\\lambda$ are hyperparameters to adjust the balance between multiple tasks and the strengths of $L _ { 2 }$ regularization. We perform the multi-task optimization of SLMRec-FAC by Algorithm 1. SLMRec-FD and SLMRec-FM follow a similar workflow.",
  "experiments": "",
  "hyperparameter": "- tau_ssl: The temperature parameter for the contrastive learning loss, typical values range from {0.1, 0.2, 0.5, 1.0}.\n- alpha: The weight for balancing the multi-task learning objectives, typically set in {0.01, 0.05, 0.1, 1.0}.\n- lambda: The L2 regularization coefficient, typical values are {0.0001, 0.001, 0.01}.\n- batch_size: The batch size for training the model, common values are {128, 256, 512, 1024, 2048}.\n- learning_rate: The learning rate for optimization, commonly set to {0.0001, 0.001, 0.01}.\n- embedding_size: The size of the embedding vectors for users and items, usually set to 64."
}
