{
  "id": "DualGNN_2021",
  "paper_title": "DualGNN: Dual Graph Neural Network for Multimedia Recommendation",
  "alias": "DualGNN",
  "year": 2021,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# IV. METHOD\n\nIn this section, we address the aforementioned challenges of modeling the users’ multi-modal preferences representation. We begin with a brief overview of our framework and then elaborate on its components.\n\nAs illustrated in Figure 2, the DualGNN framework consists of three components: 1) a single-modal representation learning module, which performs the graph network operations to capture the modal-specific user preference and micro-video representation on each modality user-microvideo bipartite graph;\n\n![](images/a059e89748242d6806c5c6664add5ed98723088dcd2426a799dd2a7c070f7922.jpg)  \nFig. 2. The overall framework of our proposed DualGNN. It consists of the single-modal representation learning module which captures the single-modal user preference on each modality user-microvideo bipartite graph, the multi-modal representation learning module that explicitly models the user’s tastes on different modalities and inductively learns the multi-modal user preference, and the prediction module to estimate the user’s preference towards the target micro-video.\n\n2) a multi-modal representation learning module that explicitly models the user’s taste on different modalities and inductively learns the multi-modal user preference; and 3) a prediction module that ranks the potential micro-videos for users by measuring the similarity of each user and micro-video pair.\n\n# A. Single-Modal Representation Learning Module\n\nFollowing the settings in MMGCN, we aim at performing the graph convolutional operations on the single-modal bipartite graph to learn the user preference in each modality (a.k.a. single-modal representation). However, we are inspired by the arguments that the feature transformation and nonlinear activation of common GCN have no positive impact on the effectiveness of collaborative filtering [9], and accordingly simplify the graph convolutional operations for the multimedia recommendation. Specifically, we discard the self-loop information propagation of each node and purely model its collaborative signal. Then, the feature transformation is also ignored to reduce the cost and facilitate the model optimization. Therefore, at the $( l + 1 )$ -th layer, the operation could be formulated as:\n\nAs a result, the informative signals are encoded into the singlemodal user and micro-video representations. The same operations are adopted to the bipartite graph of each modality. After propagating on different modalities bipartite graphs, we gain the representations of the users and micro-videos in each modality.\n\n# B. Multi-Modal Representation Learning Module\n\nTo inductively learn the particular multi-modal fusion pattern for each user, we disentangle the learning process into the information construction and aggregation operations.\n\n1) Multi-modal information construction: Inspired by previous methods on fusing multi-modal information [6], we also present several construction methods for the multi-modal information construction. At first, we initialize a parameter set $\\{ \\alpha _ { u , v } = 1 , \\alpha _ { u , a } = 1 , \\alpha _ { u , t } = 1 \\}$ for each user, where $\\alpha _ { u , v } , \\alpha _ { u , a }$ and $\\alpha _ { u , t }$ denote the user’s preference for visual, acoustic, and textual modalities, respectively.\n\na) Attentively concatenation construction: An intuitive method to construct the multi-modal information is to concatenate each single-modal representation of a user as:\n\n$$\n\\begin{array} { r l r } & { } & { \\pmb { u } _ { m } ^ { ( l + 1 ) } = \\displaystyle \\sum _ { i \\in \\mathcal { N } _ { u } } \\frac { 1 } { \\sqrt { | \\mathcal { N } _ { u } | } \\sqrt { | \\mathcal { N } _ { i } | } } \\pmb { i } _ { m } ^ { ( l ) } , } \\\\ & { } & { \\pmb { i } _ { m } ^ { ( l + 1 ) } = \\displaystyle \\sum _ { u \\in \\mathcal { N } _ { i } } \\frac { 1 } { \\sqrt { | \\mathcal { N } _ { i } | } \\sqrt { | \\mathcal { N } _ { u } | } } \\pmb { u } _ { m } ^ { ( l ) } , } \\end{array}\n$$\n\nwhere $\\mathcal { N } _ { u }$ and ${ \\mathcal { N } } _ { i }$ represent the neighbors of $u$ and $i$ in the bipartite graph, respectively. In addition, $\\pmb { u } _ { m } ^ { ( l ) } \\in \\mathbb { R } ^ { d }$ and $\\pmb { i } _ { m } ^ { ( l ) } \\in$ $\\mathbb { R } ^ { d }$ denote the representations of user and micro-video learned from the previous layer in each modality, respectively. We use the symmetric normalization $\\frac { 1 } { \\sqrt { | \\mathcal { N } _ { i } | } \\sqrt { | \\mathcal { N } _ { u } | } }$ to avoid the scale of representations increasing with graph convolution operations.\n\nBy iteratively conducting the above operations, the users and micro-videos obtain their collaborative signal from each layer. After $L$ layers propagation, we combine them with the features of each node to form the desired single-modal representations of each user and micro-video, formally,\n\n$$\n{ \\pmb u } _ { m } = \\sum _ { l = 0 } ^ { L } { \\pmb u } _ { m } ^ { ( l ) } , i _ { m } = \\sum _ { l = 0 } ^ { L } { \\pmb i } _ { m } ^ { ( l ) } .\n$$\n\n$$\n\\begin{array} { r } { \\left\\{ \\begin{array} { l l } { \\pmb { h } _ { u } = \\alpha _ { u , v } \\pmb { u } _ { v } | | \\alpha _ { u , a } \\pmb { u } _ { a } | | \\alpha _ { u , t } \\pmb { u } _ { t } , } \\\\ { \\pmb { u } _ { m u l } = \\pmb { W } _ { m } \\pmb { h } _ { u } + \\pmb { b } _ { m } , } \\end{array} \\right. } \\end{array}\n$$\n\nwhere $| |$ denotes the concatenation operation, $W _ { m } \\in \\mathbb { R } ^ { d \\times 3 d }$ and $\\pmb { b } _ { m } \\in \\mathbb { R } ^ { d }$ denote linear transformation matrix and bias, respectively. $u _ { m u l }$ denotes the constructed multi-modal representation of $u$ . In this way, each single-modal representation could be intactly considered in constructing the multi-modal information.\n\nb) Attentively sum construction: Inspired by the fact that the element-wise sum can preserve most features of the multimodal information [28], we could attentively integrate the single-modal preference of users as:\n\n$$\n{ \\pmb u } _ { m u l } = \\alpha _ { u , v } { \\pmb u } _ { v } + \\alpha _ { u , a } { \\pmb u } _ { a } + \\alpha _ { u , t } { \\pmb u } _ { t } .\n$$\n\nc) Attentively maximum construction: For user’s single modal representations, we select the maximum value of each dimension as the user’s multi-modal preference representation. Such operation can be formally defined as:\n\n$$\n\\begin{array} { r } { \\pmb { u } _ { m u l } = m a x ( \\alpha _ { u , v } \\pmb { u } _ { v } , \\alpha _ { u , a } \\pmb { u } _ { a } , \\alpha _ { u , t } \\pmb { u } _ { t } ) . } \\end{array}\n$$\n\nThis operation is based on the assumption that the most prominent single-modal representation of each user carries the richest information for its multi-modal representation.\n\n2) Multi-modal information aggregation: Following the fact that users who have interacted with the same micro-videos are generally close to each other in the multi-modal preference. We argue that the personalized fusion pattern of each user is hidden in such user co-occurrence relationship.\n\nHowever, the co-occurrence times between a user and his/her co-occurrence users is not consistent. More specially, the user may have a large number of co-occurrence times with a small group of users, while most users only co-occur a few times with the user. Thus, we argue that only users with a certain number of co-occurrences have more similar multi-modal preferences. For this purpose, we propose a Top-K sampling strategy for the user co-occurrence graph construction as follows.\n\nTop- $K$ sampling strategy: First, we define the sampled graph as $\\mathcal { G } _ { U } = \\{ \\mathcal { U } , \\mathcal { A } \\}$ , and $\\mathcal { A } = \\{ ( u , u ^ { \\prime } ) | u , u ^ { \\prime } \\in \\mathcal { U } \\}$ reflects the node pairs between $u$ and $u ^ { \\prime }$ . We sample the top- $K$ frequent users for each user from the user co-occurrence matrix $C$ , and an edge in graph $\\mathcal { G } _ { U }$ could be defined as $q _ { u u ^ { \\prime } }$ . If $q _ { u u ^ { \\prime } } = 1$ , it indicates that $C _ { u , u ^ { \\prime } }$ belongs to the top-K values of $\\mathcal { C } _ { u }$ , otherwise $q _ { u u ^ { \\prime } } \\ne 1$ . We will discuss the effect of $K$ on the model performance in the experiment section.\n\nThereafter, based on the sampled graph $\\mathcal { G } _ { U }$ , we design two aggregation methods to derive each user’s fusion pattern in the user co-occurrence graph as follows.\n\nMean aggregation: This method simply averages the representations of each user’s neighbor nodes as the aggregated information, and updates the user’s representation as:\n\n$$\n\\pmb { u } _ { m u l } ^ { ( l ^ { \\prime } + 1 ) } = \\pmb { u } _ { m u l } ^ { ( l ^ { \\prime } ) } + \\sum _ { u ^ { \\prime } \\in \\mathcal { N } _ { u , c } } \\frac { 1 } { | \\mathcal { N } _ { u , c } | } \\pmb { u } _ { m u l } ^ { \\prime ( l ^ { \\prime } ) } ,\n$$\n\nwhere $l ^ { \\prime }$ is the number of GCN layers, and $\\mathcal { N } _ { u , c }$ denotes user u’s neighbor nodes in the user co-occurrence graph.\n\nSoftmax weighted aggregation: In order to enhance the impact of neighbor users who have more co-occurrence times, we use the softmax function to compute each user’s aggregation weight:\n\n$$\nu _ { m u l } ( l ^ { \\prime } + 1 ) = u _ { m u l } ( l ^ { \\prime } ) + \\sum _ { u ^ { \\prime } \\in \\mathcal { N } _ { u , c } } \\frac { \\exp ( C _ { u , u ^ { \\prime } } ) } { \\sum _ { u ^ { \\prime } \\in \\mathcal { N } _ { u , c } } \\exp ( C _ { u , u ^ { \\prime } } ) } u _ { m u l } ^ { \\prime } ( l ^ { \\prime } ) .\n$$\n\nAfter $L ^ { \\prime }$ layers propagation, each user’s personalized fusion pattern could be mined from its neighbor nodes in the user cooccurrence graph. Noticing that we don’t design the symmetric multi-modal representation learning module for micro-videos, and the reasons can be summarized in the following two aspects. For one side, micro-videos’ features are more objective than the users’ preferences to characterize the instance. For another, the micro-videos’ fusion patterns are probably consistent with their exposure accessed by users. More specifically, since the micro-videos are collected from the same platform, we believe that they expose to users in a unified manner, which causes the micro-video’s same fusion pattern.",
  "experiments": "",
  "hyperparameter": "- embedding_size: Controls the dimensionality of the user and micro-video representations, common values not specified.\n- n_layers: The number of graph convolution layers for both single-modal and multi-modal representation learning, typical values are not specified.\n- reg_weight: Regularization weight to prevent overfitting, typical values not specified.\n- cl_weight: Weight for contrastive loss, specific range not given.\n- dropout: Dropout rate to prevent overfitting, typical values not specified.\n- learning_rate: The rate at which the model parameters are updated, typical values are {0.1, 0.01, 0.001, 0.0001, 0.00001}.\n- K: The number of co-occurrence users for multi-modal information aggregation, values of K are set as 40 for Movielens and 50 for Tiktok."
}
