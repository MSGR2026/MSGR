{
  "id": "DRAGON_2023",
  "paper_title": "Enhancing Dyadic Relations with Homogeneous Graphs for Multimodal Recommendation",
  "alias": "DRAGON",
  "year": 2023,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 3 Methodology\n\nThis section introduces the details of our proposed model DRAGON. Fig. 1 shows the overall architecture of DRAGON. There are four main components: (1) graph learning on a modality-specific heterogeneous graph to learn a uni-modal representation; (2) Multimodal representation learning module that captures user preference on different modalities and complementary information from each modality; (3) graph learning on homogeneous graphs to capture the co-occurrence relation between users and the semantic relation between items; (4) a predictor module that ranks the candidate items based on the scores calculated from final user and item representations.\n\n![](images/ceb8c960941ee4651a5131d475fcfb9a1917c0296d8b77b15171193484ab1678.jpg)  \nFig. 1. The diagram depicts the main components of DRAGON. We first learn the modality-specific representations, then the multimodal representation learning module fused the single-modality representations and utilizes the homogeneous graph to capture the internal relations\n\n# 3.1 Preliminary\n\nGiven a set of $N$ users $u \\in \\mathcal { U }$ , a set of $M$ items $i \\in \\mathcal { Z }$ . We model the dyadic relations of user interactions as a user-item bipartite graph $\\mathcal { G } = \\{ \\mathcal { U } , \\mathcal { Z } , \\mathcal { E } \\}$ , where we regard the historical interactions as the set of edges in the graph denoted by $\\varepsilon$ . Besides the user-item interactions, each item is associated with multimodal content information $m \\in \\{ v , t \\}$ , where $\\boldsymbol { v }$ and $t$ represent the visual and textual features respectively. We denote the modality feature for an item $i$ as $\\pmb { x } _ { i } ^ { m } \\in \\mathbb { R } ^ { d _ { m } }$ , where $d _ { m }$ denotes the feature dimension of modality $m$ . In this paper, we only consider the visual and textual modalities denoted by $v$ and $t$ . However, the proposed framework can be easily extended to scenarios involving more than two modalities.\n\n# 3.2 Dual Representation Learning\n\nLearning the representations of users and items is critical for the recommendation system. All representation learning-based techniques assume the existence of a common representation with consistent knowledge of different views of items [27]. Different views of an item contain specific discriminant information in addition to consistent knowledge about this item. We construct the heterogeneous and homogeneous graphs together to learn the dual representations of both user and item, which could capture both the internal association and the relationship between users and items.\n\nHeterogeneous Graph To learn modality-specific user and item representations, we construct a user-item graph for each modality, which is denoted as $\\mathcal { G } _ { m }$ . Following MMGCN [24], we maintain the same graph structure $\\mathcal { G }$ for different $\\mathcal { G } _ { m }$ , but only keep the node features associated with a specific modality $m$ . We adopt LightGCN [7] to encode $\\mathcal { G } _ { m }$ . As shown in [7], LightGCN simplifies the graph convolutional operations by excluding the feature transformation and nonlinear activation modules to improve recommendation performance and meanwhile ease the model optimization process. Specifically, the user and item representations at the $( l + 1 )$ -th graph convolution layer of $\\mathcal { G } _ { m }$ are derived as follows:\n\n$$\n\\pmb { u } _ { m } ^ { ( l + 1 ) } = \\sum _ { i \\in \\mathcal { N } _ { u } } \\frac { 1 } { \\sqrt { | \\mathcal { N } _ { u } | } \\sqrt { | \\mathcal { N } _ { i } | } } \\pmb { i } _ { m } ^ { ( l ) } , \\qquad \\pmb { i } _ { m } ^ { ( l + 1 ) } = \\sum _ { u \\in \\mathcal { N } _ { i } } \\frac { 1 } { \\sqrt { | \\mathcal { N } _ { u } | } \\sqrt { | \\mathcal { N } _ { i } | } } \\pmb { u } _ { m } ^ { ( l ) } ,\n$$\n\nWhere $\\mathcal { N } _ { u }$ and ${ \\mathcal { N } } _ { i }$ are the set of first hop neighbors of $u$ and $_ i$ in $\\mathcal { G } _ { m }$ . $\\mathbf { \\boldsymbol { u } } _ { m } ^ { ( 0 ) }$ is randomly initialized and $\\mathbf { \\Delta } _ { i _ { m } } ^ { ( 0 ) }$ is initialized with $\\mathbf { \\boldsymbol { x } } _ { i } ^ { m }$ . The symmetric normalization $\\frac { 1 } { \\sqrt { | N _ { u } | } \\sqrt { | N _ { i } | } }$ is used to normalize the modality features learned from each layer which avoids the increase of scale when operates the graph convolutional operations.\n\nAfter $L$ layers of data propagation, we combine the representations from every GCN layer using element-wise summation to derive the modality-specific representations for users and items. Formally,\n\n$$\n\\pmb { u } _ { m } = \\sum _ { l = 0 } ^ { L } \\pmb { u } _ { m } ^ { ( l ) } , \\qquad \\quad \\pmb { i } _ { m } = \\sum _ { l = 0 } ^ { L } \\pmb { i } _ { m } ^ { ( l ) } .\n$$\n\nIn such cases, the historical interactions and the modality information have been encoded into the final single-modal representations of users and items. And those operations have been applied to each modality by propagating on the modality-specific user-item bipartite graphs to learn the representations for each modality.\n\nHomogeneous Graph In addition to employing the heterogeneous graph, which encodes the dyadic relation between users and items, we argue that the recommendation performance can be further enhanced by modeling the internal relations between users or items. For the two homogeneous graphs, we preestablished and freeze them to maintain the initial co-occurrence relation and semantic meaning.\n\nUser Co-occurrence Graph. Based on the assumption that users who have interacted with similar items usually have similar preferences. We argue that the user’s preference pattern is hidden inside the co-occurrence items and we construct a homogeneous user co-occurrence graph to learn the internal relations between users. However, the numbers of co-occurrence items between users are in a large range. In a general situation, the user will have a high number of commonly interacted items with a small group of users but with few items with other users. We only consider those who have more commonly interacted items with the user to capture similar preferences. To explicitly model the item cooccurrence patterns of users, we construct a homogeneous user co-occurrence graph $\\ddot { \\mathcal { G } } = \\{ \\mathcal { U } , \\mathcal { P } _ { u } \\}$ , where $\\mathcal { P } _ { u } = \\{ e _ { u , u ^ { \\prime } } | u , u ^ { \\prime } \\in \\mathcal { U } \\}$ denotes the edges between user nodes in $\\ddot { \\mathcal { G } }$ and $e _ { u , u ^ { \\prime } }$ record the number of items that commonly interacted with u and u’.\n\nFor every user $u \\in \\mathcal { U }$ , we retain its top- $k$ users with the highest number of commonly interacted items. Specifically, we keep the edge weight if $u ^ { \\prime }$ $e _ { u , u ^ { \\prime } }$ belongs to the top- $k$ users. Otherwise, the edge weight is $0$ .\n\n$$\ne _ { u , u ^ { \\prime } } = \\left\\{ \\begin{array} { l l } { e _ { u , u ^ { \\prime } } } & { \\mathrm { ~ i f ~ } e _ { u , u ^ { \\prime } } \\in \\mathrm { t o p - k } ( e _ { u } ) , } \\\\ { 0 } & { \\mathrm { ~ o t h e r w i s e } . } \\end{array} \\right.\n$$\n\nAfter establishing the user co-occurrence graph, we incorporate the attention mechanism when performing the graph propagation. The weight used for aggregating neighboring nodes for a user is computed using the softmax function to maximize the effect of neighboring users with a higher number of commonly interacted items. The representation of $u$ learned from $\\ddot { \\mathcal { G } }$ at layer $l + 1$ is denoted as $\\boldsymbol { h } _ { \\boldsymbol { u } } ^ { ( l + 1 ) }$ , which is derived as follows:\n\n$$\n\\pmb { h } _ { u } ^ { ( l + 1 ) } = \\sum _ { u ^ { \\prime } \\in \\mathcal { N } _ { u } } \\frac { e x p ( e _ { u , u ^ { \\prime } } ) } { \\sum _ { \\hat { u } \\in \\mathcal { N } _ { u } } e x p ( e _ { u , \\hat { u } } ) } \\pmb { h } _ { u ^ { \\prime } } ^ { ( l ) } ,\n$$\n\nWhere $e _ { u , u ^ { \\prime } }$ indicates the number of common interacted items between $u$ and $u ^ { \\prime }$ and $\\mathcal { N } _ { u }$ denotes the neighbors of user $u$ in $\\tilde { \\mathcal { G } }$ . In this case, the representation of each user can be enhanced based on neighbors in the co-occurrence graph.\n\nItem Semantic Graph. Multimodal features offer rich and valuable content information about items, but previous studies [23,19] neglect the significant underlying semantic relations of item features. Inspired by [25], we argue that item features are objective and we could establish the modality-specific homogeneous item graphs based on the raw features to learn the internal relations between items. Specifically, we construct the modality-aware item semantic graph $\\hat { \\mathcal { G } } _ { m } = \\{ \\mathcal { T } , \\mathcal { P } _ { m } ^ { i } \\}$ for each modality $m$ , where $\\mathcal { P } _ { m } ^ { \\iota } = \\{ e _ { i , i ^ { \\prime } } ^ { m } | i , i ^ { \\prime } \\in \\mathcal { I } \\}$ denotes the edges between item nodes in $\\hat { \\mathcal { G } } _ { m }$ . For an edge $e _ { i , i ^ { \\prime } }$ , its weight is calculated by the cosine similarity between original modality features of $\\pmb { x } _ { i } ^ { m }$ and $\\pmb { x } _ { i ^ { \\prime } } ^ { m }$\n\n$$\ne _ { i , i ^ { \\prime } } ^ { m } = \\frac { ( \\pmb { x } _ { i } ^ { m } ) ^ { \\top } \\pmb { x } _ { i ^ { \\prime } } ^ { m } } { \\| \\pmb { x } _ { i } ^ { m } \\| \\| \\pmb { x } _ { i ^ { \\prime } } ^ { m } \\| } .\n$$\n\nThe derived $\\hat { \\mathcal { G } } _ { m }$ is a fully connected graph where edge weights are calculated based on the similarity scores of modality features of connected nodes. Next, we make the graph sparse by retaining the top- $k$ similar items of every item. As the $\\hat { \\mathcal { G } } _ { m }$ is a weighted graph, we convert it into an unweighted graph that captures the fundamental relation structure of the most related items [3]. Formally,\n\n$$\ne _ { i , i ^ { \\prime } } ^ { m } = \\left\\{ \\begin{array} { l l } { 1 } & { \\mathrm { ~ i f ~ } e _ { i , i ^ { \\prime } } ^ { m } \\in \\mathrm { t o p  – k } ( e _ { i } ^ { m } ) , } \\\\ { 0 } & { \\mathrm { ~ o t h e r w i s e } . } \\end{array} \\right.\n$$\n\nSince we get one item semantic graph for each modality, we combine them by performing weighted summation based on the importance score $\\alpha _ { m }$ that indicates the contribution of each modality and the summation is $1$ . Formally, $\\hat { \\mathcal { G } } = \\{ \\mathcal { T } , \\mathcal { P } _ { i } \\}$ , where $\\mathcal { P } _ { i } = \\{ e _ { i , i ^ { \\prime } } | i , i ^ { \\prime } \\in \\mathcal { I } \\}$ and $\\begin{array} { r } { e _ { i , i ^ { \\prime } } = \\sum _ { m \\in M } \\alpha e _ { i , i ^ { \\prime } } ^ { m } } \\end{array}$ .\n\nAfter establishing the item semantic graph, we apply the graph convolution operation on it to capture the item-item relationship:\n\n$$\nh _ { i } ^ { ( l + 1 ) } = \\sum _ { i ^ { \\prime } \\in \\mathcal { N } _ { i } } e _ { i , i ^ { \\prime } } h _ { i ^ { \\prime } } ^ { ( l ) } ,\n$$\n\nWhere ${ \\mathcal { N } } _ { i }$ denotes the neighbors of item $i$ in $\\mathcal { G } ^ { i }$ . Both $ { h _ { u } ^ { ( 0 ) } }$ and ${ h _ { i } ^ { ( 0 ) } }$ are initialized with their fused representations ${ \\pmb u } _ { f }$ and ${ \\dot { \\pmb { \\imath } } } _ { f }$ , which are introduced in the following section.\n\n# 3.3 Multimodal Fusion\n\nAn important factor influencing multimodal recommendation accuracy is multimodal fusion. As we mentioned in Section 1, previous multimodal recommendation models utilizing single-modal information perform better than multimodal information. We speculate their fusion methods may fail to capture modalityspecific characteristics and even corrupt the learned single-modality representation. We intend to learn the multi-modality that can capture complementary information which can not be contained in the single modality. Specifically, to fuse the single modal features derived from modality-specific user-item graphs, we apply the Attentive Concatenation for user multimodal embeddings that capture the user preference on different modalities and direct concatenation for item multimodal embeddings. The attention weight $\\alpha$ for users is initialized to 0.5. Formally,\n\n$$\n\\begin{array} { r } { \\pmb { u } _ { f } = \\alpha \\pmb { u } _ { v } \\parallel ( 1 - \\alpha ) \\pmb { u } _ { t } , \\qquad \\pmb { i } _ { f } = \\pmb { i } _ { v } \\parallel i _ { t } , } \\end{array}\n$$\n\nWhere $\\parallel$ denotes the concatenation operation. By performing attentive concatenation, we assume the single modality representations carry the richest information for each modality and this operation can capture the intact complementary information from each modality.\n\n# 3.4 Integration with Dual Representations\n\nWe integrate representations of users and items learned from heterogeneous (i.e., Modality-specific User-Item Graphs) and homogeneous (i.e., User Co-occurrence Graph $\\&$ Item Semantic Graph) graphs to form their dual representations such that interactions between users and items and their internal relations can be well captured. We perform element-wise summation on the outputs learned from the three graphs to generate the dual representations for $u$ and $_ i$ :\n\n$$\nz _ { u } = { \\pmb u } _ { f } + { \\pmb h } _ { u } ^ { L _ { u } } , \\qquad z _ { i } = i _ { f } + { \\pmb h } _ { i } ^ { L _ { i } } ,\n$$\n\nWhere $z _ { u }$ and $z _ { i }$ denote the final representations of user $u$ and item $_ i$ . $L _ { u }$ and $L _ { i }$ denote the number of GCN layers for the user co-occurrence graph and item semantic graph respectively.\n\n# 3.5 Optimization\n\nTo optimize the parameters of DRAGON for the recommendation task, we leverage the Bayesian Personalized Ranking (BPR) loss [17], which aims to score higher for the positive item than the negative one. We construct a triplet set $\\mathcal { R }$ including the triplet $( u , i , j )$ for each user $u$ with the positive item $i$ and a randomly sampled negative item $j$ that has no interactions with $u$ . The loss function $\\mathcal { L } _ { r e c }$ is defined as follows,\n\n$$\n\\begin{array} { l } { \\displaystyle \\mathcal { R } = \\{ ( u , i , j ) | ( u , i ) \\in \\mathcal { E } , ( u , j ) \\notin \\mathcal { E } \\} , } \\\\ { \\displaystyle \\mathcal { L } _ { r e c } = \\sum _ { ( u , i , j ) \\in \\mathcal { R } } - \\mathrm { l n } \\sigma ( y _ { u i } - y _ { u j } ) + \\lambda \\| \\Theta \\| _ { 2 } , } \\\\ { \\displaystyle ( u , i , j ) \\in \\mathcal { R } } \\end{array}\n$$\n\nWhere $y _ { u i } = z _ { u } ^ { \\top } z _ { i }$ calculates the inner product of $z _ { u }$ and $z _ { i }$ , $\\lambda$ is the $L _ { 2 }$ regularization weight, and $\\Theta$ denotes model parameters.",
  "experiments": "",
  "hyperparameter": " - Learning rate: Controls the step size in optimization, typical range {1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1}, with 1e-4 being optimal.\n- Regularization weight (λ): Controls L2 regularization to prevent overfitting, typical range {1e-5, 1e-4, 1e-3, 1e-2, 1e-1}.\n- Number of GCN layers in heterogeneous graph (L): Number of graph convolution layers for modality-specific user-item graphs, fixed to 2.\n- Number of GCN layers in homogeneous graphs (L_u and L_i): Number of layers for user co-occurrence graph and item semantic graph, fixed to 1.\n- Top-k in user co-occurrence graph: Number of top users to retain based on common interactions, set to 10.\n- Modality weight in item semantic graph (α_m): Weight for combining modality-specific item graphs, fixed to 0.1 for image."
}
