{
  "id": "LATTICE_2021",
  "paper_title": "Mining Latent Structures for Multimedia Recommendation",
  "alias": "LATTICE",
  "year": 2021,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 2 THE PROPOSED METHOD\n\nIn this section, we first formulate the problem and introduce our model in detail. As illustrated in Figure 2, there are three main components in our proposed framework: (1) a modality-aware graph structure learning layer that learns item graph structures from multimodal features and fuses multimodal graphs, (2) graph convolutional layers that learn the embeddings by injecting item-item affinities, and (3) downstream CF methods.\n\n# 2.1 Preliminaries\n\nLet $\\mathcal { U } , \\mathcal { I }$ denote the set of users and items, respectively. Each user $u \\in \\mathcal { U }$ is associated with a set of items ${ \\boldsymbol { \\mathit { I } } } ^ { u }$ with positive feedbacks which indicate the preference score $y _ { u i } = 1$ for $i \\in \\mathcal { I } ^ { u }$ . $\\boldsymbol { x } _ { u } , \\boldsymbol { x } _ { i } \\in \\mathbb { R } ^ { d }$ is the input ID embedding of $u$ and $i$ , respectively, where $d$ is the embedding dimension. Besides user-item interactions, multimodal features are offered as content information of items. We denote the modality features of item ?? as $\\pmb { e } _ { i } ^ { m } \\in \\mathbb { R } ^ { d _ { m } }$ , where $d _ { m }$ denotes the dimension of the features, $m \\in { \\mathcal { M } }$ is the modality, and $M$ is the set of modalities. The purpose of multimodal recommendation is to accurately predict users’ preferences by ranking items for each user according to predicted preference scores $\\hat { y } _ { u i }$ . In this paper, we consider visual and textual modalities denoted by ${ \\mathcal { M } } = \\{ \\mathrm { v } , \\mathrm { t } \\}$ . Please kindly note that our method is not fixed to the two modalities and multiple modalities can be involved.\n\n# 2.2 Modality-aware Latent Structure Learning\n\nMultimodal features provide rich and meaningful content information of items, while existing methods only utilize multimodal features as side information for each item, ignoring the important semantic relationships of items underlying features. In this section, we introduce how to discover the underlying latent graph structure of item graphs in order to learn better item representations. To be specific, we first construct initial $k$ -nearest-neighbor $( k \\mathrm { N N } )$ modality-aware item graphs by utilizing raw multimodal features. After that, we learn the latent graph structures from transformed, high-level features based on the initial graph. Finally, we integrate the latent graphs from multiple modalities in an adaptive way.\n\n2.2.1 Constructing initial ??NN modality-aware graphs. We first construct initial $k \\mathbf { N N }$ modality-aware graph $s ^ { m }$ by using raw features for each modality $m$ . Based on the hypothesis that similar items are more likely to interact than dissimilar items [27], we quantify the semantic relationship between two items by their similarity.\n\n![](images/40027bf323e68a512064e4495b3448e7c5d2b3e0665c627b19753f345054eb31.jpg)  \nFigure 2: The overall framework of our proposed LATTICE model. We first learn modality-aware item graphs and aggregate multiple modalities in an adaptive manner. Based on the mined latent graphs, we conduct graph convolutions to inject high-order item relationships into item embeddings, which are then combined with downstream CF methods to make recommendations.\n\nCommon options for node similarity measurement include cosine similarity [37], kernel-based functions [22], and attention mechanisms [5]. Our method is agnostic to similarity measurements, and we opt to the simple and parameter-free cosine similarity in this paper. The similarity matrix $S ^ { m } \\in \\mathbb { R } ^ { N \\times N }$ is computed by\n\n$$\nS _ { i j } ^ { m } = \\frac { ( \\pmb { e } _ { i } ^ { m } ) ^ { \\top } \\pmb { e } _ { j } ^ { m } } { \\| \\pmb { e } _ { i } ^ { m } \\| \\| \\pmb { e } _ { j } ^ { m } \\| } .\n$$\n\nTypically, the graph adjacency matrix is supposed to be non-negative but $s _ { i j }$ ranges between $[ - 1 , 1 ]$ . Thus, we suppress its negative entries to zeros. Moreover, common graph structures are much sparser other than a fully-connected graph, which is computationally demanding and might introduce noisy, unimportant edges [5]. For this purpose, we conduct $k \\mathbf { N N }$ sparsification [2] on the dense graph: for each item $i$ , we only keep edges with the top- $\\mathbf { \\nabla } \\cdot \\mathbf { k }$ confidence scores:\n\n$$\n\\widehat { S } _ { i j } ^ { m } = \\left\\{ \\begin{array} { l l } { S _ { i j } ^ { m } , \\ } & { S _ { i j } ^ { m } \\in \\mathrm { t o p } ^ { - } k ( S _ { i } ^ { m } ) , } \\\\ { 0 , \\ } & { \\mathrm { o t h e r w i s e } , } \\end{array} \\right.\n$$\n\nwhere ${ \\widehat { s } } ^ { m }$ is the resulting sparsified, directed graph adjacency matrix. To alleviate the exploding or vanishing gradient problem [21], we normalize the adjacency matrix as:\n\n$$\n\\widetilde { S } ^ { m } = ( D ^ { m } ) ^ { - \\frac { 1 } { 2 } } \\widehat { S } ^ { m } ( D ^ { m } ) ^ { - \\frac { 1 } { 2 } } ,\n$$\n\nwhere $D ^ { m } \\in \\mathbb { R } ^ { N \\times N }$ is the diagonal degree matrix of ${ \\widehat { s } } ^ { m }$ and $D _ { i i } ^ { m } =$ $\\textstyle \\sum _ { j } { \\widehat { S } } _ { i j } ^ { m }$ .\n\n2.2.2 Learning latent structures. Although we have obtained the modality-aware initial graph structures ${ \\widetilde { s } } ^ { m }$ by utilizing raw multimodal features, they may not be ideal for the recommendation task. This is because the raw multimodal features are often noisy or even incomplete due to the inevitably error-prone data measurement or collection. Additionally, initial graphs are constructed from the original multimodal features, which may not reflect the genuine graph structures after feature extraction and transformation. To this end, we propose to dynamically learn the graph structures by transformed, high-level multimodal features and combine learned structures with initial structures.\n\nFirstly, we transform raw modality features into high-level features $\\widetilde { \\pmb { e } } _ { i } ^ { m }$ :\n\n$$\n\\widetilde { \\pmb { e } } _ { i } ^ { m } = W _ { m } \\pmb { e } _ { i } ^ { m } + b _ { m } ,\n$$\n\nwhere $W _ { m } \\in \\mathbb { R } ^ { d ^ { \\prime } \\times d _ { m } }$ and $\\pmb { b } _ { m } \\in \\mathbb { R } ^ { d ^ { \\prime } }$ denote the trainable transformation matrix and bias vector, respectively. $d ^ { \\prime }$ is the dimension of high-level feature vector $\\widetilde { \\pmb { e } } _ { i } ^ { m }$ . We then dynamically infer the graph structures utilizing $\\widetilde { \\pmb { e } } _ { i } ^ { m }$ e, repeat the graph learning process described in Eqs. (1, 2, 3) and obtain the adjacency matrix $\\widetilde { A } ^ { m }$ .\n\nAlthough the initial graph could be noisy, it typically still carries rich and useful information regarding item graph structures. Also, drastic change of adjacency matrix will lead to unstable training. To keep rich information of initial item graph and stabilize the training process, we add a skip connection that combines the learned graph with the initial graph:\n\n$$\nA ^ { m } = \\lambda { \\widetilde s } ^ { m } + ( 1 - \\lambda ) { \\widetilde A } ^ { m } ,\n$$\n\nwhere $\\lambda \\in ( 0 , 1 )$ is the coefficient of skip connection that controls the amount of information from the initial structure. The obtained $A ^ { m }$ is the final graph adjacency matrix representing latent structures for modality $m$ . It is worth mentioning that both ${ \\widetilde { s } } ^ { m }$ and $\\widetilde { A } ^ { m }$ are sparsified and normalized matrices, thus the final adjacency matrix $A ^ { m }$ is also sparsified and normalized, which is computationally efficient and stabilizes gradients.\n\n2.2.3 Aggregating multimodal latent graphs. After we obtained modality-aware adjacency matrix $A ^ { m }$ for each modality $m \\in { \\mathcal { M } }$ , in this section, we explore how to fuse different modalities to compute the final latent structures. In multimedia recommendation, users usually focus on different modalities in different scenarios. For example, one may pay more attention to visual modality when selecting clothes, while focusing more on textual information when picking books, we thus introduce learnable weights to assign different importance scores to modality-specific graphs in an adaptive way:\n\n$$\nA = \\sum _ { m = 0 } ^ { | { \\cal M } | } \\alpha _ { m } A ^ { m } ,\n$$\n\nwhere $\\alpha _ { m }$ is the importance score of modality $m$ and $\\pmb { A } \\in \\mathbb { R } ^ { N \\times N }$ is the graph that represents multimodal item relationships. We apply the softmax function to keep the adjacency matrix $A$ normalized, such that Í|M |??=0 ?? $\\begin{array} { r } { \\sum _ { m = 0 } ^ { | { \\cal M } | } \\alpha _ { m } = 1 } \\end{array}$\n\n# 2.3 Graph Convolutions\n\nAfter obtained the latent structures, we perform graph convolution operations to learn better item representations by injecting itemitem affinities into the embedding process. Graph convolutions can be treated as message propagating and aggregation. Through propagating the item representations from its neighbors, one item can aggregate the information within the first-order neighborhood. Furthermore, by stacking multiple graph convolutional layers, the high-order item-item relationships can be captured.\n\nFollowing He et al. [14], Wu et al. [40], we employ simple message passing and aggregation without feature transformation and non-linear activations which is effective and computationally efficient. In the $l$ -th layer, the message passing and aggregation could be formulated as:\n\n$$\n\\pmb { h } _ { i } ^ { ( l ) } = \\sum _ { j \\in N ( i ) } A _ { i j } \\pmb { h } _ { j } ^ { ( l - 1 ) } ,\n$$\n\nwhere $N ( i )$ is the neighbor items and $\\pmb { h } _ { i } ^ { ( l ) } \\in \\mathbb { R } ^ { d }$ is the $l$ -th layer item representation of item ??. We set the input item representation ${ \\pmb h } _ { i } ^ { ( 0 ) }$ as its corresponding $\\mathrm { I D }$ embedding vector $x _ { i }$ . We utilize $\\mathrm { I D }$ embeddings of items as input representations rather than multimodal features, since we employ graph convolutions in order to directly capture item-item affinities. After stacking encodes the high-order item-item relationships that a $L$ layers,  constr ${ \\pmb h } _ { i } ^ { ( L ) }$ by multimodal information and thus can benefit the downstream CF methods.\n\n# 2.4 Combining with Collaborative Filtering\n\nDifferent from previous attempts which design sophisticated useritem aggregation strategies, LATTICE learns item representations from multimodal features and then combine them with downstream CF methods that model user-item interactions. It is flexible and could be served as a play-and-plug module for any CF methods.\n\nWe denote the output user and item embeddings from CF methods as $\\widetilde { \\pmb { x } } _ { u } , \\widetilde { \\pmb { x } } _ { i } \\in \\mathbb { R } ^ { d }$ and simply enhance item embeddings by adding normalized item embeddings ${ \\pmb h } _ { i } ^ { ( L ) }$ learned through item graph:\n\n$$\n\\widehat { \\pmb { x } } _ { i } = \\widetilde { \\pmb { x } } _ { i } + \\frac { \\pmb { h } _ { i } ^ { ( L ) } } { \\lVert \\pmb { h } _ { i } ^ { ( L ) } \\rVert _ { 2 } } .\n$$\n\nWe then compute the user-item preference score by taking inner product of user embeddings and enhanced item embeddings:\n\n$$\n\\widehat { y } _ { u i } = \\widetilde { x } _ { u } ^ { \\top } \\widehat { x } _ { i } .\n$$\n\nTable 1: Statistics of the datasets   \n\n<table><tr><td>Dataset1</td><td>#Users</td><td># Items</td><td># Interactions</td><td>Density</td></tr><tr><td>Clothing</td><td>39,387</td><td>23,033</td><td>237,488</td><td>0.00026</td></tr><tr><td>Sports</td><td>35,598</td><td>18,357</td><td>256,308</td><td>0.00039</td></tr><tr><td>Baby</td><td>19,445</td><td>7,050</td><td>139,110</td><td>0.00101</td></tr></table>\n\n1 Datasets can be accessed at http://jmcauley.ucsd.edu/data/amazon/links. html.\n\nWe conduct experiments in Section 3.3 on different downstream CF methods. The play-and-plug paradigm separates the usage of multimodal features with user-item interactions, thus alleviating the cold-start problem, where tailed items are only interacted with few users or even never interacted with users. We learn latent structures for items and the tailed items will get similar feedbacks from relevant neighbors through neighborhood aggregation. We conduct experiments in cold-start settings in Section 3.2 which proves the effectiveness of this play-and-plug paradigm.\n\n# 2.5 Optimization\n\nWe adopt Bayesian Personalized Ranking (BPR) loss [30] to compute the pair-wise ranking, which encourages the prediction of an observed entry to be higher than its unobserved counterparts:\n\n$$\n\\mathcal { L } _ { \\mathrm { B P R } } = - \\sum _ { u \\in \\mathcal { U } } \\sum _ { i \\in \\mathcal { I } _ { u } } \\sum _ { j \\notin \\mathcal { I } _ { u } } \\ln \\sigma \\left( \\hat { y } _ { u i } - \\hat { y } _ { u j } \\right) ,\n$$\n\nwhere ${ \\boldsymbol { \\mathit { I } } } ^ { u }$ indicates the observed items associated with user $u$ and $( u , i , j )$ denotes the pairwise training triples where $i \\in \\mathcal { I } ^ { u }$ is the positive item and $j \\notin \\mathcal { I } ^ { u }$ is the negative item sampled from unobserved interactions. $\\sigma ( \\cdot )$ is the sigmoid function.",
  "experiments": "",
  "hyperparameter": "- embedding_dim d: Dimensionality of user and item ID embeddings used in CF backbones and graph convolutions. Controls the capacity of latent representations. In the experiments it is fixed to 64 for all models.\n- k (k-NN neighbors): Number of nearest neighbors kept for each item when constructing sparsified modality-specific k-NN graphs from multimodal features. Controls graph sparsity and the amount of information aggregated from neighbors; k = 0 degenerates to the base CF model without item–item graphs, while moderate k yields the best performance.\n- lambda (skip connection coefficient): Weight in (0, 1) that balances the initial graph (built from raw features) and the learned graph (from transformed features) when forming the final modality-specific adjacency A^m. lambda = 0 uses only the learned graph; lambda = 1 uses only the initial graph; intermediate values provide a trade-off between stability and denoising.\n- learning_rate: Step size for Adam optimization when training LATTICE and baselines. Tuned on the validation set among {0.0001, 0.0005, 0.001, 0.005}.\n- l2_reg (L2 regularization coefficient): Strength of L2 weight decay used to regularize model parameters, helping prevent overfitting. Searched over {0, 1e-5, 1e-4, 1e-3}.\n- dropout_ratio: Dropout rate applied in the model to regularize training and mitigate overfitting. Tuned over {0.0, 0.1, ..., 0.8}.\n- num_gcn_layers L: Number of stacked graph convolution layers on the latent item graph. Controls how many hops of item–item relations are encoded (higher L captures higher-order affinities but may introduce noise or oversmoothing). Chosen as a small integer based on validation performance.\n- batch_size: Number of training triples in each mini-batch for BPR optimization. Fixed to 1024 in the reported experiments."
}
