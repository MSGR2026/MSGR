{
  "id": "GRCN_2020",
  "paper_title": "Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback",
  "alias": "GRCN",
  "year": 2020,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 2 METHODOLOGY\n\n# 2.1 Preliminary\n\nSuppose there are numbers of historical interaction records (i.e. implicit feedback) between users and items. We collect a set $\\boldsymbol { \\mathcal U }$ of $N$ users and a set $\\boldsymbol { \\mathcal { T } }$ of $M$ items from the records. Beyond the interaction signal, the multimodal features of items are extracted from their content involving the visual, acoustic, and textual modalities, which are denoted as ??, ??, and $t$ , respectively. For a item $i \\in \\mathcal { I }$ , we denote its feature vector as $\\mathbf { i } _ { m } \\in \\mathbf { \\bar { \\mathcal { R } } } ^ { M \\times D _ { m } }$ , where $m \\in \\mathcal { M } = \\{ v , a , t \\}$ is the indicator of multiple modalities and $D _ { m }$ is the dimension of the vector.\n\nTo conduct the graph convolutional operations, we construct a user-item interaction graph $\\mathcal { G } = \\{ \\mathbf { E } , \\mathbf { A } \\}$ , which follows the GCNbased recommendation [33, 38]. In particular, $\\textbf { E } \\in \\ \\mathcal { R } ^ { D \\times ( N + M ) }$ denotes the trainable embedding matrix of nodes (i.e. users and items), where $D$ represents the dimension of the embedding. And, $\\mathbf { A } \\in \\mathcal { R } ^ { N \\times M }$ is the symmetric matrix reflecting the connections of user and item pairs. Given a user $u \\in \\mathcal { U }$ and a item $i \\in \\mathcal { I }$ , we denote ${ \\bf A } _ { u , i } = 1$ if $u$ has interacted with $i$ ; otherwise, $\\mathbf { A } _ { u , i } = 0$ .\n\n# 2.2 Model Framework\n\nIn this section, we detail our proposed model. As illustrated in Figure 2, the model consists of three components: 1) the graph refining layer that adjusts the graph structure by identifying and pruning the noisy edges in interaction graph; 2) the graph convolutional layer which performs the graph convolutional operations on the refined graph to enrich the embeddings of items and users; and 3) the prediction layer that infers the interaction of each user and item pair.\n\n2.2.1 Graph Refining Layer. To refine the structure of constructed interaction graph, we work under the reasonable assumption that the content of item belonging to false-positive interaction is far from the user preference. Therefore, we introduce the prototypical network to learn user preference to the content information, and then prune the noisy edges according to the confidence of edges being the false-positive interactions.\n\n![](images/7ed80cffbcb46e2624772165a13d2b157ab990f764ddbab239c3fdda2342b882.jpg)  \nFigure 2: Schematic illustration of our proposed model. It consists of three components, namely graph refining layer, graph convolutional layer, and prediction layer.\n\nPrototypical Network. Intuitively, each user preference could be learned from the content of items which directly connect to the user node in the user-item graph. However, since there are some noisy edges in the graph, it is hard to immediately model the user preference with the neighbor nodes. Inspired by the idea of prototype learning [29], we regard the user preference as her/his prototype in a metric space and harness a prototypical network to approach it.\n\nFor this goal, the content signal of item is projected into a metric space to distill the informative features related to the user preference, as\n\n$$\n\\bar { \\mathbf { i } } _ { m } = l e a k y \\_ r e l u ( \\mathbf { W } _ { m } \\mathbf { i } _ { m } + \\mathbf { b } _ { m } )\n$$\n\nwhere ??????????_???????? (·), $\\mathbf { W } _ { m } \\in \\mathbb { R } ^ { D ^ { \\prime } \\times D _ { m } }$ and $ { \\mathbf { b } } _ { m } \\in \\mathbb { R } ^ { D ^ { \\prime } \\times 1 }$ denote the activation function [25], trainable weight matrix and bias vector, respectively. And $D ^ { \\prime }$ is the dimension of distilled feature vector $\\bar { \\mathbf i } _ { m }$\n\nThen, we introduce the neighbor routing mechanism [24] into prototypical network, to approach the prototype w.r.t. representation of user preference. Given a user, with the iterative routing operations, her/his representation is adjusted by jointly analyzing her/his similarities to its neighbors. To facilitate the description, we elaborate on the process in the single modality and do the same operations on the others.\n\nIn the initial iteration, we define a trainable vector $\\mathbf { u } _ { ( 0 ) }$ to represent the preference of user $u \\in \\mathcal { U }$ . And, we conduct the inner product between user preference and item features to calculate their similarity, formally,\n\n$$\np _ { u , i } = \\frac { \\exp ( \\bar { \\mathbf { i } } ^ { \\mathsf { T } } \\mathbf { u } _ { ( 0 ) } ) } { \\sum _ { j \\in { \\cal N } ( u ) } \\exp ( \\bar { \\mathbf { j } } ^ { \\mathsf { T } } \\mathbf { u } _ { ( 0 ) } ) } ,\n$$\n\nwhere $\\mathbf { \\Delta } _ { \\mathcal { P } u , i }$ denotes the similarity between $u$ and $i .$ . A higher value suggests that the content signal more informative to the user\n\npreference modeling. In addition, $N ( u )$ is used to represent the set of neighbors of node $u$ in the user-item graph.\n\nFollowing this, we tune the representation of user preference in the metric space via combining the weighted sum of its neighbors’ feature vectors. It is formulated as,\n\n$$\n\\mathbf { u } _ { ( 1 ) } = \\mathbf { u } _ { ( 0 ) } + \\sum _ { i \\in N ( u ) } p _ { u , i } \\bar { \\mathbf { i } } ,\n$$\n\nwhere $\\mathbf { u } _ { ( 1 ) }$ is the user representation after one iteration operation. Moreover, we normalize it to avoid its scale of increasing with iterative operations.\n\nWith the iteration $t = 2 , \\ldots , T$ , based on the output of previous iteration, the user representation is adjusted towards the prototype of her/his preference, which is recursively formulated as:\n\n$$\n\\begin{array} { r } { \\left\\{ { \\mathbf { u } } _ { ( t ) } = { \\mathbf { u } } _ { ( t - 1 ) } + \\sum _ { i \\in N ( u ) } \\mathcal { P } { u } _ { i } i \\bar { \\mathbf { i } } , } \\\\ { p _ { u , i } = \\frac { \\exp ( \\bar { \\mathbf { i } } ^ { \\mathsf { T } } \\mathbf { u } _ { ( t - 1 ) } ) } { \\sum _ { j \\in N ( u ) } \\exp ( \\bar { \\mathbf { j } } ^ { \\mathsf { T } } \\mathbf { u } _ { ( t - 1 ) } ) } . \\right. } \\end{array}\n$$\n\nFinally, it outputs user preference to the item content, as u¯ = u(?? ) . In what follows, we use $\\bar { \\bf { u } }$ to denote the user preference to the content information.\n\nPruning Operations. To identify noisy edges, we score the affinity between user preference and item content to measure the confidence of the corresponding edge being true-positive interaction in each modality. Then, we integrate the scores of each edge in multiple modalities to yield the weight and assign it to the edge, which implements the pruning operations in a soft manner.\n\nFor each modality, with the obtained user preference and distilled item features, we calculate the relative distances between them in two directions. It is formulated as,\n\n$$\n\\{ \\begin{array} { l l } { \\bar { s } _ { u  { i } } ^ { m } = \\frac { \\exp ( \\bar { \\mathbf { u } } _ { m } ^ { \\top } \\bar { \\mathbf { i } } _ { m } ) } { \\sum _ { j \\in \\mathcal { N } ( u ) } \\exp ( \\bar { \\mathbf { u } } _ { m } ^ { \\top } \\bar { \\mathbf { j } } _ { m } ) } , } \\\\ { ~ } \\\\ { \\bar { s } _ { i  { u } } ^ { m } = \\frac { \\exp ( \\bar { \\mathbf { i } } _ { m } ^ { \\top } \\bar { \\mathbf { u } } _ { m } ) } { \\sum _ { v \\in \\mathcal { N } ( i ) } \\exp ( \\bar { \\mathbf { i } } _ { m } ^ { \\top } \\bar { \\mathbf { v } } _ { m } ) } , } \\end{array} \n$$\n\nwhere $\\bar { s } _ { u  i } ^ { m }$ and $\\bar { s } _ { i  u } ^ { m }$ are the scores reflecting the affinities between $\\bar { \\mathbf { u } } _ { m }$ and $\\bar { \\mathbf i } _ { m }$ in $m$ -th modality.\n\nTo integrate the multimodal scores, we define a base vector for each user or item, as follows:\n\n$$\n\\begin{array} { r } { \\rho = [ \\rho ^ { v } , \\rho ^ { a } , \\rho ^ { t } ] , } \\end{array}\n$$\n\nwhere $\\rho$ denotes the base vector. Elements of the user’s base vector are used to measure her/his relative preferences to the different modalities. For the item’s base vector, each element represents the importance of content signal in the corresponding modality to the item representation.\n\nIncorporated base vectors, the weights for the edges are computed by fusing the multimodal scores, as\n\n$$\n\\begin{array} { r } { \\{ { s _ { u  i } } = \\operatorname* { m a x } ( \\rho _ { u } ^ { v } \\bar { s } _ { u  i } ^ { v } , \\rho _ { u } ^ { a } \\bar { s } _ { u  i } ^ { a } , \\rho _ { u } ^ { t } \\bar { s } _ { u  i } ^ { t } ) ,  } \\\\ {  { s _ { i  u } } = \\operatorname* { m a x } ( \\rho _ { i } ^ { v } \\bar { s } _ { i  u } ^ { v } , \\rho _ { i } ^ { a } \\bar { s } _ { i  u } ^ { a } , \\rho _ { i } ^ { t } \\bar { s } _ { i  u } ^ { t } ) .  } \\end{array}\n$$\n\nwhere $\\mathrm { m a x } ( \\cdot )$ denotes maximization operation selecting the max value. Besides, the combination operation is also able to implement in different forms, such as mean and maximization operations without base values.\n\nIn summary, with the base vector and obtained affinity scores, we achieve the weight for each edge to softly prune the noisy edge.\n\n2.2.2 Graph Convolutional Layer. Following the mainstream of GCN-based models [2, 33], we treat the graph convolutional operations as the message passing and aggregation. Using the graph convolutional operations, we could model the collaborative signal conveyed by user-item interaction graph. Further, by running the stacked graph convolutional layers, the high-order connectivity information is captured and aggregated. Towards the implicit feedback, the obtained weights for the edges are used to control the passed message. In particular, it corrupts the propagation of noise signal from false-positive interaction.\n\nFormally, in the $l$ -th layer, the message passing and aggregation could be formulated as,\n\n$$\n\\begin{array} { r } { \\{ \\mathbf { e } _ { u } ^ { ( l ) } = \\sum _ { i \\in \\mathcal { N } ( u ) } s _ { u  i } \\mathbf { e } _ { i } ^ { ( l - 1 ) } ,  } \\\\ {  [ \\mathbf { e } _ { i } ^ { ( l ) } = \\sum _ { u \\in \\mathcal { N } ( i ) } s _ { i  u } \\mathbf { e } _ { u } ^ { ( l - 1 ) } .  } \\end{array}\n$$\n\nwhere $\\mathbf { e } \\in \\mathbb { R } ^ { D \\times 1 }$ denotes the corresponding ID embedding vector. With this operation, we collect the collaborative signal from $l$ -hop neighbors.\n\nStacking L layers, we obtain the embedding at each layer and integrate them:\n\n$$\n\\mathbf { e } _ { u } = \\sum _ { l = 0 } ^ { L } \\mathbf { e } _ { u } ^ { ( l ) } , \\mathbf { e } _ { i } = \\sum _ { l = 0 } ^ { L } \\mathbf { e } _ { i } ^ { ( l ) } .\n$$\n\nWhereinto, e(0)?? and ${ \\bf e } _ { i } ^ { ( 0 ) }$ denote the initial ID embeddings from the embedding matrix , respectively. The enriched embeddings (i.e. and $\\mathbf { e } _ { i }$ ) are constituted by combing the embeddings from 0-th layer to $L$ -th layer. It encodes and injects the high-order connectivity information into the embedding of each node to enhance the representativeness.\n\nTable 1: Summary of the datasets. The dimensions of visual, acoustic, and textual modalities are denoted by V, A, and T, respectively.   \n\n<table><tr><td rowspan=1 colspan=1>Dataset</td><td rowspan=1 colspan=1>#Interactions</td><td rowspan=1 colspan=2>#Items#Users</td><td rowspan=1 colspan=1>Sparsity</td><td rowspan=1 colspan=1>V</td><td rowspan=1 colspan=1>A</td><td rowspan=1 colspan=1>T</td></tr><tr><td rowspan=2 colspan=1>MovielensTiktok</td><td rowspan=1 colspan=1>1,239,508</td><td rowspan=1 colspan=1>5,986</td><td rowspan=1 colspan=1>55,485</td><td rowspan=1 colspan=1>99.63%</td><td rowspan=1 colspan=1>2,048</td><td rowspan=1 colspan=1>128</td><td rowspan=1 colspan=1>100</td></tr><tr><td rowspan=1 colspan=1>726,065</td><td rowspan=1 colspan=1>76,085</td><td rowspan=1 colspan=1>36,656</td><td rowspan=1 colspan=1>99.97%</td><td rowspan=1 colspan=1>128</td><td rowspan=1 colspan=1>128</td><td rowspan=1 colspan=1>128</td></tr><tr><td rowspan=1 colspan=1>Kwai</td><td rowspan=1 colspan=1>298,492</td><td rowspan=1 colspan=1>86,483</td><td rowspan=1 colspan=1>7,010</td><td rowspan=1 colspan=1>99.95%</td><td rowspan=1 colspan=1>2,048</td><td rowspan=1 colspan=1>-</td><td rowspan=1 colspan=1>1</td></tr></table>\n\n2.2.3 Prediction Layer. To gain the representation of each user or item, we follow the idea that users have varying preferences in different modalities [38]. Specifically, we concatenate the multimodal features and the enriched ID embedding as a whole vector, formally,\n\n$$\n\\left\\{ \\begin{array} { l l } { \\mathbf { e } _ { u } ^ { * } = \\mathbf { e } _ { u } \\parallel \\bar { \\mathbf { u } } _ { v } \\parallel \\bar { \\mathbf { u } } _ { \\alpha } \\parallel \\bar { \\mathbf { u } } _ { t } , } \\\\ { \\mathbf { e } _ { i } ^ { * } = \\mathbf { e } _ { i } \\parallel \\bar { \\mathbf { i } } _ { v } \\parallel \\bar { \\mathbf { i } } _ { \\alpha } \\parallel \\bar { \\mathbf { i } } _ { t } , } \\end{array} \\right.\n$$\n\nwhere the symbol $| |$ means the concatenation operation.\n\nBeyond the collaborative signals, the representation contains the user preference to the item content, which contributes to inferring the interaction between users and items.\n\nFinally, we conduct the inner product between user and item representations, as\n\n$$\n\\begin{array} { r } { y _ { u , i } = e _ { u } ^ { \\ast } \\mathsf { T } e _ { i } ^ { \\ast } , } \\end{array}\n$$\n\nwhere the output $y _ { u , i }$ is used to estimate the user’s preference towards the target item. A higher score suggests that the user prefers the item more and vice versa.\n\n# 2.3 Optimization\n\nTo learn the parameters of the proposed model, we adopt Bayesian Personalized Ranking (BPR) [28] to conduct the pair-wise ranking. As such, we construct a triplet of one user $u$ , one observed item $i$ and one unobserved item $j$ , formally as,\n\n$$\n\\mathcal { T } = \\{ ( u , i , j ) ~ | \\mathbf { A } _ { u , i } = 1 , ~ \\mathbf { A } _ { u , j } = 0 \\} ,\n$$\n\nwhere $\\mathcal { T }$ is a triplet set for training.\n\nTherefore, the objective function can be defined as,\n\n$$\n\\mathcal { L } = \\sum _ { ( u , i , j ) \\in \\mathcal { T } } - \\ln \\phi ( y _ { u , i } - y _ { u , j } ) + \\lambda \\left. \\theta \\right. _ { 2 } ,\n$$\n\nwhere $\\phi ( \\cdot ) , \\lambda$ , and $\\theta$ represent the ?????????????? function, regularization weight and parameters of models, respectively.",
  "experiments": "",
  "hyperparameter": "- embedding_size (D): Dimension of the ID embedding vectors for users and items used in the GCN. Controls the capacity of collaborative representations. The paper fixes this to 64 for all models.\n- proj_feature_dim (D'): Dimension of the distilled multimodal feature space where item content and user prototypes are represented. Controls how rich the content-side representation is; chosen as a latent dimension tuned on validation (exact value not explicitly specified).\n- num_gcn_layers (L): Number of stacked graph convolutional layers used for message passing over the refined graph. Controls how far high-order connectivity is propagated. The experiments employ L = 2 for all models.\n- num_routing_iterations (T): Number of iterations in the neighbor routing within the prototypical network for refining user preference prototypes. Controls how strongly user prototypes are adjusted toward their interacted items; treated as a small integer tuned per dataset (examined via sensitivity study, exact best value not fixed in text).\n- learning_rate: Step size for optimizing model parameters with Adam. Searched over {0.0001, 0.001, 0.01, 0.1, 1} and selected on validation performance.\n- reg_weight_lambda: L2 regularization weight λ applied to model parameters in the BPR loss to prevent overfitting. Tuned over {0.00001, 0.0001, 0.001, 0.01, 0.1}.\n- early_stopping_patience: Number of epochs without improvement in Recall@10 on the validation set before stopping training. Set to 20 successive epochs."
}
