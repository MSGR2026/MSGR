{
  "id": "LayerGCN_2023",
  "paper_title": "Layer-refined Graph Convolutional Networks for Recommendation",
  "alias": "LayerGCN",
  "year": 2023,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# III. METHODOLOGY\n\n# A. Preliminaries\n\nNotations. Suppose $\\mathcal { G } ~ = ~ ( \\nu , \\mathcal { E } )$ be a given graph with node set $\\nu$ and edge set $\\mathcal { E }$ . We denote the cardinality of node set $\\nu$ by $\\mathcal { N } = | \\mathcal { V } |$ and the cardinality of edge set by $\\mathcal { M } = \\vert \\mathcal { E } \\vert$ . The adjacency matrix is denoted by $\\mathbf { A } \\in \\mathbb { R } ^ { N \\times N }$ , and the diagonal degree matrix is denoted by $\\mathbf { D }$ . In the recommendation domain, the user-item interaction graph can be treated by a bipartite graph with source nodes as users and target nodes as items. The number of users is denoted by $\\mathcal { N } _ { \\mathcal { U } }$ and the number of items is denoted by $\\mathcal { N } _ { \\mathcal { I } }$ . As a result, we have $\\mathcal { N } = \\mathcal { N } _ { \\mathscr { U } } + \\mathcal { N } _ { \\mathscr { T } }$ . We denote the embeddings of nodes in the ego layer by $\\mathbf { X } ^ { 0 } \\in \\mathbb { R } ^ { N \\times T }$ , where $\\tau$ is the embedding size of a node. In our following discussion, we mainly use the matrix form notation of each model.\n\nGCN. A typical feed forward propagation GCN to calculate the hidden embedding $\\mathbf { X } ^ { l + 1 }$ at layer $l + 1$ is recursively conducted as:\n\n$$\n\\mathbf { X } ^ { l + 1 } = \\sigma \\left( \\hat { \\mathbf { A } } \\mathbf { X } ^ { l } \\mathbf { W } ^ { l } \\right) ,\n$$\n\nwhere $\\sigma ( \\cdot )$ is a non-linear function, e.g., the ReLu function, $\\hat { \\bf A } \\ = \\ \\hat { \\bf D } ^ { - 1 / 2 } ( { \\bf A } + { \\bf I } ) \\hat { \\bf D } ^ { - 1 / 2 }$ is the re-normalization of the adjacency matrix A, and $\\hat { \\bf D }$ is the diagonal degree matrix of $\\mathbf { A } + \\mathbf { I }$ . For node classification, the last layer of a GCN is used to predict the label of a node via a sof tmax classifier.\n\nLightGCN. He et al. simplify the vanilla GCN for recommendation in [3]. They find both the feature filter matrix W and the non-linear activation $\\sigma ( \\cdot )$ impose adverse effects on recommendation performance. The simplified graph convolutional layer in LightGCN is defined as:\n\n$$\n\\mathbf { X } ^ { l + 1 } = ( \\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - 1 / 2 } ) \\mathbf { X } ^ { l } .\n$$\n\nThe node embeddings of the $( l + 1 )$ -th hidden layer are only linearly aggregated from the $l$ -th layer with a transition matrix $\\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - 1 / 2 }$ . The transition matrix is exactly the weighted adjacency matrix mentioned above. Other researchers also verify that the linear transformation matrix of a graph convolutional layer in a GCN only contributes to over-fitting and propose architectures such as SGC [5] and DGI [37] to solve the over-fitting problem.\n\nExisting GCN-based recommendation models typically integrate both the ego layer and all hidden layers obtained from each propagation to update the final embeddings of nodes. Specifically, we use a READOUT function to aggregate all layer features of a node to obtain the final nodes representation:\n\n$$\n{ \\bf X } = \\mathrm { R e a D o U T } ( { \\bf X } ^ { 0 } , { \\bf X } ^ { 1 } , { \\bf X } ^ { 2 } , \\cdot \\cdot \\cdot , { \\bf X } ^ { L } ) ,\n$$\n\nwhere the READOUT function can be any differentiable function and $L$ is the total number of layers in GCN. For example, LightGCN [3] uses a mean function for its final embedding updating.\n\n# B. LayerGCN\n\nNote that LightGCN defines the transition matrix as $\\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - 1 / 2 }$ . In this paper, we adopt the same transition matrix $\\hat { \\mathbf { A } } = \\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { \\hat { - } 1 / 2 }$ , which is symmetric. The adjacency matrix $\\mathbf { A } \\in \\mathbb { R } ^ { \\mathcal { N } \\times \\mathcal { N } }$ is constructed from the user-item interaction matrix R ∈ RNU ×NI :\n\n$$\n\\mathbf { A } = \\left( \\begin{array} { l l } { \\mathbf { 0 } } & { \\mathbf { R } } \\\\ { \\mathbf { R ^ { \\mathrm { T } } } } & { \\mathbf { 0 } } \\end{array} \\right) ,\n$$\n\nand each entry $R _ { u i } \\in \\mathbf { A }$ is set to 1, if user $u$ has interacted with item $i$ , otherwise, $R _ { u i }$ is set to 0.\n\n1) Degree-sensitive Edge Dropout (DegreeDrop): Following the ideas of model sparsification [12], [38], we first tackle the natural noise existing in the interaction matrix by pruning graph edges.\n\nIn CF, our ultimate objective is to learn a set of informative embeddings for users and items. Hence, we prune a set of superfluous edges in the original graph following a precalculated probability to attenuate the signals from noisy neighbors. In detail, given a specific edge $e _ { k } \\in \\mathcal { E } , ( k < \\mathcal { M } )$ which connects node $i$ and $j$ , its probability to be reserved in graph $\\mathcal { G }$ is calculated as:\n\n$$\np _ { e _ { k } } = \\frac { 1 } { \\sqrt { d _ { i } } \\sqrt { d _ { j } } } ,\n$$\n\nwhere $d _ { i }$ and $d _ { j }$ are the degrees of node $i$ and $j$ in graph $\\mathcal { G }$ , respectively. With $m$ edges to be pruned (pruning ratio: $m / \\mathcal { M } < 1 )$ , we sample $\\mathcal { M } - m$ edges from the multinomial probability distribution with parameters $\\mathcal { M } - m$ and $\\mathbf { p } = ( p _ { e _ { 0 } } , p _ { e _ { 1 } } , . . . , p _ { e _ { \\mathcal { M } - 1 } } )$ . Then, we construct a sparsified adjacency matrix $\\mathbf { A } _ { p }$ based on the sampled edges. Following [4], [12], we also perform the re-normalization trick on $\\mathbf { A } _ { p }$ , resulting as $\\hat { \\mathbf { A } } _ { p }$ . In information propagation of LayerGCN, we will use $\\hat { \\mathbf { A } } _ { p }$ instead of $\\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { \\bar { D } } ^ { - 1 / 2 }$ . However, in inference stage, we resort to $\\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - 1 / 2 }$ . Edge pruning can naturally prevent over-fitting in GCN based on the theoretical study of DropEdge [12]. However, different from DropEdge, which prunes the edges following a uniform distribution and shows slow convergence in model training, the proposed degreesensitive pruning method tends to drop the edges that connect popular nodes at both ends and converges much faster than DropEdge in practice. It is also verified in [10] that popular nodes are more likely to suffer from over-smoothing.\n\nIt is worth noting that edges of the graph can be further permuted with alternating degree-sensitive and random pruning. Alternating edge pruning methods may introduce more diversity into the graph and enjoy the fast training convergence speed.\n\n2) Layer-refined Graph Convolution (LayerGC): Recent studies [10], [39], [40] suggest that GCN does not scale well to deep architectures, since stacking multiple layers of graph convolutions leads to high complexity in back-propagation. In practice, most GCN variants, including GCN-based recommendation models, achieve the best performance with 2 layers [4], [23]. In CF, our ultimate objective is to learn an informative embedding that represents the personality/property of each user/item. However, the propagated information via multi-layers tends to be over-smoothed, making it impossible to discriminate between different nodes. Inspired by the huge success of ResNet [9], recent works [4], [10] stimulate the skip connection in ResNet that combines the smoothed node embedding at $( l + 1 )$ -th layer with a fixed weight of information from the $l$ -th layer or the ego layer. Nevertheless, in our propagation stage, we extract the information from the ego layer dynamically by designing a layer refinement mechanism. Formally, we define the information propagation in LayerGCN as:\n\n$$\n\\begin{array} { r l } & { \\mathbf { X } ^ { l + 1 } = \\hat { \\mathbf { A } } _ { p } \\mathbf { X } ^ { l } } \\\\ & { \\mathbf { X } ^ { l + 1 } = ( \\mathbf { a } ^ { l + 1 } + \\epsilon ) \\mathbf { X } ^ { l + 1 } , } \\end{array}\n$$\n\nwhere $\\mathbf { a } _ { l + 1 } \\in \\mathbb { R } ^ { N }$ is the node similarity vector between current layer and the ego layer. $\\epsilon$ is a small positive infinitesimal quantity to prevent zero vector in $\\mathbf { X } ^ { l + 1 }$ .That is:\n\n$$\n\\mathbf { a } ^ { l + 1 } = \\mathrm { S I M } ( \\mathbf { X } ^ { l + 1 } , \\mathbf { X } ^ { 0 } ) .\n$$\n\nIn our model, we use cosine similarity as our SIM function based on two principles. First, it is non-parametric. Hence, LayerGCN harnessing this function is less prone to overfitting. Second, cosine similarity is a widely used metric to evaluate the distance between embeddings and has demonstrated its promising performance in literature [34], [41]. Specifically, the cosine similarity function between two row vectors $\\mathbf { x } _ { i } = \\mathbf { X } ^ { l + 1 } [ i , : ]$ and $\\mathbf { x } _ { j } = \\mathbf { X } ^ { 0 } [ j , : ]$ is defined as:\n\n$$\n\\mathrm { S I M } ( \\mathbf { x } _ { i } , \\mathbf { x } _ { j } ) = \\frac { \\mathbf { x } _ { i } \\cdot \\mathbf { x } _ { j } } { m a x ( | | \\mathbf { x } _ { i } | | _ { 2 } | | \\mathbf { x } _ { j } | | _ { 2 } , \\epsilon ) } .\n$$\n\nThis function amplifies the fusion of hidden layers that are similar to the ego layer and reduces the influence of hidden layers that are divergent from the ego layer. We will analyze the proposed mechanism which has a variety of merits in the following section in detail.\n\nFinally, we update the nodes’ final embeddings by dropping the ego layer as its information is already refined within hidden layers. Hence Eq. 3 can be expressed as:\n\n$$\n\\mathbf { X } = \\operatorname { R e a D O U T } ( \\mathbf { X } ^ { 1 } , \\mathbf { X } ^ { 2 } , \\cdots , \\mathbf { X } ^ { L } ) .\n$$\n\nHere we use the sum aggregation for node updating. In the prediction phase, the final embedding $\\mathbf { X }$ is applied to rank the candidate items for each target user.\n\nWe conclude our proposed model in Fig. 2. First, to tackle the natural noise in the user-item interaction graph, we sparsify it by pruning a proportion of edges following a degreesensitive probability. We then perform linear forward propagation based on the pruned graph. With each propagated layer embedding, we update its representation based on its similarity with the ego layer. Finally, we aggregate all embeddings except the ego layer for final node representation. Based on the final node embedding $\\mathbf { X }$ , we rank all potential items against each user with the following score for top- $K$ recommendation:\n\n![](images/fc3b97ac6182f253fadea18054ed8eb6a41abbdd5e975b676c73d659ee601951.jpg)  \nFig. 2. Overview of the proposed LayerGCN. In layer refinement component (left), We use the ego layer $( \\mathbf { X } ^ { 0 } )$ to refine the information of the hidden layer $( \\breve { \\mathbf { X } } ^ { l } )$ before it propagates to the next layer $( \\mathbf { X } ^ { l + 1 } )$ .\n\n$$\n\\begin{array} { r } { \\hat { r } _ { u i } = \\mathbf { x } _ { u } \\mathbf { x } _ { i } ^ { T } , \\mathbf { x } _ { u } \\in \\mathbf { X } \\cap \\mathbf { x } _ { i } \\in \\mathbf { X } , } \\end{array}\n$$\n\nwhere $\\mathbf { x } _ { u } \\in \\mathbb { R } ^ { T }$ and $\\mathbf { x } _ { i } \\in \\mathbb { R } ^ { T }$ are the final embeddings of user $u$ and item $i$ , respectively. Note $\\tau$ is the embedding size of a node (i.e., user/item).\n\nThe Loss Function. We optimize the LayerGCN model under a supervised learning setting. That is, given a positive useritem pair $( u , i )$ from the interactions, we will sample a negative pair $( u , j )$ that user $u$ has never interacted with item $j$ in the training set. We adopt the pairwise Bayesian Personalized Ranking (BPR) loss [30], which encourages the prediction of positive user-item pair to be scored higher than its negative pair:\n\n$$\n\\mathcal { L } _ { b p r } = \\sum _ { ( u , i , j ) \\in \\mathcal { D } } - l o g \\sigma ( \\hat { r } _ { u i } - \\hat { r } _ { u j } ) ,\n$$\n\nwhere $\\mathcal { D }$ is the set of training instances, and each instance $( u , i , j )$ satisfies $r _ { u , i } = 1$ and $r _ { u , j } = 0$ .\n\nWe further regularize the embeddings of users and items with $L _ { 2 }$ loss to avoid over-fitting. The final loss function is defined as:\n\n$$\n\\mathcal { L } = \\mathcal { L } _ { b p r } + \\lambda \\cdot | | \\mathbf { X } ^ { 0 } | | _ { 2 } ^ { 2 } ,\n$$\n\nwhere $| | \\cdot | | _ { 2 }$ is $L _ { 2 }$ norm, $\\lambda$ is a penalty term regularizes the coefficients of the model.",
  "experiments": "",
  "hyperparameter": "- **embedding_size (τ)**: Dimensionality of user and item ID embeddings (and node representations).\n  - Controls the capacity of the latent space to represent user preferences and item properties.\n  - Typically fixed to **64** in experiments; in principle can be tuned similarly to other GCN-based CF models (e.g., 32–256).\n- **num_layers (L)**: Number of graph convolution (propagation + refinement) layers in LayerGCN.\n  - Controls how far information propagates along the user–item graph (order of neighborhoods) and the depth at which refinement is applied.\n  - Main experiments fix **L = 4**; ablation studies explore **L in [1, 8]** and show LayerGCN remains effective at deeper depths than LightGCN.\n- **lambda (λ)**: Coefficient of L2 regularization on the ego-layer embeddings X⁰ in the final loss.\n  - Controls the strength of weight decay / embedding norm penalty to prevent overfitting.\n  - For model selection, λ is tuned over **{1e-2, 1e-3, 1e-4, 1e-5}**, and in hyper-parameter studies further extended roughly from **1e-5 to 1e-1**; λ = 1e-3 often works well.\n- **degree_drop_ratio**: Degree-sensitive edge dropout (DegreeDrop) ratio used to sparsify the user–item graph during training.\n  - Controls the proportion of edges pruned based on degree-aware probabilities; higher ratios lead to sparser graphs and stronger denoising but can over-prune.\n  - In main tuning, the ratio is searched over **{0.0, 0.1, 0.2}** (0.0 = no pruning); extended studies consider **0.0, 0.05, 0.1, 0.2**, with moderate ratios (e.g., 0.1) performing best on dense datasets.\n- **early_stopping_patience**: Number of epochs without validation improvement allowed before early stopping.\n  - Controls when training is stopped to avoid overfitting and unnecessary computation.\n  - Fixed to **50** epochs in experiments.\n- **max_epochs**: Maximum number of training epochs allowed.\n  - Upper bound on total training iterations; combining with early stopping determines the actual training length.\n  - Fixed to **1000** epochs in the reported experiments."
}
