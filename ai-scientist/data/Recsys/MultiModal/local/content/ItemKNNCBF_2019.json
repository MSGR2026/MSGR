{
  "id": "ItemKNNCBF_2019",
  "paper_title": "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches",
  "alias": "ItemKNNCBF",
  "year": 2019,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 2 RESEARCH METHOD\n\n# 2.1 Collecting Reproducible Papers\n\nTo make sure that our work is not only based on individual examples of recently published research, we systematically scanned the proceedings of scientific conferences for relevant long papers in a manual process. Specifically, we included long papers in our analysis that appeared between 2015 and 2018 in the following four conference series: KDD, SIGIR, TheWebConf (WWW), and RecSys.1 We considered a paper to be relevant if it (a) proposed a deep learning based technique and (b) focused on the top-n recommendation problem. Papers on other recommendation tasks, e.g., group recommendation or session-based recommendation, were not considered in our analysis. Given our interest in top-n recommendation, we considered only papers that used for evaluation classification or ranking metrics, such as Precision, Recall, MAP. After this screening process, we ended up with a collection of 18 relevant papers.\n\nIn a next step, we tried to reproduce2 the results reported in these papers. Our approach to reproducibility is to rely as much as possible on the artifacts provided by the authors themselves, i.e., their source code and the data used in the experiments. In theory, it should be possible to reproduce published results using only the technical descriptions in the papers. In reality, there are, however many tiny details regarding the implementation of the algorithms and the evaluation procedure, e.g., regarding data splitting, that can have an impact on the experiment outcomes [39].\n\nWe therefore tried to obtain the code and the data for all relevant papers from the authors. In case these artifacts were not already publicly provided, we contacted all authors of the papers and waited 30 days for a response. In the end, we considered a paper to be reproducible, if the following conditions were met:\n\n• A working version of the source code is available or the code only has to be modified in minimal ways to work correctly.3 • At least one dataset used in the original paper is available. A further requirement here is that either the originally-used train-test splits are publicly available or that they can be reconstructed based on the information in the paper.\n\nOtherwise, we consider a paper to be non-reproducible given our specific reproduction approach. Note that we also considered works to be non-reproducible when the source code was published but contained only a skeleton version of the model with many parts and details missing. Concerning the datasets, research based solely on non-public data owned by companies or data that was gathered in some form from the web but not shared publicly, was also not considered reproducible.\n\nThe fraction of papers that were reproducible according to our relatively strict criteria per conference series are shown in Table 1.\n\nTable 1: Reproducible works on deep learning algorithms for top-n recommendation per conference series from 2015 to 2018.   \n\n<table><tr><td>Conference</td><td>Rep. ratio</td><td>Reproducible</td></tr><tr><td>KDD</td><td>3/4 (75%)</td><td>[17],[23],[48]</td></tr><tr><td>RecSys</td><td>1/7 (14%)</td><td>[53]</td></tr><tr><td>SIGIR</td><td>1/3 (30%)</td><td>[10]</td></tr><tr><td>WWW</td><td>2/4 (50%)</td><td>[14],[24]</td></tr><tr><td>Total</td><td>7/18 (39%)</td><td></td></tr></table>\n\nNon-reproducible: KDD: [43], RecSys: [41], [6], [38], [44], [21], [45], SIGIR: [32], [7], WWW: [42], [11]\n\nOverall, we could reproduce only about one third of the works, which confirms previous discussions about limited reproducibility, see, e.g., [3]. The sample size is too small to make reliable conclusions regarding the difference between conference series. The detailed statistics per year—not shown here for space reasons— however indicate that the reproducibility rate increased over the years.",
  "experiments": "",
  "hyperparameter": "- `k` (neighborhood_size): Number of nearest neighbors used in KNN-based baselines (UserKNN, ItemKNN, ItemKNN-CBF, ItemKNN-CFCBF). Typically searched in the range 5–800 via Bayesian optimization.\n- `h` (shrink_term): Regularization term in the similarity computation for KNN methods, reducing similarity for item/user pairs with few co-interactions; searched in the range 0–1000.\n- `alpha` (random_walk_damping): Exponent controlling transition probabilities in the P3α random-walk-based graph recommender; real values in [0, 2].\n- `beta` (popularity_discount): Exponent controlling the degree of popularity penalization in RP3β, which divides similarities by item popularity^β; real values in [0, 2] with β = 0 reducing to P3α.\n- `w` (content_weight): Weight balancing ratings vs. content features in the hybrid ItemKNN-CFCBF similarity, applied to feature vectors before concatenation; tuned on a validation set.\n- `num_bayes_iterations` (bayes_search_steps): Number of configurations evaluated during Bayesian hyperparameter optimization of all baselines, typically 35 trials (with 5 initial random points).\n- `train_test_split_strategy`: Evaluation protocol used to mirror original papers (e.g., random 80/20 holdout, leave-one-out, leave-last-out), which strongly affects measured performance and is carefully replicated for each reproduced model."
}
