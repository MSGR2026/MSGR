{
  "id": "SMORE_2025",
  "paper_title": "Spectrum-based Modality Representation Fusion Graph Convolutional Network for Multimodal Recommendation",
  "alias": "SMORE",
  "year": 2025,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 4 METHODOLOGY\n\nThe key components of SMORE encompass three core aspects: i) Spectrum Modality Fusion, ii) Multi-modal Graph Learning, and iii) Modality-aware Preference Module. Fig. 2 illustrates an overview of the proposed architecture.\n\n# 4.1 Spectrum Modality Fusion\n\nModality fusion often confers advantages since the fused embeddings elucidate complementary and universal characteristics of different modalities. As opposed to existing works and drawing inspiration from the field of signal processing, SMORE exploits the frequency domain for dual purposes: modality fusion and denoising. With this objective, the raw multi-modal features $\\mathbf { E } _ { i , m }$ are first projected into a shared latent space using the multi-layer perceptron (MLP), i.e.,\n\n$$\n\\mathbf { H } _ { i , m } = \\mathbf { W } _ { 1 , m } \\mathbf { E } _ { i , m } + \\mathbf { b } _ { 1 , m } ,\n$$\n\nwhere $\\mathbf { W } _ { 1 , m } \\in \\mathbb { R } ^ { d \\times d _ { m } }$ and $\\mathbf { b } _ { 1 , m } \\in \\mathbb { R } ^ { d }$ denote the projection matrix and bias vector of the MLP for each modality $m$ , respectively. Thereafter, to convert the projected multi-modal features into the frequency domain for fusion and denoising, we utilize the discrete Fourier transform (DFT) [13] for each modality such that\n\n$$\n\\widetilde { \\mathbf { H } } _ { i , m } = { \\mathcal { F } } _ { m } \\left( \\mathbf { H } _ { i , m } \\right) \\in \\mathbb { C } ^ { n \\times d } ,\n$$\n\nwhere\n\n$$\n\\mathcal { F } _ { m } : \\widetilde { h } _ { k } = \\sum _ { j = 0 } ^ { n - 1 } h _ { j } \\exp \\left( - \\frac { 2 \\pi \\jmath } { n } j k \\right) , \\quad 0 \\leq k \\leq n - 1\n$$\n\ndenotes the one-dimensional DFT function along the sequence and spatial dimensions of the textual and image modality, respectively, $\\jmath = \\sqrt { - 1 }$ , and $\\widetilde { h } _ { k }$ denotes the modality spectrum features at frequency $2 \\pi k / n$ , with $k$ being the frequency-bin index. While the DFT has been widely applied for frequency conversion, quadratic complexity is incurred due to the computation of $N$ components. Instead, we employ the fast Fourier transform (FFT), which decomposes the DFT matrix into a series of sparse matrix products [33], thereby reducing the complexity to a logarithmic scale.\n\nWith the spectral features of each modality, denoising is performed. Since dynamic noise may be present across different modalities and leveraging the discriminative spectrum generated from the FFT [18], we introduce a modality-specific dynamic filter\n\n$$\n\\widehat { \\mathbf { H } } _ { i , m } = \\delta _ { m } \\left( \\widetilde { \\mathbf { H } } _ { i , m } \\right) = \\mathbf { W } _ { 2 , m } ^ { c } \\odot \\widetilde { \\mathbf { H } } _ { i , m } ,\n$$\n\nwhere $\\delta _ { m }$ is defined as the modal-specific transfer function of the filter, $\\odot$ denotes the point-wise product, and $\\mathbf { W } _ { 2 , m } ^ { c } \\in \\mathbb { C } ^ { n \\times d }$ denotes the trainable complex weight of the filter. The adaptive filter functions as a frequency selector that seeks to suppress noise-related irrelevant information. We can formulate the fusion process similarly to acquire the cross-modality fusion spectrum such that\n\n$$\n\\widehat { \\mathbf { H } } _ { i , f } = \\delta _ { f } \\left( \\underset { m \\in { \\mathcal { M } } } { \\Pi } \\widetilde { \\mathbf { H } } _ { i , m } \\right) ,\n$$\n\nwhere, as opposed to matrix multiplication, $\\Pi$ is defined as the pointwise product operator, and $\\delta _ { f }$ is defined as the transfer function of the dynamic fusion filter. It is important to note that in the frequency domain, the point-wise product operation is equivalent to the circular convolution operation in the spatial domain. As a result, the rich correlations between the sequence and spatial modality (e.g., text and image) are captured, while minimizing noise contamination during fusion. More importantly, in contrast to advanced fusion methods (e.g., co-attention [35]), which require a quadratic time complexity, fusing in the frequency domain allows SMORE to achieve logarithmic runtime due to the efficient FFT and point-wise aggregation. In this aspect, we can achieve both efficiency and effectiveness in fusing modalities and denoising.\n\nThereafter, the spectrum of the uni-modal and fused modality features are projected back into the original feature space using the inverse discrete Fourier transform (IDFT)\n\nefficacy of a multi-modal recommender can be influenced substantially by both collaborative and semantically associated signals. These works construct individual graphs for each modality and aggregate them via learnable weights before performing message propagation on the fusion graph. By aggregating and disregarding uni-modality graph, distinct modality preferences are obscured.\n\n# 4.2 Multi-modal Graph Learning\n\n$$\n\\mathcal { F } _ { m } ^ { - 1 } : h _ { j } = \\frac { 1 } { n } \\sum _ { k = 0 } ^ { n - 1 } \\tilde { h } _ { k } \\exp \\left( \\frac { 2 \\pi \\ j } { n } j k \\right) , \\quad 0 \\leq j \\leq n - 1 ,\n$$\n\n$$\n\\dot { \\bf H } _ { i , m } = \\mathcal { F } _ { m } ^ { - 1 } ( \\widehat { \\bf H } _ { i , m } ) \\in \\mathbb { R } ^ { n \\times d } , \\quad \\dot { \\bf H } _ { i , f } = \\mathcal { F } _ { m } ^ { - 1 } ( \\widehat { \\bf H } _ { i , f } ) \\in \\mathbb { R } ^ { n \\times d } .\n$$\n\nAs will be illustrated in Section 5.5, the above empowers SMORE to execute modality fusion and denoising effectively, extracting only essential uni-modal and fused features through filtering in the frequency domain.\n\n4.2.1 Item-Item Modal-Specific and Fusion Views. Having acquired the denoised and fused modality representations, the semantically correlated modality features can be distilled through graph convolutional operations. As highlighted in [42, 43, 50], the\n\nIn contrast, we emphasize the importance of capturing both uni-modal and fusion preferences by proposing a new multi-modal graph learning module that distinctively constructs modal-specific and fusion graphs. We first establish item-item affinities by computing the similarity of each raw modality features. Henceforth, we attain modality similarity matrix $S _ { m }$ such that the similarity between (item) row $a$ and (item) column $^ { b }$ entry of $\\mathbf { E } _ { i , m }$ is given by\n\n$$\ns _ { a , b } ^ { m } = \\frac { \\left( e _ { a } ^ { m } \\right) ^ { \\mathrm { T } } e _ { b } ^ { m } } { \\Vert e _ { a } ^ { m } \\Vert \\Vert e _ { b } ^ { m } \\Vert } .\n$$\n\nTo ensure that the uni-modal vital features are captured, we perform graph sparsification [2] by retaining $K$ edges with the highest similarity scores such that\n\n$$\n\\dot { s } _ { a , b } ^ { m } = \\left\\{ \\begin{array} { l l } { s _ { a , b } ^ { m } , } & { s _ { a , b } ^ { m } \\in \\mathrm { t o p } \\cdot K _ { m } ( \\{ s _ { a , c } ^ { m } , c \\in \\mathcal { I } \\} ) ; } \\\\ { 0 , } & { \\mathrm { o t h e r w i s e } , } \\end{array} \\right.\n$$\n\nwhere ${ \\dot { s } } _ { a , b } ^ { m }$ denotes the degree of similarity (edge weights) between items $a$ and $^ { b }$ in modality $m$ . To mitigate the gradient vanishing/exploding problem [17], we then normalize the similarity matrix\n\n$$\n\\ddot { S } _ { m } = \\mathbf { D } _ { m } ^ { - 1 / 2 } \\dot { S } _ { m } \\mathbf { D } _ { m } ^ { - 1 / 2 } ,\n$$\n\n$\\mathbf { D } _ { m } ^ { - 1 / 2 }$ is defined as the degree matrix of ${ \\dot { S } } _ { m }$\n\nHaving constructed the modality-specific graph, we adopt the max-pooling strategy to retain the highest complementary strength between different $m$ modality graphs, thereby preserving prominent cross-modality features. The fusion affinity matrix can be defined as the max edge weights between items $a$ and $^ { b }$ , i.e.,\n\n$$\n\\ddot { S } _ { a , b } ^ { f } = \\operatorname* { m a x } _ { m , m ^ { \\prime } \\in \\mathcal { M } } \\left( \\ddot { S } _ { a } ^ { m } , \\ddot { S } _ { b } ^ { m ^ { \\prime } } \\right) , \\quad m \\neq m ^ { \\prime } .\n$$\n\nPrior to unimodal and fusion feature propagation, we extract preference-related modality features based on behavioral guidance\n\n$$\n\\begin{array} { r l } & { \\ddot { \\mathbf { H } } _ { i , m } = f _ { g a t e } ^ { m } \\left( \\mathbf { E } _ { i , i d } , \\dot { \\mathbf { H } } _ { i , m } \\right) = \\mathbf { E } _ { i , i d } \\odot \\sigma \\left( \\mathbf { W } _ { 3 , m } \\dot { \\mathbf { H } } _ { i , m } + \\mathbf { b } _ { 3 , m } \\right) , } \\\\ & { \\ddot { \\mathbf { H } } _ { i , f } = f _ { g a t e } ^ { c } \\left( \\mathbf { E } _ { i , i d } , \\dot { \\mathbf { H } } _ { i , f } \\right) = \\mathbf { E } _ { i , i d } \\odot \\sigma \\left( \\mathbf { W } _ { 4 , f } \\dot { \\mathbf { H } } _ { i , f } + \\mathbf { b } _ { 4 , f } \\right) , } \\end{array}\n$$\n\nwhere $\\mathbf { W } _ { ( . ) } \\in \\mathbb { R } ^ { d \\times d }$ and $\\mathbf { b } _ { ( . ) } \\in \\mathbb { R } ^ { d }$ are the trainable parameters and $\\sigma$ is the non-linearity sigmoid gate function.\n\nInspired by the simplicity and efficacy of LightGCN [12], the item uni-modal $\\ddot { \\mathrm { H } } _ { i , m }$ and fusion features $\\ddot { \\mathrm { \\bf H } } _ { i , f }$ are propagated through a shallow light graph convolutional layer with the propagation rule being\n\n$$\n\\begin{array} { r } { \\overline { { \\mathbf { H } } } _ { i , m } = \\ddot { S } _ { m } \\ddot { \\mathbf { H } } _ { i , m } , \\qquad \\overline { { \\mathbf { H } } } _ { i , f } = \\ddot { S } _ { f } \\ddot { \\mathbf { H } } _ { i , f } . } \\end{array}\n$$\n\nLikewise, we can compute the user modality features through a weighted-sum aggregation layer defined by\n\n$$\n\\bar { h } _ { u , m } = \\sum _ { i \\in { \\cal N } _ { u } } \\frac { 1 } { \\sqrt { | { \\cal N } _ { u } | | { \\cal N } _ { i } | } } \\bar { h } _ { i , m } , \\qquad \\bar { h } _ { u , f } = \\sum _ { i \\in { \\cal N } _ { u } } \\frac { 1 } { \\sqrt { | { \\cal N } _ { u } | | { \\cal N } _ { i } | } } \\bar { h } _ { i , m } ,\n$$\n\nwhere $\\bar { h } _ { u , m }$ and $\\bar { h } _ { u , f }$ are the user uni and fusion modality features, respectively, and $1 / \\sqrt { | N _ { u } | | N _ { i } | }$ is the symmetric normalization term to avoid overscaling. It is useful to note that employing a shallow layer in the item-item view is sufficient for capturing relevant semantic associative signals since stacking multiple layers may induce undesirable high-order latent noise [41]. By concatenating $\\overline { { \\mathbf { H } } } _ { u , m }$ with $\\overline { { \\mathbf { H } } } _ { i , m }$ , and $\\bar { \\bar { \\mathbf { H } } } _ { u , f }$ with $\\overline { { \\mathbf { H } } } _ { i , f }$ , we can obtain the enriched uni-modal and fusion features for both the users and items, denoted as $\\overline { { \\mathbf { H } } } _ { m }$ and $\\overline { { \\mathbf { H } } } _ { f } \\in \\mathbb { R } ^ { d \\times ( | \\mathcal { U } | + | \\mathcal { I } | ) }$ , respectively.\n\n4.2.2 User-Item Behavioral View. The focus of this view is on encoding the high-order collaborative signals from users’ historical interactions. It has been verified that the collaborative signals are highly influential in delineating users’ behavioral patterns [9, 12, 21, 51]. On this basis, we recursively propagate longrange collaborative signals in the interaction graph resulting in the behavioral embedding of the users and items given by\n\n$$\n\\mathbf { E } _ { i d } ^ { ( l ) } = ( \\mathbf { D } ^ { - 1 / 2 } \\mathbf { A } \\mathbf { D } ^ { - 1 / 2 } ) \\mathbf { E } _ { i d } ^ { ( l - 1 ) } , \\quad \\mathbf { A } = \\left[ \\begin{array} { c c } { 0 } & { \\mathbf { Y } } \\\\ { \\mathbf { Y } ^ { \\top } } & { 0 } \\end{array} \\right] .\n$$\n\nHe re, E(?? −1)???? de notes the ID embeddings at the previous layer, and $\\mathbf { D } ^ { - 1 / 2 }$ is the diagonal degree matrix corresponding to the adjacency matrix A. To obtain the overall high-order behavioral features of users and items, we aggregate the hidden layers by applying the mean function giving\n\n$$\n\\overline { { \\mathbf { E } } } _ { i d } = \\frac { 1 } { L + 1 } \\sum _ { i = 0 } ^ { L } \\mathbf { E } _ { i d } ^ { ( l ) } , \\qquad \\overline { { \\mathbf { E } } } _ { i d } \\in \\mathbb { R } ^ { d \\times ( | \\mathcal { U } | + | \\mathcal { T } | ) } .\n$$\n\n# 4.3 Modality-Aware Preference Module\n\nWith the high-order behavioral and enriched (uni-modal and fused) features $\\overline { { \\mathbf { E } } } _ { i d }$ , $\\overline { { \\mathbf { H } } } _ { m }$ and $\\overline { { \\mathbf { H } } } _ { f }$ , modality preferences are distilled for each user. In line with the diversity observed in real-world scenarios, a user may be inclined toward a single modality, while some may exhibit a mixture of fusion preferences. As such, we utilize complementary signals encapsulated in the fusion embeddings to weigh the uni-modal features, effectively striking a balance between the uni-modal and fusion preferences. To this end, we define\n\n$$\n\\alpha _ { m } = \\mathrm { s o f t m a x } ( \\mathbf { p } _ { m } ^ { \\top } \\operatorname { t a n h } ( \\mathbf { W } _ { 5 , m } \\overline { { \\mathbf { H } } } _ { f } + \\mathbf { b } _ { 5 , m } ) )\n$$\n\nas the modal-specific attention weights, where $p _ { ( \\cdot ) } \\in \\mathbb { R } ^ { d }$ is the attention vector. These weights are subsequently used to weigh the semantically associated uni-modal features to obtain the final aggregated uni-modal features given by\n\n$$\n\\mathbf { H } _ { m } ^ { * } = \\sum _ { m \\in M } \\alpha _ { m } \\overline { { \\mathbf { H } } } _ { m } .\n$$\n\nWe next extract modality preferences derived from user collaborative information by feeding the high-order behavioral signal through a uni-modal and fusion gate function. This results in explicit uni-modal and fusion preferences given, respectively, by\n\n$$\n\\begin{array} { r l } & { \\mathbf { Q } _ { m } = \\psi _ { p r e f } ^ { m } \\left( \\overline { { \\mathbf { E } } } _ { i d } \\right) = \\sigma ( \\mathbf { W } _ { 6 , m } \\overline { { \\mathbf { E } } } _ { i d } + \\mathbf { b } _ { 6 , m } ) , } \\\\ & { \\mathbf { Q } _ { f } = \\psi _ { p r e f } ^ { f } \\left( \\overline { { \\mathbf { E } } } _ { i d } \\right) = \\sigma ( \\mathbf { W } _ { 7 , f } \\overline { { \\mathbf { E } } } _ { i d } + \\mathbf { b } _ { 7 , f } ) , } \\end{array}\n$$\n\nwhere $\\sigma$ is the non-linearity function. The overall multi-modal side features (distilled from the explicit uni-modal and fusion preferences) are then derived as\n\n$$\n\\mathbf { H } _ { s } = { \\frac { 1 } { | M | } } \\left( \\sum _ { m \\in M } \\mathbf { H } _ { m } ^ { * } \\odot \\mathbf { Q } _ { m } \\right) + \\left( \\mathbf { H } _ { f } \\odot \\mathbf { Q } _ { f } \\right) .\n$$\n\nTo maximize mutual information across high-order behavioral and modality-side information, we then incorporate an InfoNCE contrastive task [24] with\n\n$$\n\\mathcal { L } _ { c l } ^ { u } = \\sum _ { u \\in \\mathcal { U } } - \\log \\frac { \\exp ( \\bar { e } _ { u , i d } \\cdot \\overline { { h } } _ { u , s } / \\tau ) } { \\sum _ { v \\in \\mathcal { U } } \\exp ( \\bar { e } _ { v , i d } \\cdot \\overline { { h } } _ { v , s } / \\tau ) }\n$$\n\nbeing the user contrastive loss and $\\tau$ being the hyperparameter temperature that regulates the degree of smoothness in the distribution.\n\nTable 1: Dataset Statistics   \n\n<table><tr><td>Dataset</td><td>#User</td><td>#Item</td><td>#Interaction</td><td>Density</td></tr><tr><td>Baby</td><td>19,445</td><td>7,050</td><td>160,792</td><td>0.117%</td></tr><tr><td>Sports</td><td>35,598</td><td>18,357</td><td>296,337</td><td>0.045%</td></tr><tr><td>Clothing</td><td>39,387</td><td>23,033</td><td>278.,677</td><td>0.031%</td></tr></table>\n\nThis task ensures the preservation of essential features distilled from behavioral and modality views. Similarly, we can obtain the item contrastive loss $\\mathcal { L } _ { c l } ^ { i }$ by substituting users with items as defined in Eq (22). Thereafter, the overall contrastive loss is governed by $\\mathcal { L } _ { c l } \\bar { = } \\mathcal { L } _ { c l } ^ { u } + \\mathcal { L } _ { c l } ^ { i }$ .\n\n# 4.4 Prediction and Optimization\n\nBy capitalizing on the refined uni-modal and complementary fused features, we acquire the final representations of the user and item\n\n$$\ne _ { u } ^ { * } = \\bar { e } _ { u , i d } + h _ { u , s } , \\quad e _ { i } ^ { * } = \\bar { e } _ { i , i d } + h _ { i , s } .\n$$\n\nThe estimated likelihood is then computed as $\\widehat { y } ( u , i ) = e _ { u } ^ { * \\top } e _ { i }$ . For bmodel optimization, we employ the BPR loss [28] to reconstruct the historical data, which prioritizes higher scores for observed items, i.e.,\n\n$$\n\\mathcal { L } _ { b p r } = \\sum _ { ( u , i , j ) \\in O } - \\mathrm { l n } \\sigma \\Big ( \\widehat { y } _ { u i } - \\widehat { y } _ { u j } \\Big ) .\n$$\n\nHere, $O = \\{ ( u , i , j ) | ( u , i ) \\in O ^ { + } , ( u , j ) \\in O ^ { - } \\}$ represents the set of interactions, comprising observed $O ^ { + }$ and unobserved $O ^ { - }$ interactions, and $\\sigma$ denotes the sigmoid function. We then perform joint optimization in conjunction with the contrastive loss such that the overall loss function is given by\n\n$$\n\\begin{array} { r } { \\mathcal { L } = \\mathcal { L } _ { b p r } + \\lambda _ { 1 } \\mathcal { L } _ { c l } + \\lambda _ { 2 } | | \\Theta | | _ { 2 } ^ { 2 } , } \\end{array}\n$$\n\nwhere $\\lambda _ { 1 }$ and $\\lambda _ { 2 }$ regulates the influence of the contrastive task and the L2 regularization term, respectively.",
  "experiments": "",
  "hyperparameter": "- λ1: Weight for the contrastive loss in the overall loss function. Typical values: 0.01 to 0.03.\n- Km: Number of top edges to retain for graph sparsification in modality-specific graphs. Typical range: 10 to 40.\n- d: Embedding dimension for users and items. Typical value: 64.\n- Batch size: Number of samples per training batch. Typical value: 2048."
}
