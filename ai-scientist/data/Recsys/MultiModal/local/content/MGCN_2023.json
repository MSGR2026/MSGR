{
  "id": "MGCN_2023",
  "paper_title": "Multi-View Graph Convolutional Network for Multimedia Recommendation",
  "alias": "MGCN",
  "year": 2023,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# 2 MODEL\n\n# 2.1 Problem Definition\n\nLet $\\boldsymbol { \\mathcal { U } } = \\{ \\boldsymbol { u } \\}$ donate the user set and ${ \\cal T } = \\{ i \\}$ donate the item set. The input ID embeddings of user $u$ and item $i$ are $\\mathbf { E } _ { i d } \\in \\mathbb { R } ^ { d \\times ( | U | + | I | ) }$ $d$ is the embedding dimension. Then, we denote each item modality features as $\\mathbf { E } _ { i , m } ~ \\in ~ \\mathbb { R } ^ { d _ { m } \\times | I | }$ , where $d _ { m }$ is the dimension of the features, $m \\in { \\mathcal { M } }$ is the modality, and $\\mathcal { M }$ is the set of modalities. In this paper, we mainly consider visual and textual modalities denoted by $\\boldsymbol { \\mathcal { M } } = \\{ \\boldsymbol { v } , t \\}$ . Please kindly note that our method is not fixed to the two modalities and multiple modalities can be involved.\n\nNext, user historical behavior data is denoted as $\\mathcal { R } \\in \\{ 0 , 1 \\} ^ { | U | \\times | I | }$ , where each entry $\\mathcal { R } _ { u , i } = 1$ if user $u$ clicked item ??, otherwise $R _ { u , i } = 0$ Naturally, the historical interaction data $\\mathcal { R }$ can be regarded as a sparse behavior graph $\\mathcal { G } = \\{ \\mathcal { V } , \\mathcal { E } \\}$ , where $\\mathcal { V } = \\{ \\mathcal { U } \\cup \\mathcal { I } \\}$ denotes the set of nodes and $\\mathcal { E } = \\{ ( u , i ) | u \\in \\mathcal { U } , i \\in J , R u i = 1 \\}$ denotes the set of edges. The purpose of the multimedia recommendation is to accurately predict users’ preferences by ranking items for each user according to predicted preferences scores $\\hat { y } _ { u i }$ .\n\n# 2.2 Behavior-Guided Purifier\n\nModality information provides rich and meaningful content information of items, while inevitably containing modality noise as well. To avoid noise contamination, we propose a behavior-guided purifier. Specifically, we first transform raw item modality features $\\mathbf { E } _ { i , m }$ into high-level features $\\dot { \\bf E } _ { i , m }$ :\n\n![](images/1453a6d2e4c53c1225b82e1d4eae2ff5b6f122b00c882b9eae68afcbc7f7c8e9.jpg)  \nFigure 2: (a) The overall framework. (b) The behavior-guided purifier filters out modality noise with the help of behavior information. (c) The behavior-aware fuser adaptively fuses modality features according to user modality preferences.\n\n$$\n\\dot { \\bf E } _ { i , m } = { \\bf W } _ { 1 } { \\bf E } _ { i , m } + { \\bf b } _ { 1 } ,\n$$\n\nwhere $\\mathbf { W } _ { 1 } \\in \\mathbb { R } ^ { d \\times d _ { m } }$ and $\\mathbf { b } _ { 1 } \\in \\mathbb { R } ^ { d }$ denote the trainable transformation matrix and the bias vector.\n\nThen the preference-relevant modality features are separated from the modality features, with the guidance of behavior features:\n\n$$\n\\begin{array} { r } { \\ddot { \\mathbf { E } } _ { i , m } = f _ { g a t e } ^ { m } ( \\mathbf { E } _ { i , i d } , \\dot { \\mathbf { E } } _ { i , m } ) = \\mathbf { E } _ { i , i d } \\odot \\sigma ( \\mathbf { W } _ { 2 } \\dot { \\mathbf { E } } _ { i , m } + \\mathbf { b } _ { 2 } ) , } \\end{array}\n$$\n\nwhere $\\mathbf { W } _ { 2 } \\in \\mathbb { R } ^ { d \\times d }$ and $\\mathbf { b } _ { 2 } \\in \\mathbb { R } ^ { d }$ are learnable parameters, $\\odot$ represents the element-wise product and $\\sigma$ is the sigmoid nonlinearity. With the guidance of behavior features that are encoded in the ID embedding $\\mathbf { E } _ { i , i d }$ , we separate the preference-relevant modality features $\\ddot { \\mathbf { E } } _ { i , m }$ from item’s representation $\\dot { \\bf E } _ { i , m }$ . For users, we obtain their modality features by aggregating the interacted items’ modality features. We detail the process in Section 2.3.\n\n# 2.3 Multi-View Information Encoder\n\nAccording to [20, 30], both the collaborative signals and the semantically correlative signals can significantly influence the efficacy of multimedia recommendation. Thus, inspired by [30], we design a multi-view information encoder to enhance the discriminability of features. It captures collaborative signals from the view of the user-item relationship, and semantically correlative signals from the view of the item-item relationship.\n\n2.3.1 User-Item View. In particular, to capture high-order collaborative signals, we construct a GCN module to propagate ID embeddings of users and items over the interaction graph. The message propagation stage at $l$ -th graph convolution layer can be formulated as:\n\n$$\n{ \\bf E } _ { i d } ^ { ( l ) } = { \\bf E } _ { i d } ^ { ( l - 1 ) } \\mathcal { L } ,\n$$\n\nwher e E (?? ) represents the enhanced representations of users and items in $l \\cdot$ -th graph convolution layer, $\\mathbf { E } _ { i d } ^ { ( 0 ) }$ are the initial ID embeddings, that is e?????? (0) = ?????? and e?????? (0) = e???? . $\\mathcal { L }$ represents the\n\nLaplacian matrix for the user-item graph, which is formulated as:\n\n$$\n\\mathcal { L } = \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\mathcal { A } \\mathbf { D } ^ { - \\frac { 1 } { 2 } } , \\quad \\mathrm { a n d } \\quad \\mathcal { A } = \\left| \\begin{array} { c c } { 0 } & { \\mathrm { ~ R ~ } } \\\\ { \\mathrm { R } ^ { \\mathrm { T } } } & { 0 } \\end{array} \\right| ,\n$$\n\nwhere $\\mathcal { R }$ is the user-item interaction matrix, and 0 is the all-zero matrix; $\\mathcal { A }$ is the adjacency matrix and $\\mathbf { D }$ is the diagonal degree matrix; as such, the nonzero off-diagonal entry $\\mathcal { L } _ { u i } = 1 / \\sqrt { | \\mathcal { N } _ { u } | | \\mathcal { N } _ { i } | }$ , which avoids the scale of embeddings increasing with graph convolution operations. $\\mathcal { N } _ { u }$ represents the set of user’s $u$ neighbors, ${ \\cal N } _ { i }$ represents the set of item’s $u$ neighbors, $| \\mathcal { N } _ { u } |$ and $| N _ { i } |$ denote the size of $\\mathcal { N } _ { u }$ and ${ \\cal N } _ { i }$ .\n\nThe representations of the $l$ -th layer encode the $l$ -order neighbors’ information. By aggregating high-order neighbor information, the final representations $\\bar { \\mathbf { E } } _ { i d }$ are obtained:\n\n$$\n\\bar { \\mathbf { E } } _ { i d } = \\frac { 1 } { L + 1 } \\sum _ { i = 0 } ^ { L } \\mathbf { E } _ { i d } ^ { ( l ) } .\n$$\n\n2.3.2 Item-Item View. Similar to the view of user-item, graph convolution operations on item-item affinity graphs can capture semantically correlative signals, thereby enriching item modality features. However, propagating modality feature in a dense affinity graph is computationally demanding and may introduce noise through unimportant edges. Thus, we conduct KNN sparsification [5] on the dense graph.\n\nSpecifically, we first quantify the item-item affinities based on the similarity of each raw modality feature. Considering the computational complexity, cosine similarity has been selected. Then, a fully-connected graph is constructed, to indicate item-item affinities in modality $m$ . The element in row $a$ and column $^ { b }$ of the affinity graph $S _ { m }$ is:\n\n$$\ns _ { a , b } ^ { m } = \\frac { ( e _ { a } ^ { m } ) ^ { \\mathrm { T } } e _ { b } ^ { m } } { \\Vert e _ { a } ^ { m } \\Vert \\Vert e _ { b } ^ { m } \\Vert } ,\n$$\n\nwhere $s _ { a , b } ^ { m }$ represents the similarity between item $a$ and item $^ { b }$ in modality $m$ .\n\nBy performing graph convolution operations, we capture the common modality features of adjacent nodes, which can be used to enhance the target node. To capture the most relevant features from neighbors, for each item $a$ , we only preserve $K$ edges with the greatest similarity:\n\n$$\n\\dot { s } _ { a , b } ^ { m } = \\left\\{ \\begin{array} { r l } { { \\displaystyle s _ { a , b } ^ { m } , } } & { { \\displaystyle s _ { a , b } ^ { m } \\in \\mathrm { t o p } { - K } ( \\{ s _ { a , c } , c \\in \\bar { Z } \\} ) , } } \\\\ { { \\displaystyle 0 , } } & { { \\mathrm { o t h e r w i s e } , } } \\end{array} \\right.\n$$\n\nwhere ??¤????,?? represents the edge weight between item $a$ and item $^ { b }$ in $m$ modality. Same to the user-item view, we normalize the item-item affinity matrix to alleviate the exploding gradient problem:\n\n$$\n\\ddot { S } _ { m } = \\mathbf { D } _ { m } ^ { - \\frac { 1 } { 2 } } \\dot { S } _ { m } \\mathbf { D } _ { m } ^ { - \\frac { 1 } { 2 } } ,\n$$\n\nwhere $\\mathbf { D } ^ { \\mathbf { m } }$ is the diagonal degree matrix of ${ \\dot { S } } _ { m }$ . Then, we propagate all items’ modality features $\\bar { \\mathbf { E } } _ { i , m }$ through a GCN module on the corresponding item-item affinity matrix $\\ddot { S } _ { m }$ :\n\n$$\n\\bar { \\mathbf { E } } _ { i , m } = \\ddot { S } _ { m } \\ddot { \\mathbf { E } } _ { i , m } .\n$$\n\nIt is capable of enriching the feature by capturing common features of similar items. However, it should be noted that in the item-item view, the semantic similarity of node modality features is significantly decreasing with the propagation path increasing. Stacking multiple graph convolution layers not only leads to the node oversmoothing issue, but also easily captures noisy features. Therefore, in this study, we constructed a shallow GCN module to propagate modality information over ${ \\dot { S } } _ { m }$ (we set the graph convolution layer to 1, and prove the effect in Section 3.3).\n\nFinally, we obtain user modality features by aggregating interacted item modality features. User $u$ ’s modality feature $\\bar { \\mathbf { e } } _ { u , m }$ is expressed as:\n\n$$\n\\bar { \\mathbf { e } } _ { u , m } = \\sum _ { i \\in N _ { u } } \\frac { 1 } { \\sqrt { | N _ { u } | | N _ { i } | } } \\bar { \\mathbf { e } } _ { i , m } ,\n$$\n\nBy concatenating user modality features $\\bar { \\mathbf { E } } _ { u , m }$ with item modality features $\\bar { \\mathbf { E } } _ { i , m }$ , the final modality feature $\\bar { \\mathbf { E } } _ { m } \\in \\mathbb { R } ^ { d \\times ( | U | + | I | ) }$ is obtained.\n\n# 2.4 Behavior-Aware Fuser\n\nTo accurately capture items’ features in different modalities, we design a behavior-aware fuser. It allows flexible fusion weight allocation based on user modality preferences, which can be distilled from the behavior features. Moreover, to encourage the model to comprehensively explore user preference, a self-supervised task is introduced in the fusion process. The task is expected to maximize the mutual information [12, 13] between behavior features and fused multimodal features.\n\nSpecifically, the modality preferences ${ \\bf P } _ { m }$ are first distilled from user behavior features:\n\n$$\n\\mathbf { P } _ { m } = \\sigma ( \\mathbf { W } _ { 3 } \\bar { \\mathbf { E } } _ { i d } + \\mathbf { b } _ { 3 } ) ,\n$$\n\nwhere $\\mathbf { W } _ { 3 } \\in \\mathbb { R } ^ { d \\times d }$ and $\\mathbf { b } _ { 3 } \\in \\mathbb { R } ^ { d }$ are learnable parameters, $\\sigma$ is the sigmoid nonlinearity, which learns a nonlinear gate to model user modality features.\n\nThere are both modality-shared and modality-specific features possessed across all modalities. For modality-shared features, user attention remains consistent, as this aligns with the intended purpose of the user’s purchase. For this reason, we first extract the modality-shared features through attention mechanisms [28, 35], where the attention weight of each modality features $\\bar { \\mathbf { E } } _ { m }$ are calculated as:\n\n$$\n\\alpha _ { m } = \\mathrm { s o f t m a x } (  { \\mathbf { q } } _ { 1 } ^ { \\mathrm { T } } \\mathrm { t a n h } (  { \\mathbf { W } } _ { 4 } \\bar {  { \\mathbf { E } } } _ { m } +  { \\mathbf { b } } _ { 4 } ) ) ,\n$$\n\nwhere ${ \\bf q } _ { 1 } \\in \\mathbb { R } ^ { d }$ denotes attention vector and $\\mathbf { W } _ { 4 } \\in \\mathbb { R } ^ { d \\times d }$ , $\\mathbf { b } _ { 4 } \\in \\mathbb { R } ^ { d }$ denote the weight matrix and the bias vector, respectively. Notice that these parameters are shared for all modalities. Then, the modality-shared features $\\mathbf { E } _ { s }$ are obtained:\n\n$$\n\\mathbf { E } _ { s } = \\sum _ { m \\in \\mathcal { M } } \\alpha _ { m } \\bar { \\mathbf { E } } _ { m } .\n$$\n\nThen, the modality-specific features $\\tilde { \\mathbf { E } } _ { m }$ are obtained by subtracting the modality-shared features $\\tilde { \\mathbf { E } } _ { s }$ :\n\n$$\n\\tilde { \\mathbf { E } } _ { m } = \\bar { \\mathbf { E } } _ { m } - \\mathbf { E } _ { s } .\n$$\n\nFinally, we adaptively fuse the modality-specific features $\\tilde { \\mathbf { E } } _ { m }$ , and combine them with modality-shared features $\\mathbf { E } _ { s }$ as the final features E?????? :\n\n$$\n\\mathbf { E } _ { m u l } = \\mathbf { E } _ { s } + \\frac { 1 } { | \\mathcal { M } | } \\sum _ { m \\in \\mathcal { M } } \\tilde { \\mathbf { E } } _ { m } \\odot \\mathbf { P } _ { m } .\n$$\n\nIn order to promote the exploration of behavior and multimodal information, a self-supervised auxiliary task has been devised. The mathematical expression of this task is as follows:\n\n$$\n\\begin{array} { r l r } & { } & { \\mathcal { L } _ { C } = \\displaystyle \\sum _ { u \\in \\mathcal { U } } - \\log \\frac { \\exp ( e _ { u , m u l } \\cdot \\bar { e } _ { u , i d } / \\tau ) } { \\sum _ { v \\in \\mathcal { U } } \\exp ( e _ { v , m u l } \\cdot \\bar { e } _ { v , i d } / \\tau ) } } \\\\ & { } & { \\quad + \\displaystyle \\sum _ { i \\in \\mathcal { I } } - \\log \\frac { \\exp ( e _ { i , m u l } \\cdot \\bar { e } _ { i , i d } / \\tau ) } { \\sum _ { j \\in \\mathcal { I } } \\exp ( e _ { j , m u l } \\cdot \\bar { e } _ { j , i d } / \\tau ) } , } \\end{array}\n$$\n\nwhere $\\tau$ is the temperature hyper-parameter of softmax.\n\n# 2.5 Predictor\n\nBased on the enhanced behavior features and multimodal features, we form the final representations of users and items:\n\n$$\n\\begin{array} { l } { { \\bf e } _ { u } = \\bar { \\bf e } _ { u , i d } + { \\bf e } _ { u , m u l } , } \\\\ { { \\bf e } _ { i } = \\bar { \\bf e } _ { i , i d } + { \\bf e } _ { i , m u l } . } \\end{array}\n$$\n\nFollowed [11], the inner product is adopted to determine the likelihood of interaction between user $u$ and item $i$ :\n\n$$\nf _ { p r e d i c t } ( u , i ) = \\hat { y } _ { u i } = \\mathbf { e } _ { u } ^ { \\mathrm { T } } \\mathbf { e } _ { i } .\n$$\n\n# 2.6 Optimization\n\nDuring the phase of model training, we adopt the Bayesian Personalized Ranking (BPR) loss $\\mathcal { L } _ { B P R }$ as the basic optimization task, which assumes that users prefer historically interacted items over unclicked ones. And it is combined with auxiliary self-supervised tasks to jointly update the representations of users and items:\n\n$$\n\\begin{array} { r } { \\mathcal { L } = \\mathcal { L } _ { B P R } + \\lambda _ { C } \\mathcal { L } _ { C } + \\lambda _ { E } \\Vert \\mathbf { E } \\Vert _ { 2 } , } \\end{array}\n$$\n\nwhere $\\mathbf { E }$ is the set of model parameters; $\\lambda _ { C }$ and $\\lambda _ { E }$ are hyperparameters to control the effect of the contrastive auxiliary task and the $L _ { 2 }$ regularization, respectively.\n\nTable 1: Statistics of the experimental datasets   \n\n<table><tr><td>Dataset</td><td>#User</td><td>#Item</td><td>#behavior</td><td>Density</td></tr><tr><td>Baby</td><td>19,445</td><td>7,050</td><td>160,792</td><td>0.117%</td></tr><tr><td>Sports</td><td>35,598</td><td>18,357</td><td>296,337</td><td>0.045%</td></tr><tr><td>Clothing</td><td>39,387</td><td>23,033</td><td>278,677</td><td>0.031%</td></tr></table>",
  "experiments": "",
  "hyperparameter": "- `k`: The number of nearest neighbors in item-item graph construction (typically 15).\n- `lambda_C`: The weight for the self-supervised task in the loss function (usually around 0.01).\n- `embedding_size`: The size of the embeddings for items and users, typically set to 64.\n- `batch_size`: The number of samples per training batch (commonly 2048).\n- `learning_rate`: The learning rate for model optimization, usually set to 1e-4 for Adam optimizer.\n- `epochs`: The number of training epochs (typically 1000 for convergence)."
}
