{
  "id": "MVGAE_2021",
  "paper_title": "Multi-Modal Variational Graph Auto-Encoder for Recommendation Systems",
  "alias": "MVGAE",
  "year": 2021,
  "domain": "Recsys",
  "task": "MultiModalRecommendation",
  "introduction": "",
  "method": "# III. METHODOLOGY\n\nIn this section, we will introduce our proposed MVGAE in details. The framework is shown in Fig. 1, which consists of a hierarchical multi-modal encoder and an inner product decoder. To better exploit the information of each modality, we use modality-specific encoders to infer modality-specific latent variables. Then we resort to conditional independence assumption to derive a tractable objective for MVGAE, where single-modal latent variables are fused by a product-of-experts (PoE) module to get feature-based latent variable. Another PoE module is utilized to fuse feature-based and collaborative-based latent variables hierarchically. Finally, the inner product decoder is adopted to reconstruct the user-item adjacency matrix such that the final prediction could be made according to the learned latent embeddings. Notations used in this article are summarized in Table I.\n\n![](images/c66c18d7054a8d0f79cfe8020eb6760ba6e9817c61f6b3d2353c927ca8af550f.jpg)  \nFig. 1. Framework of MVGAE. First, features of each modality $\\mathbf { X } _ { m }$ and user-item adjacency matrix A are encoded into modality-specific latent variables. Second, feature-based latent variable is obtained by taking the product of modality-specific variables with a PoE module, and then feature-based and collaborative-based latent variables are fused with a higher-level PoE module hierarchically to compute the joint latent variables Z, where sample $\\hat { \\mathbf { Z } }$ is drawn with the reparamerization trick. Third, the link between user $_ u$ and item $_ i$ is decoded by an inner product decoder.\n\nTABLE I NOTATIONS USED IN OUR METHOD   \n\n<table><tr><td>Notation</td><td>Description</td></tr><tr><td>u∈u</td><td>user u,in user setU with |U| nodes</td></tr><tr><td>i∈I</td><td>itemi,in item set I with |Z|nodes</td></tr><tr><td>d</td><td>dimension of latent embeddings</td></tr><tr><td>G= (v,ε)</td><td>user-item bipartite graph</td></tr><tr><td>V=u+I</td><td>vertex set of G</td></tr><tr><td>N = |U|+||</td><td>number of nodes in G</td></tr><tr><td>k∈{1,...,N}</td><td>index for nodes in G</td></tr><tr><td>m∈{i.,...,M} AERNXN</td><td>index for modalities</td></tr><tr><td></td><td>the adjacency matrix of G matrix of initial user (item) node features</td></tr><tr><td>Um (Vm) Xm =[Um,Vm]</td><td>matrix of initial node features for G</td></tr><tr><td>ZmERN×2d</td><td></td></tr><tr><td></td><td>matrix of latent variables for modality m</td></tr><tr><td>μm∈RNxd Om ∈RN×d</td><td>mean for Zm</td></tr><tr><td></td><td>standard deviation for Zm</td></tr><tr><td>μ{f,c} eRNxd</td><td>mean for feature,collaborative</td></tr><tr><td>T{fC}∈RNxd</td><td>standard deviation for feature,collaborative</td></tr><tr><td>ZERN×2d</td><td>matrix of joint latent variables</td></tr><tr><td>μERNxd</td><td>mean for Z</td></tr><tr><td>geRNxd</td><td>standard deviation for Z</td></tr></table>\n\n![](images/a6d09cc3e0c9001da090b90c8abfa28654adec59177b9ea8e7ce22881c777c72.jpg)  \nFig. 2. Diagram of the moGCN-based layers with aggreg lity-specific encoder which is coon and self-combination operations. $\\bar { \\mathbf { x } } _ { k } ^ { ( 0 ) }$ ed byis the $k$ x(L)k is employed to infer the latent variable for node k which is parameterized by $\\pmb { \\mu } _ { k }$ and $\\sigma _ { k }$ .\n\n# A. Multi-Modal Encoder\n\n1) Modality-Specific Encoder: Given the user-item bipartite graph $G$ , we have the adjacency matrix A where for each element $\\mathbf { A } _ { i j }$ in this matrix, if the node $i$ and $j$ are adjacent, and then ${ \\bf A } _ { i j } = 1$ , otherwise ${ \\bf A } _ { i j } = 0$ . Furthermore, we assume the diagonal elements to be 1 as in [33]. Therefore, the adjacency matrix denotes an annular graph where each node has a self-loop to better enhance its own features during message aggregation. For modality $m$ , the matrix of initial node features $\\mathbf { X } _ { m }$ is concatenated by node features of users ${ \\mathbf { U } } _ { m }$ and items ${ \\mathbf { V } _ { m } }$ . ${ \\mathbf { U } } _ { m }$ is randomly sampled from Gaussian distributions initially and ${ \\mathbf { V } } _ { m }$ is the feature matrix extracted from original item contents. Our modality-specific encoder aims to infer the modality-specific latent variables using A and $\\mathbf { X } _ { m }$ . Given the latent variables $\\mathbf { Z } _ { m }$ , nodes in $G$ are assumed to be conditional independent [33], where our modality-specific inference model can be represented as:\n\n$$\nq _ { \\phi } ( \\mathbf { Z } _ { m } | \\mathbf { X } _ { m } , \\mathbf { A } ) = \\prod _ { k = 1 } ^ { N } q _ { \\phi } \\left( \\mathbf { z } _ { m } ^ { k } | \\mathbf { X } _ { m } , \\mathbf { A } \\right)\n$$\n\n$$\nq _ { \\phi } \\left( \\mathbf { z } _ { m } ^ { k } | \\mathbf { X } _ { m } , \\mathbf { A } \\right) = { \\mathcal { N } } \\left( \\mathbf { z } _ { m } ^ { k } | { \\boldsymbol { \\mu } } _ { m } ^ { k } , \\mathrm { d i a g } \\left( { \\boldsymbol { \\sigma } } _ { m } ^ { k } ^ { 2 } \\right) \\right) ,\n$$\n\nwhere $N = | \\mathcal { U } | + | \\mathcal { T } |$ is the number of nodes and $k \\in$ $\\{ 1 , \\ldots , N \\}$ is the index of nodes. $\\mathbf { z } _ { m } ^ { k }$ denotes the latent variable for the $k$ th node with the matrix form $\\mathbf { Z } _ { m }$ w.r.t. modality $m$ . Here, we assume each node as a Gaussian variable, $\\pmb { \\mu } _ { m }$ is the matrix of mean vectors, similarly, $\\log ( { \\pmb { \\sigma } _ { m } } ^ { 2 } )$ is the matrix of log of diagonal variance vectors.\n\nThe modality-specific inference model is parameterized by GCN-based layers. The details are presented in Fig. 2. To better capture modality-specific features and interactions, the GCN-based layer is well-designed which contains two parts: aggregation and self-combination. We focus on modality $m$ in the following of this section. Moreover, multiple GCN-based layers could be stacked to explore the higher-order connectivity information. In this way, the representations of nodes can be recursively calculated, where we denote $l$ as the propagation steps.\n\nFor a node $k$ in $G$ , we employ a mean aggregation function to fuse the content features of its neighbors, where the aggregation based on the message-passing mechanism of GCNs helps to gather features from its neighbors. $\\hat { \\mathbf { x } } _ { k } ^ { ( l ) }$ is the representation of node $k$ after lth layer aggregations which calculates as:\n\n$$\n\\hat { \\mathbf { x } } _ { k } ^ { ( l ) } = \\mathrm { L e a k y R e L U } \\left( \\frac { 1 } { | \\mathbb { N } _ { k } | } \\sum _ { j \\in \\mathbb { N } _ { k } } \\mathbf { W } _ { m , 1 } ^ { ( l ) } \\mathbf { x } _ { j } ^ { ( l ) } \\right) ,\n$$\n\nwhere $\\mathbb { N } _ { k } = \\{ j | ( k , j ) \\in \\mathcal { E } \\}$ denotes the neighbors of node $k$ . $\\mathbf { x } _ { j } ^ { ( l ) }$ is the feature of node $j$ after the lth layer. $\\mathrm { W } _ { m , 1 } ^ { ( l ) }$ is the feature 1transformation matrix. During the aggregation, for a user, features of his or her interacted items converge such that preferred aspects of items for the user could be strengthened in the latent variable. With multi-layer aggregation, the embedding process can gather more information and the sparsity problem could be alleviated.\n\nSelf-combination is employed to amplify its own feature $\\mathbf { x } _ { k } ^ { ( l ) }$ of node $k$ to conduct deeper layers of the modality-specific encoder inspired by residual connections [45]. The combination operation between $\\hat { \\mathbf { x } } _ { k } ^ { ( l ) }$ and x(l)k can be represented as:\n\n$$\n\\begin{array} { r } { \\phantom { \\frac { 1 } { k } } \\tilde { \\mathbf { x } } _ { k } ^ { ( l ) } = \\mathrm { L e a k y R e L U } \\left( \\mathbf { W } _ { m , 2 } ^ { ( l ) } \\cdot \\mathbf { x } _ { k } ^ { ( l ) } \\right) \\phantom { x x x x x x x x x x x x x x } } \\\\ { \\phantom { \\frac { 1 } { k } } \\mathbf { x } _ { k } ^ { ( l + 1 ) } = \\mathrm { L e a k y R e L U } \\left( \\mathbf { W } _ { m , 3 } ^ { ( l ) } \\left( \\hat { \\mathbf { x } } _ { k } ^ { ( l ) } \\| \\tilde { \\mathbf { x } } _ { k } ^ { ( l ) } \\right) \\right) , } \\end{array}\n$$\n\nwhere W(l) and W(l)m, are the weight matrix of linear transform layers. xˆ(l)k 2 3is the lth messages from neighbors, and $\\tilde { \\mathbf { x } } _ { k } ^ { ( l ) }$ is the lth transformed feature representation of itself. LeakyReLU [46] is adopted as the activation function.\n\nThe node $k$ can receive messages from its $L$ -hop neighbors after stacking $L$ GCN-based propagation layers, where we obtain x(L)k . Then we use two separate GCN-based layers to learn $\\mathbf { Z } _ { m }$ , parameterized by $\\pmb { \\mu } _ { m }$ and $\\log ( { \\pmb { \\sigma } _ { m } } ^ { 2 } )$ as in variational auto-encoders [21]. These two GCN-based layers remove activation function LeakyReLU in Eq. (6) so as to learn both positive and negative numerics of $\\pmb { \\mu } _ { m }$ and $\\log ( { \\pmb { \\sigma } _ { m } } ^ { 2 } )$ .\n\n2) Multi-Modal Fusion: A straightforward way to infer approximated posteriors of multi-modal features is to adopt the early fusion strategy [47], which takes the concatenated features from all modalities as input and outputs the parameters of joint feature-based posteriors. However, such an approach can only acquire an entangled uncertainty estimation from concatenated modalities, which fails to distinguish modalities with multiple uncertainty levels. Thus, to infer the joint feature-based latent variables $\\mathbf { Z } _ { f }$ based on useful and complementary information from all modalities, we assume the observations of $M$ modalities, $\\mathbf { X } _ { 1 } , \\ldots , \\mathbf { X } _ { M }$ , are conditional independent given $\\mathbf { Z } _ { f }$ inspired by [22]. Then the generative model becomes:\n\n$$\np \\left( \\mathbf { X } _ { 1 } , \\ldots , \\mathbf { X } _ { M } | \\mathbf { Z } _ { f } \\right) = p \\left( \\mathbf { X } _ { 1 } | \\mathbf { Z } _ { f } \\right) p \\left( \\mathbf { X } _ { 2 } | \\mathbf { Z } _ { f } \\right) \\ldots p \\left( \\mathbf { X } _ { M } | \\mathbf { Z } _ { f } \\right) .\n$$\n\nLet $\\mathbf { X } = \\mathbf { X } _ { 1 } , \\dots , \\mathbf { X } _ { M }$ , and we omit A in the inference and gen1erative models for brevity of writing. The evidence lower bound (ELBO) of log marginal likelihood for all modalities $\\mathcal { L } ( \\mathbf { X } )$ becomes:\n\n$$\n\\mathbb { E } _ { q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } ) } \\left[ \\sum _ { \\mathbf { X } _ { m } \\in \\mathbf { X } } \\log p \\left( \\mathbf { X } _ { m } | \\mathbf { Z } _ { f } \\right) \\right] - \\mathrm { K L } \\left[ q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } ) , p ( \\mathbf { Z } _ { f } ) \\right] .\n$$\n\nTherefore, the problem of tackling the KL divergence term in $\\mathcal { L } ( \\mathbf { X } )$ comes to resolve the approximated joint featurebased posteriors $q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } )$ . Knowing that the optimal value of $q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } )$ are the true posteriors $p ( \\mathbf { Z } _ { f } | \\mathbf { X } )$ , we could transform the problem to find the relation among $p ( \\mathbf { Z } _ { f } | \\mathbf { X } )$ and single-modality posteriors $p _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { \\mathbf { m } } )$ . Here, with the conditional independence assumption, we have:\n\n$$\n\\begin{array} { r l r } {  { p ( \\mathbf { Z } _ { f } | \\mathbf { X } ) } = \\frac { p ( \\mathbf { X } | \\mathbf { Z } _ { f } ) p ( \\mathbf { Z } _ { f } ) } { p ( \\mathbf { X } ) } = \\frac { p ( \\mathbf { Z } _ { f } ) } { p ( \\mathbf { X } ) } \\prod _ { m = 1 } ^ { M } p ( \\mathbf { X } _ { m } | \\mathbf { Z } _ { f } ) }  \\\\ & { } & { = \\frac { p ( \\mathbf { Z } _ { f } ) } { p ( \\mathbf { X } ) } \\prod _ { m = 1 } ^ { M } \\frac { p ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } ) p ( \\mathbf { X } _ { m } ) } { p ( \\mathbf { Z } _ { f } ) } . } \\end{array}\n$$\n\nFurthermore, based on VAEs, $p ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } )$ could be approximated with $q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } ) p ( \\mathbf { Z } _ { f } )$ , where $q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } )$ are the variational posteriors for the $m$ th modality and $p ( \\mathbf { Z } _ { f } )$ are the priors. With this approximation, we can avoid the quotient term and the joint feature-based posteriors become:\n\n$$\n\\begin{array} { r l r } {  { p ( \\mathbf { Z } _ { f } | \\mathbf { X } ) \\propto \\frac { \\prod _ { m = 1 } ^ { M } p ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } ) } { \\prod _ { m = 1 } ^ { M - 1 } p ( \\mathbf { Z } _ { f } ) } \\approx \\frac { \\prod _ { m = 1 } ^ { M } q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } ) p ( \\mathbf { Z } _ { f } ) } { \\prod _ { m = 1 } ^ { M - 1 } p ( \\mathbf { Z } _ { f } ) } \\ } } \\\\ & { } & \\\\ & { } & { \\qquad = p ( \\mathbf { Z } _ { f } ) \\prod _ { m = 1 } ^ { M } q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { m } ) . \\qquad ( 9 } \\end{array}\n$$\n\nFrom this equation, the approximated joint feature-based posteriors are shown to be factorized into a product of modality-specific variational posteriors and we omit the priors $p ( \\mathbf { Z } _ { f } )$ for simplification. This derivation would be easily extended to any subset of modalities yielding $q _ { \\phi } ( \\mathbf { Z } _ { f } | \\mathbf { X } _ { S } ) \\propto$ $\\begin{array} { r } { \\prod _ { m = 1 } ^ { | S | } q _ { \\phi } ( \\mathbf { Z } _ { m } | \\mathbf { X } _ { m } ) , S \\subset \\{ 1 , \\ldots , M \\} } \\end{array}$ which can easily deal =1with modality missing problem.\n\nBy using the property of Gaussian distribution, parameters for the joint feature-based latent variables of multiple modalities can be computed by taking the product distribution:\n\n$$\n\\begin{array} { r l } & { \\mu _ { f } = \\frac { \\sum _ { m = 1 } ^ { M } \\left( \\mu _ { m } \\odot \\left( 1 / \\sigma _ { m } ^ { 2 } \\right) \\right) } { \\sum _ { m = 1 } ^ { M } \\left( 1 / \\sigma _ { m } ^ { 2 } \\right) } } \\\\ & { \\sigma _ { f } = \\mathrm { s q r t } \\left( \\frac { 1 } { \\sum _ { m = 1 } ^ { M } \\left( 1 / \\sigma _ { m } ^ { 2 } \\right) } \\right) , } \\end{array}\n$$\n\nwhere $\\pmb { \\mu } _ { m }$ and $\\pmb { \\sigma } _ { m } ^ { 2 }$ are the parameters of the $m$ th Gaussian expert. By applying PoE, the joint feature-based mean vectors $\\mu _ { f }$ which represent semantic information are in essence a weighted summation of modality-specific $\\pmb { \\mu } _ { m }$ with the weight to be the precision $1 / \\sigma _ { m } ^ { 2 }$ of each modality. It indicates that for the expert that has lower variance, it contains more useful information for recommendations and has more influence under the joint feature-based representation.\n\nTo further exploit the collaborative patterns, we exploit another GCN-based encoder to model interactions between users and items, where we obtain the collaborative-based latent variable $\\mathbf { Z } _ { C }$ parameterized by $\\pmb { \\mu } _ { C }$ and $\\sigma _ { C } ^ { 2 }$ . Then we fuse the feature-based and collaborative-based latent embeddings, i.e., $\\mathbf { Z } _ { f }$ and $\\mathbf { Z } _ { C }$ , with another PoE principle hierarchically. Therefore, the approximated joint posteriors of $\\mathbf { Z }$ could be obtained as:\n\n$$\nq _ { \\phi } ( { \\bf Z } | { \\bf X } , { \\bf X } _ { C } ) = q _ { \\phi } ( { \\bf Z } _ { f } | { \\bf X } ) q _ { \\phi } ( { \\bf Z } _ { C } | { \\bf X } _ { C } ) ,\n$$\n\nwhere $\\mathbf { X }$ is the multi-modal item features and $\\mathbf { X } _ { C }$ is the collaborative-based features which are randomly initialized and propagate with GCN to capture collaborative structures as in [34]. Specifically, the collaborative-based encoder can expressively model of high-order connectivity in the user-item graph, which effectively injects the collaborative signal into the embedding process during the propagation. Moreover, with the PoE module, more collaborative information could be employed for more dense datasets by giving more importance to collaborative-based embeddings during the higher-level PoE fusion. On the other hand, more item feature information could be exploited for more sparse datasets.\n\n# B. Inner Product Decoder\n\nAfter getting the approximated joint posteriors of $\\mathbf { Z }$ which are parameterized by $\\pmb { \\mu }$ and $\\sigma$ , the decoder is aimed at reconstructing the adjacency matrix A such that the learned joint latent embeddings could best predict the interactions between users and items. Now that $\\mathbf { Z }$ capture information of users’ preferences of interactions and item features, the similarities between users and items could be calculated by the distance between the joint embeddings of users and items. Here, an inner product between the joint latent embeddings is adopted to decode the adjacency matrix:\n\n$$\np ( \\mathbf { A } | \\mathbf { Z } ) = \\prod _ { k _ { 1 } = 1 } ^ { N } \\prod _ { k _ { 2 } = 1 } ^ { N } \\mathbf { z } _ { k _ { 1 } } ^ { \\top } \\mathbf { z } _ { k _ { 2 } } ,\n$$\n\nwhere ${ \\bf z } _ { k _ { 1 } }$ and ${ \\bf z } _ { k _ { 2 } }$ are the latent embeddings for node $k _ { 1 }$ and $k _ { 2 }$ . During training, the latent embedding of node $k$ 1is sampled 2from $\\mathcal { N } ( \\mathbf { z } _ { k } | \\mu _ { k } , \\sigma _ { k } ^ { 2 } )$ . The sampling operation is a corruption of the joint latent mean vectors such that they could be more robust to noise. While during testing, the latent embedding is fixed as the learned mean vector $\\pmb { \\mu } _ { k }$ , which captures the semantic information, to make link predictions [33].\n\nIt is worth noting that for sparse datasets, the large variance of the node embeddings may cause the positive inner product of two node embeddings to become negative during the sampling in reparameterization [21] in the training phase, or vice versa. Therefore, a Sigmoid function is added to the latent embeddings before the inner product operation. With the activation function, which maps variables to positive spaces, the inner product operation will be more robust and accurate. While for more dense datasets, the nonlinear mapping function compresses and deforms the original latent space, making the similarity calculation less accurate. Therefore, the Sigmoid function is selectively added to more sparse datasets.",
  "experiments": "",
  "hyperparameter": "- embedding_size (d): Dimensionality of the latent embeddings for users and items. Controls the representation capacity of MVGAE; fixed to 64 for all datasets in the experiments.\n- num_gcn_layers (L): Depth of the common GCN-based propagation in the modality-specific encoders. Determines how many hops of neighbors are aggregated; typically set between 1 and 3, with L = 2 used by default.\n- beta: Trade-off coefficient between the reconstruction (BPR) term and the KL-divergence regularization in the ELBO-style objective. Controls how much information from item features is allowed into the latent variables; tuned over {0.01, 0.1, 1} and usually set to 1.\n- learning_rate: Step size for the Adam optimizer. Tuned over {0.0001, 0.001, 0.01, 0.1}; typical values are 0.001 (Movies, AliShop) and 0.005 (Electronics).\n- l2_reg (weight_decay): Coefficient of L2 normalization/weight decay on model parameters to prevent overfitting. Searched in {0.0001, 0.001, 0.01, 0.1, 0}, with dataset-specific best choices (e.g., 0.001 or 0.0001).\n- epsilon_std (lambda): Standard deviation of the Gaussian noise used in the reparameterization trick for sampling latent embeddings. Controls corruption strength and robustness; set to 0.1 by default.\n- num_negative_samples: Number of negative items sampled per positive interaction when forming BPR triplets. Default is 1 for most datasets; increased to 10 negatives on Movies to mitigate class imbalance.\n- batch_size: Number of interactions used per optimization step. Fixed to 1024 for all experiments, balancing convergence speed and memory usage."
}
