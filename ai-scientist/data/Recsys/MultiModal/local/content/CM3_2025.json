{
    "id": "CM3_2025",
    "paper_title": "CM3: Calibrating Multimodal Recommendation",
    "alias": "CM3",
    "year": 2025,
    "domain": "Recsys",
    "task": "MultiModalRecommendation",
    "idea": "CM3 introduces a spherical Bézier multimodal fusion that interpolates an infinite number of modality features on a hypersphere via De Casteljau’s algorithm, and a calibrated uniformity loss that amplifies repulsion between dissimilar items by a factor of e^{2t(1-φ)} while leaving similar items closer, enabling finer-grained item embedding distributions.",
    "introduction": "# 1 Introduction\n\nThe advent of multimodal learning has intensified attention on multimodal recommender systems, which leverage heterogeneous data modalities (e.g., visual and textual information) associated with items to achieve effective recommendation [1-3]. Within this burgeoning field, contrastive learning has emerged as a promising paradigm for enhancing the learning of user and item representations from multimodal data. In fact, the contrastive learning framework is predicated upon two fundamental principles: alignment and uniformity [4]. In the context of multimodal recommendation, alignment ensures consistency between representations derived from distinct modalities or positive user-item pairs, while uniformity promotes an equitable distribution of user and item representations across\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/3fc8fb99ebfa69dad77299104dad21266f2a04e620824f41a84cfdad4a60bdd2.jpg)\n\n\n\nFigure 1: Training dynamics of alignment loss ( $l_{\\mathrm{align}}$ ) and item uniformity loss ( $l_{\\mathrm{uniform-item}}$ ) for various multimodal models. Optimal validation performance, indicated by stars, is accompanied by corresponding changes in loss values (denoted by colored arrows). Performance is quantified using Recall@20, shown in brackets after each model name.\n\n\na unit hypersphere. Prior research [5], which relies exclusively on user-item interactions, has established that directly optimizing alignment and uniformity can significantly enhance recommendation performance. However, these principles remain under-explored in multimodal recommendation, where the integration of diverse feature modalities necessitates delicate consideration.\n\nFirstly, we demonstrate the contrary in optimizing alignment and uniformity in current multimodal recommender models. According to Theorem 1 of DirectAU [5], perfectly aligned and uniform encoders, if they exist, are the global minimizers of the Bayesian Personalized Ranking (BPR) loss [6]. This implies that the BPR loss inherently promotes lower alignment between positive user-item pairs and uniformity between user-user and item-item pairs. However, our empirical analysis reveals that this theoretical optimum is not achieved in multimodal recommendation. We illustrate this by plotting the training evolution of alignment and uniformity metrics for five representative multimodal models with Clothing dataset in Fig. 1. Among these, BM3 [7] utilizes contrastive learning techniques to derive user and item representations, while VBPR [8] and FREEDOM [9] employ the BPR loss for model optimization. It is noteworthy that LGMRec [10] and DA-MRS [11] incorporates both contrastive learning and BPR loss for model optimization. As depicted in Fig. 1, all multimodal models exhibited a distinct bias towards optimizing uniformity, thereby compromising alignment, during the latter stages of training. This unexpected finding reveals\n\na divergence from the typical optimization behavior seen in general recommender systems, as documented in [5].\n\nSecondly, we delve into the underlying mechanisms that precipitate the observed behavior. Given two pairs of interactions for  $u$  as  $(u,i)$  and  $(u,j)$ , based on the research of [4],  $l_{\\mathrm{align}}$  minimizes both  $\\mathbb{E}_{(u,i)\\sim p_{\\mathrm{pos}}}\\left[\\| f(u) - f(i)\\|_2^2\\right]$  and  $\\mathbb{E}_{(u,j)\\sim p_{\\mathrm{pos}}}\\left[\\| f(u) - f(j)\\|_2^2\\right]$ , while  $l_{\\mathrm{uniform}}$  minimizes  $\\mathbb{E}_{(i,j)\\sim p_{\\mathrm{item}}}\\left[e^{-t\\| f(i) - f(j)\\|_2^2}\\right]$ . If  $u$  is perfectly aligned with both items,  $\\mathbb{E}_{(i,j)\\sim p_{\\mathrm{item}}}\\left[\\| f(i) - f(j)\\|_2^2\\right]$  tends to be minimized. However, this conflicts with the objective of  $l_{\\mathrm{uniform}}$ , which aims to maximize  $\\| f(i) - f(j)\\|_2^2$ . Consequently, models face challenges in balancing the optimization of these competing objectives. Furthermore, the incorporation of multimodal information further deteriorates item uniformity optimization, as items with similar multimodal features cluster more tightly in the embedding space than items with randomly generated multimodal data, as can be evidenced by Table 6 in the Appendix.\n\nTo address this issue, we propose a Calibrated MultiModal Model  $(\\mathrm{CM}^3)$  that enhances recommendation efficacy by modulating item uniformity via the utilization of multimodal information. Specifically, we initially compute a similarity score based on the multimodal features of items. This score is subsequently integrated into the uniformity loss, aiming to repel dissimilar items while maintaining proximity between similar items. We further provide a theoretical analysis demonstrating the pivotal role of the similarity score in determining the behavior of the calibrated uniformity loss with respect to items. To quantify similarity by leveraging the intrinsic information of each modality, we propose a Spherical Bézier fusion method that integrates multimodal data into a unified vector. The item-item similarity score is then derived from this composite vector. This approach ensures that the resulting vectors retain hyperspherical properties, as each constituent modality vector already lies on the hypersphere. Our key contributions are as follows:\n\n- We elucidate the inherent dilemma faced by conventional multimodal recommendation models in simultaneously optimizing the alignment of positive interactions and maintaining uniformity between user-user and item-item relationships.\n\n- We introduce a novel calibrated recommendation model,  $\\mathrm{CM}^3$ , which refines inter-item relations within the uniformity loss function by utilizing multimodal features. In  $\\mathrm{CM}^3$ , we design a spherical Bézier fusion method to blend data from all modalities, preserving semantics by integrating multimodal features along the shortest path on a spherical surface.\n\n- We conduct comprehensive empirical evaluations on real-world datasets, demonstrating that  $\\mathrm{CM}^3$  significantly outperforms state-of-the-art multimodal recommender systems. To gain a nuanced understanding of  $\\mathrm{CM}^3$ 's efficacy, we also perform extensive ablation studies under various evaluation configurations.",
    "method": "# 3 Calibrated Multimodal Recommendation\n\n# 3.1 Overview of  $\\mathbf{CM}^3$\n\nThe crux of multimodal models lies in their capacity to learn informative user and item representations for recommendation, leveraging rich multimodal features. To this end,  $\\mathrm{CM}^3$  implements a bifurcated strategy to derive item representations: i) the augmentation of multimodal features through a Spherical Bezier Multimodal Fusion technique, which facilitates the integration and transformation of diverse data modalities in a hyperspherical manifold space; and ii) the refinement of low-dimensional item embeddings via the application of a meticulously calibrated uniformity loss function. This novel uniformity loss enables the model to distill the multifaceted nature of multimodal information into a refined and discriminative representational framework, thereby enhancing the model's capacity to capture nuanced item characteristics and inter-item relationships.\n\nWe elucidate the constituent components of  $\\mathrm{CM}^3$  in the subsequent subsections, accentuating the innovative design elements while concisely referencing the foundational mechanisms upon which  $\\mathrm{CM}^3$  is constructed, such as Graph Convolutional Networks (GCN). Fig. 2 presents an overview of  $\\mathrm{CM}^3$ .\n\n# 3.2 Notations\n\nConsider a dataset  $\\mathcal{D}$  defined by the tuple  $\\mathcal{D} = \\{\\mathcal{G},\\mathrm{X}^0,\\dots ,\\mathrm{X}^{|M| - 1}\\}$ , where  $\\mathcal{G} = (\\mathcal{V},\\mathcal{E})$  denotes the interaction bipartite graph. The set  $\\mathcal{M}$  encompasses all available modalities pertinent to the items under consideration. Within this framework,  $\\mathcal{E}$  and  $\\mathcal{V}$  denote the edge set and node set of the graph, respectively, encapsulating the interactions between users and items. More formally, an edge  $\\varepsilon_{ui} = 1$  within  $\\mathcal{G}$  signifies the existence of an interaction between a user  $u$  and an item  $i$ . The node set  $\\mathcal{V}$  is defined as the union of user and item sets, such that  $\\mathcal{V} = \\mathcal{U}\\cup \\mathcal{I}$ , where  $u\\in \\mathcal{U}$  represents a user and  $i\\in \\mathcal{I}$  denotes an item. For each modality  $m\\in \\mathcal{M}$ , we define a feature matrix  $\\mathbf{X}^{m}\\in \\mathbb{R}^{|I|\\times d_{m}}$ , where  $|\\varOmega|$  represents the cardinality of the item set and  $d_{m}$  signifies the original dimensionality of the feature space for modality  $m$ .\n\nGiven the dataset  $\\mathcal{D}$ , the objective of a multimodal recommender system is to generate a ranked list of items for each user  $u$ , predicated on a preference score function. This function, denoted as  $\\hat{y}_{ui}$ , quantifies the predicted affinity between user  $u$  and item  $i$ , and is formally defined as:  $\\hat{y}_{ui} = f_{\\Theta}(u,i,\\mathbf{x}_i^0,\\dots ,\\mathbf{x}_i^{|\\mathcal{M}| - 1})$ . The model  $f_{\\Theta}(\\cdot)$  is parameterized by  $\\Theta$ , a set of trainable parameters.\n\n# 3.3 Spherical Bézier Multimodal Fusion\n\n3.3.1 Multimodal Feature Projection. The multimodal features extracted from pretrained models are often tangentially related to the downstream task and typically characterized by high dimensionality. To address these challenges, we employ Deep Neural Networks (DNNs) to project each individual modality feature into its corresponding low-dimensional space. This dimensionality reduction not only mitigates computational complexity but also enhances the\n\nrelevance of the features to the task at hand. Specifically, given a unimodal feature matrix of items, denoted as  $\\mathbf{X}^m\\in \\mathbb{R}^{|\\mathcal{I}|\\times d_m}$ , we derive the latent unimodal representation through the following equation:\n\n$$\n\\widetilde {\\mathbf {X}} ^ {m} = \\sigma \\left(\\mathbf {X} ^ {m} \\mathbf {W} _ {1} ^ {m} + \\mathbf {b} _ {1} ^ {m}\\right) \\mathbf {W} _ {2} ^ {m}, \\tag {1}\n$$\n\nwhere  $\\sigma (\\cdot)$  denotes an activation function, such as the 'Leaky_relu' function.  $\\mathbf{W}_1^m\\in \\mathbb{R}^{d_m\\times d_1}$ ,  $\\mathbf{W}_2^m\\in \\mathbb{R}^{d_1\\times d}$ , and  $\\mathbf{b}_1^m\\in \\mathbb{R}^{d_1}$  represent the trainable weight matrices and bias vector, respectively. Here,  $d_{1}$  and  $d$  indicate the vector dimensions.\n\n3.3.2 Infinite Multimodal Fusion. Given the unimodal representations derived from Equation (1) using distinct pre-trained encoders, a multimodality gap may arise. To address this, we propose an advanced interpolation method based on Mixup to effectively fuse the representations. Mixup [49-51] is a technique that linearly interpolates pairs of data points, creating synthetic samples to enrich the training set. Empirical studies have consistently demonstrated its effectiveness in improving the generalization and robustness of neural networks. While traditional Mixup typically leverages both feature vectors and labels from two samples for interpolation. In this work, we extend this approach to enable infinite multimodal fusion. First, in the absence of labels, we interpolate multimodal features corresponding to the same item (item as label) for fusion. Second, we employ De Casteljau's algorithm to iteratively combine an infinite number of multimodal features. This method ensures that the interpolated vector traverses a Bézier curve defined by the multimodal vectors while remaining constrained within the hyperspherical manifold. Given a set of unimodal features  $[\\tilde{\\mathbf{x}}_i^m]$ , where  $m \\in \\mathcal{M}$ , for an item  $i$ , the mixed feature can be computed as:\n\n$$\nh \\left(\\left[ \\tilde {\\mathbf {x}} _ {i} ^ {m} \\right]\\right) = \\underbrace {f \\left(\\tilde {\\mathbf {x}} _ {i} ^ {| \\mathcal {M} | - 1} , f \\left(\\cdots , f \\left(\\tilde {\\mathbf {x}} _ {i} ^ {2} , f \\left(\\tilde {\\mathbf {x}} _ {i} ^ {1} , \\tilde {\\mathbf {x}} _ {i} ^ {0}\\right)\\right) \\cdots\\right)\\right)} _ {| \\mathcal {M} | - 1}, \\tag {2}\n$$\n\nwhere  $f(\\vec{a},\\vec{b})$  denotes the spherical interpolation function that is defined as:\n\n$$\nf (\\vec {a}, \\vec {b}) = \\frac {\\sin (\\lambda \\theta)}{\\sin (\\theta)} \\vec {a} + \\frac {\\sin ((1 - \\lambda) \\theta)}{\\sin (\\theta)} \\vec {b}. \\tag {3}\n$$\n\nIn this equation,  $\\theta = \\cos^{-1}(\\vec{a},\\vec{b})$  represents the angle between vectors  $\\vec{a}$  and  $\\vec{b}$ , and  $\\lambda$  is sampled from a Beta distribution with hyperparameter  $\\alpha$ , such that  $\\lambda \\sim \\mathrm{Beta}(\\alpha ,\\alpha)$ .\n\nPROPOSITION 1. Given that all vectors in  $[\\tilde{\\mathbf{x}}_i^m ]$  lie on the hypersphere, the mixed feature defined by Equation (2) also lies on the hypersphere.\n\nProof. The proof of this proposition is straightforward and is provided in the Appendix A.  $\\square$\n\n# 3.4 Enhancing User and Item Representations via Graph Learning\n\nTo adeptly capture higher-order interactions between users and items, as well as the intricate semantic relationships among items, we employ the widely acknowledged graph learning paradigm [9, 25]. This methodology facilitates the derivation of user and item representations from both user-item and item-item graphs.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/53476c8b9c08297126a00a0cce75232bccd6a417eb2cf1c19d66b36a3ad22bbd.jpg)\n\n\n\n(a) Spherical Bézier Multimodal Fusion\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/50729edac95274945225254a15f5318fe291e1c706b472f6a3be23cf11489344.jpg)\n\n\n\n(b) User & Item Representation Learning\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/fb70815f9adc6acb53de6135cf1d785632dcbfe285085555d36c06de6003cf0b.jpg)\n\n\n\n(c) Differences between the Standard and Calibrated Uniformity\n\n\n\nFigure 2: Overview of our proposed  $\\mathbf{CM}^3$ . (a) We encode the product images and textual descriptions using pre-trained models, then mix multimodal features with a Spherical Bézier Fusion. (b) Initial item and user embeddings are enhanced through user-item and item-item graphs using GNNs. Alignment, as well as standard and calibrated uniformity losses, are used to optimize the distributions of user and item representations on a unit hypersphere. (c) A toy example demonstrates the differences between standard uniformity and our calibrated uniformity losses over items.\n\n\n3.4.1 Graph Learning on User-Item Graph. We first concatenate the  $|\\mathcal{M}| + 1$  latent features into a single vector to signify item representation:\n\n$$\n\\widetilde {\\mathbf {X}} = \\operatorname {C o n c a t} \\left(\\widetilde {\\mathbf {X}} ^ {0}, \\dots , \\widetilde {\\mathbf {X}} ^ {| \\mathcal {M} | - 1}, h ([ \\widetilde {\\mathbf {X}} ^ {m} ])\\right), \\tag {4}\n$$\n\nwhere  $h([\\widetilde{\\mathbf{X}}^m])$  is the mixed features via Equation (2) at matrix view. The dimension of  $\\widetilde{\\mathbf{X}}$  is  $\\mathbb{R}^{|\\mathcal{I}| \\times \\ell}$ , where  $\\ell = |\\mathcal{M}|d + d$ .\n\nTo accommodate user preference and attend to both the modality-specific latent feature and the multi-modality shared feature, we formulate a user ID embedding matrix, represented as  $\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{U}| \\times \\ell}$ . For the propagation of information within the convolutional network, we employ LightGCN [52]. Specifically, the representations of user  $u$  and item  $i$  at the  $(l + 1)$ -th graph convolution layer of  $\\mathcal{G}$  are derived as follows:\n\n$$\n\\mathbf {e} _ {u} ^ {(l + 1)} = \\sum_ {i \\in \\mathcal {N} _ {u}} \\frac {1}{\\sqrt {\\left| \\mathcal {N} _ {u} \\right|} \\sqrt {\\left| \\mathcal {N} _ {i} \\right|}} \\widetilde {\\mathbf {x}} _ {i} ^ {(l)}; \\tag {5}\n$$\n\n$$\n\\widetilde {\\mathbf {x}} _ {i} ^ {(l + 1)} = \\sum_ {u \\in \\mathcal {N} _ {i}} \\frac {1}{\\sqrt {| \\mathcal {N} _ {u} |} \\sqrt {| \\mathcal {N} _ {i} |}} \\mathbf {e} _ {u} ^ {(l)},\n$$\n\nwhere  $\\mathcal{N}_u$  and  $\\mathcal{N}_i$  denote the set of first hop neighbors of  $u$  and  $i$  in  $\\mathcal{G}$ , respectively. Employing  $L_{ui}$  layers of convolutional operations, we extract all representations from the hidden layers to formulate the final representations for both users and items:\n\n$$\n\\begin{array}{c} \\mathbf {E} = \\operatorname {R E A D O U T} \\left(\\mathbf {E} ^ {0}, \\mathbf {E} ^ {1}, \\dots , \\mathbf {E} ^ {L u i}\\right); \\\\ \\widetilde {\\sim} \\quad \\widetilde {\\sim} _ {0}, \\widetilde {\\sim} _ {1}, \\widetilde {\\sim} _ {L} \\end{array} \\tag {6}\n$$\n\n$$\n\\widetilde {\\mathbf {X}} = \\text {R E A D O U T} (\\widetilde {\\mathbf {X}} ^ {0}, \\widetilde {\\mathbf {X}} ^ {1}, \\dots , \\widetilde {\\mathbf {X}} ^ {L _ {u i}}),\n$$\n\nwhere the READOUT function can be any differentiable function. We use the sum function to derive the representations.\n\n3.4.2 User Preference Mining. To distinguish user preferences among multimodal features, we partition user embeddings into  $|\\mathcal{M}| + 1$  segments, each corresponding to the modality features as defined in Equation (4). A learnable weight matrix  $\\mathbf{W}_3 \\in \\mathbb{R}^{|\\mathcal{U}| \\times (|\\mathcal{M}| + 1) \\times 1}$  is initialized and employed to compute the final user representation. Following the reshaping of both  $\\mathbf{E}$  to the dimensions of  $\\mathbb{R}^{|\\mathcal{U}| \\times (|\\mathcal{M}| + 1) \\times d}$ , we calculate:\n\n$$\n\\widehat {\\mathbf {E}} = \\mathbf {W} _ {3} \\mathbf {E}. \\tag {7}\n$$\n\nSubsequently, having obtained the differentiated user preferences, we reshape the representation back to its original dimensions as:  $\\widehat{\\mathbf{E}} = \\widehat{\\mathbf{E}}.\\mathrm{view}(|\\mathcal{U}|,\\ell)$\n\n3.4.3 Graph Learning on Item-Item Graph. To further elucidate the high-order relationships between items, we adhere to the methodology outlined in existing work [9] to construct an item-item graph S based on multimodal features. Subsequently, we perform graph convolutions on the items using the item-item graph S to derive the final item representations. The detailed procedures involved in this process are not elaborated upon here, as they are analogous to those used in the user-item graph. With the last layer's representation  $\\widetilde{\\mathbf{X}}^{L_{ii}}$  ( $L_{ii}$  is the number of layers), we establish a residual connection with the initial item representation  $(\\widetilde{\\mathbf{X}}^0)$  to obtain the item final representation:\n\n$$\n\\widehat {\\mathbf {X}} = \\overline {{\\mathbf {X}}} ^ {L _ {i i}} + \\widetilde {\\mathbf {X}} ^ {0}. \\tag {8}\n$$\n\n# 3.5 Alignment and Calibrated Uniformity\n\nConsider a positive pair  $(u, i)$  of user and item with corresponding representations  $\\mathbf{u}$  and  $\\mathbf{i}$ , respectively. The alignment and uniformity\n\nlosses are defined as follows:\n\n$$\n\\begin{array}{l} l _ {\\mathrm {a l i g n}} (u, i) = \\underset {(\\mathbf {u}, \\mathbf {i}) \\sim p _ {\\mathrm {p o s}}} {\\mathbb {E}} | | \\mathbf {u} - \\mathbf {i} | | ^ {2}; \\\\ l _ {\\text {u n i f o r m}} (i, i ^ {\\prime}) = \\log_ {\\mathbf {i}, i ^ {\\prime} \\sim p _ {\\text {i t e m}}} \\mathbb {E} _ {e ^ {- t | | \\mathbf {i} - \\mathbf {i} ^ {\\prime} | | ^ {2}},} \\tag {9} \\\\ \\end{array}\n$$\n\nwhere  $t > 0$  is a temperature parameter,  $p_{\\mathrm{pos}}$ ,  $p_{\\mathrm{user}}$ , and  $p_{\\mathrm{item}}$  denote the distributions of positive user-item pairs, users, and items, respectively.  $\\mathbf{u}'$  and  $\\mathbf{i}'$  represent the embeddings of user  $u'$  and item  $i'$ . The alignment loss serves to bring positive pairs  $(u,i)$  closer in the embedding space, while the uniformity loss repels users from other users and items from other items, promoting a uniform distribution of representations.\n\nWe propose that the relationships between items should be differentiated. Consequently, we modify the uniformity loss for items as follows:\n\n$$\nl _ {\\text {c a l - u n i f o r m}} \\left(i, i ^ {\\prime}\\right) = \\log_ {\\mathbb {E} _ {\\mathbf {i}, \\mathbf {i} ^ {\\prime} \\sim p _ {\\text {i t e m}}}} e ^ {- t \\left(\\left\\| \\mathbf {i} - \\mathbf {i} ^ {\\prime} \\right\\| ^ {2} - 2 + 2 s \\left(\\bar {\\mathbf {i}}, \\bar {\\mathbf {i}} ^ {\\prime}\\right)\\right)}, \\tag {10}\n$$\n\nwhere  $s(\\cdot)$  is a function that computes a clamped similarity score between two vectors, which can be pre-calculated before loss computation.  $\\bar{\\mathbf{i}}$  represents any level of representation for item  $i$ . In this context, we utilize the mixed features of  $i$ , defined as  $\\bar{\\mathbf{i}} = h([\\bar{\\mathbf{x}}_i^m])$ .\n\nTHEOREM 1 (CALIBRATED UNIFORMITY AMPLIFICATION). Let  $\\mathcal{I}$  be the set of all items, and let  $\\varphi = s(\\bar{\\mathbf{i}},\\bar{\\mathbf{i}}^{\\prime})$  denote the similarity between a specific pair of items  $i,i^{\\prime}\\in \\mathcal{I}$ . Consider the calibrated uniformity loss function  $l_{\\mathrm{cal-uniform}}$  defined above, the following statement holds:\n\nThe calibrated uniformity loss  $l_{\\mathrm{cal-uniform}}$  amplifies the repulsion between items  $i$  and  $i'$  by a factor of  $e^{2t(1 - \\varphi)}$  relative to the standard uniformity loss.\n\nPROOF. Note that for  $\\mathbf{i},\\mathbf{i}'\\in S^d$  , where  $S^d$  is a unit hypersphere, we have:  $\\| \\mathbf{i} - \\mathbf{i}'\\| ^2 = 2 - 2\\cdot \\mathbf{i}^\\top \\mathbf{i}'$\n\nRelation between  $l_{\\mathrm{cal - uniform}}$  and  $l_{\\mathrm{uniform}}$ :\n\n$$\n\\frac {e ^ {- t \\left(\\left| \\mathbf {i} - \\mathbf {i} ^ {\\prime} \\right| \\right| ^ {2} - 2 + 2 \\varphi)}}{e ^ {- t \\left(\\left| \\mathbf {i} - \\mathbf {i} ^ {\\prime} \\right| \\right| ^ {2}})} = \\frac {e ^ {- t \\left(2 - 2 \\cdot \\mathbf {i} ^ {\\top} \\mathbf {i} ^ {\\prime} - 2 + 2 \\varphi\\right)}}{e ^ {- t \\left(2 - 2 \\cdot \\mathbf {i} ^ {\\top} \\mathbf {i} ^ {\\prime}\\right)}} = e ^ {2 t (1 - \\varphi)} \\tag {11}\n$$\n\nGiven the clamped similarity score between items is bounded within the interval [0, 1], the calibrated uniformity loss  $l_{\\mathrm{cal-uniform}}$  degenerates to the standard uniformity loss  $l_{\\mathrm{uniform}}$  iff  $\\varphi = 1$ . Conversely, for  $\\varphi \\neq 1$ ,  $l_{\\mathrm{cal-uniform}}$  imposes a more stringent repulsion  $(\\because e^{1 - \\varphi} > 1)$  between dissimilar items compared to the standard uniformity loss. Consequently, this mechanism promotes items that are similar to themselves to be positioned closer together on the hypersphere.\n\n# 3.6 Model Optimization and Recommendation\n\nFor model optimization, we adopt the alignment for positive pairs and the uniformity loss for users with representation of  $\\widehat{\\mathbf{E}}$ , but with the calibrated uniformity loss on items based on representation of  $\\widehat{\\mathbf{X}}$ . The final loss:\n\n$$\n\\mathcal {L} = l _ {\\text {a l i g n}} (u, i) + \\gamma \\left(l _ {\\text {u n i f o r m}} (u, u ^ {\\prime}) + l _ {\\text {c a l - u n i f o r m}} (i, i ^ {\\prime})\\right). \\tag {12}\n$$\n\nTo generate item recommendations for a user, we calculate the score for a possible interaction between  $u$  and  $i$  as:\n\n$$\n\\hat {y} _ {u i} = \\widehat {\\mathbf {e}} _ {u} ^ {\\top} \\widehat {\\mathbf {x}} _ {i}. \\tag {13}\n$$\n\nA high score suggests that the user prefers the item. Based on these scores, we select the top- $k$  items as recommendations for user  $u$ .\n\n\nTable 1: Statistics of the experimental datasets.\n\n\n<table><tr><td>Dataset</td><td># Users</td><td># Items</td><td># Interactions</td><td>Sparsity</td></tr><tr><td>Baby</td><td>19,445</td><td>7,050</td><td>160,792</td><td>99.88%</td></tr><tr><td>Sports</td><td>35,598</td><td>18,357</td><td>296,337</td><td>99.95%</td></tr><tr><td>Clothing</td><td>39,387</td><td>23,033</td><td>278,677</td><td>99.97%</td></tr><tr><td>Electronics</td><td>192,403</td><td>63,001</td><td>1,689,188</td><td>99.99%</td></tr><tr><td>MicroLens</td><td>98,129</td><td>17,228</td><td>705,174</td><td>99.96%</td></tr></table>\n\n# 3.7 Computational Complexity Analysis\n\nThe computational complexity associated with the alignment and uniformity computations is equivalent for both DirectAU and  $\\mathrm{CM}^3$ , with the exception of similarity score calculations  $(O\\left(\\sum_{m}^{M}|\\mathcal{I}|(d_{m}d_{1} + d_{1}d) + d^{2})\\right))$  and graph learning of item-item Graph  $(O(L_{ii}|\\mathcal{I}|))$ . Given that the additional computational cost does not substantially increase runtime relative to the baseline DirectAU, we can infer that  $\\mathrm{CM}^3$ 's computational complexity is of the same order as DirectAU.",
    "experiments": "# 4 Experiment Settings\n\n# 4.1 Datasets\n\nFollowing existing research [7-9, 25], we conduct experiments on the Amazon review dataset, which contains both product descriptions and multi-view images. The multimodal information inherent in these datasets provides an ideal context for the rigorous evaluation of multimodal recommendation algorithms. Our experimental design incorporates four distinct category-specific datasets: Baby, Sports, Clothing, and Electronics. To ensure data quality and relevance, we applied a 5-core filtering process to both item and user data, effectively removing entries with insufficient interactions. To further investigate the generalization capabilities of our model, we employ the MicroLens dataset [53], which comprises data collected from a short-video platform. The key statistical characteristics of these refined datasets are summarized in Table 1, offering a quantitative overview of the data used in our experimental procedures. For the utilization of multimodal information from Amazon datasets, we adhered to established preprocessing protocols as described in [9, 54].\n\n# 4.2Baselines\n\nTo demonstrate the efficacy of our proposed method, we conduct a comprehensive comparison against the following widely-adopted baselines in general CF models (i.e., MF [6], LightGCN [52], SelfCF [55], DirectAU [5]) and multimodal recommendation (i.e., VBPR [8], MMGCN [20], GRCN [21], LATTICE [25], SLMRec [28], BM3 [7], FREEDOM [9], LGMRec [10], DA-MRS [11], MIG-GT [56]). We briefly summarize their key points as follows: MF [6] utilizes BPR loss to enhance latent representations of users and items within a matrix factorization framework. LightGCN [52] incorporates a simplified GCNs to derive item and user representations through neighbor information aggregation and propagation. SelfCF [55] employs three contrastive view perturbations within a self-supervised learning paradigm to generate latent representations of items and users. The \"embedding dropout\" method from SelfCF is adopted here due to its reported superior performance. DirectAU [5] establishes a direct link between standard BPR loss and the minimization of alignment and uniformity, proposing a simple yet\n\neffective approach to optimize these properties for enhanced recommendation performance. VBPR [8] extracts visual representations using pre-trained CNNs and concatenates these with item embeddings to model user preferences. MMGCN [20] leverages GCNs on modality-specific interaction graphs to derive user preferences in recommendation tasks. GRCN [21] employs user preference and item content affinity to refine the user interaction graph, aiming to mitigate false-positive interactions and prevent noise propagation along edges. LATTICE [25] models item-item relationships across feature modalities, fusing them to construct a semantic item-item graph. GCNs are applied to both the fused item-item and user-item graphs for more effective embedding learning. SLMRec [28] advances multimedia recommendation by using self-supervised learning to capture richer user and item relationships, leading to more accurate recommendation performance. BM3 [7] augments latent representations of items and users through a dropout strategy, introducing a novel self-supervised learning approach to derive high-quality user and item representations. FREEDOM [9] addresses the limitations of LATTICE by proposing to freeze the item-item graph and further denoising the user-item graphs, enhancing computational efficiency and representation quality. LGMRec [10] simultaneously learns local and global user interests for effectively recommendation. The local graph captures collaborative and multimodal embeddings, while the global graph represents multiple user group interests, addressing sparsity issues in recommendations. DA-MRS [11] introduces a denoising and alignment framework designed to mitigate noise within multimodal content and user feedback, while also facilitating their alignment through fine-grained guidance. MIG-GT [56] aims to integrate information from various data modalities using graph neural networks, enhanced by global transformers to capture broader dependencies and improve recommendation accuracy.\n\n# 4.3 Evaluation Metrics and Scenarios\n\nFollowing established methodologies [5, 7, 25], we randomly partition each dataset into training, validation, and test sets at a ratio of 8:1:1. To assess algorithm performance in top-  $k$  recommendation scenarios, we utilize standard evaluation metrics commonly employed in recommendation systems, namely Recall  $(\\mathbb{R}@\\boldsymbol{k})$  and Normalized Discounted Cumulative Gain (NDCG, shorted as  $\\mathbf{N}@\\boldsymbol{k}$ ). The parameter  $k$  is set to 10 and 20. We employ two distinct data splitting strategies to evaluate our model under both general and cold-start conditions, following the protocols established by [9, 25].\n\nWarm-Start Evaluation. For each user in the dataset, we implement a stratified random sampling approach to partition their historical interactions. The dataset is segregated into three mutually exclusive subsets: training, validation, and testing, with a ratio of 8:1:1, respectively. This methodology ensures: i). A minimum of five interactions per user in the processed dataset. ii). At least one sample for both validation and testing phases. iii). A minimum of three interactions for model training.\n\nCold-Start Evaluation. To simulate cold-start conditions, we adopt the following procedure: i). Random selection of  $20\\%$  of items from the complete item pool. ii). Equal bifurcation of the selected items into validation  $(10\\%)$  and test  $(10\\%)$  sets. iii). Assignment of user-item interactions to training, validation, or testing sets based\n\non the item's designated partition. This approach ensures that items in the validation and test sets remain unseen during the training phase, accurately replicating the challenges inherent in cold-start scenarios where no prior information is available for a subset of items during the recommendation process.\n\n# 4.4 Implementation details\n\nWe set the embedding dimension of  $d$  as  $d = 64$  and utilize the Xavier initialization method [57] for user embedding initialization. To minimize the proposed loss function, we optimize the model using the Adam optimizer [58] with a learning rate of 0.001. For the baseline methods, we strictly adhere to the hyperparameter tuning procedures outlined in their respective original papers. Regarding our proposed method, we employ a grid search to identify the optimal combination of hyperparameters across all datasets. Specifically, we explore the trade-off  $\\gamma$  between alignment and uniformity loss within the range [0.2, 3.0] with increments of 0.2. The model selection is based on the highest R@20 score achieved on the validation data. The training process is limited to a maximum of 100 epochs, with early stopping implemented after 10 epochs. Our implementation is based on the MMRec framework [59].\n\n# 5 Experiment Results\n\n# 5.1 Performance Comparison\n\n5.1.1 Warm-Start Evaluation of  $\\mathsf{CM}^3$ . Experimental results from different algorithms are presented in Table 2 and Table 3, from which we observe the following phenomena. Firstly, our proposed  $\\mathrm{CM}^3$  achieves the best results in terms of Recall and NDCG across all datasets. Quantitatively,  $\\mathrm{CM}^3$  achieved an average NDCG@20 improvement of  $11.95\\%$  over DA-MRS and  $8.56\\%$  over MIG-GT, respectively, when evaluated across all available datasets. The consistent improvement over all baselines demonstrates the superiority of our  $\\mathrm{CM}^3$ , even on the largest \"Electronics\" dataset. Secondly, the efficacy of incorporating multimodal information in recommendation models may be attenuated when applied to large-scale datasets. For example, on \"Electronics\" dataset, almost all evaluated multimodal approaches except MIG-GT demonstrate inferior performance compared to DirectAU and SelfCF, highlighting notable limitations in their methodologies. This observation suggests that in larger datasets such as \"Electronics\", user-item interaction data assumes a more pivotal role in recommendation accuracy than in smaller datasets. The imposition of a uniformity loss between user-user and item-item pairs plays a crucial role in their differentiation. By leveraging this principle in conjunction with multimodal features, our proposed  $\\mathrm{CM}^3$  framework demonstrates superior performance across all baselines, on the \"Electronics\" dataset. Specifically,  $\\mathrm{CM}^3$  demonstrates a substantial improvement of  $13.97\\%$  in NDCG@20 compared to DirectAU on this dataset. This significant performance gain underscores the efficacy of our approach in integrating uniformity constraints with multimodal information for enhanced recommendation accuracy.\n\n5.1.2 Cold-Start Evaluation of  $CM^3$ . Multimodal recommendation models, by incorporating additional information beyond user-item interactions, mitigate the challenges posed by data sparsity. Fig. 3\n\n\nTable 2: Performance comparison of different recommendation methods in terms of Recall@20 and NDCG@20. The best results are indicated in bold text, and the second-best results are underlined. \\*\\* denotes that the improvements (Imp.) are statistically significant compared of the best baseline in a paired t-test with  $p <   0.05$\n\n\n<table><tr><td>Dataset</td><td colspan=\"2\">Baby</td><td colspan=\"2\">Sports</td><td colspan=\"2\">Clothing</td><td colspan=\"2\">Electronics</td><td colspan=\"2\">Microlens</td></tr><tr><td>Metric</td><td>R@20</td><td>N@20</td><td>R@20</td><td>N@20</td><td>R@20</td><td>N@20</td><td>R@20</td><td>N@20</td><td>R@20</td><td>N@20</td></tr><tr><td>MF</td><td>0.0575</td><td>0.0249</td><td>0.0653</td><td>0.0298</td><td>0.0303</td><td>0.0138</td><td>0.0367</td><td>0.0161</td><td>0.0959</td><td>0.0408</td></tr><tr><td>LightGCN</td><td>0.0754</td><td>0.0328</td><td>0.0864</td><td>0.0387</td><td>0.0544</td><td>0.0243</td><td>0.0540</td><td>0.0250</td><td>0.1075</td><td>0.0467</td></tr><tr><td>SelfCF</td><td>0.0822</td><td>0.0357</td><td>0.0955</td><td>0.0427</td><td>0.0616</td><td>0.0275</td><td>0.0653</td><td>0.0306</td><td>0.1125</td><td>0.0473</td></tr><tr><td>DirectAU</td><td>0.0804</td><td>0.0367</td><td>0.1017</td><td>0.0464</td><td>0.0669</td><td>0.0298</td><td>0.0666</td><td>0.0315</td><td>0.1186</td><td>0.0524</td></tr><tr><td>VBPR</td><td>0.0663</td><td>0.0284</td><td>0.0856</td><td>0.0384</td><td>0.0415</td><td>0.0192</td><td>0.0458</td><td>0.0202</td><td>0.1026</td><td>0.0441</td></tr><tr><td>MMGCN</td><td>0.0660</td><td>0.0282</td><td>0.0636</td><td>0.0270</td><td>0.0361</td><td>0.0154</td><td>0.0331</td><td>0.0141</td><td>0.0701</td><td>0.0279</td></tr><tr><td>GRCN</td><td>0.0824</td><td>0.0358</td><td>0.0919</td><td>0.0413</td><td>0.0657</td><td>0.0284</td><td>0.0529</td><td>0.0241</td><td>0.1070</td><td>0.0460</td></tr><tr><td>LATTICE</td><td>0.0850</td><td>0.0370</td><td>0.0953</td><td>0.0421</td><td>0.0733</td><td>0.0330</td><td>OOM</td><td>OOM</td><td>0.1089</td><td>0.0473</td></tr><tr><td>SLMRec</td><td>0.0810</td><td>0.0357</td><td>0.1017</td><td>0.0462</td><td>0.0810</td><td>0.0357</td><td>0.0651</td><td>0.0303</td><td>0.1190</td><td>0.0511</td></tr><tr><td>BM3</td><td>0.0883</td><td>0.0383</td><td>0.0980</td><td>0.0438</td><td>0.0621</td><td>0.0281</td><td>0.0648</td><td>0.0302</td><td>0.0981</td><td>0.0400</td></tr><tr><td>FREEDOM</td><td>0.0992</td><td>0.0424</td><td>0.1089</td><td>0.0481</td><td>0.0941</td><td>0.0420</td><td>0.0601</td><td>0.0273</td><td>0.1032</td><td>0.0437</td></tr><tr><td>LGMRec</td><td>0.1002</td><td>0.0440</td><td>0.1068</td><td>0.0480</td><td>0.0828</td><td>0.0371</td><td>0.0625</td><td>0.0287</td><td>0.1132</td><td>0.0489</td></tr><tr><td>DA-MRS</td><td>0.0966</td><td>0.0426</td><td>0.1078</td><td>0.0475</td><td>0.0924</td><td>0.0415</td><td>OOM</td><td>OOM</td><td>0.1196</td><td>0.0520</td></tr><tr><td>MIG-GT</td><td>0.1021</td><td>0.0452</td><td>0.1130</td><td>0.0511</td><td>0.0934</td><td>0.0422</td><td>0.0696</td><td>0.0320</td><td>0.1189</td><td>0.0523</td></tr><tr><td>CM³</td><td>0.1034</td><td>0.0470*</td><td>0.1222*</td><td>0.0567*</td><td>0.1006*</td><td>0.0463*</td><td>0.0760*</td><td>0.0359*</td><td>0.1258*</td><td>0.0554*</td></tr><tr><td>Imp.</td><td>1.27%</td><td>3.98%</td><td>8.14%</td><td>10.96%</td><td>6.91%</td><td>9.72%</td><td>9.20%</td><td>12.19%</td><td>5.18%</td><td>5.73%</td></tr></table>\n\n\n- 'OOM' denotes an Out-Of-Memory condition encountered on a Tesla V100 GPU with 32 GB of memory.\n\n\n\nTable 3: Performance comparison of different recommendation methods in terms of Recall@10 and NDCG@10. The best results are indicated in bold text, and the second-best results are underlined.\n\n\n<table><tr><td>Dataset</td><td colspan=\"2\">Baby</td><td colspan=\"2\">Sports</td><td colspan=\"2\">Clothing</td><td colspan=\"2\">Electronics</td><td colspan=\"2\">Microlens</td></tr><tr><td>Metric</td><td>R@10</td><td>N@10</td><td>R@10</td><td>N@10</td><td>R@10</td><td>N@10</td><td>R@10</td><td>N@10</td><td>R@10</td><td>N@10</td></tr><tr><td>MF</td><td>0.0357</td><td>0.0192</td><td>0.0432</td><td>0.0241</td><td>0.0206</td><td>0.0114</td><td>0.0235</td><td>0.0127</td><td>0.0624</td><td>0.0322</td></tr><tr><td>LightGCN</td><td>0.0479</td><td>0.0257</td><td>0.0569</td><td>0.0311</td><td>0.0361</td><td>0.0197</td><td>0.0363</td><td>0.0204</td><td>0.0720</td><td>0.0376</td></tr><tr><td>SelfCF</td><td>0.0521</td><td>0.0279</td><td>0.0630</td><td>0.0344</td><td>0.0415</td><td>0.0224</td><td>0.0442</td><td>0.0251</td><td>0.0723</td><td>0.0369</td></tr><tr><td>DirectAU</td><td>0.0543</td><td>0.0300</td><td>0.0682</td><td>0.0379</td><td>0.0443</td><td>0.0240</td><td>0.0460</td><td>0.0262</td><td>0.0817</td><td>0.0429</td></tr><tr><td>VBPR</td><td>0.0423</td><td>0.0223</td><td>0.0558</td><td>0.0307</td><td>0.0281</td><td>0.0158</td><td>0.0293</td><td>0.0159</td><td>0.0677</td><td>0.0351</td></tr><tr><td>MMGCN</td><td>0.0421</td><td>0.0220</td><td>0.0401</td><td>0.0209</td><td>0.0227</td><td>0.0120</td><td>0.0207</td><td>0.0109</td><td>0.0421</td><td>0.0207</td></tr><tr><td>GRCN</td><td>0.0532</td><td>0.0282</td><td>0.0599</td><td>0.0330</td><td>0.0421</td><td>0.0224</td><td>0.0349</td><td>0.0194</td><td>0.0702</td><td>0.0365</td></tr><tr><td>LATTICE</td><td>0.0547</td><td>0.0292</td><td>0.0620</td><td>0.0335</td><td>0.0492</td><td>0.0268</td><td>OOM</td><td>OOM</td><td>0.0726</td><td>0.0380</td></tr><tr><td>SLMRec</td><td>0.0547</td><td>0.0285</td><td>0.0676</td><td>0.0374</td><td>0.0540</td><td>0.0285</td><td>0.0443</td><td>0.0249</td><td>0.0778</td><td>0.0405</td></tr><tr><td>BM3</td><td>0.0564</td><td>0.0301</td><td>0.0656</td><td>0.0355</td><td>0.0422</td><td>0.0231</td><td>0.0437</td><td>0.0247</td><td>0.0606</td><td>0.0304</td></tr><tr><td>FREEDOM</td><td>0.0627</td><td>0.0330</td><td>0.0717</td><td>0.0385</td><td>0.0629</td><td>0.0341</td><td>0.0396</td><td>0.0220</td><td>0.0674</td><td>0.0345</td></tr><tr><td>LGMRec</td><td>0.0644</td><td>0.0349</td><td>0.0720</td><td>0.0390</td><td>0.0555</td><td>0.0302</td><td>0.0417</td><td>0.0233</td><td>0.0748</td><td>0.0390</td></tr><tr><td>DA-MRS</td><td>0.0626</td><td>0.0339</td><td>0.0708</td><td>0.0379</td><td>0.0633</td><td>0.0342</td><td>OOM</td><td>OOM</td><td>0.0801</td><td>0.0419</td></tr><tr><td>MIG-GT</td><td>0.0665</td><td>0.0361</td><td>0.0753</td><td>0.0414</td><td>0.0636</td><td>0.0347</td><td>0.0467</td><td>0.0261</td><td>0.0806</td><td>0.0426</td></tr><tr><td>CM3</td><td>0.0692*</td><td>0.0381*</td><td>0.0837*</td><td>0.0467*</td><td>0.0701*</td><td>0.0386*</td><td>0.0519*</td><td>0.0297*</td><td>0.0852*</td><td>0.0450*</td></tr><tr><td>Imp.</td><td>4.06%</td><td>5.54%</td><td>11.16%</td><td>12.80%</td><td>10.22%</td><td>11.24%</td><td>11.13%</td><td>13.36%</td><td>4.28%</td><td>4.89%</td></tr></table>\n\n\n- 'OOM' denotes an Out-Of-Memory condition encountered on a Tesla V100 GPU with 32 GB of memory.\n\n\nillustrates the recommendation performance of our proposed  $\\mathrm{CM}^3$  and three representative models.\n\nThe figure elucidates the subsequent facets: Firstly, incorporating multimodal features into the training loss function can enhance the robustness of recommendation models in cold-start scenarios. For example, VBPR concatenates multimodal features with item IDs for item representation learning. While LATTICE solely utilizes multimodal features to construct the item-item graph, VBPR achieves competitive performance on smaller datasets like \"Baby\" and \"Sports\". This suggests that directly incorporating multimodal\n\nfeatures in the loss function might be beneficial. Secondly, GCNs have the potential to propagate information and gradients to unseen items (cold-start items) during training, even if they haven't been observed in user-item interactions. This can alleviate the cold-start problem, particularly when the user-item graph is large and sparsely connected. Partial validation for this can be observed on \"Clothing\" dataset in Fig. 3. Thirdly, we observe that  $\\mathrm{CM}^3$  significantly outperforms baseline models. In addition to the aforementioned advantages of  $\\mathrm{CM}^3$ , we hypothesize that the calibrated uniformity loss plays a crucial role in adjusting the item distribution. This\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/a8589b4c9a1e31bf4d0e07b7265faf8082272c75eb95b5a5b7643b5b21201a56.jpg)\n\n\n\nFigure 3: Performance of  $\\mathrm{CM}^3$  compared with representative baselines under cold-start settings.\n\n\n\nTable 4: Performance comparison of  $\\mathbf{CM}^3$  variants under different component ablation settings.\n\n\n<table><tr><td>Dataset</td><td>Metric</td><td>CM3w/o F</td><td>CM3w LF</td><td>CM3w StdU</td><td>CM3</td></tr><tr><td rowspan=\"2\">Baby</td><td>R@20</td><td>0.0956</td><td>0.0991</td><td>0.0890</td><td>0.1034</td></tr><tr><td>N@20</td><td>0.0446</td><td>0.0460</td><td>0.0414</td><td>0.0470</td></tr><tr><td rowspan=\"2\">Sports</td><td>R@20</td><td>0.1164</td><td>0.1180</td><td>0.1120</td><td>0.1222</td></tr><tr><td>N@20</td><td>0.0543</td><td>0.0539</td><td>0.0531</td><td>0.0567</td></tr><tr><td rowspan=\"2\">Clothing</td><td>R@20</td><td>0.0981</td><td>0.0994</td><td>0.0993</td><td>0.1006</td></tr><tr><td>N@20</td><td>0.0457</td><td>0.0458</td><td>0.0460</td><td>0.0463</td></tr></table>\n\n\nTable 5: Performance comparison of  $\\mathbf{CM}^3$  variants under different unimodal/multimodal features.\n\n\n<table><tr><td>Dataset</td><td>Metric</td><td>CM3w/o V</td><td>CM3w/o T</td><td>CM3</td><td>CM3MLLM</td></tr><tr><td rowspan=\"2\">Baby</td><td>R@20</td><td>0.0847</td><td>0.0860</td><td>0.1034</td><td>0.1062</td></tr><tr><td>N@20</td><td>0.0378</td><td>0.0382</td><td>0.0470</td><td>0.0477</td></tr><tr><td rowspan=\"2\">Sports</td><td>R@20</td><td>0.1035</td><td>0.1018</td><td>0.1222</td><td>0.1246</td></tr><tr><td>N@20</td><td>0.0472</td><td>0.0465</td><td>0.0567</td><td>0.0576</td></tr><tr><td rowspan=\"2\">Clothing</td><td>R@20</td><td>0.0725</td><td>0.0679</td><td>0.1006</td><td>0.1065</td></tr><tr><td>N@20</td><td>0.0336</td><td>0.0311</td><td>0.0463</td><td>0.0488</td></tr></table>\n\nadjustment enables unseen items to receive a loss signal, thereby facilitating the learning of their representations.\n\n# 5.2 Ablation Study\n\nTo gain a comprehensive understanding of  $\\mathrm{CM}^3$ , we conduct ablation studies to investigate the impact of each component on recommendation performance.\n\n5.2.1 Component Ablation. In this study, we explore the contributions of the spherical Bézier fusion and calibrated uniformity loss in comparison to the linear interpolated fusion and standard uniformity loss. We consider the following variants while fixing all other settings.\n\n-  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{oF}}$  indicates that we only remove the proposed fusion strategies during  $\\mathrm{CM}^3$  training.\n\n-  $\\mathbf{CM}^3_{\\mathbf{wLF}}$  means that the multimodal features is fused with the conventional linear interpolation.\n\n-  $\\mathbf{CM}^3_{\\mathbf{wStdU}}$  represents that the calibrated uniformity loss is substituted by the standard uniformity loss.\n\nTable 4 presents the experimental results of the aforementioned model variants across three datasets. Analysis of these results yields several noteworthy observations: i). The full version of our model\n\nconsistently outperforms all ablation settings across every dataset and evaluation metric. This finding suggests that each component of the model contributes positively to its overall performance. ii). Each dataset reacts differently to the removal of model components. For example, on \"Baby\" and \"Sports\" datasets, removing the calibrated uniformity leads to a significant performance drop, whereas the drop on \"Clothing\" dataset is less pronounced. Linear fusion performs comparably to our method on \"Clothing\" dataset, but shows inferior results on the other two datasets. iii). Comparative analysis between the full model and the variant without fusion reveals that the proposed spherical Bézier fusion serves as an effective default strategy for enhancing recommendation accuracy.\n\n5.2.2 Multimodal Feature Ablation. In this study, we investigate the impact of unimodal features on recommendation performance. Specifically, we consider the following variants of  $\\mathrm{CM}^3$ , either incorporating only unimodal features or utilizing features extracted with Multimodal Large Language Models (MLLMs).\n\nSpecifically, we leverage Meta's \"Llama-3.2-11B-Vision\" [60] for converting visual content into textual captions. Text embeddings for items are subsequently generated from item captions using the \"e5-mistral-7b-instruct\" model [61]. The resulting embedding vectors, each of dimension 4,096, are used to represent both image-derived and text-based item descriptions.\n\n-  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{o}\\mathbf{V}}$  represents that  $\\mathbf{CM}^3$  is trained without the visual features of items.\n\n-  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{oT}}$  denotes that  $\\mathrm{CM}^3$  is trained without textual features.\n\n-  $\\mathbf{CM}^3_{\\mathbf{MLM}}$  indicates that  $\\mathrm{CM}^3$  is trained utilizing multimodal features derived from MLLMs.\n\nTable 5 reports the recommendation accuracy of  $\\mathbf{CM}^3$  and its two variants on three datasets. From experiment results, we observe that: i). Generally,  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{o}\\mathbf{V}}$ , which excluding the visual features clearly perform better than its counterpart  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{o}\\mathbf{T}}$  on \"Sports\" and \"Clothing\" datasets. This observation suggests that textual features are essential to ensure recommendation performance. ii). An exception is that  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{o}\\mathbf{T}}$  performs slightly better than  $\\mathbf{CM}^3_{\\mathbf{w} / \\mathbf{o}\\mathbf{V}}$  on \"Baby\" dataset. We guess that product images in the Baby category may provide discriminative information for well modeling item representations. In summary, visual and textual features complement each other from different perspectives, allowing  $\\mathbf{CM}^3$  to achieve the best results across all three datasets. It was further noted that  $\\mathbf{CM}^3$ s performance on the Clothing dataset was enhanced by  $5.40\\%$  in NDCG@20 via the use of features extracted from MLLMs.\n\n# 5.3 Item Representation Distribution\n\nTo investigate how  $\\mathrm{CM}^3$  enforces the distribution of item representations, we generated two plots in Fig. 4 based on Sports dataset. The first shows feature distributions using Gaussian kernel density estimation (KDE) in  $\\mathbb{R}^2$ , with lighter colors indicating a higher density of points. The second is a KDE plot of the angles, calculated as  $\\arctan 2(y,x)$  for each point  $(x,y)\\in S^1$ . As depicted in the figure, the application of contrastive loss encourages a more uniform distribution among item representations (DA-MRS and  $\\mathrm{CM}^3$  over VBPR). Notably, the proposed calibrated uniformity loss provides a fine-grained adjustment, mitigating the tendency towards excessive uniformity that can occur with standard uniformity loss.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/996b0a950421b0ae2d585279b799d1d45df2bdbaad99cb8fba5a8805dfc77310.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/2b035ee29e46378102b4e54738a048289590ff718ddd1869f680b533e7c48234.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/652195cf37cb895043e66a17fa6373e23b9a5f55224aece76a3ef8295801e1b8.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/e9360c9fdbf41ab3f5d0792ae9d65d9c91790b0a61f6330100f3703d8c944a59.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/490572ea6cc6f9e84492575fa008cc00a60b38ed6145fd9aa3d614eda8766080.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/321d9fab270316662161bfc33f22a50397b3ac6b564b60f91e0d96b0de34f1a0.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/eaacad46846f7b64bb8f058f643edbd0912a54986554ceb7f0ead8f157b594a7.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/64b3ac5475f62715d3b94364bb1c22948646fe0f2c580ae5188b8db0674b830a.jpg)\n\n\n\nFigure 4: Distribution of item representations via KDE plot, with lighter areas indicating a higher concentration of points.\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/68adae77f624bedea8bd48e45f07a2081f65113e1380bc6eb4c709fddbd74f4e.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/f3a8a534-ab35-4953-b2d2-2911a943eec6/42f39c683df1c56e5c5ea641e6e579549768822c47c2d28480b5e1c2dbd3cbfa.jpg)\n\n\n\nFigure 5: Performance analysis of  $\\mathbf{CM}^3$  across varying alignment-uniformity trade-offs  $\\gamma$ .\n\n\n# 5.4 Hyperparameter Sensitivity Study\n\nTo investigate the influence of the trade-off factor  $\\gamma$  in the loss function, we conduct experiments to examine the sensitivity of  $\\mathrm{CM}^3$  with respect to  $\\gamma$  across three datasets. From Fig. 5, we observe the following: i). As  $\\gamma$  increases from 0.4 to 0.8, the recommendation accuracy improves dramatically, suggesting that uniformity plays a crucial role in learning user and item representations. ii). The behavior of the datasets diverges notably when the parameter  $\\gamma$  exceeds 0.8. The metrics  $R@20$  and  $N@20$  maintain relatively stable and high values as  $\\gamma$  increases beyond 0.8. This phenomenon underscores the critical role of the uniformity loss in these larger datasets, suggesting that a higher degree of uniformity constraint continues to benefit model performance. In contrast, our  $\\mathrm{CM}^3$  model exhibits a gradual performance decline when  $\\gamma$  surpasses 0.8. This observation suggests that the optimal balance between alignment and uniformity for the smaller dataset is achieved at a lower  $\\gamma$  value. This aligns with previous research by [5], which demonstrated that excessive optimization of uniformity can be detrimental to recommendation performance. Refer the Appendix for more details.",
    "hyperparameter": "embedding dimension d = 64; learning rate = 0.001 (Adam); alignment-uniformity trade-off γ searched in [0.2, 3.0] with step 0.2, optimal around 0.8; Beta(α, α) for λ in spherical interpolation (α not explicitly reported); temperature t > 0 in uniformity losses; max epochs = 100 with early stop at 10; graph convolution layers L_ui and L_ii (exact values not specified)."
  }