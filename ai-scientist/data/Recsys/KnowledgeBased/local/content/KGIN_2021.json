{
  "id": "KGIN_2021",
  "paper_title": "Learning Intents behind Interactions with Knowledge Graph for Recommendation",
  "alias": "KGIN",
  "year": 2021,
  "domain": "Recsys",
  "task": "KnowledgeAwareRecommendation",
  "idea": "",
  "introduction": "# 1 INTRODUCTION\n\nKnowledge graph (KG) has shown great potential in improving the accuracy and explainability of recommendation. The rich entity and relation information in KG can supplement the relational modeling between users and items. They not only reveal various relatedness among items (e.g., co-directed by a person), but also can be used to interpret user preference (e.g., attributing a user's choice of a movie to its director).\n\nLearning high-quality user and item representations from such structural knowledge has become the theme of knowledge-aware recommendation. Earlier works [1, 4, 51] generate embeddings from KG triplets and treat them as prior or content information to supplement item representations. Some follow-on studies [15, 44, 49] enrich the interactions with multi-hop paths from users to items for better characterizing user-item relations. However, these methods struggle to obtain high-quality paths, suffering from various issues like labor-intensive feature engineering [44], poor transferability to different domains [15, 17], and/or unstable performance [49]. More recently, a technical trend [38, 39, 41, 47] is to develop end-to-end models founded on graph neural networks (GNNs) [9, 13, 19, 34]. The key idea is to utilize the information aggregation scheme, which can effectively integrate multi-hop neighbors into representations. Benefiting from the integration of connectivity modeling and representation learning, these GNN-based models achieve promising performance for recommendation.\n\nDespite their effectiveness, we argue that current GNN-based methods fall short in modeling two factors: (1) User Intents. To the best of our knowledge, none of these studies consider user-item relations at a finer-grained level of intents. An important fact\n\nhas been ignored: a user typically has multiple intents, driving the user to consume different items. Taking the right of Figure 1 as an example, intent  \\(p_1\\)  emphasizes a combination of director ( \\(r_1\\) ) and star ( \\(r_2\\) ) aspects that drives user  \\(u_1\\)  to watch the movies  \\(i_1\\)  and  \\(i_5\\) ; while another intent  \\(p_2\\)  highlights the star ( \\(r_2\\) ) and partner ( \\(r_3\\) ) aspects the user to select movie  \\(i_2\\) . Ignoring the existence of user intents limits the modeling of user-item interactions. (2) Relational Paths. In these studies, the information aggregation schemes are mostly node-based – that is, to collect the information from neighboring nodes, without differentiating which paths it comes from. Moreover, KG relations are typically modeled in decay factors [38, 41] of adjacent matrix, in order to control the influences of neighbors. As shown in the left of Figure 2, the representation of  \\(u_1\\)  mixes the signal from the one-, two-, and three-hop neighbors (i.e.,  \\(\\{i_1, i_2\\}\\) ,  \\(\\{v_1, v_2, v_3\\}\\) ,  \\(\\{v_3\\}\\) , respectively). It fails to preserve the relation dependencies and sequences carried by paths (e.g.,  \\((p_1, r_2, r_3)\\)  in the three-hop path from  \\(v_3\\)  to  \\(u_1\\) ). Hence, such node-based schemes are insufficient to capture the interactions among relations.\n\nIn this paper, we focus on exploring user intents behind user-item interactions by using item KG, so as to improve the performance and interpretability of recommendation. We propose a new model, Knowledge Graph-based Intent Network (KGIN), which consists of two components to solve the foregoing limitations correspondingly: (1) User Intent Modeling. Each user-item interaction is enriched with the underlying intents. While we can express these intents as latent vectors, their semantics are opaque to understand. Hence, we associate each intent with a distribution over KG relations, accounting for the importance of relation combination. Technically, the intent embedding is an attentive combination of relation embeddings, where important relations are assigned with larger attribution scores. Moreover, an independence constraint is introduced to encourage significant differences among intents for better interpretability. (2) Relational Path-aware Aggregation. Unlike the node-based aggregation mechanism, we view a relational path as an information channel, and embed each channel into a representation vector. As user-intent-item triplets and KG triplets are heterogeneous, we set different aggregation strategies for these two parts, so as to distill behavioral patterns of users and relatedness of items respectively. In a nutshell, this relational modeling allows us to identify influential intents, and encode relation dependencies and semantics of paths into the representations. We conduct extensive experiments on three real-world datasets. Experimental results show that our KGIN outperforms the state-of-the-art methods such as KGAT [41], KGNN-LS [38], and CKAN [47]. Furthermore, KGIN is able to interpret user behaviors at the granularity of intents.\n\nWe summarize the contributions of this work as:\n\n- Revealing user intents behind the interactions in KG-based recommendation, for better model capacity and interpretability;  \n- Proposing a new model, KGIN, which considers user-item relationships at the finer granularity of intents and long-range semantics of relational paths under the GNN paradigm;  \n- Conducting empirical studies on three benchmark datasets to demonstrate the superiority of KGIN.\n\n![](images/8e422746188e7c747ed306d73cb2408cfa1a2591545481ce7553865b86376378.jpg)  \nFigure 2: An example of the node-based and relational path-aware aggregation schemes, where a (dashed or solid) arrow is an information flow among nodes. Best viewed in color.\n\n![](images/31d9b6c3e200a094595ff27ac90bb09bccf7e1adfd622cdff81faf689a3014e5.jpg)\n",
  "method": "# 3 METHODOLOGY\n\nWe now present the proposed Knowledge Graph-based Intent Network (KGIN). Figure 3 displays the working flow of KGIN. It consists of two key components: (1) user intent modeling, which uses multiple latent intents to profile user-item relationships and formulates each intent as an attentive combination of KG relations, meanwhile encourages different intents to be independent of each others; and (2) relational path-aware aggregation, which highlights the relation dependence in long-range connectivity, so\n\nas to preserve the holistic semantics of relational paths. KGIN ultimately yields high-quality representations of users and items.\n\n## 3.1 User Intent Modeling\n\nUnlike the previous GNN-based studies [38, 41, 47] that assume no or only one interact-with relation between users and items, we aim to capture the intuition that behaviors of users are influenced by multiple intents. Here we frame the intent as the reason of users' choices to items, which reflects the commonality of all users' behaviors. Taking movie recommendation as an example, possible intents are diverse considerations on movie attributes, such as the combination of star and partner, or director and genre. Different intents abstract different behavioral patterns of users. This can supercharge the widely-used collaborative filtering [26] effect with the finer-grained assumption - users driven by similar intents would have similar preference on items. Such intuition motivates us to model user-item relations at the granularity of intents.\n\nAssuming  \\(\\mathcal{P}\\)  as the set of intents shared by all users, we can slice a uniform user-item relation into the  \\(|\\mathcal{P}|\\)  intents, and decompose each  \\((u,i)\\)  pair into  \\(\\{(u,p,i)|p\\in \\mathcal{P}\\}\\) . As a result, we reorganize the user-item interaction data as a heterogeneous graph, termed intent graph (IG), which differs from the homogeneous collaborative graph adopted in previous works [41, 47].\n\n3.1.1 Representation Learning of Intentions. Although we can express these intents with latent vectors, it is hard to identify the semantics of each intent explicitly. One straightforward solution is to couple each intent with one KG relation, which is proposed by KTUP [4]. However, this solution only considers single relations in isolation, without accounting for the interactions and combinations of relations, thereby fails to refine high-level concepts of user intents. For example, the combination of relations  \\(r_1\\)  and  \\(r_2\\)  is influential to intent  \\(p_1\\) , while relations  \\(r_3\\)  and  \\(r_4\\)  contributes more to intent  \\(p_2\\) . Hence, we assign each intent  \\(p \\in \\mathcal{P}\\)  with a distribution over KG relations - technically, exert an attention strategy to create the intent embedding as:\n\n\\[\n\\mathbf {e} _ {p} = \\sum_ {r \\in \\mathcal {R}} \\alpha (r, p) \\mathbf {e} _ {r}, \\tag {1}\n\\]\n\nwhere  \\(\\mathbf{e}_r\\)  is the ID embedding of relation  \\(r\\) , which is assigned with an attention score  \\(\\alpha(r, p)\\)  to quantify its importance, formally:\n\n\\[\n\\alpha (r, p) = \\frac {\\exp \\left(w _ {r p}\\right)}{\\sum_ {r ^ {\\prime} \\in \\mathcal {R}} \\exp \\left(w _ {r ^ {\\prime} p}\\right)}, \\tag {2}\n\\]\n\nwhere  \\(w_{rp}\\)  is a trainable weight specific to certain relation  \\(r\\)  and certain intent  \\(p\\) . Here we use the weights for simplicity, and leave the further exploration of complex attention modules in future work. It is worthwhile mentioning that the attentions are not tailored to a single user, but refine common patterns of all users.\n\n3.1.2 Independence Modeling of Intents. Different intents should contain different information about user preference [23, 24]. If one intent can be inferred by the others, it might be redundant and less informative to describe user-item relation; in contrast, the intent with unique information will offer a useful angle to characterize behavioral patterns of users. Hence, for better model capacity and explainability, we encourage the representations of intents to differ from each others.\n\nHere we introduce a module of independence modeling to guide the representation learning of independent intents. This module can be simply achieved by applying a statistical measure, such as mutual information [2], Pearson correlation [33], and distance correlation [32, 33, 43], as a regularizer. Here we offer two implementations:\n\n- Mutual information. We minimize the mutual information between the representations of any two different intents, so as to quantify their independence. Such an idea coincides with contrastive learning [7, 12]. More formally, the independence modeling is:\n\n\\[\n\\mathcal {L} _ {\\mathrm {I N D}} = \\sum_ {p \\in \\mathcal {P}} - \\log \\frac {\\exp \\left(s \\left(\\mathbf {e} _ {p} , \\mathbf {e} _ {p}\\right) / \\tau\\right)}{\\sum_ {p ^ {\\prime} \\in \\mathcal {P}} \\exp \\left(s \\left(\\mathbf {e} _ {p} , \\mathbf {e} _ {p ^ {\\prime}}\\right) / \\tau\\right)}, \\tag {3}\n\\]\n\nwhere  \\(s(\\cdot)\\)  is the function measuring the associations of any two intent representations, which is set as cosine similarity function here; and  \\(\\tau\\)  is the hyper-parameter to the temperature in softmax function.\n\n- Distance correlation. It measures both linear and nonlinear associations of any two variables, whose coefficient is zero if and only if these variables are independent. Minimizing the distance correlation of user intents enables us to reduce the dependence of different intents, which is formulated as:\n\n\\[\n\\mathcal {L} _ {\\mathrm {I N D}} = \\sum_ {p, p ^ {\\prime} \\in \\mathscr {P}, p \\neq p ^ {\\prime}} d C o r \\left(\\mathbf {e} _ {p}, \\mathbf {e} _ {p ^ {\\prime}}\\right), \\tag {4}\n\\]\n\nwhere  \\(dCor(\\cdot)\\)  is the distance correlation between intents  \\(p\\)  and  \\(p'\\) :\n\n\\[\nd C o r \\left(\\mathbf {e} _ {p}, \\mathbf {e} _ {p ^ {\\prime}}\\right) = \\frac {d C o v \\left(\\mathbf {e} _ {p} , \\mathbf {e} _ {p ^ {\\prime}}\\right)}{\\sqrt {d V a r \\left(\\mathbf {e} _ {p}\\right) \\cdot d V a r \\left(\\mathbf {e} _ {p ^ {\\prime}}\\right)}}, \\tag {5}\n\\]\n\nwhere  \\(dCov(\\cdot)\\)  is the distance covariance of two representations, and  \\(dVar(\\cdot)\\)  is the distance variance of each intent representation.\n\nOptimizing this loss allows us to encourage the divergence among different intents and makes these intents have distinct boundary, thus endows better explainability of user intents.\n\n## 3.2 Relational Path-aware Aggregation\n\nHaving modeled the user intents, we move on to the representation learning of users and items under the GNN-base paradigm. Previous GNN-based recommender models [38, 39, 41] have shown that the neighborhood aggregation scheme is a promising end-to-end way to integrate multi-hop neighbors into representations. More specifically, the representation vector of an ego node is computed by recursively aggregating and transforming representations of its multi-hop neighbors.\n\nHowever, we argue that current aggregation schemes are mostly node-based, which limit the benefit of the structural knowledge, due to two issues: (1) The aggregators focus on combining the information of neighborhood, without distinguishing which paths they originate from. Considering the example in Figure 2, there are three information channels between the ego node  \\(u_{1}\\)  and its 2-hop neighbor  \\(v_{2}\\) :  \\(u_{1} \\stackrel{p_{1}}{\\leftarrow} i_{1} \\stackrel{r_{2}}{\\leftarrow} v_{2}\\)  and  \\(u_{1} \\stackrel{p_{2}}{\\leftarrow} i_{2} \\stackrel{r_{2}}{\\leftarrow} v_{2}\\) . When constructing the neural messages passed by  \\(v_{2}\\) , the node-based aggregators largely transform and rescale  \\(v_{2}\\) 's representation by decay factors, without considering influences of different\n\n![](images/2b77910d9ab572514da50c074599882fd41a2d49cda692dbf017f52dd7cc07bf.jpg)  \nFigure 3: Illustration of the proposed KGIN framework. Best viewed in color.\n\nchannels. Hence, they are insufficient to preserve the structural information in representations. Moreover, (2) current node-based aggregators usually model KG relations in the decay factors via attention networks [38, 41, 47] to control how much information is propagated from neighbors. This limits the contributions of KG relations to node representations. Moreover, no relation dependency  \\((e.g.,(p_2,r_2,r_3)\\)  in path  \\(u_{1}\\xleftarrow{p_{2}}i_{2}\\xleftarrow{r_{2}}v_{2}\\xleftarrow{r_{3}}v_{3})\\)  is captured in an explicit fashion. Hence, we aim to devise a relational path-aware aggregation scheme to solve these two limitations.\n\n3.2.1 Aggregation Layer over Intent Graph. We first move on to refine collaborative information from IG. As mentioned, the CF effect [26] is powerful to characterize user patterns, by assuming that behavioral similar users would have similar preference on items. This inspires us to treat personal history (i.e., the items a user has adopted before) as the pre-existing features of individual users. Moreover, in our IG, we can capture finer-grained patterns at a granular level of user intents, by assuming that users with similar intents would exhibit similar preference towards items. Considering a user  \\(u\\)  in IG, we use  \\(\\mathcal{N}_u = \\{(p,i) | (u,p,i) \\in C\\}\\)  to represent the intent-aware history and the first-order connectivity around  \\(u\\) . Technically, we can integrate the intent-aware information from historical items to create the representation of user  \\(u\\)  as:\n\n\\[\n\\mathbf {e} _ {u} ^ {(1)} = f _ {\\mathrm {I G}} \\left(\\left\\{\\left(\\mathbf {e} _ {u} ^ {(0)}, \\mathbf {e} _ {p}, \\mathbf {e} _ {i} ^ {(0)}\\right) \\mid (p, i) \\in \\mathcal {N} _ {u} \\right\\}\\right), \\tag {6}\n\\]\n\nwhere  \\(\\mathbf{e}_u^{(1)}\\in \\mathbb{R}^d\\)  is the representation of user  \\(u\\) ; and  \\(f_{\\mathrm{G}}(\\cdot)\\)  is the aggregator function to characterize each first-order connection  \\((u,p,i)\\) . Here we implement  \\(f_{\\mathrm{G}}(\\cdot)\\)  as:\n\n\\[\n\\mathbf {e} _ {u} ^ {(1)} = \\frac {1}{| \\mathcal {N} _ {u} |} \\sum_ {(p, i) \\in \\mathcal {N} _ {u}} \\beta (u, p) \\mathbf {e} _ {p} \\odot \\mathbf {e} _ {i} ^ {(0)}, \\tag {7}\n\\]\n\nwhere  \\(\\mathbf{e}_i^{(0)}\\)  is the ID embedding of item  \\(i\\) ;  \\(\\odot\\)  is the element-wise product. We harness it with two insights. (1) For a given a user, different intents will have varying contributions to motivate her behaviors. Hence, we introduce an attention score  \\(\\beta(u, p)\\)  to differentiate the importance of intent  \\(p\\)  as:\n\n\\[\n\\beta (u, p) = \\frac {\\exp \\left(\\mathbf {e} _ {p} ^ {\\top} \\mathbf {e} _ {u} ^ {(0)}\\right)}{\\sum_ {p ^ {\\prime} \\in \\mathcal {P}} \\exp \\left(\\mathbf {e} _ {p ^ {\\prime}} ^ {\\top} \\mathbf {e} _ {u} ^ {(0)}\\right)}, \\tag {8}\n\\]\n\nwhere  \\(\\mathbf{e}_u^{(0)}\\in \\mathbb{R}^d\\)  is the ID embedding of user  \\(u\\)  to make the importance score personalized. (2) Unlike the ideas of using the decay factors [38, 41, 47] or regularization terms [41] in previous studies, we highlight the role of intent relations during the aggregation. Hence we construct the item  \\(i\\) 's message via the\n\nelement-wise product  \\(\\beta (u,p)\\mathbf{e}_p\\odot \\mathbf{e}_i^{(0)}\\)  . As a result, we are able to explicitly express the first-order intent-aware information in the user representations.\n\n3.2.2 Aggregation Layer over Knowledge Graph. We then focus on the aggregation scheme in KG. As one entity can be involved in multiple KG triplets, it can take other connected entities as its attributes, which reflect the content similarity among items. For example, movie The Hobbit  \\(I\\)  can be described by its director Peter Jackson and star Martin Freeman. More formally, we use  \\(\\mathcal{N}_i = \\{(r,v) | (i,r,v) \\in \\mathcal{G}\\}\\)  to represent the attributes and the first-order connectivity about item  \\(i\\) , and then integrate the relation-aware information from connected entities to generate the representation of item  \\(i\\) :\n\n\\[\n\\mathbf {e} _ {i} ^ {(1)} = f _ {\\mathrm {K G}} \\left(\\left\\{\\left(\\mathbf {e} _ {i} ^ {(0)}, \\mathbf {e} _ {r}, \\mathbf {e} _ {v} ^ {(0)}\\right) \\mid (r, v) \\in \\mathcal {N} _ {i} \\right\\}\\right) \\tag {9}\n\\]\n\nwhere  \\(\\mathbf{e}_i^{(1)}\\in \\mathbb{R}^d\\)  is the representation collecting the information from the first-order connectivity; and  \\(f_{\\mathrm{KG}}(\\cdot)\\)  is the aggregation function to extract and integrate information from each connection  \\((i,r,v)\\) . Here we account for the relational context in the aggregator. Intuitively, each KG entity has different semantics and meanings in different relational contexts. For instance, entity Quentin Tarantino expresses signals pertinent to the director and star concepts in two triplets (Quentin Tarantino, director, Django Unchained) and (Quentin Tarantino, star, Django Unchained), respectively. However, previous studies [38, 41, 47] only model KG relations in the decay factors via the attention mechanism, in order to control the contributions of Quentin Tarantino to the representation of Django Unchained. Instead, we model the relational context in the aggregator as:\n\n\\[\n\\mathbf {e} _ {i} ^ {(1)} = \\frac {1}{| N _ {i} |} \\sum_ {(r, v) \\in N _ {i}} \\mathbf {e} _ {r} \\odot \\mathbf {e} _ {v} ^ {(0)}, \\tag {10}\n\\]\n\nwhere  \\(\\mathbf{e}_v^{(0)}\\)  is the ID embedding of entity  \\(v\\) . For each triplet  \\((i, r, v)\\) , we devise a relational message  \\(\\mathbf{e}_r \\odot \\mathbf{e}_v^{(0)}\\)  by modeling the relation  \\(r\\)  as the projection or rotation operator [30]. As a result, the relational message is able to reveal different meanings carried by the triplets, even when they get the same entities. Analogously, we can obtain the representation  \\(\\mathbf{e}_v^{(1)}\\)  of each KG entity  \\(v \\in \\mathcal{V}\\) .\n\n3.2.3 Capturing Relational Paths. Having modeled the first-order connectivity in Equations (6) and (9), we further stack more aggregation layers to gather the influential signals from higher-order neighbors. Technically, we recursively formulate the\n\nrepresentations of user  \\(u\\)  and item  \\(i\\)  after  \\(l\\)  layers as:\n\n\\[\n\\begin{array}{l} \\mathbf {e} _ {u} ^ {(l)} = f _ {\\mathrm {I G}} \\left(\\left\\{\\left(\\mathbf {e} _ {u} ^ {(l - 1)}, \\mathbf {e} _ {p}, \\mathbf {e} _ {i} ^ {(l - 1)}\\right) | (p, i) \\in \\mathcal {N} _ {u} \\right\\}\\right), \\\\ \\mathbf {e} _ {i} ^ {(l)} = f _ {\\mathrm {K G}} \\left(\\left\\{\\left(\\mathbf {e} _ {i} ^ {(l - 1)}, \\mathbf {e} _ {r}, \\mathbf {e} _ {v} ^ {(l - 1)}\\right) | (r, v) \\in \\mathcal {N} _ {i} \\right\\}\\right), \\tag {11} \\\\ \\end{array}\n\\]\n\nwhere  \\(\\mathbf{e}_u^{(l-1)}, \\mathbf{e}_i^{(l-1)}, \\mathbf{e}_v^{(l-1)}\\)  separately denote the representations of user  \\(u\\) , item  \\(i\\) , and entity  \\(v\\) , which memorize the relational signals being propagated from their  \\((l-1)\\) -hop neighbors. Benefiting from our relational modeling, these representations are able to store the holistic semantics of multi-hop paths, and highlight the relational dependencies. Let  \\(s = i \\xrightarrow{r_1} s_1 \\xrightarrow{r_2} \\cdots s_{l-1} \\xrightarrow{r_l} s_l\\)  be a  \\(l\\) -hop path rooted at item  \\(i\\) , which contains a sequence of connected triplets. Its relational path is represented as the sequence of relations merely, i.e.,  \\((r_1, r_2, \\cdots, r_l)\\) . We can rewrite the representation  \\(\\mathbf{e}_i^{(l)}\\)  as follows:\n\n\\[\n\\mathbf {e} _ {i} ^ {(l)} = \\sum_ {s \\in \\mathcal {N} _ {i} ^ {l}} \\frac {\\mathbf {e} _ {r _ {1}}}{| \\mathcal {N} _ {s _ {1}} |} \\odot \\frac {\\mathbf {e} _ {r _ {2}}}{| \\mathcal {N} _ {s _ {2}} |} \\odot \\dots \\odot \\frac {\\mathbf {e} _ {r _ {l}}}{| \\mathcal {N} _ {s _ {l}} |} \\odot \\mathbf {e} _ {s _ {l}} ^ {(0)}, \\tag {12}\n\\]\n\nwhere  \\(\\mathcal{N}_i^l\\)  is the set of all  \\(i\\) 's  \\(l\\) -hop paths. Clearly, this representation reflects the interactions among relations and preserves the holistic semantics of paths. This is significantly different from the current aggregation mechanism adopted in knowledge-aware recommenders, which overlook the importance of KG relations and thus fail to capture the relational paths.\n\n## 3.3 Model Prediction\n\nAfter  \\(L\\)  layers, we obtain the representations of user  \\(u\\)  and item  \\(i\\)  at different layers and then sum them up as the final representations:\n\n\\[\n\\mathbf {e} _ {u} ^ {*} = \\mathbf {e} _ {u} ^ {(0)} + \\dots + \\mathbf {e} _ {u} ^ {(L)}, \\quad \\mathbf {e} _ {i} ^ {*} = \\mathbf {e} _ {i} ^ {(0)} + \\dots + \\mathbf {e} _ {i} ^ {(L)}. \\tag {13}\n\\]\n\nBy doing so, the intent-aware relationships and the KG relation dependencies of paths are encoded in the final representations.\n\nThereafter, we employ the inner product on the user and item representations to predict how likely the user would adopt the item:\n\n\\[\n\\hat {y} _ {u i} = \\mathbf {e} _ {u} ^ {* \\top} \\mathbf {e} _ {i} ^ {*}. \\tag {14}\n\\]\n\n## 3.4 Model Optimization\n\nWe opt for the pairwise BPR loss [26] to reconstruct the historical data. Specifically, it considers that for a given user, her historical items should be assigned with higher prediction scores than the unobserved items:\n\n\\[\n\\mathcal {L} _ {\\mathrm {B P R}} = \\sum_ {(u, i, j) \\in O} - \\ln \\sigma \\left(\\hat {y} _ {u i} - \\hat {y} _ {u j}\\right), \\tag {15}\n\\]\n\nwhere  \\(O = \\{(u,i,j)|(u,i)\\in O^{+},(u,j)\\in O^{-}\\}\\)  is the training dataset consisting of the observed interactions  \\(O^{+}\\)  and unobserved counterparts  \\(O^{-}\\) ;  \\(\\sigma (\\cdot)\\)  is the sigmoid function. By combining the independence loss and BPR loss, we minimize the following objective function to learn the model parameter:\n\n\\[\n\\mathcal {L} _ {\\mathrm {K G I N}} = \\mathcal {L} _ {\\mathrm {B P R}} + \\lambda_ {1} \\mathcal {L} _ {\\mathrm {I N D}} + \\lambda_ {2} \\| \\Theta \\| _ {2} ^ {2}, \\tag {16}\n\\]\n\nwhere  \\(\\Theta = \\{\\mathbf{e}_u^{(0)},\\mathbf{e}_v^{(0)},\\mathbf{e}_r,\\mathbf{e}_p,\\mathbf{w}|u\\in \\mathcal{U},v\\in \\mathcal{V},p\\in \\mathcal{P}\\}\\)  is the set of model parameters (note that the item set  \\(\\mathcal{I}\\subset \\mathcal{V}\\) );  \\(\\lambda_{1}\\)  and  \\(\\lambda_{2}\\)  are two hyperparameters to control the independence loss (Equation (6)) and  \\(L_{2}\\)  regularization term, respectively.\n\nTable 1: Statistics of the datasets.  \n\n<table><tr><td></td><td></td><td>Amazon-Book</td><td>Last-FM</td><td>Alibaba-iFashion</td></tr><tr><td rowspan=\"3\">User-Item Interaction</td><td>#Users</td><td>70,679</td><td>23,566</td><td>114,737</td></tr><tr><td>#Items</td><td>24,915</td><td>48,123</td><td>30,040</td></tr><tr><td>#Interactions</td><td>847,733</td><td>3,034,796</td><td>1,781,093</td></tr><tr><td rowspan=\"3\">Knowledge Graph</td><td>#Entities</td><td>88,572</td><td>58,266</td><td>59,156</td></tr><tr><td>#Relations</td><td>39</td><td>9</td><td>51</td></tr><tr><td>#Triplets</td><td>2,557,746</td><td>464,567</td><td>279,155</td></tr></table>\n\n## 3.5 Model Analysis\n\n3.5.1 Model Size. Recent studies [48] have shown that using nonlinear feature transformations possibly makes GNNs difficult to train. Hence, in the aggregation scheme of KGIN, we discard the nonlinear activation functions and feature transformation matrices. Hence, the model parameters of KGIN consist of (1) ID embeddings of users, KG entities (including items), and KG relations  \\(\\{\\mathbf{e}_u^{(0)},\\mathbf{e}_v^{(0)},\\mathbf{e}_r|u\\in \\mathcal{U},v\\in \\mathcal{V}\\}\\) ; and (2) ID embeddings of user intents  \\(\\{\\mathbf{e}_p|p\\in \\mathcal{P}\\}\\)  and the attention weights  \\(\\mathbf{w}\\) .\n\n3.5.2 Time Complexity. The time cost of KGIN mainly comes from the user intent modeling and aggregation scheme. In the aggregations over IG, the computational complexity of user representations is  \\(O(L|C|d)\\) , where  \\(L\\) ,  \\(|C|\\) , and  \\(d\\)  denote the number of layers, the number of triplets in IG, and the embedding size, respectively. In the aggregation over KG, the time cost of updating entity representations is  \\(O(L|\\mathcal{G}|d)\\) , where  \\(|\\mathcal{G}|\\)  is the number of KG triplets. As for the independence modeling, the cost of distance correlation is  \\(O(|\\mathcal{P}|(|\\mathcal{P}| - 1)/2)\\) , where  \\(|\\mathcal{P}|\\)  is the number of user intents. In total, the time complexity of the whole training epoch is  \\(O(L|C|d + L|\\mathcal{G}|d + |\\mathcal{P}|(|\\mathcal{P}| - 1)/2)\\) . Under the same experimental settings (i.e., representation sizes at different layers), KGIN has comparable complexity to KGAT and CKAN.\n",
  "experiments": "# 4 EXPERIMENTS\n\nWe provide empirical results to demonstrate the effectiveness of our proposed KGIN. The experiments are designed to answer the following research questions:\n\n- RQ1: How does KGIN perform, comparing to the state-of-the-art knowledge-aware recommender models?  \n- RQ2: What is the impact of the designs (e.g., the number and independence of user intents, the depth of relational paths) on the improvement of KGIN's relational modeling?  \n- RQ3: Can KGIN provide insights on user intents and give an intuitive impression of explainability?\n\n## 4.1 Experimental Settings\n\n4.1.1 Dataset Description. We use three benchmark datasets for book, music, and fashion outfit recommendation in the experiments: (1) We use the Amazon-Book and Last-FM datasets released by KGAT [41]; And (2) we further introduce the Alibaba-iFashion dataset [8] to investigate the effectiveness of item knowledge. This is a fashion outfit dataset collected from Alibaba online shopping systems. The outfits are viewed as the items being recommended to users, where each outfit consists of multiple fashion staffs (e.g., tops, bottoms, shoes, accessories), and these staffs follow a fashion taxonomy and are assigned with different fashion categories (e.g., jeans, T-shirts). We extract such attributes as the KG data of outfits.\n\nMoreover, in order to ensure the data quality, we adopt the 10-core setting, i.e., discarding users and items with less than ten interactions, and filtering out KG entities involved less than ten triplets. The statistics of datasets are summarized in Table 1, where we only list the number of canonical relations and construct triplets with the inverse relations in experiments. Closely Following prior studies [41, 45], we use the same data partition. In the training phase, each observed user-item interaction is a positive instance, while an item that the user did not adopt before is randomly sampled to pair the user as a negative instance.\n\n4.1.2 Evaluation Metrics. In the evaluation phase, we conduct the all-ranking strategy [20], rather than sampled metrics like leaving one item out [44] or sampling a smaller set of users [38, 39, 47]. To be more specific, for each user, the full items that she has not adopted before are viewed as negative, and the relevant items in the testing set are treated as positive. All these items are ranked based on the predictions of recommender model. To evaluate top-K recommendation, we adopt the protocols [20]: recall@K and ndcg@K, where K is set as 20 by default. We report the average metrics for all users in the testing set.\n\n4.1.3 Alternative Baselines. We compare KGIN with the state-of-the-art methods, covering KG-free (MF), embedding-based (CKE), and GNN-based (KGAT, KGNN-LS, CKAN, and RGCN) methods:\n\n- MF [26] (matrix factorization) only considers the user-item interactions, while leaving KG untouched. Technically, it uses ID embeddings of users and items to perform the prediction.  \n- CKE [51] is a representative embedding-based method, which leverages KG embeddings of entities derived from TransR [22] as ID embeddings of items under the MF framework. Where, KG relations are only used as the constraints in TransR to regularize the representations of endpoints.  \n- KGNN-LS [38] is a GNN-based model, which converts KG into user-specific graphs, and then considers user preference on KG relations and label smoothness in the information aggregation phase, so as to generate user-specific item representations. It models relations in decay factors.  \n- KGAT [41] is a state-of-the-art GNN-based recommender. It applies an attentive neighborhood aggregation mechanism on a holistic graph, which combines KG with the user-item graph, to generate user and item representations. User-item relationships and KG relations serve as the attentive weights in adjacent matrix  \n- CKAN [47] is built upon KGNN-LS, which utilizes different neighborhood aggregation schemes on the user-item graph and KG respectively, to obtain user and item embeddings.  \n- R-GCN [27] is originally proposed for the knowledge graph completion task, which views various KG relations as different channels of information flow when aggregating neighboring nodes. Here we transfer it to the recommendation task.\n\n4.1.4 Parameter Settings. We implement our KGIN model in PyTorch, and have released our implementations (code, datasets, parameter settings, and training logs) to facilitate reproducibility. For a fair comparison, we fix the size of ID embeddings  \\(d\\)  as 64, the optimizer as Adam [18], and the batch size as 1024 for all methods. A grid search is conducted to confirm the optimal settings for each method - more specifically, the learning rate is tuned\n\nTable 2: Overall performance comparison.  \n\n<table><tr><td></td><td colspan=\"2\">Amazon-Book</td><td colspan=\"2\">Last-FM</td><td colspan=\"2\">Alibaba-iFashion</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>MF</td><td>0.1300</td><td>0.0678</td><td>0.0724</td><td>0.0617</td><td>0.1095</td><td>0.0670</td></tr><tr><td>CKE</td><td>0.1342</td><td>0.0698</td><td>0.0732</td><td>0.0630</td><td>0.1103</td><td>0.0676</td></tr><tr><td>KGAT</td><td>0.1487</td><td>0.0799</td><td>0.0873</td><td>0.0744</td><td>0.1030</td><td>0.0627</td></tr><tr><td>KGNN-LS</td><td>0.1362</td><td>0.0560</td><td>0.0880</td><td>0.0642</td><td>0.1039</td><td>0.0557</td></tr><tr><td>CKAN</td><td>0.1442</td><td>0.0698</td><td>0.0812</td><td>0.0660</td><td>0.0970</td><td>0.0509</td></tr><tr><td>R-GCN</td><td>0.1220</td><td>0.0646</td><td>0.0743</td><td>0.0631</td><td>0.0860</td><td>0.0515</td></tr><tr><td>KGIN-3</td><td>0.1687*</td><td>0.0915*</td><td>0.0978*</td><td>0.0848*</td><td>0.1147*</td><td>0.0716*</td></tr><tr><td>%Imp.</td><td>13.44%</td><td>14.51%</td><td>11.13%</td><td>13.97%</td><td>3.98%</td><td>5.91%</td></tr></table>\n\nin  \\(\\{10^{-4}, 10^{-3}, 10^{-2}\\}\\) , the coefficients of additional constraints (e.g.,  \\(L_{2}\\)  regularization in all methods, independence modeling in KGIN, TransR in CKE and KGAT, label smoothness in KGNN-LS) are searched in  \\(\\{10^{-5}, 10^{-4}, \\dots, 10^{-1}\\}\\) , and the number of GNN layers  \\(L\\)  is tuned in  \\(\\{1, 2, 3\\}\\)  for GNN-based methods. Moreover, for KGNN-LS and CKAN, we set the size of neighborhood as 16 and the batch size as 128. We initialize model parameters with Xavier [11], while using the pre-trained ID embeddings of MF as the initialization of KGAT.\n\nThe detailed settings of KGIN are provided in Appendix A.1. We observe that using Equations (3) and (4) have similar trends and performance, hence report the results of Equation (3). We use KGIN-3 to denote the recommender model with three relational path aggregation layers, and similar notations for others. Without specification, we fix the number of user intents  \\(|\\mathcal{P}|\\)  as 4 and the number of relational path aggregation layers  \\(L\\)  as 3. Moreover, in Sections 4.3.1 and 4.3.2, we study their influence by varying  \\(K\\)  in  \\(\\{1,2,4,8\\}\\)  and  \\(L\\)  in  \\(\\{1,2,3\\}\\) , respectively.\n\n## 4.2 Performance Comparison (RQ1)\n\nWe begin with the comparison w.r.t. recall@20 and ndcg@20. The empirical results are reported in Table 2, where %Imp. denotes the relative improvements of the best performing method (starred) over the strongest baselines (underlined). We find that:\n\n- KGIN consistently outperforms all baselines across three datasets in terms of all measures. More specifically, it achieves significant improvements over the strongest baselines w.r.t. ndcg@20 by  \\(14.51\\%\\) ,  \\(13.97\\%\\) , and  \\(5.91\\%\\)  in Amazon-Book, Last-FM, and Alibaba-iFashion, respectively. This demonstrates the rationality and effectiveness of KGIN. We attribute these improvements to the relational modeling of KGIN: (1) By uncovering user intents, KGIN is able to better characterize the relationships between users and items, and result in more powerful representations of users and items. In contrast, all baselines ignore the hidden user intents, and model user-item edges as a homogeneous channel to collect information; (2) Benefiting from our relational path aggregation scheme, KGIN can preserve the holistic semantics of paths and collect more informative signals from KG, than the GNN-based baselines (i.e., KGAT, CKAN, KGNN-LS); (3) Applying different aggregation schemes on IG and KG makes KGIN better able to encode the collaborative signals and item knowledge into user and item representations.\n\n- Jointly analyzing KGIN across the three datasets, we find that the improvement on Amazon-Book is more significant than that on Alibaba-iFashion. This is reasonable since (1) both interaction and\n\nTable 3: Impact of presence of user intents and KG relations.  \n\n<table><tr><td></td><td>Amazon-Book recall</td><td>ndcg</td><td>Last-FM recall</td><td>ndcg</td><td>Alibaba-iFashion recall</td><td>ndcg</td></tr><tr><td>w/o I&amp;R</td><td>0.1518</td><td>0.0816</td><td>0.0802</td><td>0.0669</td><td>0.0862</td><td>0.0530</td></tr><tr><td>w/o I</td><td>0.1627</td><td>0.0870</td><td>0.0942</td><td>0.0819</td><td>0.1103</td><td>0.0678</td></tr></table>\n\nTable 4: Impact of the number of layers  \\(L\\)  .  \n\n<table><tr><td></td><td colspan=\"2\">Amazon-Book</td><td colspan=\"2\">Last-FM</td><td colspan=\"2\">Alibaba-iFashion</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>KGIN-1</td><td>0.1455</td><td>0.0766</td><td>0.0831</td><td>0.0707</td><td>0.1045</td><td>0.0638</td></tr><tr><td>KGIN-2</td><td>0.1652</td><td>0.0892</td><td>0.0920</td><td>0.0791</td><td>0.1162</td><td>0.0723</td></tr><tr><td>KGIN-3</td><td>0.1687</td><td>0.0915</td><td>0.0978</td><td>0.0848</td><td>0.1147</td><td>0.0716</td></tr></table>\n\nKG data on Amazon-Book offer denser and richer information than that on Alibaba-iFashion; and (2) in Alibaba-iFashion, the first-order connectivity (fashion outfit, including, fashion staff) dominate the KG triplets. This indicates that KGIN is good at fulfilling the potentials of long-range connectivity.\n\n- Leaving KG untapped limits the performance of MF. By simply incorporating KG embeddings into MF, CKE performs better than MF. Such findings are consistent with prior studies [4], indicating the importance of side information like KG.  \n- GNN-based methods (i.e., KGAT, CKAN, KGNN-LS) outperform CKE in Amazon-Book and Last-FM, suggesting the importance of modeling long-range connectivity. These improvements come from using the local structure of a node - more specifically, multihop neighborhood - to improve the representation learning. However, in Alibaba-iFashion, the performance of CKE is better than them. Some possible reasons are: (1) These GNN-based methods involve additional nonlinear feature transformation, which are rather heavy and burdensome to train, thus degrades performance [14, 48]; (2) TransR in CKE successfully captures the major first-order connectivity in Alibaba-iFashion.  \n- The results of KGAT, KGNN-LS, and CKAN are at the same level, being better than R-GCN. Although the utility of transforming neighbors' information via KG relations in R-GCN is better than working as decay factors in the others, R-GCN is not originally designed for recommendation and thus fails to model user-item relationships properly.\n\n## 4.3 Relational Modeling of KGIN (RQ2)\n\nAs the relational modeling is at the core of KGIN, we also conduct ablation studies to investigate the effectiveness - specifically, how the presence of user intents and KG relations, the number of relational path aggregation layers, the granularity of user intents, and the independence of user intents influence our model.\n\n4.3.1 Impact of Presence of User Intents & KG Relations. We first answer the question: Is it of importance to consider user intents or KG relations? Towards this end, two variants are constructed by (1) discarding all user intents and KG relations, termed KGIN-  \\(3_{\\mathrm{W / O I&R}}\\) , and (2) removing all user intents only ( \\(|\\mathcal{P}| = 0\\) ), termed KGIN-  \\(3_{\\mathrm{W / O I}}\\) . We summarize the results in Table 3.\n\nObviously, compared with KGIN-3 in Table 2, removing all relations (i.e., KGIN-  \\(3_{\\mathrm{w/o}}\\)  I&R) dramatically reduces the predictive accuracy, indicating the necessity of relational modeling. To be more specific, KGIN-  \\(3_{\\mathrm{w/o}}\\)  I&R only propagates nodes' information in one\n\nTable 5: Impact of independence modeling.  \n\n<table><tr><td></td><td>Amazon-Book\nw/ Ind</td><td>w/o Ind</td><td>Last-FM\nw/ Ind</td><td>w/o Ind</td><td>Alibaba-iFashion\nw/ Ind</td><td>w/o Ind</td></tr><tr><td>distance correlation</td><td>0.0389</td><td>0.3490</td><td>0.0365</td><td>0.4944</td><td>0.0112</td><td>0.3121</td></tr></table>\n\n![](images/93e9831842d29eab5f21a1552dda566532efc7dbe01fe5d1e7d81fffb067962d.jpg)  \n(a) Amazon-Book\n\n![](images/808affa412057cd18d012735472d67cd1176c8ff1caf1e3c9ad98ebe537dde6b.jpg)  \n(b) Last-FM  \nFigure 4: Impact of intent number  \\((|\\mathcal{P}|)\\) . Best viewed in color.\n\nspace, without preserving any relational semantics, thus distorts the internal relationships among nodes. Analogously, leaving hidden user intents unexplored (i.e., KGIN-  \\(3_{\\mathrm{w / o1}}\\)  ) also downgrades the performance. Although KGIN-  \\(3_{\\mathrm{w / o1}}\\)  retains the modeling of KG relations, it only considers coarser-gained preference of users and thus leads to suboptimal user representations.\n\n4.3.2 Impact of Model Depth. We then consider varying the number of relational path aggregation layers. Stacking more layers is able to integrate the information carried by longer-range connectivity (i.e., longer paths) into node representations. Here we search  \\(L\\)  in the range of  \\(\\{1, 2, 3\\}\\)  and summarize the results in Table 4. We observe that:\n\n- Increasing the model depth is able to enhance the predictive results in most cases. To be more specific, KGIN-2 substantially achieves significant improvements over KGIN-1. We attribute such improvements to two reasons: (1) Stacking more layers explore more relevant items connected by some KG triplets and deepens the understanding of user interest. KGIN-1 only takes the first-order connectivity (e.g., user-intent-item triplets, KG triplets) into consideration, while KGIN-2 reveals the two-hop paths; (2) More information pertinent to user intents are derived from longer relational paths, thus better profiling user preference on items.  \n- Continuing one more exploration beyond KGIN-2, the results of KGIN-3 are consistently better in Amazon-Book and LastFM. This empirically shows that higher-order connectivity is complementary to the second-order one, thus resulting in better node representations.  \n- However, the results of KGIN-3 are worse than KGIN-2 in Alibaba-iFashion. This again admits the inherent characteristics of Alibaba-iFashion - most of KG triplets are the first-order connectivity (fashion outfit, including, fashion staff) of items, which have been captured in KGIN-2.\n\n4.3.3 Impact of Intent Modeling. To analyze the influence of intents number, we vary  \\(|\\mathcal{P}|\\)  in range of  \\(\\{1,2,4,8\\}\\)  and illustrate the performance changing curves on Amazon-Book and Last-FM datasets in Figure 4. We find that:\n\n- Increasing the intent number enhances the performance in most cases. Specifically, when only modeling a coarse-grained relation\n\n![](images/6facffff844babfdee598a1d1433af4285334dca325bedfeca33c8e4a7e0fba6.jpg)  \nFigure 5: Explanations of user intents and real cases in Amazon-Book (left) and Last-FM (right). Best viewed in color.\n\n![](images/ed8e6859ae8f10fe85d45e5ff1a9642bd8566ee05567806efc28dcd8ddbc28de.jpg)\n\n(i.e.,  \\(|\\mathcal{P}| = 1\\) ), KGIN-3 performs poor across the board. This again emphasizes the benefits of exploring multiple user intents.\n\n- In Amazon-Book, continuing one more partition beyond  \\(|\\mathcal{P}| = 4\\)  impairs the accuracy. One possible reason is that independence modeling encourages the irrelevance among intents, but also makes some intents too fine-grained to carry useful information. We leave the exploration of intent granularity to future work.  \n- Interestingly, comparing to the results on Amazon-Book, setting  \\(|\\mathcal{P}| = 8\\)  improves the accuracy on Last-FM, although Amazon-Book contains richer set of KG relations as compared to Last-FM. We attribute this to the difference of two datasets. In particular, KG in Last-FM is converted from the attributes of albums, songs, and artists, while KG in Amazon-Book is extracted from Freebase and contains noisy relations irrelevant to user behaviors.\n\nWe also conduct an ablation study to investigate the influence of independence modeling (cf. Section 3.1.2). Specifically, we disable this module to build a variant KGIN-  \\(3_{\\mathrm{w/o Ind}}\\) , and show the results w.r.t. distance correlation in Table 5. Clearly, while approaching the comparable performance of recommendation to KGIN-3, KGIN-  \\(3_{\\mathrm{w/o Ind}}\\)  achieves larger correlation coefficients and fails to differentiate user intents, which are still opaque to understand user behaviors.\n\n## 4.4 Explainability of KGIN (RQ3)\n\nIn this section, we present the semantics of user intents, and offer two examples of Amazon-Book and Last-FM to give an intuitive impression of our explainability. As shown in Figure 5, we have the following observations:\n\n- KGIN first induces intents - the commonality of all users - with various combinations of KG relations. For an intent, the weight of a relation reflects its importance to influence user behaviors. For example, in Amazon-Book, the top two relations of the first intent  \\(p_1\\)  are theater.play_GENre and theater.plays.in-this-genre, while date-of-the-first-performance and fictional-universe are assigned with the highest scores for the second intent  \\(p_3\\) . Clearly, the learned intents abstract the shared reasons of user's choices. Moreover, thanks to the independence modeling, the intents tend to have distinct boundary, thus describing user behaviors from different and independent angles. However,  \\(p_1\\)  and  \\(p_3\\)  are highly relevant. This makes sense since only 9 relations exist in Last-FM.  \n- It can be found that some relations get high weights in multiple intents, like version in Last-FM. This indicates that such relations are common factors pertinent to user behaviors. Combining it with other relations like featured-artist, KGIN induces the intent  \\(p_1\\)  as the special version of music created by a certain artist.\n\n- KGIN creates instance-wise explanations for each interaction — the personalization of a single user. For the interaction  \\(u_{231}\\) \\(i_{21904}\\)  in Amazon-Book, KGIN searches the most influential intent  \\(p_1\\)  based on the attention scores (cf. Equation (8)). Thus, it explains this behavior as User  \\(u_{231}\\)  selects music  \\(i_{21904}\\)  since it matches her interest on the featured artist and certain version.\n",
  "hyperparameter": ""
}