{
  "id": "KGNNLS_2019",
  "paper_title": "Knowledge Graph Neural Network with Label Smoothness Regularization for Recommendation",
  "alias": "KGNNLS",
  "year": 2019,
  "domain": "Recsys",
  "task": "KnowledgeAwareRecommendation",
  "idea": "The paper proposes KGNN-LS, which combines knowledge-aware graph neural networks with label smoothness (LS) regularization for recommendation. The core innovation is using a user-specific relation scoring function to transform the KG into personalized weighted graphs, then applying GNN layers for representation learning while simultaneously regularizing edge weights through a leave-one-out label propagation scheme. This dual approach leverages KG structure on both the feature side (through GNN propagation) and label side (through label smoothness), with the LS regularization helping to learn appropriate edge weights that satisfy the smoothness assumption that adjacent entities in the KG should have similar relevancy labels.",
  "introduction": "# 1 INTRODUCTION\n\nRecommender systems are widely used in Internet applications to meet user's personalized interests and alleviate the information overload [4, 29, 32]. Traditional recommender systems that are based on collaborative filtering [13, 22] usually suffer from the cold-start problem and have trouble recommending brand new items that have not yet been heavily explored by the users. The sparsity issue can be addressed by introducing additional sources of information such as user/item profiles [23] or social networks [22].\n\nKnowledge graphs (KGs) capture structured information and relations between a set of entities [8, 9, 18, 24-28, 33, 34, 36]. KGs are heterogeneous graphs in which nodes correspond to entities (e.g., items or products, as well as their properties and characteristics) and edges correspond to relations. KGs provide connectivity information between items via different types of relations and thus capture semantic relatedness between the items.\n\nThe core challenge in utilizing KGs in recommender systems is to learn how to capture user-specific item-item relatedness captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods [8, 33, 36], embedding-based methods [9, 26, 27, 34], and hybrid methods [18, 24, 28]. However, these approaches rely on manual feature engineering, are unable to perform end-to-end training, and have poor scalability. Graph Neural Networks (GNNs), which aggregate node feature information from node's local network neighborhood using neural networks, represent a promising advancement in graph-based representation learning [3, 5-7, 11, 15]. Recently, several works developed GNNs architecture for recommender systems [14, 19, 28, 31, 32], but these approaches are mostly designed for homogeneous bipartite user-item interaction graphs or user-/item-similarity graphs. It remains an open question how to extend GNNs architecture to heterogeneous knowledge graphs.\n\nIn this paper, we develop Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) that extends GNNs architecture to knowledge graphs to simultaneously capture semantic relationships between the items as well as personalized user preferences and interests. To account for the relational heterogeneity in KGs, similar to [28], we use a trainable and personalized relation scoring function that transforms the KG into a user-specific weighted graph, which characterizes both the semantic information of the KG as well as user's personalized interests. For example, in the movie recommendation setting the relation scoring function could learn that a given user really cares about \"director\" relation between movies and persons, while somebody else may care more\n\nabout the \"lead actor\" relation. Using this personalized weighted graph, we then apply a graph neural network that for every item node computes its embedding by aggregating node feature information over the local network neighborhood of the item node. This way the embedding of each item captures it's local KG structure in a user-personalized way.\n\nA significant difference between our approach and traditional GNNs is that the edge weights in the graph are not given as input. We set them using user-specific relation scoring function that is trained in a supervised fashion. However, the added flexibility of edge weights makes the learning process prone to overfitting, since the only source of supervised signal for the relation scoring function is coming from user-item interactions (which are sparse in general). To remedy this problem, we develop a technique for regularization of edge weights during the learning process, which leads to better generalization. We develop an approach based on label smoothness [35, 38], which assumes that adjacent entities in the KG are likely to have similar user relevancy labels/scores. In our context this assumption means that users tend to have similar preferences to items that are nearby in the KG. We prove that label smoothness regularization is equivalent to label propagation and we design a leave-one-out loss function for label propagation to provide extra supervised signal for learning the edge scoring function. We show that the knowledge-aware graph neural networks and label smoothness regularization can be unified under the same framework, where label smoothness can be seen as a natural choice of regularization on knowledge-aware graph neural networks.\n\nWe apply the proposed method to four real-world datasets of movie, book, music, and restaurant recommendations, in which the first three datasets are public datasets and the last is from Meituan-Dianping Group. Experiments show that our method achieves significant gains over state-of-the-art methods in recommendation accuracy. We also show that our method maintains strong recommendation performance in the cold-start scenarios where user-item interactions are sparse.\n",
  "method": "# 4 OUR APPROACH\n\nIn this section, we first introduce knowledge-aware graph neural networks and label smoothness regularization, respectively, then we present the unified model.\n\n## 4.1 Preliminaries: Knowledge-aware Graph Neural Networks\n\nThe first step of our approach is to transform a heterogeneous KG into a user-personalized weighted graph that characterizes user's preferences. To this end, similar to [28], we use a user-specific relation scoring function  \\(s_u(r)\\)  that provides the importance of relation  \\(r\\)  for user  \\(u\\) :  \\(s_u(r) = g(\\mathbf{u},\\mathbf{r})\\) , where  \\(\\mathbf{u}\\)  and  \\(\\mathbf{r}\\)  are feature vectors of user  \\(u\\)  and relation type  \\(r\\) , respectively, and  \\(g\\)  is a differentiable function such as inner product. Intuitively,  \\(s_u(r)\\)  characterizes the importance of relation  \\(r\\)  to user  \\(u\\) . For example, a user may be more interested in directors of movies, but another user may care more about the lead actors of movies.\n\nGiven user-specific relation scoring function  \\(s_u(\\cdot)\\)  of user  \\(u\\) , knowledge graph  \\(\\mathcal{G}\\)  can therefore be transformed into a user-specific adjacency matrix  \\(\\mathbf{A}_u \\in \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{E}|}\\) , in which the  \\((i,j)\\) -entry  \\(A_u^{ij} = s_u(re_i, e_j)\\) , and  \\(r_{e_i, e_j}\\)  is the relation between entities  \\(e_i\\)  and  \\(e_j\\)  in  \\(\\mathcal{G}\\) .<sup>1</sup>  \\(A_u^{ij} = 0\\)  if there is no relation between  \\(e_i\\)  and  \\(e_j\\) . See the left two subfigures in Figure 1 for illustration. We also denote the raw feature matrix of entities as  \\(\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{E}| \\times d_0}\\) , where  \\(d_0\\)  is the dimension of raw entity features. Then we use multiple feed forward layers<sup>2</sup> to update the entity representation matrix by aggregating representations of neighboring entities. Specifically, the layer-wise forward propagation can be expressed as\n\n\\[\n\\mathbf {H} _ {l + 1} = \\sigma \\left(\\mathbf {D} _ {u} ^ {- 1 / 2} \\mathbf {A} _ {u} \\mathbf {D} _ {u} ^ {- 1 / 2} \\mathbf {H} _ {l} \\mathbf {W} _ {l}\\right), l = 0, 1, \\dots , L - 1. \\tag {1}\n\\]\n\nIn Eq. (1),  \\(\\mathbf{H}_l\\)  is the matrix of hidden representations of entities in layer  \\(l\\) , and  \\(\\mathbf{H}_0 = \\mathbf{E}\\) .  \\(\\mathbf{A}_u\\)  is to aggregate representation vectors of neighboring entities. In this paper, we set  \\(\\mathbf{A}_u \\gets \\mathbf{A}_u + \\mathbf{I}\\) , i.e., adding self-connection to each entity, to ensure that old representation vector of the entity itself is taken into consideration when updating entity representations.  \\(\\mathbf{D}_u\\)  is a diagonal degree matrix with entries  \\(D_u^{ii} = \\sum_j A_u^{ij}\\) , therefore,  \\(\\mathbf{D}_u^{-1/2}\\)  is used to normalize  \\(\\mathbf{A}_u\\)  and keep the entity representation matrix  \\(\\mathbf{H}_l\\)  stable.  \\(\\mathbf{W}_l \\in \\mathbb{R}^{d_l \\times d_{l+1}}\\)  is the layer-specific trainable weight matrix,  \\(\\sigma\\)  is a non-linear activation function, and  \\(L\\)  is the number of layers.\n\nA single GNN layer computes the representation of an entity via a transformed mixture of itself and its immediate neighbors in the KG. We can therefore naturally extend the model to multiple layers to explore users' potential interests in a broader and deeper way. The final output is  \\(\\mathbf{H}_L\\in \\mathbb{R}^{|\\mathcal{E}|\\times d_L}\\) , which is the entity representations that mix the initial features of themselves and their neighbors up to  \\(L\\)  hops away. Finally, the predicted engagement probability of user  \\(u\\)  with item  \\(v\\)  is calculated by  \\(\\hat{y}_{uv} = f(\\mathbf{u},\\mathbf{v}_u)\\) , where  \\(\\mathbf{v}_u\\)  (i.e., the  \\(v\\) -th row of  \\(\\mathbf{H}_L\\) ) is the final representation vector of item  \\(v\\) , and  \\(f\\)  is a differentiable prediction function, for example, inner product or a multilayer perceptron. Note that  \\(\\mathbf{v}_u\\)  is user-specific since the adjacency matrix  \\(\\mathbf{A}_u\\)  is user-specific. Furthermore, note that the system is end-to-end trainable where the gradients flow from  \\(f(\\cdot)\\)  via GNN (parameter matrix  \\(\\mathbf{W}\\) ) to  \\(g(\\cdot)\\)  and eventually to representations of users  \\(u\\)  and items  \\(v\\) .\n\n### 4.2 Label Smoothness Regularization\n\nIt is worth noticing a significant difference between our model and GNNs: In traditional GNNs, edge weights of the input graph are fixed; but in our model, edge weights  \\(\\mathbf{D}_u^{-1/2}\\mathbf{A}_u\\mathbf{D}_u^{-1/2}\\)  in Eq. (1) are learnable (including possible parameters of function  \\(g\\)  and feature vectors of users and relations) and also requires supervised training like W. Though enhancing the fitting ability of the model, this will inevitably make the optimization process prone to overfitting, since the only source of supervised signal is from user-item interactions outside GNN layers. Moreover, edge weights do play an essential role in representation learning on graphs, as highlighted by a large amount of prior works [10, 20, 21, 35, 38]. Therefore, more regularization on edge weights is needed to assist the learning of entity representations and to help generalize to unobserved interactions more efficiently.\n\nLet's see how an ideal set of edge weights should be like. Consider a real-valued label function  \\(l_{u}:\\mathcal{E}\\to \\mathbb{R}\\)  on  \\(\\mathcal{G}\\) , which is constrained to take a specific value  \\(l_{u}(v) = y_{uv}\\)  at node  \\(v\\in \\mathcal{V}\\subseteq \\mathcal{E}\\) . In our context,  \\(l_{u}(v) = 1\\)  if user  \\(u\\)  finds the item  \\(v\\)  relevant and has engaged with it, otherwise  \\(l_{u}(v) = 0\\) . Intuitively, we hope that adjacent entities in the KG are likely to have similar relevancy labels, which is known as label smoothness assumption. This motivates our choice of energy function  \\(E\\) :\n\n\\[\nE \\left(l _ {u}, \\mathrm {A} _ {u}\\right) = \\frac {1}{2} \\sum_ {e _ {i} \\in \\mathcal {E}, e _ {j} \\in \\mathcal {E}} A _ {u} ^ {i j} \\left(l _ {u} \\left(e _ {i}\\right) - l _ {u} \\left(e _ {j}\\right)\\right) ^ {2}. \\tag {2}\n\\]\n\nWe show that the minimum-energy label function is harmonic by the following theorem:\n\nTHEOREM 1. The minimum-energy label function\n\n\\[\nl _ {u} ^ {*} = \\underset {l _ {u}: l _ {u} (v) = y _ {u v}, \\forall v \\in \\mathcal {V}} {\\arg \\min } E \\left(l _ {u}, \\mathrm {A} _ {u}\\right) \\tag {3}\n\\]\n\nw.r.t. Eq. (2) is harmonic, i.e.,  \\(l_{u}^{*}\\)  satisfies\n\n\\[\nl _ {u} ^ {*} (e _ {i}) = \\frac {1}{D _ {u} ^ {i i}} \\sum_ {e _ {j} \\in \\mathcal {E}} A _ {u} ^ {i j} l _ {u} ^ {*} (e _ {j}), \\forall e _ {i} \\in \\mathcal {E} \\backslash \\mathcal {V}. \\tag {4}\n\\]\n\nProof. Taking the derivative of the following equation\n\n\\[\nE (l _ {u}, \\mathbf {A} _ {u}) = \\frac {1}{2} \\sum_ {i, j} A _ {u} ^ {i j} \\left(l _ {u} (e _ {i}) - l _ {u} (e _ {j})\\right) ^ {2}\n\\]\n\nwith respect to  \\(l_{u}(e_{i})\\)  where  \\(e_i\\in \\mathcal{E}\\backslash \\mathcal{V}\\)  , we have\n\n\\[\n\\frac {\\partial E \\left(l _ {u} , \\mathbf {A} _ {u}\\right)}{\\partial l _ {u} \\left(e _ {i}\\right)} = \\sum_ {j} A _ {u} ^ {i j} \\left(l _ {u} \\left(e _ {i}\\right) - l _ {u} \\left(e _ {j}\\right)\\right).\n\\]\n\nThe minimum-energy label function  \\(l_{u}^{*}\\)  should satisfy that\n\n\\[\n\\left. \\frac {\\partial E \\left(l _ {u} , \\mathbf {A} _ {u}\\right)}{\\partial l _ {u} \\left(e _ {i}\\right)} \\right| _ {l _ {u} = l _ {u} ^ {*}} = 0.\n\\]\n\nTherefore, we have\n\n\\[\nl _ {u} ^ {*} (e _ {i}) = \\frac {1}{\\sum_ {j} A _ {u} ^ {i j}} \\sum_ {j} A _ {u} ^ {i j} l _ {u} ^ {*} (e _ {j}) = \\frac {1}{D _ {u} ^ {i i}} \\sum_ {j} A _ {u} ^ {i j} l _ {u} ^ {*} (e _ {j}), \\forall e _ {i} \\in \\mathcal {E} \\backslash \\mathcal {V}.\n\\]\n\n![](images/7d38b6e666e3e3dbae64567d53e019b349fa07f037e3e7ef59dd6278f145727e.jpg)\n\nThe harmonic property indicates that the value of  \\(l_{u}^{*}\\)  at each non-item entity  \\(e_i \\in \\mathcal{E} \\backslash \\mathcal{V}\\)  is the average of its neighboring entities, which leads to the following label propagation scheme [39]:\n\nTHEOREM 2. Repeating the following two steps:\n\n(1) Propagate labels for all entities:  \\(l_{u}(\\mathcal{E}) \\gets \\mathbf{D}_{u}^{-1}\\mathbf{A}_{u}l_{u}(\\mathcal{E})\\) , where  \\(l_{u}(\\mathcal{E})\\)  is the vector of labels for all entities;  \n(2) Reset labels of all items to initial labels:  \\(l_{u}(\\mathcal{V}) \\gets \\Upsilon[u, \\mathcal{V}]^{\\top}\\) , where  \\(l_{u}(\\mathcal{V})\\)  is the vector of labels for all items and  \\(\\Upsilon[u, \\mathcal{V}] = [y_{uv_1}, y_{uv_2}, \\dots]\\)  are initial labels;\n\nwill lead to  \\(l_{u}\\rightarrow l_{u}^{*}\\)\n\nPROOF. Let  \\(l_{u}(\\mathcal{E}) = \\left[ \\begin{array}{c} l_{u}(\\mathcal{V}) \\\\ l_{u}(\\mathcal{E} \\backslash \\mathcal{V}) \\end{array} \\right]\\) . Since  \\(l_{u}(\\mathcal{V})\\)  is fixed on  \\(\\Upsilon[u, \\mathcal{V}]\\) ,\n\nwe are only interested in  \\(l_{u}(\\mathcal{E}\\backslash \\mathcal{V})\\) . We denote  \\(\\mathbf{P} = \\mathbf{D}_u^{-1}\\mathbf{A}_u\\)  (the subscript  \\(u\\)  is omitted from  \\(\\mathbf{P}\\)  for ease of notation), and partition matrix  \\(\\mathbf{P}\\)  into sub-matrices according to the partition of  \\(l_{u}\\) :\n\n\\[\n\\mathbf {P} = \\left[ \\begin{array}{c c} \\mathbf {P} _ {V V} & \\mathbf {P} _ {V E} \\\\ \\mathbf {P} _ {E V} & \\mathbf {P} _ {E E} \\end{array} \\right].\n\\]\n\nThen the label propagation scheme is equivalent to\n\n\\[\nl _ {u} (\\mathcal {E} \\backslash \\mathcal {V}) \\leftarrow \\mathrm {P} _ {E V} \\mathrm {Y} [ u, \\mathcal {V} ] ^ {\\top} + \\mathrm {P} _ {E E} l _ {u} (\\mathcal {E} \\backslash \\mathcal {V}). \\tag {5}\n\\]\n\nRepeat the above procedure, we have\n\n\\[\nl _ {u} (\\mathcal {E} \\backslash \\mathcal {V}) = \\lim  _ {n \\rightarrow \\infty} \\left(\\mathrm {P} _ {E E}\\right) ^ {n} l _ {u} ^ {(0)} (\\mathcal {E} \\backslash \\mathcal {V}) + \\left(\\sum_ {i = 1} ^ {n} \\left(\\mathrm {P} _ {E E}\\right) ^ {i - 1}\\right) \\mathrm {P} _ {E V} \\mathrm {Y} [ u, \\mathcal {V} ] ^ {\\top}, \\tag {6}\n\\]\n\nwhere  \\(l_{u}^{(0)}(\\mathcal{E}\\backslash \\mathcal{V})\\)  is the initial value for  \\(l_{u}(\\mathcal{E}\\backslash \\mathcal{V})\\) . Now we show that  \\(\\lim_{n\\to \\infty}(\\mathbf{P}_{EE})^n l_u^{(0)}(\\mathcal{E}\\backslash \\mathcal{V}) = 0\\) . Since  \\(\\mathbf{P}\\)  is row-normalized and  \\(\\mathbf{P}_{EE}\\)  is a sub-matrix of  \\(\\mathbf{P}\\) , we have\n\n\\[\n\\exists \\epsilon <   1, \\sum_ {j} \\mathbf {P} _ {E E} [ i, j ] \\leq \\epsilon ,\n\\]\n\n![](images/eccdd35b80f89700858c52b6b14a3c26cfb89662c1ef683c5bf835488e313795.jpg)  \nFigure 2: (a) Analogy of a physical equilibrium model for recommender systems; (b)-(d) Illustration of the effect of the KG; (e) Illustration of the effect of label smoothness regularization.\n\nfor all possible row index  \\(i\\) . Therefore,\n\n\\[\n\\begin{array}{l} \\sum_ {j} \\left(\\mathbf {P} _ {E E}\\right) ^ {n} [ i, j ] = \\sum_ {j} \\left(\\left(\\mathbf {P} _ {E E}\\right) ^ {(n - 1)} \\mathbf {P} _ {E E}\\right) [ i, j ] \\\\ = \\sum_ {j} \\sum_ {k} \\left(\\mathbf {P} _ {E E}\\right) ^ {(n - 1)} [ i, k ] \\mathbf {P} _ {E E} [ k, j ] \\\\ = \\sum_ {k} \\left(\\mathbf {P} _ {E E}\\right) ^ {(n - 1)} [ i, k ] \\sum_ {j} \\mathbf {P} _ {E E} [ k, j ] \\\\ \\leq \\sum_ {k} (\\mathbf {P} _ {E E}) ^ {(n - 1)} [ i, k ] \\epsilon \\\\ \\leq \\dots \\leq \\epsilon^ {n}. \\\\ \\end{array}\n\\]\n\nAs  \\(n\\)  goes infinity, the row sum of  \\((\\mathbf{P}_{EE})^n\\)  converges to zero, which implies that  \\((\\mathbf{P}_{EE})^n l_u^{(0)}(\\mathcal{E}\\backslash \\mathcal{V})\\to 0\\) . It's clear that the choice of initial value  \\(l_{u}^{(0)}(\\mathcal{E}\\backslash \\mathcal{V})\\)  does not affect the convergence.\n\nSince  \\(lim_{n\\to \\infty}(\\mathbf{P}_{EE})^n l_u^{(0)}(\\mathcal{E}\\backslash \\mathcal{V}) = 0\\)  , Eq. (6) becomes\n\n\\[\nl _ {u} (\\mathcal {E} \\backslash \\mathcal {V}) = \\lim  _ {n \\to \\infty} \\left(\\sum_ {i = 1} ^ {n} \\left(\\mathrm {P} _ {E E}\\right) ^ {i - 1}\\right) \\mathrm {P} _ {E V} \\mathrm {Y} [ u, \\mathcal {V} ] ^ {\\top}.\n\\]\n\nDenote\n\n\\[\n\\mathrm {T} = \\lim  _ {n \\rightarrow \\infty} \\sum_ {i = 1} ^ {n} \\left(\\mathrm {P} _ {E E}\\right) ^ {i - 1} = \\sum_ {i = 1} ^ {\\infty} \\left(\\mathrm {P} _ {E E}\\right) ^ {i - 1},\n\\]\n\nand we have\n\n\\[\n\\mathbf {T} - \\mathbf {T P} _ {E E} = \\sum_ {i = 1} ^ {\\infty} \\left(\\mathbf {P} _ {E E}\\right) ^ {i - 1} - \\sum_ {i = 1} ^ {\\infty} \\left(\\mathbf {P} _ {E E}\\right) ^ {i} = \\mathbf {I}.\n\\]\n\nTherefore, we derive that\n\n\\[\n\\mathbf {T} = (I - \\mathbf {P} _ {E E}) ^ {- 1},\n\\]\n\nand\n\n\\[\nl _ {u} (\\mathcal {E} \\backslash \\mathcal {V}) = (I - \\mathbf {P} _ {E E}) ^ {- 1} \\mathbf {P} _ {E V} \\mathbf {Y} [ u, \\mathcal {V} ] ^ {\\top}.\n\\]\n\nThis is the unique fixed point and therefore the unique solution to Eq. (5). Repeating the steps in Theorem 2 leads to\n\n\\[\nl _ {u} (\\mathcal {E}) \\to l _ {u} ^ {*} (\\mathcal {E}) = \\left[ \\begin{array}{c} \\mathbf {Y} [ u, \\mathcal {V} ] ^ {\\top} \\\\ (I - \\mathbf {P} _ {E E}) ^ {- 1} \\mathbf {P} _ {E V} \\mathbf {Y} [ u, \\mathcal {V} ] ^ {\\top} \\end{array} \\right].\n\\]\n\n![](images/44f241bf0cf4ef9b3e9809ff5b6d5a91f2a10053ac567dbe44f91687983e338a.jpg)\n\nTheorem 2 provides a way for reaching the minimum-energy of relevancy label function  \\(E\\) . However,  \\(l_{u}^{*}\\)  does not provide any signal for updating the edge weights matrix  \\(\\mathbf{A}_u\\) , since the labeled part of  \\(l_{u}^{*}\\) , i.e.,  \\(l_{u}^{*}(\\mathcal{V})\\) , equals their true relevancy labels  \\(\\Upsilon[u, \\mathcal{V}]\\) ;\n\nMoreover, we do not know true relevancy labels for the unlabeled nodes  \\(l_u^* (\\mathcal{E}\\backslash \\mathcal{V})\\)\n\nTo solve the issue, we propose minimizing the leave-one-out loss [35]. Suppose we hold out a single item  \\(v\\)  and treat it unlabeled. Then we predict its label by using the rest of (labeled) items and (unlabeled) non-item entities. The prediction process is identical to label propagation in Theorem 2, except that the label of item  \\(v\\)  is hidden and needs to be calculated. This way, the difference between the true relevancy label of  \\(v\\)  (i.e.,  \\(y_{uv}\\) ) and the predicted label  \\(\\hat{l}_u(v)\\)  serves as a supervised signal for regularizing edge weights:\n\n\\[\nR (\\mathbf {A}) = \\sum_ {u} R (\\mathbf {A} _ {u}) = \\sum_ {u} \\sum_ {v} J \\left(y _ {u v}, \\hat {l} _ {u} (v)\\right), \\tag {7}\n\\]\n\nwhere  \\(J\\)  is the cross-entropy loss function. Given the regularization in Eq. (7), an ideal edge weight matrix  \\(\\mathbf{A}\\)  should reproduce the true relevancy label of each held-out item while also satisfying the smoothness of relevancy labels.\n\n### 4.3 The Unified Loss Function\n\nCombining knowledge-aware graph neural networks and LS regularization, we reach the following complete loss function:\n\n\\[\n\\min  _ {\\mathbf {W}, \\mathbf {A}} \\mathcal {L} = \\min  _ {\\mathbf {W}, \\mathbf {A}} \\sum_ {u, v} J \\left(y _ {u v}, \\hat {y} _ {u v}\\right) + \\lambda R (\\mathbf {A}) + \\gamma \\| \\mathcal {F} \\| _ {2} ^ {2}, \\tag {8}\n\\]\n\nwhere  \\(\\| \\mathcal{F}\\| _2^2\\)  is the L2-regularizer,  \\(\\lambda\\)  and  \\(\\gamma\\)  are balancing hyperparameters. In Eq. (8), the first term corresponds to the part of GNN that learns the transformation matrix  \\(\\mathbf{W}\\)  and edge weights A simultaneously, while the second term  \\(R(\\cdot)\\)  corresponds to the part of label smoothness that can be seen as adding constraint on edge weights A. Therefore,  \\(R(\\cdot)\\)  serves as regularization on A to assist GNN in learning edge weights.\n\nIt is also worth noticing that the first term can be seen as feature propagation on the KG while the second term  \\(R(\\cdot)\\)  can be seen as label propagation on the KG. A recommender for a specific user  \\(u\\)  is actually a mapping from item features to user-item interaction labels, i.e.,  \\(\\mathcal{F}_u: \\mathrm{E}_v \\to y_{uv}\\)  where  \\(\\mathrm{E}_v\\)  is the feature vector of item  \\(v\\) . Therefore, Eq. (8) utilizes the structural information of the KG on both the feature side and the label side of  \\(\\mathcal{F}_u\\)  to capture users' higher-order preferences.\n\n### 4.4 Discussion\n\nHow can the knowledge graph help find users' interests? To intuitively understand the role of the KG, we make an analogy with a\n\n<table><tr><td></td><td>Movie</td><td>Book</td><td>Music</td><td>Restaurant</td></tr><tr><td># users</td><td>138,159</td><td>19,676</td><td>1,872</td><td>2,298,698</td></tr><tr><td># items</td><td>16,954</td><td>20,003</td><td>3,846</td><td>1,362</td></tr><tr><td># interactions</td><td>13,501,622</td><td>172,576</td><td>42,346</td><td>23,416,418</td></tr><tr><td># entities</td><td>102,569</td><td>25,787</td><td>9,366</td><td>28,115</td></tr><tr><td># relations</td><td>32</td><td>18</td><td>60</td><td>7</td></tr><tr><td># KG triples</td><td>499,474</td><td>60,787</td><td>15,518</td><td>160,519</td></tr></table>\n\nTable 2: Statistics of the four datasets: MovieLens-20M (movie), Book-Crossing (book), Last.FM (music), and Dianping-Food (restaurant).\n\nphysical equilibrium model as shown in Figure 2. Each entity/item is seen as a particle, while the supervised positive user-relevancy signal acts as the force pulling the observed positive items up from the decision boundary and the negative items signal acts as the force pushing the unobserved items down. Without the KG (Figure 2a), these items are only loosely connected with each other through the collaborative filtering effect (which is not drawn here for clarity). In contrast, edges in the KG serve as the rubber bands that impose explicit constraints on connected entities. When number of layers is  \\(L = 1\\)  (Figure 2b), representation of each entity is a mixture of itself and its immediate neighbors, therefore, optimizing on the positive items will simultaneously pull their immediate neighbors up together. The upward force goes deeper in the KG with the increase of  \\(L\\)  (Figure 2c), which helps explore users' long-distance interests and pull up more positive items. It is also interesting to note that the proximity constraint exerted by the KG is personalized since the strength of the rubber band (i.e.,  \\(s_u(r)\\) ) is user-specific and relation-specific: One user may prefer relation  \\(r_1\\)  (Figure 2b) while another user (with same observed items but different unobserved items) may prefer relation  \\(r_2\\)  (Figure 2d).\n\nDespite the force exerted by edges in the KG, edge weights may be set inappropriately, for example, too small to pull up the unobserved items (i.e., rubber bands are too weak). Next, we show by Figure 2e that how the label smoothness assumption helps regularizing the learning of edge weights. Suppose we hold out the positive sample in the upper left and we intend to reproduce its label by the rest of items. Since the true relevancy label of the held-out sample is 1 and the upper right sample has the largest label value, the LS regularization term  \\(R(\\mathbf{A})\\)  would enforce the edges with arrows to be large so that the label can \"flow\" from the blue one to the striped one as much as possible. As a result, this will tighten the rubber bands (denoted by arrows) and encourage the model to pull up the two upper pink items to a greater extent.\n",
  "experiments": "# 5 EXPERIMENTS\n\nIn this section, we evaluate the proposed KGNN-LS model, and present its performance on four real-world scenarios: movie, book, music, and restaurant recommendations.\n\n## 5.1 Datasets\n\nWe utilize the following four datasets in our experiments for movie, book, music, and restaurant recommendations, respectively, in which the first three are public datasets and the last one is from\n\n![](images/cbf01374967c79f29b395e413f80581e7c737ad98dc865be8229990fca982efc.jpg)  \n(a) MovieLens-20M\n\n![](images/f38bd830272d1ea69095cf5f124fe014105f82f58c2c92cbca7de20e662b1422.jpg)  \n(b) Last.FM  \nFigure 3: Probability distribution of the shortest path distance between two randomly sampled items in the KG under the circumstance that (1) they have no common user in the dataset; (2) they have common user(s) in the dataset.\n\nMeituan-Dianping Group. We use Satori<sup>3</sup>, a commercial KG built by Microsoft, to construct sub-KGs for MovieLens-20M, Book-Crossing, and Last.FM datasets. The KG for Dianping-Food dataset is constructed by the internal toolkit of Meituan-Dianping Group. Further details of datasets are provided in Appendix A.\n\n- MovieLens-20M \\(^4\\)  is a widely used benchmark dataset in movie recommendations, which consists of approximately 20 million explicit ratings (ranging from 1 to 5) on the MovieLens website. The corresponding KG contains 102,569 entities, 499,474 edges and 32 relation-types.  \n- Book-Crossing<sup>5</sup> contains 1 million ratings (ranging from 0 to 10) of books in the Book-Crossing community. The corresponding KG contains 25,787 entities, 60,787 edges and 18 relation-types.  \n- Last.FM<sup>6</sup> contains musician listening information from a set of 2 thousand users from Last.fm online music system. The corresponding KG contains 9,366 entities, 15,518 edges and 60 relation-types.  \n- Dianping-Food is provided by Dianping.com<sup>7</sup>, which contains over 10 million interactions (including clicking, buying, and adding to favorites) between approximately 2 million users and 1 thousand restaurants. The corresponding KG contains 28,115 entities, 160,519 edges and 7 relation-types.\n\nThe statistics of the four datasets are shown in Table 2.\n\n## 5.2Baselines\n\nWe compare the proposed KGNN-LS model with the following baselines for recommender systems, in which the first two baselines are KG-free while the rest are all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.\n\n- SVD [12] is a classic CF-based model using inner product to model user-item interactions. We use the unbiased version (i.e., the predicted engaging probability is modeled as  \\(y_{uv} = \\mathbf{u}^{\\top}\\mathbf{v}\\) ). The dimension and learning rate for the four datasets are set as:  \\(d = 8\\) ,  \\(\\eta = 0.5\\)  for MovieLens-20M, Book-Crossing;  \\(d = 8\\) ,  \\(\\eta = 0.1\\)  for Last.FM;  \\(d = 32\\) ,  \\(\\eta = 0.1\\)  for Dianping-Food.\n\n3https://searchengineand.com/library/bing/bing-satori  \n4https://grouplens.org/datasets/movielens/  \n<sup>5</sup>http://www2.informatik.uni-reiburg.de/~cziegler/BX/  \n\\(^{6}\\) https://grouplens.org/datasets/hetrec-2011/  \n<sup>7</sup>https://www.dianping.com/\n\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">MovieLens-20M</td><td colspan=\"4\">Book-Crossing</td><td colspan=\"4\">Last.FM</td><td colspan=\"4\">Dianping-Food</td></tr><tr><td>R@2</td><td>R@10</td><td>R@50</td><td>R@100</td><td>R@2</td><td>R@10</td><td>R@50</td><td>R@100</td><td>R@2</td><td>R@10</td><td>R@50</td><td>R@100</td><td>R@2</td><td>R@10</td><td>R@50</td><td>R@100</td></tr><tr><td>SVD</td><td>0.036</td><td>0.124</td><td>0.277</td><td>0.401</td><td>0.027</td><td>0.046</td><td>0.077</td><td>0.109</td><td>0.029</td><td>0.098</td><td>0.240</td><td>0.332</td><td>0.039</td><td>0.152</td><td>0.329</td><td>0.451</td></tr><tr><td>LibFM</td><td>0.039</td><td>0.121</td><td>0.271</td><td>0.388</td><td>0.033</td><td>0.062</td><td>0.092</td><td>0.124</td><td>0.030</td><td>0.103</td><td>0.263</td><td>0.330</td><td>0.043</td><td>0.156</td><td>0.332</td><td>0.448</td></tr><tr><td>LibFM + TransE</td><td>0.041</td><td>0.125</td><td>0.280</td><td>0.396</td><td>0.037</td><td>0.064</td><td>0.097</td><td>0.130</td><td>0.032</td><td>0.102</td><td>0.259</td><td>0.326</td><td>0.044</td><td>0.161</td><td>0.343</td><td>0.455</td></tr><tr><td>PER</td><td>0.022</td><td>0.077</td><td>0.160</td><td>0.243</td><td>0.022</td><td>0.041</td><td>0.064</td><td>0.070</td><td>0.014</td><td>0.052</td><td>0.116</td><td>0.176</td><td>0.023</td><td>0.102</td><td>0.256</td><td>0.354</td></tr><tr><td>CKE</td><td>0.034</td><td>0.107</td><td>0.244</td><td>0.322</td><td>0.028</td><td>0.051</td><td>0.079</td><td>0.112</td><td>0.023</td><td>0.070</td><td>0.180</td><td>0.296</td><td>0.034</td><td>0.138</td><td>0.305</td><td>0.437</td></tr><tr><td>RippleNet</td><td>0.045</td><td>0.130</td><td>0.278</td><td>0.447</td><td>0.036</td><td>0.074</td><td>0.107</td><td>0.127</td><td>0.032</td><td>0.101</td><td>0.242</td><td>0.336</td><td>0.040</td><td>0.155</td><td>0.328</td><td>0.440</td></tr><tr><td>KGNN-LS</td><td>0.043</td><td>0.155</td><td>0.321</td><td>0.458</td><td>0.045</td><td>0.082</td><td>0.117</td><td>0.149</td><td>0.044</td><td>0.122</td><td>0.277</td><td>0.370</td><td>0.047</td><td>0.170</td><td>0.340</td><td>0.487</td></tr></table>\n\nTable 3: The results of Recall@K in top-K recommendation.  \n\n<table><tr><td>Model</td><td>Movie</td><td>Book</td><td>Music</td><td>Restaurant</td></tr><tr><td>SVD</td><td>0.963</td><td>0.672</td><td>0.769</td><td>0.838</td></tr><tr><td>LibFM</td><td>0.959</td><td>0.691</td><td>0.778</td><td>0.837</td></tr><tr><td>LibFM + TransE</td><td>0.966</td><td>0.698</td><td>0.777</td><td>0.839</td></tr><tr><td>PER</td><td>0.832</td><td>0.617</td><td>0.633</td><td>0.746</td></tr><tr><td>CKE</td><td>0.924</td><td>0.677</td><td>0.744</td><td>0.802</td></tr><tr><td>RippleNet</td><td>0.960</td><td>0.727</td><td>0.770</td><td>0.833</td></tr><tr><td>KGNN-LS</td><td>0.979</td><td>0.744</td><td>0.803</td><td>0.850</td></tr></table>\n\nTable 4: The results of AUC in CTR prediction.\n\n- LibFM [16] is a widely used feature-based factorization model for CTR prediction. We concatenate user ID and item ID as input for LibFM. The dimension is set as  \\(\\{1, 1, 8\\}\\)  and the number of training epochs is 50 for all datasets.  \n- LibFM + TransE extends LibFM by attaching an entity representation learned by TransE [2] to each user-item pair. The dimension of TransE is 32 for all datasets.  \n- PER [33] is a representative of path-based methods, which treats the KG as heterogeneous information networks and extracts meta-path based features to represent the connectivity between users and items. We use manually designed \"user-item-attribute-item\" as meta-paths, i.e., \"user-movie-directormovie\", \"user-movie-genre-movie\", and \"user-movie-star-movie\" for MovieLens-20M; \"user-book-author-book\" and \"user-book-genre-book\" for Book-Crossing, \"user-musician-date_of_birth-musician\" (date of birth is discretized), \"user-musician-country-musician\", and \"user-musician-genre-musician\" for Last.FM; \"user-restaurant-dish-restaurant\", \"user-restaurant-business_area-restaurant\", \"user-restaurant-tag-restaurant\" for Dianping-Food. The settings of dimension and learning rate are the same as SVD.  \n- CKE [34] is a representative of embedding-based methods, which combines CF with structural, textual, and visual knowledge in a unified framework. We implement CKE as CF plus a structural knowledge module in this paper. The dimension of embedding for the four datasets are 64, 128, 64, 64. The training weight for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.  \n- RippleNet [24] is a representative of hybrid methods, which is a memory-network-like approach that propagates users' preferences on the KG for recommendation. For RippleNet,  \\(d = 8\\) ,  \\(H = 2\\) ,  \\(\\lambda_1 = 10^{-6}\\) ,  \\(\\lambda_2 = 0.01\\) ,  \\(\\eta = 0.01\\)  for MovieLens20M;  \\(d = 16\\) ,  \\(H = 3\\) ,  \\(\\lambda_1 = 10^{-5}\\) ,  \\(\\lambda_2 = 0.02\\) ,  \\(\\eta = 0.005\\)  for\n\nLast.FM;  \\(d = 32\\) ,  \\(H = 2\\) ,  \\(\\lambda_{1} = 10^{-7}\\) ,  \\(\\lambda_{2} = 0.02\\) ,  \\(\\eta = 0.01\\)  for Dianping-Food.\n\n## 5.3 Validating the Connection between  \\(\\mathcal{G}\\)  and Y\n\nTo validate the connection between the knowledge graph  \\(\\mathcal{G}\\)  and user-item interaction Y, we conduct an empirical study where we investigate the correlation between the shortest path distance of two randomly sampled items in the KG and whether they have common user(s) in the dataset, that is there exist user(s) that interacted with both items. For MovieLens-20M and Last.FM, we randomly sample ten thousand item pairs that have no common users and have at least one common user, respectively, then count the distribution of their shortest path distances in the KG. The results are presented in Figure 3, which clearly show that if two items have common user(s) in the dataset, they are likely to be more close in the KG. For example, if two movies have common user(s) in MovieLens-20M, there is a probability of 0.92 that they will be within 2 hops in the KG, while the probability is 0.80 if they have no common user. This finding empirically demonstrates that exploiting the proximity structure of the KG can assist making recommendations. This also justifies our motivation to use label smoothness regularization to help learn entity representations.\n\n## 5.4 Results\n\n5.4.1 Comparison with Baselines. We evaluate our method in two experiment scenarios: (1) In top- \\(K\\)  recommendation, we use the trained model to select  \\(K\\)  items with highest predicted click probability for each user in the test set, and choose  \\(\\text{Recall}@\\text{K}\\)  to evaluate the recommended sets. (2) In click-through rate (CTR) prediction, we apply the trained model to predict each piece of user-item pair in the test set (including positive items and randomly selected negative items). We use AUC as the evaluation metric in CTR prediction.\n\nThe results of top-  \\(K\\)  recommendation and CTR prediction are presented in Tables 3 and 4, respectively, which show that KGNNLS outperforms baselines by a significant margin. For example, the AUC of KGNN-LS surpasses baselines by  \\(5.1\\%\\) \\(6.9\\%\\) \\(8.3\\%\\)  and  \\(4.3\\%\\)  on average in MovieLens-20M, Book-Crossing, Last.FM, and Dianping-Food datasets, respectively.\n\nWe also show daily performance of KGNN-LS and baselines on Dianping-Food to investigate performance stability. Figure 4 shows their AUC score from September 1, 2018 to September 30, 2018. We notice that the curve of KGNN-LS is consistently above baselines over the test period; Moreover, the performance of KGNN-LS is also\n\n![](images/73102be32084b9ef07c73297460b3972dd95ae44d20fccbbec26e0838a60ebb8.jpg)  \nFigure 4: Daily AUC of all methods on Dianping-Food in September 2018.\n\n![](images/39f34f89fe7131675e220da8c1a1efa07936d15a33bd7244b205d8a39172a27f.jpg)  \nFigure 5: Effectiveness of LS regularization on Last.FM.\n\n![](images/a6bc308d0f2d344e2d895ebd50069d7dfe3f8e3dc67a09c3b5472ffdccb02ff7.jpg)  \nFigure 6: Running time of all methods w.r.t. KG size on MovieLens-20M.\n\n<table><tr><td>r</td><td>20%</td><td>40%</td><td>60%</td><td>80%</td><td>100%</td></tr><tr><td>SVD</td><td>0.882</td><td>0.913</td><td>0.938</td><td>0.955</td><td>0.963</td></tr><tr><td>LibFM</td><td>0.902</td><td>0.923</td><td>0.938</td><td>0.950</td><td>0.959</td></tr><tr><td>LibFM+TransE</td><td>0.914</td><td>0.935</td><td>0.949</td><td>0.960</td><td>0.966</td></tr><tr><td>PER</td><td>0.802</td><td>0.814</td><td>0.821</td><td>0.828</td><td>0.832</td></tr><tr><td>CKE</td><td>0.898</td><td>0.910</td><td>0.916</td><td>0.921</td><td>0.924</td></tr><tr><td>RippleNet</td><td>0.921</td><td>0.937</td><td>0.947</td><td>0.955</td><td>0.960</td></tr><tr><td>KGNN-LS</td><td>0.961</td><td>0.970</td><td>0.974</td><td>0.977</td><td>0.979</td></tr></table>\n\nwith low variance, which suggests that KGNN-LS is also robust and stable in practice.\n\n5.4.2 Effectiveness of LS Regularization. Is the proposed LS regularization helpful in improving the performance of GNN? To study the effectiveness of LS regularization, we fix the dimension of hidden layers as 4, 8, and 16, then vary  \\(\\lambda\\)  from 0 to 5 to see how performance changes. The results of  \\(R@10\\)  in Last.FM dataset are plotted in Figure 5. It is clear that the performance of KGNN-LS with a non-zero  \\(\\lambda\\)  is better than  \\(\\lambda = 0\\)  (the case of Wang et al. [28]), which justifies our claim that LS regularization can assist learning the edge weights in a KG and achieve better generalization in recommender systems. But note that a too large  \\(\\lambda\\)  is less favorable, since it overwhelms the overall loss and misleads the direction of gradients. According to the experiment results, we find that a  \\(\\lambda\\)  between 0.1 and 1.0 is preferable in most cases.\n\n5.4.3 Results in cold-start scenarios. One major goal of using KGs in recommender systems is to alleviate the sparsity issue. To investigate the performance of KGNN-LS in cold-start scenarios, we vary the size of training set of MovieLens-20M from  \\(r = 100\\%\\)  to  \\(r = 20\\%\\)  (while the validation and test set are kept fixed), and report the results of AUC in Table 5. When  \\(r = 20\\%\\) , AUC decreases by  \\(8.4\\%\\) ,  \\(5.9\\%\\) ,  \\(5.4\\%\\) ,  \\(3.6\\%\\) ,  \\(2.8\\%\\) , and  \\(4.1\\%\\)  for the six baselines compared to the model trained on full training data ( \\(r = 100\\%\\) ), but the performance decrease of KGNN-LS is only  \\(1.8\\%\\) . This demonstrates that KGNN-LS still maintains predictive performance even when user-item interactions are sparse.\n\n5.4.4 Hyper-parameters Sensitivity. We first analyze the sensitivity of KGNN-LS to the number of GNN layers  \\(L\\) . We vary  \\(L\\)  from 1 to 4 while keeping other hyper-parameters fixed. The results are shown in Table 6. We find that the model performs poorly when  \\(L = 4\\) , which is because a larger  \\(L\\)  will mix too many entity embeddings\n\nTable 5: AUC of all methods w.r.t. the ratio of training set  \\(r\\)  .  \n\n<table><tr><td>L</td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>MovieLens-20M</td><td>0.155</td><td>0.146</td><td>0.122</td><td>0.011</td></tr><tr><td>Book-Crossing</td><td>0.077</td><td>0.082</td><td>0.043</td><td>0.008</td></tr><tr><td>Last.FM</td><td>0.122</td><td>0.106</td><td>0.105</td><td>0.057</td></tr><tr><td>Dianping-Food</td><td>0.165</td><td>0.170</td><td>0.061</td><td>0.036</td></tr></table>\n\nTable 6:  \\(R@{10}\\)  w.r.t. the number of layers  \\(L\\)  .  \n\n<table><tr><td>d</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>128</td></tr><tr><td>MovieLens-20M</td><td>0.134</td><td>0.141</td><td>0.143</td><td>0.155</td><td>0.155</td><td>0.151</td></tr><tr><td>Book-Crossing</td><td>0.065</td><td>0.073</td><td>0.077</td><td>0.081</td><td>0.082</td><td>0.080</td></tr><tr><td>Last.FM</td><td>0.111</td><td>0.116</td><td>0.122</td><td>0.109</td><td>0.102</td><td>0.107</td></tr><tr><td>Dianping-Food</td><td>0.155</td><td>0.170</td><td>0.167</td><td>0.166</td><td>0.163</td><td>0.161</td></tr></table>\n\nTable 7:  \\(R@10\\)  w.r.t. the dimension of hidden layers  \\(d\\) .\n\nin a given entity, which over-smoothes the representation learning on KGs. KGNN-LS achieves the best performance when  \\(L = 1\\)  or 2 in the four datasets.\n\nWe also examine the impact of the dimension of hidden layers  \\(d\\)  on the performance of KGNN-LS. The result in shown in Table 7. We observe that the performance is boosted with the increase of  \\(d\\)  at the beginning, because more bits in hidden layers can improve the model capacity. However, the performance drops when  \\(d\\)  further increases, since a too large dimension may overfit datasets. The best performance is achieved when  \\(d = 8 \\sim 64\\) .\n\n## 5.5 Running Time Analysis\n\nWe also investigate the running time of our method with respect to the size of KG. We run experiments on a Microsoft Azure virtual machine with 1 NVIDIA Tesla M60 GPU, 12 Intel Xeon CPUs (E5-2690 v3 @2.60GHz), and 128GB of RAM. The size of the KG is increased by up to five times the original one by extracting more triples from Satori, and the running times of all methods on MovieLens-20M are reported in Figure 6. Note that the trend of a curve matters more than the real values, since the values are largely dependent on the minibatch size and the number of epochs (yet we did try to align the configurations of all methods). The result show that KGNN-LS exhibits strong scalability even when the KG is large.\n",
  "hyperparameter": "Number of GNN layers L: 1-2 (best performance, L=4 causes over-smoothing); Embedding dimension d: 8-64 (best range, with specific values: d=8 for MovieLens-20M and Book-Crossing, d=32 for Dianping-Food); Label smoothness regularization weight λ: 0.1-1.0 (preferable range); L2 regularization weight γ; Learning rate η: 0.5 for MovieLens-20M and Book-Crossing, 0.1 for Last.FM and Dianping-Food (for SVD baseline, similar for other methods); For RippleNet baseline: H=2-3 hops, λ1=10^-7 to 10^-5, λ2=0.01-0.02; Training epochs: 50 for LibFM; TransE dimension: 32 for all datasets."
}