{
  "id": "CFKG_2018",
  "paper_title": "Learning heterogeneous knowledge base embeddings for explainable recommendation",
  "alias": "CFKG",
  "year": 2018,
  "domain": "Recsys",
  "task": "KnowledgeAwareRecommendation",
  "idea": "",
  "introduction": "## 1. Introduction\n\nMost of the existing collaborative filtering (CF)-based recommendation systems work with various unstructured data such as ratings, reviews, or images to profile the users for personalized recommendation. Though effective, it is difficult for existing approaches to model the explicit relationship between different information that we know about the users and items. In this paper, we would like to ask a key question, i.e., \"can we extend the power of CF upon large-scale structured user behaviors?\" The main challenge to answer this question is how to effectively integrate different types\n\nof user behaviors and item properties, while preserving the internal relationship between them to enhance the final performance of personalized recommendation.\n\nFortunately, the emerging success on knowledge-base embeddings (KBE) may shed some light on this problem, where heterogeneous information can be projected into a unified low-dimensional embedding space. By encoding the rich information from multi-type user behaviors and item properties into the final user/item embeddings, we can enhance the recommendation performance while preserving the internal structure of the knowledge.\n\nEquipping recommender systems with structured knowledge also helps the system to generate informed explanations for the recommended items. Researchers have shown that providing personalized explanations for the recommendations helps to improve the persuasiveness, efficiency, effectiveness, and transparency of recommender systems [1-7]. By preserving the knowledge structure about users, items, and heterogenous entities, we can conduct fuzzy reasoning over the knowledge-base embeddings (KBE) to generate tailored explanations for each user.\n\nInspired by the above motivations, in this paper, we design a novel explainable CF framework over knowledge graphs. The main building block is an integration of traditional CF with the learning of knowledge-base embeddings. More specifically, we first define the concept of user-item knowledge graph, which encodes our knowledge about the user behaviors and item properties as a relational graph structure. The user-item knowledge graph focuses on how to depict different types of user behaviors and item properties over heterogeneous entities in a unified framework. Then, we extend the design philosophy of CF to learn over the knowledge graph for personalized recommendation. For each recommended item, we further conduct fuzzy reasoning over the paths in the knowledge graph based on soft matching to construct personalized explanations.\n\nContributions. The main contributions of this paper can be summarized as follows:\n\nWe propose to integrate heterogeneous multi-type user behaviors and knowledge of the items into a unified graph structure for recommendation.  \n- Based on the user-item knowledge graph, we extend traditional CF to learn over the heterogeneous knowledge for recommendation, which helps to capture the user preferences more comprehensively.  \nWe further propose a soft matching algorithm to construct explanations regarding the recommended items by searching over the paths in the graph embedding space.\n\nIn the following part of the paper, we first present related work in Section 2, and then provide the problem formalization in Section 3. Section 4 goes over the model for CF over knowledge graphs, and in Section 5 the soft matching method for generating explanations is illustrated. Experimental setup and discussion of the results are provided in Section 6 and Section 7, respectively. We conclude the work and point out some of the future research directions in Section 8.\n",
  "method": "## 4. Collaborative Filtering on Knowledge Graphs\n\nWe now describe our model for explainable recommendation. Our model is a CF model built on user-item knowledge graph. In this section, we first introduce how to model the entities and relations as a product knowledge graph, and then we discuss how to optimize the model parameters for recommendation.\n\n### 4.1. Relation Modeling as Entity Translations\n\nAs discussed previously, we assume that the product knowledge can be represented as a set of triplets  \\(S = \\{(e_h, e_t, r)\\}\\) , where  \\(r\\)  is the relation from entity  \\(e_h\\)  to entity  \\(e_t\\) . Because an entity can be associated with one or more other entities through a single or multiple relations, we propose to separate the modeling of entity and relation for CF. Specifically, we project each entity to a low-dimensional latent space and treat each relation as a translation function that converts one entity to another. Inspired by [21], we represent  \\(e_h\\)  and  \\(e_t\\)  as latent vectors  \\(e_h \\in \\mathbb{R}^d\\)  and  \\(e_t \\in \\mathbb{R}^d\\) , and model their relationship  \\(r\\)  as a linear projection from  \\(e_h\\)  to  \\(e_t\\)  parameterized by  \\(r \\in \\mathbb{R}^d\\) , namely,\n\n\\[\n\\boldsymbol {e} _ {t} = \\operatorname {t r a n s} \\left(\\boldsymbol {e} _ {h}, r\\right) = \\boldsymbol {e} _ {h} + \\boldsymbol {r} \\tag {1}\n\\]\n\nTo learn the entity embeddings, we can construct a product knowledge graph by linking entities with the translation function in the latent space. An example generation process of such a graph is shown in Figure 1.\n\n![](images/f8a9eb096e99488bde2aa09dd592fb01972f65a5ea3ec090da243c31dc4bd428.jpg)  \nFigure 1. The construction process of knowledge graph with our model. Each entity is represented with a latent vector, and each relation is modeled as a linear translation from one entity to another entity parameterized by the relation embedding.\n\nSolving Equation (1) for all  \\((e_h, e_t, r) \\in S\\) , however, is infeasible in practice. On one hand, a trivial solution that constructs a single latent vector for all entities with the same type will lead to inferior recommendation performance as it ignores the differences between users and items. On the other hand, deriving a solution that assigns different latent vectors for all entities in  \\(S\\)  is mathematically impossible because an entity can be linked to multiple entities with a single relationship. For example, we cannot find a single vector for Also_viewed that translates an item to multiple items that have different latent representations.\n\nTo solve the problems, we propose to relax the constraints of Equation (1) and adopt an embedding-based generative framework to learn it. Empirically, we want the translation model  \\(\\text{trans}(e_h, r) \\approx e_t\\)  for an observed relation triplet  \\((e_h, e_t, r) \\in S\\)  and  \\(\\text{trans}(e_h, r) \\neq e_t'\\)  for an unobserved triplet\n\n\\((e_h, e_t', r) \\notin S\\) . In other words, we want  \\(\\text{trans}(e_h, r)\\)  to assign high probability for observing  \\(e_t\\)  but low probability for observing  \\(e_t'\\) , which is exactly the goal of the embedding-based generative framework. The embedding-based generative framework is first proposed by Mikolov et al. [29] and has been widely used in word embedding [29,30], recommendation [31,32], and information retrieval tasks [33,34]. Formally, for an observed relation triplet  \\((e_h, e_t, r) \\in S\\) , we can learn the translation model  \\(\\text{trans}(e_h, r)\\)  by optimizing the generative probability of  \\(e_t\\)  given  \\(\\text{trans}(e_h, r)\\) , which is defined as:\n\n\\[\nP \\left(e _ {t} \\mid \\text {t r a n s} \\left(e _ {h}, r\\right)\\right) = \\frac {\\exp \\left(\\boldsymbol {e} _ {t} \\cdot \\text {t r a n s} \\left(e _ {h} , r\\right)\\right)}{\\sum_ {e _ {t} ^ {\\prime} \\in E _ {t}} \\exp \\left(\\boldsymbol {e} _ {t} ^ {\\prime} \\cdot \\text {t r a n s} \\left(e _ {h} , r\\right)\\right)} \\tag {2}\n\\]\n\nwhere  \\(E_{t}\\)  is the set of all possible entities that share the same type with  \\(e_{t}\\) .\n\nBecause Equation (2) is a softmax function of  \\(e_t\\)  over  \\(E_t\\) , the maximization of  \\(P(e_t | \\text{trans}(e_h, r))\\)  will explicitly increase the similarity of  \\(\\text{trans}(e_h, r)\\)  and  \\(e_t\\)  but decrease the similarity between  \\(\\text{trans}(e_h, r)\\)  and other entities. In this way, we convert Equation (1) into an optimization problem that can be solved with iterative optimization algorithms such as gradient descent. Another advantage of the proposed model is that it provides a theoretically principled method to conduct soft match between tail entities and the translation model. This is important for the extraction of recommendation explanations, which will be discussed in Section 5.\n\n### 4.2. Optimization Algorithm\n\nFor model optimization, we learn the representations of entities and relations by maximizing the likelihood of all observed relation triplets. Let  \\(S\\)  be the set of observed triplets  \\((e_h, e_t, r)\\)  in the training data, then we can compute the likelihood of  \\(S\\)  defined as\n\n\\[\n\\mathcal {L} (S) = \\log \\prod_ {\\left(e _ {h}, e _ {t}, r\\right) \\in S} P \\left(e _ {t} \\mid \\text {t r a n s} \\left(e _ {h}, r\\right)\\right) \\tag {3}\n\\]\n\nwhere  \\(P(e_t|trans(e_h,r))\\)  is the posterior probability of  \\(e_t\\)  computed with Equation (2).\n\nThe computation cost of  \\(\\mathcal{L}(S)\\) , however, is prohibitive in practice because of the softmax function. For efficient training, we adopt a negative sampling strategy to approximate  \\(P(e_t | \\text{trans}(e_h, r))\\)  [35]. Specifically, for each observed relation triplet  \\((e_h, e_t, r)\\) , we randomly sample a set of \"negative\" entities with the same type of  \\(e_t\\) . Then the log likelihood of  \\((e_h, e_t, r)\\)  is approximated as\n\n\\[\n\\log P \\left(e _ {t} \\mid \\text {t r a n s} \\left(e _ {h}, r\\right)\\right) \\approx \\log \\sigma \\left(\\boldsymbol {e} _ {t} \\cdot \\text {t r a n s} \\left(e _ {h}, r\\right)\\right) + k \\cdot \\mathbb {E} _ {\\boldsymbol {e} _ {t} ^ {\\prime} \\sim P _ {t}} \\left[ \\log \\sigma \\left(- \\boldsymbol {e} _ {t} ^ {\\prime} \\cdot \\text {t r a n s} \\left(e _ {h}, r\\right)\\right) \\right] \\tag {4}\n\\]\n\nwhere  \\(k\\)  is the number of negative samples,  \\(P_{t}\\)  is a predefined noisy distribution over entities with the type of  \\(e_t\\) , and  \\(\\sigma(x)\\)  is a sigmoid function as  \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) . Therefore,  \\(\\mathcal{L}(S)\\)  can be reformulated as the sum of the log-likelihood of  \\((e_h, e_t, r) \\in S\\)  as\n\n\\[\n\\mathcal {L} (S) = \\sum_ {\\left(e _ {h}, e _ {t}, r\\right) \\in S} \\log \\sigma \\left(\\boldsymbol {e} _ {t} \\cdot \\operatorname {t r a n s} \\left(e _ {h}, r\\right)\\right) + k \\cdot \\mathbb {E} _ {e _ {t} ^ {\\prime} \\sim P _ {t}} [ \\log \\sigma \\left(- \\boldsymbol {e} _ {t} ^ {\\prime} \\cdot \\operatorname {t r a n s} \\left(e _ {h}, r\\right)\\right) ] \\tag {5}\n\\]\n\nWe also tested the  \\(\\ell_2\\) -norm loss function used in TransE model and it does not provide any improvement compared to our inner product-based model with log-likelihood loss, and it is also difficult for  \\(\\ell_2\\) -norm loss to generate expansions, as a result, we adopt our loss function for embedding, recommendation, and explanation in this work. To better illustrate the relationship between our model and a traditional CF method based on matrix factorization, we conduct the following analysis. Inspired by [36], we derive the local objective for the maximization of Equation (5) on a specific relation triplet  \\((e_h, e_t, r)\\) :\n\n\\[\n\\ell (e _ {h}, e _ {t}, r) = \\# (e _ {h}, e _ {t}, r) \\cdot \\log \\sigma (\\boldsymbol {e} _ {t} \\cdot t r a n s (e _ {h}, r)) + k \\cdot \\# (e _ {h}, r) \\cdot P _ {t} (e _ {t}) \\cdot \\log \\sigma (- \\boldsymbol {e} _ {t} \\cdot t r a n s (e _ {h}, r)) \\quad (6)\n\\]\n\nwhere  \\(\\# (e_h,e_t,r)\\)  and  \\(\\# (e_h,r)\\)  are the frequency of  \\((e_h,e_t,r)\\)  and  \\((e_h,r)\\)  in the training data. If we further compute the partial derivative of  \\(\\ell (e_h,e_t,r)\\)  with respect to  \\(x = e_{t}\\cdot trans(e_{h},r)\\) , we have\n\n\\[\n\\frac {\\partial \\ell \\left(e _ {h} , e _ {t} , r\\right)}{\\partial x} = \\# \\left(e _ {h}, e _ {t}, r\\right) \\cdot \\sigma (- x) - k \\cdot \\# \\left(e _ {h}, r\\right) \\cdot P _ {t} \\left(e _ {t}\\right) \\cdot \\sigma (x) \\tag {7}\n\\]\n\nWhen the training process has converged, the partial derivative of  \\(\\ell(e_h, e_t, r)\\)  should be 0, and then we have\n\n\\[\nx = \\boldsymbol {e} _ {t} \\cdot \\operatorname {t r a n s} \\left(e _ {h}, r\\right) = \\log \\left(\\frac {\\# \\left(e _ {h} , e _ {t} , r\\right)}{\\# \\left(e _ {h} , r\\right)} \\cdot \\frac {1}{P _ {t} \\left(e _ {t}\\right)}\\right) - \\log k \\tag {8}\n\\]\n\nAs we can see, the left-hand side is the product of the latent vectors for  \\(e_t\\)  and  \\(\\text{trans}(e_h, r)\\) ; and the right-hand side of Equation (8) is a shifted version of the pointwise mutual information between  \\(e_t\\)  and  \\((e_h, r)\\) . Therefore, maximizing the log likelihood of observed triplet set  \\(S\\)  with negative sampling is actually factorizing the matrix of mutual information between the head-tail entity pairs  \\((e_h, e_t)\\)  of relation  \\(r\\) . From this perspective, our model is a variation of factorization methods that can jointly factorize multiple relation matrix on a product knowledge graph.\n\nAs shown in Equation (8), the final objective of the proposed model is controlled by the noisy distribution  \\(P_{t}\\) . Similar to previous studies [30,33,35], we notice that the relationships with tail entities that have high frequency in the collection reveal less information about the properties of the head entity. Therefore, we define the noisy probability  \\(P_{t}(e_{t})\\)  for each relation  \\(r\\)  (except Purchase) as the frequency distribution of  \\((e_{h}, e_{t}, r)\\)  in  \\(S\\)  so that the mutual information on frequent tail entities will be penalized in optimization process. For Purchase, however, we define  \\(P_{t}\\)  as a uniform distribution to avoid unnecessary biases toward certain items.\n",
  "experiments": "## 6. Experimental Setup\n\nIn this section, we introduce the test bed of our experiments and discuss our evaluation settings in details.\n\n### 6.1. Datasets\n\nWe conducted experiments on the Amazon review dataset [37], which contains product reviews in 24 categories on Amazon.com and rich metadata such as prices, brands, etc. Specifically, we used the 5-core data of CDs and Vinyl, Clothing, Cell Phones, and Beauty, in which each user or item has at least 5 associated reviews.\n\nStatistics about entities and relations used in our experiments are shown in Table 1. Overall, the interactions between users, items and other entities are highly sparse. For each dataset, we randomly sampled  \\(70\\%\\)  of user purchase as the training data and used the rest  \\(30\\%\\)  as the test set. This means that each user has at least 3 reviews observed in the training process and 2 reviews hidden for evaluation purposes. Thus, the objective of product recommendation is to find and recommend items that are purchased by the user in the test set.\n\nTable 1. Statistics of the 5-core datasets for CDs & Vinyl, Clothing, Cell Phones & Accessories, and Beauty in Amazon.  \n\n<table><tr><td></td><td>CDs &amp; Vinyl</td><td>Clothing</td><td>Cell Phones &amp; Accessories</td><td>Beauty</td></tr><tr><td>Entities</td><td></td><td></td><td></td><td></td></tr><tr><td>#Reviews</td><td>1,097,591</td><td>278,677</td><td>194,439</td><td>198,502</td></tr><tr><td>#Words per review</td><td>174.57 ± 177.05</td><td>62.21 ± 60.16</td><td>93.50 ±131.65</td><td>90.90 ± 91.86</td></tr><tr><td>#Users</td><td>75,258</td><td>39,387</td><td>27,879</td><td>22,363</td></tr><tr><td>#Items</td><td>64,443</td><td>23,033</td><td>10,429</td><td>12,101</td></tr><tr><td>#Brands</td><td>1414</td><td>1182</td><td>955</td><td>2077</td></tr><tr><td>#Categories</td><td>770</td><td>1,193</td><td>206</td><td>248</td></tr><tr><td>Density</td><td>0.023%</td><td>0.031%</td><td>0.067%</td><td>0.074%</td></tr><tr><td>Relations</td><td></td><td></td><td></td><td></td></tr><tr><td>#Purchase per user</td><td>14.58 ± 39.13</td><td>7.08 ± 3.59</td><td>6.97 ± 4.55</td><td>8.88 ± 8.16</td></tr><tr><td>#Mention per user</td><td>2545.92 ± 10,942.31</td><td>440.20 ± 452.38</td><td>652.08 ± 1335.76</td><td>806.89 ± 1344.08</td></tr><tr><td>#Mention per item</td><td>2973.19 ± 5490.93</td><td>752.75 ± 909.42</td><td>1743.16 ± 3482.76</td><td>1491.16 ± 2553.93</td></tr><tr><td>#Also_bought per item</td><td>57.28 ± 39.22</td><td>61.35 ± 32.99</td><td>56.53 ± 35.82</td><td>73.65 ± 30.69</td></tr><tr><td>#Also_viewed per item</td><td>0.27 ± 1.86</td><td>6.29 ± 6.17</td><td>1.24 ± 4.29</td><td>12.84 ± 8.97</td></tr><tr><td>#Bought_together per item</td><td>0.68 ± 0.80</td><td>0.69 ± 0.90</td><td>0.81 ± 0.77</td><td>0.75 ± 0.72</td></tr><tr><td>#Produced_by per item</td><td>0.21 ± 0.41</td><td>0.17 ± 0.38</td><td>0.52 ± 0.50</td><td>0.83 ± 0.38</td></tr><tr><td>#Belongs_to per item</td><td>7.25 ± 3.13</td><td>6.72 ± 2.15</td><td>3.49 ± 1.08</td><td>4.11 ± 0.70</td></tr></table>\n\n### 6.2. Evaluation\n\nTo verify the effectiveness of the proposed model, we adopt six representative and state-of-the-art methods as baselines for performance comparison. Three of them are traditional recommendation methods based on matrix factorization (BPR [38], BPR-HFT [39], and VBPR [40]), and the other three are deep models for product recommendation (DeepCoNN [32], CKE [24], and JRL [31]).\n\n- BPR: The Bayesian personalized ranking [38] model is a popular method for top-N recommendation that learns latent representations of users and items by optimizing the pairwise preferences between different user-item pairs. In this paper, we adopt matrix factorization as the prediction component for BPR.  \n- BPR-HFT: The hidden factors and topics (HFT) model [39] integrates latent factors with topic models for recommendation. The original HFT model is optimized for rating prediction tasks. For fair comparison, we learn the model parameters under the pairwise ranking framework of BPR for top-N recommendation.  \n- VBPR: The visual Bayesian personalized ranking [40] model is a state-of-the-art method that incorporate product image features into the framework of BPR for recommendation.  \n- TransRec: The translation-based recommendation approach proposed in [25], which takes items as entities and users as relations, and leveraged translation-based embeddings to learn the similarity between user and items for personalized recommendation. We adopted  \\(L_{2}\\)  loss function, which was reported to have better performance in [25]. Notice that TransRec is different from our model because our model treats both items and users as entities, and learns embedding representations for different types of knowledge (e.g., brands, categories) as well as their relationships.  \n- DeepCoNN: The Deep Cooperative Neural Networks model for recommendation [32] is a neural model that applies a convolutional neural network (CNN) over the textual reviews to jointly model users and items for recommendation.  \n- CKE: The collaborative KBE model is a state-of-the-art neural model [24] that integrates text, images, and knowledge base for recommendation. It is similar to our model as they both use text and structured product knowledge, but it builds separate models on each type of data to construct item representations while our model constructs a knowledge graph that jointly embeds all entities and relations.\n\n- JRL: The joint representation learning model [31] is a state-of-the-art neural recommender, which leverage multi-model information including text, images and ratings for Top-N recommendation.\n\nThe performance evaluation is conducted on the test set where only purchased items are considered to be relevant to the corresponding user. Specifically, we adopt four ranking measures for top-N recommendation, which are the Normalized Discounted Cumulative Gain (NDCG), Precision (Prec.), Recall, and the percentage of users that have at least one correct recommendation (Hit-Ratio, HR). All ranking metrics are computed based on the top-10 results for each test user. Significant test is conducted based on the Fisher randomization test [41].\n\n### 6.3. Parameter Settings\n\nOur model is trained with stochastic gradient descent on a Nvidia Titan X GPU. We set the initial learning rate as 0.5 and gradually decrease it to 0.0 during the training process. We set the batch size as 64 and clip the norm of batch gradients with 5. For each dataset, we train the model for 20 epochs and set the negative sampling number as 5. We tune the dimension of embeddings from 10 to 500 ([10, 50, 100, 200, 300, 400, 500]) and report the best performance of each model in Section 7.\n\nWe also conduct five-fold cross-validation on the training data to tune the hyper-parameters for baselines. For BPR-HFT, the best performance is achieved when the number of topics is 10. For BPR and VBPR, the regularization coefficient  \\(\\lambda = 10\\)  worked the best in most cases. Similar to our model, we tune the number of latent factors (the embedding size) from 10 to 500 and only report the best performance of each baseline.\n\n## 7. Results and Discussion\n\nIn this section, we discuss the experimental results. We first compare the performance of our model with the baseline methods on top-N recommendation. Then we conduct case studies to show the effectiveness of our model for recommendation explanation.\n\n### 7.1. Recommendation Performance\n\nOur experiments mainly focus on two research questions:\n\nRQ1: Does incorporating knowledge-base in our model produce better recommendation performance?  \nRQ2: Which types of product knowledge are most useful for top-N recommendation?  \n- RQ3: What is the computational efficiency of our model compared to other recommendation algorithms?\n\nTo answer RQ1, we report the results of our model and the baseline methods in Table 2. As shown in Table 2, the deep models with rich auxiliary information (DeepCoNN, CKE, and JRL) perform better in general than the shallow methods (BPR, BPR-HFT, VBPR, TransRec) on most datasets, which is coherent with previous studies [24,31,32]. Among different neural baselines, JRL obtains the best performance in our experiments. It produced  \\(80\\%\\)  or more improvements over the matrix factorization baselines and  \\(10\\%\\)  or more over the other deep recommendation models. Overall, our model outperformed all the baseline models consistently and significantly. It obtained  \\(5.6\\%\\)  NDCG improvement over the best baseline (i.e., JRL) on CDs and Vinyl,  \\(78.16\\%\\)  on Clothing,  \\(23.05\\%\\)  on Cell Phones, and  \\(45.56\\%\\)  on Beauty. This shows that the proposed model can effectively incorporate product knowledge graph and is highly competitive for top-N recommendation.\n\nFigure 3 depicts the recommendation performance of our model and baseline methods with different embedding sizes on  \\(CDs\\ & \\text{Vinyl}\\)  and Beauty datasets. Observations on the other two datasets are similar. As shown in Figure 3, the recommendation methods based on shallow models (BPR, BPR-HFT, and VBPR) obtain the best NDCG when the embedding size is fairly small (from 10 to 100), and larger embedding sizes usually hurt the performance of these models. In contrast\n\nto the shallow models, the results of neural models (i.e., JRL, DeepCoNN, CKE, and our model) show positive correlations with the increase of embedding sizes. For example, the NDCG of the best baseline (JRL) and our model improves when the embedding size increases from 10 to 300, and remains stable afterwards. Overall, our model is robust to the variation of embedding sizes and consistently outperformed the baselines.\n\n![](images/f979d631249dfe2e86357ca9214531d6d26f10bb2cfb6cc2a806b50f1fb91e76.jpg)  \nFigure 3. The NDCG@10 performance of our model (ECFKG, Explainable Collaborative Filtering over Knowledge Graph) and the baseline methods under different embedding sizes.\n\n![](images/0567374f3094068f8e41e4d5e496c883bbb6872a7d8b4bdcedad9df00eeabfff.jpg)\n\nIn Table 2, we see that the CKE model did not perform as well as we expected. Although it has incorporated reviews, images and all other product knowledge described in this paper, the CKE model did not perform as well as JRL and our model. One possible reason is that CKE only considers heterogeneous information in the construction of item representations, but it does not directly leverage the information for user modeling. Another potential reason is that CKE separately constructs three latent spaces for text, image and other product knowledge, which makes it difficult for information from different types of data to propagate among the entities. Either way, this indicates that the embedding-based relation modeling of our model is a better way to incorporate structured knowledge for product recommendation.\n\nFrom the results we can also see that datasets of different density result in different performance in our model. In particular, denser datasets (Cell Phone and Beauty) generally get better ranking performance than sparser datasets (CD & Vinyl and Clothing) in our model, which means more sufficient information can help to learn better models in our algorithm.\n\nTo answer RQ2, we experiment the performance of our model when using different relations. Because we eventually need to provide item recommendations for users, our approach would at least need the Purchase relation to model the user purchase histories. As a result, we train and test our model built with only the Purchase relation, as well as Purchase plus one another relation separately. As shown in Table 3, the relative performance of our models built on different relations varies considerably on the four datasets, which makes it difficult to conclude which type of product knowledge is the globally most useful one. This, however, is not surprising because the value of relation data depends on the properties of the candidate products. On CDs and Vinyl, where most products are music CDs, the CD covers did not reveal much information, and people often express their tastes and preferences in the reviews they wrote. Thus Mention turns out to be the most useful relation. On Clothing, however, reviews are not as important as the appearance or picture of the clothes, instead, it is easier to capture item similarities from the items that have been clicked and viewed by the same user. Therefore, adding Also_view relation produces the largest performance improvement for our model on Clothing.\n\nTable 2. Performance of the baselines and our model on top-10 recommendation. All the values in the table are percentage numbers with  \\(\\%\\)  omitted, and all differences are significant at  \\(p < 0.05\\) . The stared numbers (*) indicate the best baseline performances, and the bolded numbers indicate the best performance of each column. The last line shows the percentage improvement of our model against the best baseline (i.e., JRL), which are significant at  \\(p < 0.001\\) .  \n\n<table><tr><td colspan=\"2\">Dataset</td><td colspan=\"4\">CDs and Vinyl</td><td colspan=\"4\">Clothing</td><td colspan=\"4\">Cell Phones</td><td colspan=\"4\">Beauty</td></tr><tr><td>Measures (%)</td><td>NDCG</td><td>Recall</td><td>HR</td><td>Prec.</td><td>NDCG</td><td>Recall</td><td>HR</td><td>Prec.</td><td>NDCG</td><td>Recall</td><td>HR</td><td>Prec.</td><td>NDCG</td><td>Recall</td><td>HR</td><td>Prec.</td><td></td></tr><tr><td>BPR</td><td>2.009</td><td>2.679</td><td>8.554</td><td>1.085</td><td>0.601</td><td>1.046</td><td>1.767</td><td>0.185</td><td>1.998</td><td>3.258</td><td>5.273</td><td>0.595</td><td>2.753</td><td>4.241</td><td>8.241</td><td>1.143</td><td></td></tr><tr><td>BPR-HFT</td><td>2.661</td><td>3.570</td><td>9.926</td><td>1.268</td><td>1.067</td><td>1.819</td><td>2.872</td><td>0.297</td><td>3.151</td><td>5.307</td><td>8.125</td><td>0.860</td><td>2.934</td><td>4.459</td><td>8.268</td><td>1.132</td><td></td></tr><tr><td>VBPR</td><td>0.631</td><td>0.845</td><td>2.930</td><td>0.328</td><td>0.560</td><td>0.968</td><td>1.557</td><td>0.166</td><td>1.797</td><td>3.489</td><td>5.002</td><td>0.507</td><td>1.901</td><td>2.786</td><td>5.961</td><td>0.902</td><td></td></tr><tr><td>TransRec</td><td>3.372</td><td>5.283</td><td>11.956</td><td>1.837</td><td>1.245</td><td>2.078</td><td>3.116</td><td>0.312</td><td>3.361</td><td>6.279</td><td>8.725</td><td>0.962</td><td>3.218</td><td>4.853</td><td>9.867</td><td>1.285</td><td></td></tr><tr><td>DeepCoNN</td><td>4.218</td><td>6.001</td><td>13.857</td><td>1.681</td><td>1.310</td><td>2.332</td><td>3.286</td><td>0.229</td><td>3.636</td><td>6.353</td><td>9.913</td><td>0.999</td><td>3.359</td><td>5.429</td><td>9.807</td><td>1.200</td><td></td></tr><tr><td>CKE</td><td>4.620</td><td>6.483</td><td>14.541</td><td>1.779</td><td>1.502</td><td>2.509</td><td>4.275</td><td>0.388</td><td>3.995</td><td>7.005</td><td>10.809</td><td>1.070</td><td>3.717</td><td>5.938</td><td>11.043</td><td>1.371</td><td></td></tr><tr><td>JRL</td><td>5.378 *</td><td>7.545 *</td><td>16.774 *</td><td>2.085 *</td><td>1.735 *</td><td>2.989 *</td><td>4.634 *</td><td>0.442 *</td><td>4.364 *</td><td>7.510 *</td><td>10.940 *</td><td>1.096 *</td><td>4.396 *</td><td>6.949 *</td><td>12.776 *</td><td>1.546 *</td><td></td></tr><tr><td>Our model</td><td>5.563</td><td>7.949</td><td>17.556</td><td>2.192</td><td>3.091</td><td>5.466</td><td>7.972</td><td>0.763</td><td>5.370</td><td>9.498</td><td>13.455</td><td>1.325</td><td>6.399</td><td>10.411</td><td>17.498</td><td>1.986</td><td></td></tr><tr><td>Improvement</td><td>3.44</td><td>5.35</td><td>4.66</td><td>5.13</td><td>78.16</td><td>82.87</td><td>72.03</td><td>72.62</td><td>23.05</td><td>26.47</td><td>22.99</td><td>20.89</td><td>45.56</td><td>49.82</td><td>36.96</td><td>28.46</td><td></td></tr></table>\n\nTable 3. Performance of our model on top-10 recommendation when incorporating Purchase with other types of relation separately. And the bolded numbers indicate the best performance of each column. All the values in the table are percentage numbers with  \\(\\%\\)  ' omitted, and all differences are significant at  \\(p < {0.05}\\)  .  \n\n<table><tr><td>Relations</td><td colspan=\"4\">CDs and Vinyl</td><td colspan=\"4\">Clothing</td><td colspan=\"4\">Cell Phones</td><td colspan=\"4\">Beauty</td></tr><tr><td>Measures(%)</td><td>NDCG</td><td>Recall</td><td>HT</td><td>Prec</td><td>NDCG</td><td>Recall</td><td>HT</td><td>Prec</td><td>NDCG</td><td>Recall</td><td>HT</td><td>Prec</td><td>NDCG</td><td>Recall</td><td>HT</td><td>Prec</td></tr><tr><td>Purchase only</td><td>1.725</td><td>2.319</td><td>7.052</td><td>0.818</td><td>0.974</td><td>1.665</td><td>2.651</td><td>0.254</td><td>2.581</td><td>4.526</td><td>6.611</td><td>0.649</td><td>2.482</td><td>3.834</td><td>7.432</td><td>0.948</td></tr><tr><td>+Also_view</td><td>1.722</td><td>2.356</td><td>6.967</td><td>0.817</td><td>1.800</td><td>3.130</td><td>4.672</td><td>0.448</td><td>2.555</td><td>4.367</td><td>6.417</td><td>0.630</td><td>4.592</td><td>7.505</td><td>12.901</td><td>1.511</td></tr><tr><td>+Also_bought</td><td>3.641</td><td>5.285</td><td>12.332</td><td>1.458</td><td>1.352</td><td>2.419</td><td>3.580</td><td>0.343</td><td>4.095</td><td>7.129</td><td>10.051</td><td>0.986</td><td>4.301</td><td>6.994</td><td>11.908</td><td>1.408</td></tr><tr><td>+Bought_together</td><td>1.962</td><td>2.712</td><td>7.473</td><td>0.861</td><td>0.694</td><td>1.284</td><td>2.026</td><td>0.189</td><td>3.173</td><td>5.572</td><td>7.952</td><td>0.784</td><td>3.341</td><td>5.337</td><td>9.556</td><td>1.181</td></tr><tr><td>+Produced_by</td><td>1.719</td><td>2.318</td><td>6.842</td><td>0.792</td><td>0.579</td><td>1.044</td><td>1.630</td><td>0.155</td><td>2.852</td><td>4.982</td><td>7.274</td><td>0.719</td><td>3.707</td><td>5.939</td><td>10.660</td><td>1.287</td></tr><tr><td>+Belongs_to</td><td>2.799</td><td>4.028</td><td>10.297</td><td>1.200</td><td>1.453</td><td>2.570</td><td>3.961</td><td>0.376</td><td>2.807</td><td>4.892</td><td>7.242</td><td>0.717</td><td>3.347</td><td>5.382</td><td>9.994</td><td>1.193</td></tr><tr><td>+Mention</td><td>3.822</td><td>5.185</td><td>12.828</td><td>1.628</td><td>1.019</td><td>1.754</td><td>2.780</td><td>0.265</td><td>3.387</td><td>5.806</td><td>8.548</td><td>0.848</td><td>3.658</td><td>5.727</td><td>10.549</td><td>1.305</td></tr><tr><td>+all (our model)</td><td>5.563</td><td>7.949</td><td>17.556</td><td>2.192</td><td>3.091</td><td>5.466</td><td>7.972</td><td>0.763</td><td>5.370</td><td>9.498</td><td>13.455</td><td>1.325</td><td>6.370</td><td>10.341</td><td>17.131</td><td>1.959</td></tr></table>\n\nOverall, it is difficult to find a product knowledge that is universally useful for recommending products from all categories. However, we see that by modeling all of the heterogenous relation types, our final model outperforms all the baselines and outperforms all the simplified versions of our model with one or two types of relation, which implies that our KBE approach to recommendation is scalable to new relation types, and it has the ability to leverage very heterogeneous information sources in a unified manner.\n\nTo answer RQ3, we compare the training efficiency of different methods in our experiments on the same Nvidia Titan X GPU platform. The testing procedures for all methods are quite efficient, and generating the recommendation list for a particular user on the largest CDs & Vinyl dataset requires less than 20 milliseconds. This is because after the model has learned the embeddings of the users and items, generating the recommendation list does not require re-computation of the embeddings and only needs to calculate their similarity. In terms of training efficiency, shallow models can be much more efficient than deep neural models, which is not surprising. For example, BPR or BPR-HFT can be trained within 30 minutes on the largest CDs & Vinyl dataset, while deep models such as DeepCoNN, CKE, and JRL takes about 10 hours on the same dataset, but they also bring much better recommendation performance. Our Explainable CF over Knowledge Graph (ECFKG) approach takes comparable training time on the largest dataset (about 10 hours), while achieving better recommendation performance than other deep neural baselines, which is a good balance between efficiency and effectiveness.\n\n### 7.2. Case Study for Explanation Generation\n\nTo show the ability of our model to generate knowledge-enhanced explanations, we conduct case study for a test user (i.e., A1P27BGF8NAI29) from Cell Phones, for whom we have examined that the first recommendation (i.e., B009RXU59C) provided by the system is correct. We plot the translation process of this user to other entity subspaces with Purchase, Mention, Bought_together, and Belongs_to relations, as shown in Figure 4. We also show the translation of the first recommended item B009RXU59C (B9C) using the same relations. The top 5 entities retrieved by our system for each translation are listed along with their probabilities computed based on Equation (10).\n\n![](images/a159be76600b489294a81d18259dd0680266a0695b308c44f6279af5586c4393.jpg)  \nFigure 4. Example explanation paths between the user A1P27BGF8NAI29 and the item B009RXU59C (B9C) in Cell Phones.\n\nAs we can see in Figure 4, there are three explanation paths between the user A1P27BGF8NAI29 and the item B9C. The first and second paths are constructed by Purchase and Bought_together. According to our model, the user is linked to B008RDIOTU (BTU) and B00HNGB1YS (BYS) through Purchase+Bought_together with probabilities as  \\(27.80\\%\\)  and  \\(5.48\\%\\) . The item B9C is linked to BTU and BYS through Bought_together directly with probabilities as  \\(26.57\\%\\)  and  \\(3.41\\%\\) . The third path is constructed by Purchase and Belongs_to. The user is linked to the category Chargers with probability as  \\(2.53\\%\\)  and B9C is linked to Chargers with probability as  \\(30.90\\%\\) . Therefore, we can create three natural\n\nlanguage explanations for the recommendation of  \\(B9C\\)  by describing these explanation paths with simple templates. Example sentences and the corresponding confidence are listed below:\n\n-  \\(B9C\\)  is recommended because the user often purchases items that are bought with BTU together, and  \\(B9C\\)  is also frequently bought with BTU together ( \\(27.80\\% \\times 26.57\\% = 7.39\\%\\) ).  \n-  \\(B9C\\)  is recommended because the user often purchases items that are bought with  \\(BYS\\)  together, and  \\(B9C\\)  is also frequently bought with  \\(BYS\\)  together ( \\(5.48\\% \\times 3.41\\% = 0.19\\%\\) ).  \n-  \\(B9C\\)  is recommended because the user often purchases items related to the category Chargers, and  \\(B9C\\)  belongs to the category Chargers  \\((2.53\\% \\times 30.90\\% = 0.78\\%)\\) .\n\nAmong the explanation sentences, the best explanation should be the one with the highest confidence, which is the first sentence in this case.\n\nTo better evaluate the quality of these recommendation explanations, we look at the details of each product shown in Figure 4. On Amazon.com, B9C is a High-speed Wall Charger by New Trend for tablets, BTU is an iPad mini Keyboard Case, and BYS is an iPad Air Keyboard Case. If the generated explanations are reasonable, this means that the user has purchased some items that were frequently co-purchased with iPad accessories. Also, this indicates that there is a high probability that the user has an iPad. For validation proposes, we list the five training reviews written by the user in Table 4.\n\nAs we can see in Table 4, the user has purchased several tablet accessories such as Bluetooth headsets and portable charger. The second review even explicitly mentions that the user has possessed an iPad and expresses concerns about the \"running-out-of-juice\" problem. Therefore, it is reasonable to believe that the user is likely to purchase stuff that are frequently co-purchased with iPad accessories such as iPad mini Keyboard Case (BTU) or iPad Air Keyboard Case (BYS), which are recommended as top items in our algorithm, and are also well-explained by the explanation paths in the knowledge graph.\n\nTable 4. The reviews written by the user A1P27BGF8NAI29 in the training data of Cell Phones.  \n\n<table><tr><td>Review of Jabra VOX Coreded Stereo Wired Headsets\n... I like to listen to music at work, but I must wear some sort of headset so that I do not create a disturbance. So, I have a broad experience in headsets ...</td></tr><tr><td>Review of OXA Juice Mini M1 2600mAh\n... I recently had an experience, where I was about town and need to recharge my iPad, and so I tried this thing out.\nI plugged in the iPad, and it quickly charged it up, and at my next destination it was ready to go ...</td></tr><tr><td>Review of OXA 8000mAh Solar External Battery Pack Portable\n... This amazing gadget is a solar powered charger for your small electronic device. This charger is (according to my ruler) 5-1/4 inches by 3 inches by about 6 inches tall. So, it is a bit big to place in the pocket ...</td></tr><tr><td>Review of OXA Bluetooth Wristwatch Bracelet\n... I was far from thrilled with Bluetooth headset that I had, so I decided to give this device a try. Pros: The bracelet is not bad looking, ...</td></tr><tr><td>Review of OXA Mini Portable Wireless Bluetooth Speaker\n... This little gadget is a Bluetooth speaker. It&#x27;s fantastic! This speaker fits comfortably in the palm of your hand, ...</td></tr></table>\n",
  "hyperparameter": ""
}