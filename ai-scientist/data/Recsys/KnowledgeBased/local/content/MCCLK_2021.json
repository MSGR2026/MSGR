{
  "id": "MCCLK_2021",
  "paper_title": "Multi-Context Collaborative Knowledge Learning for Recommendation",
  "alias": "MCCLK",
  "year": 2021,
  "domain": "Recsys",
  "task": "KnowledgeAwareRecommendation",
  "idea": "CKE (Collaborative Knowledge Embedding) integrates three types of knowledge (structural, textual, and visual) from knowledge bases into collaborative filtering for recommendation. The core innovation is using Bayesian TransR for structural embedding (handling network heterogeneity), Bayesian SDAE for textual embedding, and Bayesian SCAE (convolutional auto-encoder) for visual embedding, then jointly learning these embeddings with CF through a unified optimization framework. The item latent vector combines CF-specific offset vectors with all three knowledge embeddings, enabling complementary information fusion through collaborative joint learning rather than separate training.",
  "introduction": "# Introduction\nRecommender systems are crucial for online services, with collaborative filtering (CF) being a successful core technique. However, CF suffers from poor performance due to sparse user-item interactions and inability to recommend new items (cold start). Hybrid systems integrating CF with auxiliary information have become popular to address these issues.\n\nKnowledge bases (e.g., YAGO, DBpedia, Satori) provide rich heterogeneous information—structured, unstructured, and multi-semantic data—making them valuable for enhancing recommendations. Previous works leveraging knowledge bases have limitations: they only use network structure (ignoring textual/visual information) and rely on tedious manual feature engineering.\n\nThis paper proposes **Collaborative Knowledge Base Embedding (CKE)**, a framework that integrates CF with knowledge base embedding to automatically extract item semantic representations from structural, textual, and visual knowledge. Key contributions:\n1. First work to leverage three types of knowledge (structural, textual, visual) from knowledge bases for recommendations.\n2. Uses heterogeneous network embedding and deep learning to automatically extract semantic representations, avoiding manual feature engineering.\n3. Jointly learns CF latent representations and knowledge base embeddings in a unified model, capturing both user-item implicit relations and item semantic features.\n4. Extensive experiments on two real datasets show CKE outperforms state-of-the-art baselines.",
  "method": "# Method\n## 1. Preliminary Definitions\n### 1.1 User Implicit Feedback\n- Implicit feedback matrix \\( R \\in \\mathbb{R}^{m×n} \\) (m users, n items), where \\( R_{ij}=1 \\) if user \\( i \\) interacted with item \\( j \\), 0 otherwise.\n- \\( R_{ij}=1 \\) does not guarantee user preference; \\( R_{ij}=0 \\) includes both negative feedback and unawareness.\n\n### 1.2 Knowledge Base Components\n- **Structural Knowledge**: Heterogeneous network with entities (items, attributes) and relations (e.g., \"movie-genre\", \"book-author\").\n- **Textual Knowledge**: Text summaries (e.g., movie plots, book descriptions) of items.\n- **Visual Knowledge**: Visual content (e.g., movie posters, book covers) of items.\n\n### 1.3 Problem Formulation\nGiven a knowledge base (structural + textual + visual knowledge) and user implicit feedback, generate a ranked list of items each user is interested in.\n\n## 2. CKE Framework Overview\nCKE has two core steps:\n1. **Knowledge Base Embedding**: Extract item semantic representations from three knowledge types.\n2. **Collaborative Joint Learning**: Integrate CF with knowledge base embeddings to learn user/item latent vectors.\n\n## 3. Knowledge Base Embedding\n### 3.1 Structural Embedding (Bayesian TransR)\n- **TransR**: Maps entities and relations to distinct spaces (entity space → relation-specific space via projection matrix \\( M_r \\)) to handle network heterogeneity.\n- For triple \\((v_h, r, v_t)\\) (head entity \\( v_h \\), relation \\( r \\), tail entity \\( v_t \\)):\n  - Project entities: \\( v_h^r = v_h M_r \\), \\( v_t^r = v_t M_r \\).\n  - Score function: \\( f_r(v_h, v_t) = \\|v_h^r + r - v_t^r\\|_2^2 \\).\n- **Bayesian Extension**: Adds Gaussian priors to parameters (entities, relations, projection matrices) for regularization, optimizing pairwise triple ranking probability.\n- Item structural representation: Embedding vector \\( v_j \\) of item entity \\( j \\) from Bayesian TransR.\n\n### 3.2 Textual Embedding (Bayesian Stacked Denoising Auto-Encoder, SDAE)\n- **SDAE**: Unsupervised deep learning model that learns latent representations by reconstructing clean text from corrupted input.\n- Architecture: Encoder (corrupted text → latent vector) + Decoder (latent vector → clean text).\n- Bayesian Extension: Adds Gaussian priors to weights/biases, with the middle layer output as item textual representation \\( X_{\\frac{L_t}{2}, j*} \\) ( \\( L_t \\) = number of layers).\n\n### 3.3 Visual Embedding (Bayesian Stacked Convolutional Auto-Encoder, SCAE)\n- **SCAE**: Replaces fully connected layers in SDAE with convolutional layers to preserve image spatial locality and reduce parameters.\n- Architecture: Encoder (corrupted image → convolutional feature maps → latent vector) + Decoder (latent vector → deconvolutional layers → clean image).\n- Bayesian Extension: Gaussian priors for weights/biases, with the middle layer output as item visual representation \\( Z_{\\frac{L_v}{2}, j*} \\) ( \\( L_v \\) = number of layers).\n\n## 4. Collaborative Joint Learning\n### 4.1 Item Latent Vector Integration\nItem latent vector combines knowledge base embeddings and a CF-specific offset vector:\n\\[ e_j = \\eta_j + v_j + X_{\\frac{L_t}{2}, j*} + Z_{\\frac{L_v}{2}, j*} \\]\n- \\( \\eta_j \\): Item offset vector (CF latent features), \\( v_j \\): structural representation, \\( X_{\\frac{L_t}{2}, j*} \\): textual representation, \\( Z_{\\frac{L_v}{2}, j*} \\): visual representation.\n\n### 4.2 Pair-Wise Preference Learning\n- Preference probability: For user \\( i \\), item \\( j \\) (interacted) is preferred over \\( j' \\) (non-interacted):\n\\[ p(j > j'; i|\\theta) = \\sigma(u_i^T e_j - u_i^T e_j') \\]\n- \\( u_i \\): User latent vector, \\( \\sigma \\): sigmoid function.\n\n### 4.3 Model Training\n- Objective: Maximize log-likelihood of pairwise preferences and knowledge base embedding reconstruction.\n- Optimization: Stochastic Gradient Descent (SGD) with random sampling of triples \\((i, j, j')\\) ( \\( R_{ij}=1 \\), \\( R_{ij'}=0 \\)) and knowledge base quadruples.\n\n### 4.4 Prediction\nRecommend items to user \\( i \\) by ranking items by \\( u_i^T e_j \\) in descending order.",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nTwo real datasets from different domains, with knowledge extracted from Satori knowledge base:\n| Dataset        | #Users   | #Items  | #Interactions | Structural Knowledge (Nodes/Edges) | Textual Items | Visual Items |\n|----------------|----------|---------|---------------|-----------------------------------|---------------|--------------|\n| MovieLens-1M   | 5,883    | 3,230   | 226,101       | 84,011 / 169,368                  | 2,752         | 2,958        |\n| IntentBooks    | 92,564   | 18,475  | 897,871       | 26,337 / 57,408                   | 17,331        | 16,719       |\n- MovieLens-1M: Positive ratings (5-star) as implicit feedback; items mapped to Satori entities (92% match precision).\n- IntentBooks: User book interests from Bing search logs (91.5% precision); items are Satori entities.\n\n### 1.2 Evaluation Protocol\n- **Split**: 70% of user interactions as training set, 30% as test set; 5 repetitions, average reported.\n- **Metrics**: MAP@K (Mean Average Precision) and Recall@K (K=20,40,60,80,100).\n- **Hyperparameters**: Tuned via validation set (Table 2), latent dimensions 100–150, regularization coefficients 0.001–0.1.\n\n### 1.3 Baselines\n#### 3.1 Structural Knowledge Baselines\n- BPRMF: CF-only (Bayesian Personalized Ranking Matrix Factorization).\n- BPRMF+TransE: BPRMF + TransE (network embedding ignoring heterogeneity).\n- PRP: PageRank with Priors (homogeneous graph-based).\n- PER: Personalized Entity Recommendation (meta-path features from heterogeneous networks).\n- LIBFM(S): Factorization machine with structural knowledge as raw features.\n\n#### 3.2 Textual Knowledge Baselines\n- LIBFM(T): LIBFM with textual bag-of-words as features.\n- CMF(T): Collective Matrix Factorization (user-item + item-word matrices).\n- CTR: Collaborative Topic Regression (CF + topic modeling).\n\n#### 3.3 Visual Knowledge Baselines\n- LIBFM(V): LIBFM with flattened image pixels as features.\n- CMF(V): Collective Matrix Factorization (user-item + item-pixel matrices).\n- BPRMF+SDAE(V): BPRMF + SDAE (no convolutional layers for visual embedding).\n\n#### 3.4 Full Framework Baselines\n- CKE(ST)/CKE(SV)/CKE(TV): CKE with 2 types of knowledge (structural+textual/structural+visual/textual+visual).\n- LIBFM(STV): LIBFM with all three knowledge types as raw features.\n- BPRMF+STV: BPRMF + separate knowledge base embeddings (no joint learning).\n\n## 2. Main Results\n### 2.1 Structural Knowledge Usage\n- CKE(S) (CKE with structural knowledge) outperforms all baselines:\n  - Beats BPRMF (CF-only) by 15–20% in Recall@K, proving structural knowledge’s value.\n  - Outperforms BPRMF+TransE, confirming TransR’s advantage in handling network heterogeneity.\n  - Surpasses PER/LIBFM(S), showing embedding-based representation is better than manual features/meta-paths.\n\n### 2.2 Textual Knowledge Usage\n- CKE(T) (CKE with textual knowledge) outperforms CTR (state-of-the-art textual CF):\n  - Deep learning (SDAE) extracts better textual semantics than topic modeling (CTR).\n  - CMF(T) performs poorly, as direct matrix factorization of item-word matrices underutilizes textual information.\n\n### 2.3 Visual Knowledge Usage\n- CKE(V) (CKE with visual knowledge) outperforms BPRMF+SDAE(V):\n  - Convolutional layers in SCAE better capture visual spatial features than fully connected SDAE.\n  - Visual knowledge provides limited but significant improvements (5–8% in Recall@K) compared to structural/textual knowledge.\n\n### 2.4 Full Framework Performance\n- CKE(STV) (all three knowledge types) outperforms CKE(ST)/CKE(SV)/CKE(TV):\n  - Each knowledge type contributes complementary information; visual knowledge adds marginal gains.\n  - Beats LIBFM(STV) by 10–15% in MAP@K, proving embedding-based representation is superior to raw features.\n  - Outperforms BPRMF+STV, verifying the value of joint learning (unified optimization for CF and knowledge embedding).",
  "hyperparameter": "Latent dimensions: 100-150 for user/item vectors and knowledge embeddings; Regularization coefficients: 0.001-0.1 for Bayesian priors; SDAE layers (L_t): middle layer output used as textual representation; SCAE layers (L_v): middle layer output used as visual representation; Training split: 70% training, 30% test; Evaluation K values: 20, 40, 60, 80, 100 for MAP@K and Recall@K metrics; Optimization: Stochastic Gradient Descent (SGD) with random sampling of user-item triples"
}