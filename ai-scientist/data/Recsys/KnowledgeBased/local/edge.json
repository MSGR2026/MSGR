[
  {
    "source": "RippleNet_2018",
    "target": "KGAT_2019",
    "type": "in-domain",
    "similarities": "1. Both models leverage knowledge graph embeddings to enrich entity representations—RippleNet’s entity_embedding and relation_embedding modules can be reused verbatim in KGAT, as both initialize entity/relation vectors of the same dimension and apply Xavier initialization.\n\n2. Both perform multi-hop neighbor aggregation over the KG—RippleNet’s _build_ripple_sets (building user-specific fixed-size neighbor tables) can be adapted for KGAT’s _build_ckg by simply extending the adjacency lists to include user nodes and sampling neighbors into fixed-size tensors (adj_entity / adj_relation) that are already implemented.\n\n3. Both use attention to weight neighbors—RippleNet’s _key_addressing that computes softmax( relation_emb ⊙ user_emb ) can be reused as a template for KGAT’s _compute_attention; only the query vector changes from user_emb to the projected entity vector, and the key-value pair becomes (W_r e_t , W_r e_h + e_r ) instead of (relation, tail).\n\n4. Both optimize with BPR loss plus L2 regularization—RippleNet’s calculate_loss (BPRLoss on pos/neg scores plus EmbLoss on sampled embeddings) can be copied for KGAT’s rec_loss term; the negative-sampling batch construction and reg_loss computation remain identical.",
    "differences": "1. KGAT requires a **new TransR embedding layer**—relation-specific projection matrices trans_w (shape n_relations × d×k) and the _get_kg_embedding method that projects entities into relation space via batched matrix multiply; this entire component is absent in RippleNet.\n\n2. KGAT needs **graph-convolution-style propagation layers**—a loop over L layers that maintains entity_emb_list and recursively aggregates neighbor representations with layer-specific weight matrices; RippleNet’s single forward loop must be replaced by the _propagate method that returns a list of layer-wise embeddings.\n\n3. KGAT introduces **three optional aggregators** (GCN, GraphSage, Bi-Interaction) with distinct linear layers and activation (LeakyReLU); RippleNet’s single transform_matrix must be replaced by a ModuleList of layer-specific aggregators and a switch in _apply_aggregator.\n\n4. KGAT concatenates **all-layer representations** before prediction—torch.cat(entity_emb_list, dim=1) in forward; RippleNet outputs only the final-hop vector, so the final score computation needs to be rewritten to accept the stacked (L+1)*embedding_size vector and perform an inner product with the equivalently stacked item vector.\n\n5. KGAT jointly trains **KG embedding loss** (TransR pairwise ranking on triplets) alongside BPR—an additional calculate_kg_loss method and alternative optimization step are required; RippleNet’s training loop contains no knowledge-graph-specific loss term.",
    "rank": "rank1"
  },
  {
    "source": "CFKG_2018",
    "target": "KGAT_2019",
    "type": "in-domain",
    "similarities": "1. Both models unify user-item interactions and knowledge triplets into a single heterogeneous graph where users are treated as special entities; CFKG's `user_embedding` and `entity_embedding` tables can be merged into KGAT's single `entity_embedding` table by mapping user IDs to entity space (user_id + n_items).\n2. Both adopt translation-based relation modeling: CFKG uses TransE-style e_h + r ≈ e_t with inner-product score, while KGAT uses TransR-style W_r·e_h + e_r ≈ W_r·e_t with L2 score; CFKG's `_get_score()` method can be upgraded to KGAT's `_get_kg_embedding()` by adding relation-specific projection matrices.\n3. Both employ negative-sampling BPR loss for optimization; CFKG's `InnerProductLoss` (softplus on positive/negative scores) can be directly reused as KGAT's `rec_loss` and `kg_loss` modules.\n4. Both share the same knowledge-graph data loader interface (`HEAD_ENTITY_ID`, `RELATION_ID`, `TAIL_ENTITY_ID`) and pairwise training loop; CFKG's `calculate_loss()` skeleton (separate RS & KG batches) can be kept, only adding the extra propagation step before score computation.",
    "differences": "1. KGAT requires multi-hop attentive propagation: implement `_propagate()` that performs L-layer neighbor sampling (`adj_entity`, `adj_relation`), attention weight computation (`_compute_attention()` with TransR projections), and neighbor aggregation; CFKG has no propagation component.\n2. KGAT needs TransR relation-specific projection matrices `trans_w` (size n_relations × embedding_size × kg_embedding_size) to project entities into relation space before translation; CFKG only stores a single vector per relation.\n3. KGAT concatenates representations from all propagation layers (`layer-aggregation`) to form final user/item vectors, requiring `_propagate()` to return a list of embeddings per layer; CFKG uses only the 0-th layer embedding.\n4. KGAT introduces three optional aggregators (GCN, GraphSage, Bi-Interaction) implemented in `_apply_aggregator()` with distinct weight matrices per layer; CFKG has no aggregation function—its score is simply (e_h + e_r) ⊙ e_t.",
    "rank": "rank3"
  },
  {
    "source": "MCCLK_2021",
    "target": "KGAT_2019",
    "type": "in-domain",
    "similarities": "1. **TransR-based Knowledge Graph Embedding**: Both papers use TransR (relation-specific projection matrices) to embed entities and relations. MCCLK's `relation_embedding` and projection logic in `_get_kg_embedding()` can be reused for KGAT's TransR implementation, specifically the `trans_w` embedding layer and entity projection operations.\n\n2. **Graph-based Propagation Architecture**: Both employ multi-hop neighbor aggregation. MCCLK's `GraphConv` class with `Aggregator` modules for entity propagation can be adapted for KGAT's attentive embedding propagation layers, particularly the neighbor sampling and aggregation logic in `_propagate()`.\n\n3. **BPR Loss for Recommendation**: Both use Bayesian Personalized Ranking loss for implicit feedback. MCCLK's `BPRLoss` implementation in `calculate_loss()` can be directly reused for KGAT's `calculate_rs_loss()`.\n\n4. **Collaborative Knowledge Graph Construction**: Both integrate user-item interactions into KG. MCCLK's `get_norm_inter_matrix()` for building user-item-entity graphs can guide KGAT's `_build_ckg()` implementation, especially the adjacency matrix construction logic.\n\n5. **Multi-layer Embedding Propagation**: Both stack multiple propagation layers. MCCLK's layer-wise propagation in `GraphConv.forward()` with residual connections (`entity_res_emb`) can inform KGAT's layer aggregation mechanism in `_propagate()`.",
    "differences": "1. **Knowledge-aware Attention Mechanism**: KGAT requires implementing relation-specific attention scores using `tanh` activation (Equation 4-5), which is absent in MCCLK. New implementation needed: `_compute_attention()` method with relation projection and softmax normalization over neighbors.\n\n2. **TransR Optimization Strategy**: KGAT uses pairwise ranking loss for KG embedding (Equation 2), while MCCLK doesn't optimize TransR separately. New component: `calculate_kg_loss()` with negative sampling for triplets and score computation via projected entity distances.\n\n3. **Layer-wise Representation Concatenation**: KGAT concatenates embeddings from all propagation layers (Equation 11), while MCCLK uses averaging. New implementation: Modify `_propagate()` to return a list of layer embeddings and concatenate them in `forward()`.\n\n4. **Aggregator Function Variants**: KGAT supports three aggregators (GCN, GraphSage, Bi-Interaction) with different transformation matrices per layer (Equation 6-8), while MCCLK uses simple mean aggregation. New modules: `aggregator_layers` with layer-specific weight matrices and aggregator type selection in `_apply_aggregator()`.\n\n5. **Alternative Training Procedure**: KGAT alternates between KG embedding and recommendation losses (Section 3.4.1), requiring dual optimization loops. New implementation: Separate `calculate_kg_loss()` and `calculate_rs_loss()` with alternating parameter updates in training loop.",
    "rank": "rank5"
  },
  {
    "source": "RippleNet_2018",
    "target": "KGNNLS_2019",
    "type": "in-domain",
    "similarities": "1. Both models leverage knowledge graph structure by learning entity and relation embeddings using nn.Embedding layers with identical initialization (xavier_normal_initialization), enabling direct reuse of embedding initialization code from RippleNet\n2. Both employ multi-hop neighborhood aggregation over KG - RippleNet's _build_ripple_sets() method for K-hop expansion can be adapted to construct adjacency matrices for KGNNLS's GNN layers\n3. Both use inner product for scoring user-item interactions (RippleNet: torch.sum(user_repr * item_emb, dim=1), KGNNLS: torch.sum(user_emb * item_embs, dim=1)), allowing reuse of final prediction computation\n4. Both implement knowledge graph preprocessing through dataset.kg_graph() and build adjacency structures (RippleNet's kg_dict vs KGNNLS's kg_dict), enabling reuse of KG parsing and neighbor lookup logic\n5. Both incorporate L2 regularization on embeddings using EmbLoss(), with RippleNet's reg_loss computation serving as template for KGNNLS's regularization term",
    "differences": "1. KGNNLS requires implementing user-specific relation scoring function s_u(r) = g(u,r) using user_embedding and relation_embedding, which RippleNet lacks - needs new _compute_user_relation_scores() method with sigmoid normalization\n2. KGNNLS needs GNN layer implementation with normalized adjacency matrices (D^{-1/2} A D^{-1/2}) - requires new _normalize_adjacency_matrix() using scipy.sparse and torch.sparse.mm for propagation, absent in RippleNet's attention-based aggregation\n3. KGNNLS requires label smoothness regularization via leave-one-out label propagation - needs new _calculate_ls_loss() and _label_propagation() methods implementing iterative D^{-1} A label propagation with cross-entropy loss, completely missing from RippleNet\n4. KGNNLS processes each user separately with personalized adjacency matrices in forward pass, requiring batch-wise processing loop - RippleNet processes all users jointly through batched ripple sets, necessitating architectural change in forward() implementation\n5. KGNNLS uses BCEWithLogitsLoss for pointwise classification while RippleNet uses BPRLoss for pairwise ranking - requires changing loss computation in calculate_loss() and input type from PAIRWISE to POINTWISE",
    "rank": "rank1"
  },
  {
    "source": "MCCLK_2021",
    "target": "KGNNLS_2019",
    "type": "in-domain",
    "similarities": "1. **KG-based entity aggregation**: Both papers use graph convolution to propagate entity embeddings via relation edges; MCCLK's `Aggregator` class and `GraphConv` module can be reused to implement KGNNLS's Eq.(1) propagation by replacing the attention-weighted `scatter_mean` with user-specific relation scores `s_u(r)` as edge weights.\n2. **User-specific graph construction**: MCCLK already builds a user-item–entity sparse graph (`inter_matrix` + `kg_graph`); the same COO format and `torch.sparse.FloatTensor` creation in `build_adj()` can be reused to construct KGNNLS's per-user weighted adjacency `A_u`.\n3. **Embedding lookup & MLP transform**: MCCLK's `entity_embedding` and `relation_embedding` plus the `gnn_layers` (linear layers) map directly to KGNNLS's `W_l` matrices—only the forward loop over `n_layers` needs to be kept, removing the multi-view stacking.\n4. **BPR/pointwise loss infrastructure**: MCCLK’s `BPRLoss` and `EmbLoss` utilities are already imported; KGNNLS’s binary cross-entropy loss can plug into the same `calculate_loss` skeleton by swapping `BPRLoss` for `BCEWithLogitsLoss`.\n5. **Sparse dropout & normalization**: `SparseDropout`, `xavier_normal_initialization`, and the `D^{-1/2} A D^{-1/2}` Laplacian normalization routine in `build_adj()` can be copied verbatim to normalize KGNNLS’s `A_u + I`.",
    "differences": "1. **Per-user weighted adjacency matrix**: KGNNLS requires a distinct sparse adjacency `A_u` per user computed on-the-fly via `s_u(r)=sigmoid(u·r)`; MCCLK uses a single shared KG adjacency. New `_build_user_adjacency_matrix()` must generate a list of `batch_size` COO matrices with `relation_scores` as values.\n2. **Label-smoothing regularization term**: KGNNLS adds a leave-one-out label-propagation loss `R(A)` (Eq.7) that is absent in MCCLK. Implement `_calculate_ls_loss()` which (i) performs 3-step power iteration `P=D^{-1}A`, (ii) resets known item labels, and (iii) returns cross-entropy between predicted and true labels.\n3. **Relation-specific attention replaced by user-relation score**: MCCLK computes attention via `calculate_sim_hrt` using entity-relation interaction; KGNNLS uses only `u·r` inner product. Remove the attention branch in `Aggregator.forward` and feed the pre-computed `relation_scores` as edge weights into `scatter_mean`.\n4. **Pointwise training loop vs pairwise BPR**: MCCLK samples triplets `(u,i,j)` and uses `BPRLoss`; KGNNLS needs pointwise labels `(u,i,y_ui)`. Replace the `PAIRWISE` data loader with `POINTWISE`, change `calculate_loss` input dict to contain `LABEL_FIELD`, and compute `BCEWithLogitsLoss` on raw scores.\n5. **No multi-view contrastive learning**: MCCLK’s `local_level_loss` and `global_level_loss_*` contrastive terms and the separate LightGCN branch are unused in KGNNLS; strip out `light_gcn()`, `fc1/2/3` projectors, and all `temperature`-based NT-Xent losses.",
    "rank": "rank2"
  },
  {
    "source": "RippleNet_2018",
    "target": "KGCN_2019",
    "type": "in-domain",
    "similarities": "1. Both models use entity and relation embeddings from the KG—RippleNet’s `entity_embedding` and `relation_embedding` tables can be reused verbatim; only an additional `user_embedding` table needs to be added for KGCN.\n2. Attention-style neighbor weighting is computed with the same inner-product scorer: reuse RippleNet’s `_key_addressing` logic (user_emb • relation_emb → softmax) inside KGCN’s `_compute_user_relation_score`; the only change is removing the Gumbel option.\n3. Fixed-size neighbor sampling to guarantee regular mini-batch shapes—RippleNet’s `_build_ripple_sets` already pads/truncates to `n_memory`; identical sampling strategy can be copied into KGCN’s `_build_kg_adjacency` with `neighbor_sample_size = n_memory`.\n4. BPR-style pairwise loss (RippleNet) vs. point-wise BCE (KGCN): the existing `BPRLoss` and `EmbLoss` classes from RippleNet can be kept; simply switch the loss call in `calculate_loss` and feed 0/1 labels instead of pairs.\n5. Xavier initialization and L2 regularization on embeddings—reuse the same initialization call and `reg_loss` computation already present in RippleNet.",
    "differences": "1. KGCN needs an explicit `user_embedding` matrix (RippleNet has none); create `nn.Embedding(n_users, embedding_size)` and add it to the optimizer.\n2. KGCN builds a static adjacency sampler (`adj_entity`, `adj_relation`) once at preprocessing, whereas RippleNet dynamically constructs user-specific ripple sets every batch; implement `_build_kg_adjacency` and store two fixed `[n_entities, K]` tensors.\n3. KGCN propagates entity features layer-by-layer over the whole KG receptive field (Algorithm 1 loops h=H…0); write a new `_propagate` method that iteratively calls `_aggregate_neighbors` for each hop and handles the exponentially growing tensor views.\n4. Three aggregator architectures (sum/concat/neighbor) with different `nn.Linear` shapes must be implemented; add a `ModuleList` of aggregator layers and branch inside `_aggregate_neighbors` according to `aggregator_type`.\n5. KGCN uses point-wise sigmoid output for explicit 0/1 labels; replace RippleNet’s ranking score with `torch.sigmoid(torch.sum(user_emb * final_item_emb, dim=1))` and adapt `predict`/`full_sort_predict` accordingly.",
    "rank": "rank1"
  },
  {
    "source": "MCCLK_2021",
    "target": "KGCN_2019",
    "type": "in-domain",
    "similarities": "1. Both models use knowledge-graph neighbor sampling and user-specific attention: MCCLK's `Aggregator.calculate_sim_hrt()` computes relation-aware attention scores via entity-relation interaction norms, which can be reused to implement KGCN's π_r^u = g(u,r) by replacing the norm-based score with a simple inner-product between user and relation embeddings.\n2. Both stack multi-hop entity representations: MCCLK keeps `entity_res_emb = [entity_emb]` per hop and finally `mean(dim=1)`; KGCN can reuse this pattern—store `entity_representations[h]` for each layer h and return the 0-order slice after H iterations.\n3. Both adopt parameter-sharing relation embeddings: MCCLK's `self.relation_embedding = nn.Embedding(n_relations, embedding_size)` and its retrieval `relation_emb[edge_type]` can be directly copied for KGCN's relation lookup in `_compute_user_relation_score`.\n4. Both apply dropout & normalization on aggregated embeddings: MCCLK's `mess_dropout` + `F.normalize` after each hop can be inserted into KGCN's `_aggregate_neighbors` right after the aggregator layer to stabilize training.",
    "differences": "1. KGCN needs a fixed-size neighbor sampler per entity: implement `_build_kg_adjacency` that constructs two dense matrices `adj_entity` (K neighbors) and `adj_relation` (their relations) with padding/self-loop, plus random re-sampling every epoch—MCCLK has no such fixed-K sampling.\n2. KGCN propagates representations inside a mini-batch receptive field: write `_get_receptive_field` that expands `[batch_size]` → `[batch_size*K^H]` indices across H hops; MCCLK processes the full graph via `torch_scatter.scatter_mean` and does not build layer-wise receptive lists.\n3. KGCN requires three distinct aggregator architectures (sum/concat/neighbor) selectable by config: add `if self.aggregator_type == 'concat'` branches in `_aggregate_neighbors` with differently sized `nn.Linear` layers; MCCLK only uses a single mean-plus-attention aggregator.\n4. KGCN uses pointwise BCE loss with negative sampling while MCCLK uses pairwise BPR: replace MCCLK's `BPRLoss` with `nn.BCEWithLogitsLoss`, feed `label` field, and sample T^u negatives per user via `torch.randint`—no negative sampling routine exists in MCCLK.",
    "rank": "rank2"
  },
  {
    "source": "MCCLK_2021",
    "target": "CFKG_2018",
    "type": "in-domain",
    "similarities": "1. Both models embed users, items, and KG entities into the same latent space via `nn.Embedding` layers; MCCLK’s `user_embedding`, `entity_embedding`, and `relation_embedding` tables can be directly reused for CFKG’s `user_embedding`, `entity_embedding`, and `relation_embedding` (last index reserved for the “purchase” relation).\n2. Both adopt pairwise negative-sampling training; MCCLK’s BPR loss (`BPRLoss`) or BCE loss (`BCEWithLogitsLoss`) can be swapped for CFKG’s `InnerProductLoss` by simply replacing the score computation in `calculate_loss` with CFKG’s inner-product score and keeping the existing negative-item sampling pipeline.\n3. Both construct a unified graph that merges user-item interactions and KG triples; MCCLK’s `get_edges` and `get_norm_inter_matrix` utilities that convert `dataset.inter_matrix` and `dataset.kg_graph` into sparse COO tensors can be reused to feed CFKG’s sampler without change.\n4. Both use a translation-style score: MCCLK already computes `user_gcn_emb[user] + relation_emb[purchase_idx]` inside the score term; this tensor operation can be extracted and used as CFKG’s `h_e + r_e` for the inner-product score, so the core score tensor code is already present.",
    "differences": "1. CFKG abandons any graph convolution or neighbor aggregation; the entire `GraphConv`, `Aggregator`, `light_gcn`, and contrastive-learning modules in MCCLK must be removed and replaced by a simple embedding lookup plus score computation.\n2. CFKG introduces a separate relation vector for “purchase” (last row of `relation_embedding`) that is NOT updated by neighbor messages; a new constant index (`self.n_relations`) must be reserved and ensured to be excluded from any KG propagation logic.\n3. CFKG’s loss is not BPR/BCE but a soft-plus margin loss on inner products (`F.softplus(-pos_score) + F.softplus(neg_score)`); the existing `rec_loss` module in MCCLK must be replaced by the provided `InnerProductLoss` class.\n4. CFKG samples one KG triple and one user-item pair per batch, concatenates them, and computes a single translation score; MCCLK’s current batch iterator that separates CF and KG losses must be rewritten to return a joint batch dict containing `{USER_ID, ITEM_ID, NEG_ITEM_ID, HEAD_ENTITY_ID, RELATION_ID, TAIL_ENTITY_ID, NEG_TAIL_ENTITY_ID}` and feed them into one unified loss call.",
    "rank": "rank2"
  },
  {
    "source": "KGNNLS_2019",
    "target": "KGIN_2021",
    "type": "in-domain",
    "similarities": "1. Both models use KG-aware entity embeddings initialized via nn.Embedding and aggregate multi-hop neighbors through GNN layers; KGNNLS’s _gnn_forward() with sparse.mm() and layer-wise Linear() can be reused for KGIN’s entity propagation after removing the user-specific adjacency logic.\n2. Both adopt relation embeddings (relation_embedding.weight) and leverage them during propagation; KGNNLS’s relation scoring (sigmoid(inner-product)) can be adapted to compute KGIN’s intent-relation attention weights α(r,p) by replacing the user vector with intent vector and keeping the inner-product scorer.\n3. Both stack L layers and sum/add residual representations across hops; KGNNLS’s residual loop over self.gnn_layers and final torch.add(entity_res_emb, entity_emb) can be directly copied for KGIN’s layer-wise aggregation.\n4. Both use inner-product user-item predictor (torch.sum(user*item, dim=1)) and BPR-style pairwise loss; KGNNLS’s rec_loss_fn (BCEWithLogitsLoss) can be swapped for KGIN’s BPRLoss with minimal code change.",
    "differences": "1. KGIN introduces latent intents (n_factors intent embeddings) and an intent-relation attention matrix (disen_weight_att) that must be newly created and trained; KGNNLS has no intent concept—this entire IntentEmbedding + attention submodule is missing and needs fresh implementation.\n2. KGIN performs two parallel aggregations—Intent Graph (user←(p,i) with element-wise product β(u,p)e_p⊙e_i) and KG entity aggregation (e_r⊙e_v)—whereas KGNNLS only has single entity propagation; new Aggregator() class with separate IG/KG forward paths must be written.\n3. KGIN adds an independence regularizer (MutualInformation / DistanceCorrelation) over intent embeddings; KGNNLS’s LS regularizer (_calculate_ls_loss with label propagation) is completely different and must be replaced by cor_loss computation in GraphConv.\n4. KGIN constructs a joint user-item-entity graph (get_norm_inter_matrix) and samples negative items per user for BPR, while KGNNLS keeps user and KG graphs separate and uses pointwise labels; the interaction-matrix building, negative sampling, and pairwise batch generation logic are new components to implement.",
    "rank": "rank1"
  },
  {
    "source": "KGAT_2019",
    "target": "KGIN_2021",
    "type": "in-domain",
    "similarities": "1. Both models build a Collaborative Knowledge Graph (CKG) by merging user-item interactions with KG triplets—KGAT’s `_build_ckg()` that maps users to entity space (line 92 `user_entity_id = self.n_items + user`) and creates bidirectional edges can be reused as-is for KGIN’s intent graph construction.\n2. Both use multi-layer embedding propagation with residual/sum aggregation—KGAT’s `_propagate()` loop (lines 316-354) that maintains a list of layer-wise embeddings (`entity_emb_list`) and the final concatenation (`torch.cat(user_emb_list, dim=1)`) can be refactored to return the sum instead of concat to match KGIN’s Eq. 13.\n3. Both adopt BPR pairwise loss and `EmbLoss` L2 regularizer—KGAT’s `calculate_loss()` (lines 434-474) that jointly optimizes `rec_loss`, `kg_loss`, and `reg_loss` can be copied; only the additional independence regularizer `cor_loss` needs to be appended.\n4. Both represent relations as learnable embeddings (`relation_embedding` in KGAT line 68) and use them to weight neighbor messages—KGAT’s attention computation `_compute_attention()` (lines 244-268) can be simplified to element-wise product (Eq. 10) by removing the tanh and softmax to fit KGIN’s relational message `e_r ⊙ e_v`.\n5. Both sample fixed-size neighbors via padding/random sampling—KGAT’s neighbor sampling logic in `_build_ckg()` (lines 130-140) with `neighbor_sample_size=8` can be directly reused for KGIN’s KG and intent-graph adjacency lists.",
    "differences": "1. KGIN introduces latent user intents (`latent_embedding`, `n_factors`) modeled as an attention combination of KG relations (`disen_weight_att` Eq. 1-2)—this requires implementing intent embedding table `self.latent_embedding` and attention parameters `disen_weight_att` not present in KGAT.\n2. KGIN splits propagation into two distinct graphs: Intent Graph (IG) for user-item-intent triplets and KG for item-entity-relation triplets—KGAT’s single `_propagate()` must be duplicated into two separate aggregation functions (`f_IG` and `f_KG`) with different message types (`β(u,p)e_p ⊙ e_i` vs `e_r ⊙ e_v`).\n3. KGIN enforces intent independence via a configurable regularizer (`calculate_cor_loss` with mutual-information, distance-correlation, or cosine)—this entire module (`GraphConv.calculate_cor_loss`, lines 120-180) is new and must be added to the loss term with weight `sim_decay`.\n4. KGIN uses sum-pooling of layer-wise embeddings (Eq. 13) instead of KGAT’s concatenation—change KGAT’s final `torch.cat(..., dim=1)` to a simple sum across the layer list and remove the dimension expansion in prediction layers.\n5. KGIN adopts sparse adjacency matrices (`interact_mat`, `edge_index`, `edge_type`) and `scatter_mean` for efficient neighbor aggregation, whereas KGAT uses dense neighbor lookup tables—replace KGAT’s dense `adj_entity`/`adj_relation` tensors with `torch.sparse.FloatTensor` and `torch_scatter.scatter_mean` operations in the aggregator.",
    "rank": "rank2"
  },
  {
    "source": "MCCLK_2021",
    "target": "KGIN_2021",
    "type": "in-domain",
    "similarities": "1. Both models adopt a two-stage GNN architecture: a GraphConv module that performs relation-aware neighborhood aggregation via scatter_mean on (entity[tail]*relation_emb[edge_type]), and a hop-wise residual stacking (entity_res_emb = torch.add(entity_res_emb, entity_emb))—the MCCLK implementation in gcn.convs[i](…) can be reused almost verbatim for KGIN’s Aggregator.forward().\n2. KG propagation mechanics are identical: head/tail indexing from the same edge_index tensor, relation embedding lookup by edge_type, and L-hop message passing with dropout & L2-normalize; MCCLK’s edge_sampling, SparseDropout, and mess_dropout pipelines can be copied directly into KGIN’s GraphConv.\n3. Interaction-matrix preprocessing is shared: get_norm_inter_matrix(mode='si') that builds a sparse D^{-1}A user→item matrix, plus get_edges() that converts a scipy COO kg_graph into torch.LongTensor edge_index/edge_type—both routines are already implemented in MCCLK and can be imported without change.\n4. BPR pairwise loss and reg_loss (EmbLoss) are used in both; the training loop structure (calculate_loss → mf_loss + reg_weight*reg_loss + aux_loss) is the same, so MCCLK’s BPRLoss instantiation and negative-item sampling logic can be retained.",
    "differences": "1. KGIN introduces latent user intents: an additional n_factors embedding table latent_emb and an attention matrix disen_weight_att ∈ ℝ^{n_factors × n_relations} that computes intent representations e_p = softmax(w_{r,p})·e_r; this entire intent-generation module is absent in MCCLK and must be newly implemented.\n2. Independence regularization for intents (L_IND) is required: KGIN offers three variants—MutualInformation, DistanceCorrelation, or CosineSimilarity—computed over disen_weight_att; MCCLK only has contrastive losses, so a new calculate_cor_loss() function and its integration into the total loss need to be coded.\n3. Aggregation formula differs: KGIN uses element-wise product e_r ⊙ e_v for KG messages and β(u,p)*e_p ⊙ e_i for intent-graph messages, whereas MCCLK uses pure sum aggregation; the Aggregator.forward() must be rewritten to include these products and the personalized attention β(u,p)=softmax(e_p^T e_u).\n4. User-embedding enrichment path: KGIN fuses the intent-attentive signal via user_agg = (score @ disen_weight)*user_agg + user_agg, where score=softmax(user_emb @ latent_emb^T); MCCLK simply does sparse-mm(interact_mat, entity_emb) plus an optional relation-score residual—this extra weighted fusion step has to be added.",
    "rank": "rank3"
  },
  {
    "source": "RippleNet_2018",
    "target": "MKR_2019",
    "type": "in-domain",
    "similarities": "1. **Entity & Relation Embedding Layer Reuse**: Both models use `nn.Embedding` for entity and relation representations with identical initialization (`xavier_normal_initialization`). The `entity_embedding` and `relation_embedding` modules from RippleNet can be directly reused in MKR for KG-related components, as both map discrete IDs to dense vectors of size `embedding_size`.\n\n2. **Knowledge Graph Sampling & Batching**: The `dataset.kg_graph(form='coo')` and negative sampling logic in RippleNet's `_build_ripple_sets` can be adapted for MKR's KGE module. The COO-format KG parsing and minibatch sampling of triples `(h, r, t)` are structurally identical, requiring only minor changes to feed MKR’s cross&compress units instead of RippleNet’s attention aggregation.\n\n3. **Loss Function Structure**: Both employ pairwise ranking losses (BPR in RippleNet, margin-based in MKR) plus L2 regularization (`EmbLoss`). The `reg_loss` computation in RippleNet’s `calculate_loss` can be refactored to MKR’s multi-task loss, regularizing shared embeddings (users/items/entities/relations) with the same `reg_weight` hyperparameter.\n\n4. **User/Item ID Embedding Lookup**: RippleNet’s `entity_embedding(items)` for item vectors mirrors MKR’s `item_embedding(item)` when items are treated as entities. The embedding lookup pattern (`embedding_layer(LongTensor(ids))`) is identical, enabling reuse of indexing and tensor shaping code.",
    "differences": "1. **Cross&Compress Unit – NEW Module**: MKR requires implementing the `CrossCompressUnit` class (Eq. 1-3) that performs outer-product `v·e^T` and compression with four weight vectors (`w_vv`, `w_ev`, `w_ve`, `w_ee`). This bilinear interaction layer is absent in RippleNet; it must be coded from scratch, including efficient batch-wise matrix operations without explicit `d×d` materialization.\n\n2. **Multi-Task Training Loop**: Unlike RippleNet’s single `calculate_loss`, MKR alternates between RS and KGE tasks (Algorithm 1). A new training script must sample two minibatches per iteration—user-item pairs with labels `(u,i,y)` and KG triples with negative sampling `(h,r,t,t')`—and update shared parameters via two backward passes with different loss weights (`kg_weight`, `reg_weight`).\n\n3. **Item–Entity Association Handling**: RippleNet pre-computes ripple sets per user; MKR dynamically samples associated entities `e~S(v)` for each item during training. Implement a `sample_associated_entities(item_ids)` utility that maps items to their KG neighbors on-the-fly, storing `item_to_entities` dict built from `dataset.kg_graph`—a data structure not needed in RippleNet.\n\n4. **Separate Pathway Architectures**: MKR splits user features through an MLP (`user_mlp`) and item/entity features through cross&compress units, later concatenated for prediction. RippleNet’s single `transform_matrix` after attention aggregation must be replaced by two distinct networks: `predict_mlp` (user+item→score) and `tail_mlp` (head+relation→tail), each with configurable depth (`H`, `K`).",
    "rank": "rank1"
  },
  {
    "source": "MCCLK_2021",
    "target": "MKR_2019",
    "type": "in-domain",
    "similarities": "1. Both models jointly train recommendation (RS) and knowledge-graph embedding (KGE) tasks end-to-end; reuse MCCLK’s multi-task training loop (Algorithm 1 lines 2-11 pattern) and its BPR/BCE loss handlers (recbole.model.loss) with minimal changes—just swap the negative-sampling sampler to MKR’s triple-wise one.\n\n2. Entity/relation lookup and ID→vector tables are identical: MCCLK’s entity_embedding & relation_embedding nn.Embedding objects can be copied; only rename item_embedding→entity_embedding for items that are also KG entities.\n\n3. Message-passing over the KG subgraph is conceptually similar: MCCLK’s GraphConv + Aggregator (attention-based neighbour pooling) can be stripped down to a 1-hop version and used inside MKR’s KGE module to produce head & tail representations instead of the current MLP-only design.\n\n4. Regularisation & dropout plumbing (EmbLoss, SparseDropout, mess_dropout) is already implemented; keep the same reg_weight schedule and L2 term for fair comparison.\n\n5. Dataset iterators that yield (user,item,label) and (h,r,t) tuples are already available in RecBole; add a second NegKG_Sampler that draws corrupted tails exactly like MCCLK’s negative sampling in scatter_mean edges.",
    "differences": "1. MKR’s core Cross-&-Compress unit (Eq. 2) is **not** in MCCLK—must implement CrossCompressUnit(nn.Module) that performs outer-product v·e^T followed by four compressed projections (w_VV, w_EV, w_VE, w_EE) and outputs dual vectors; this is a brand-new layer with 4d parameters per layer.\n\n2. MKR uses **separate** L-layer deep towers for users (pure MLP) and items (cross-&-compress) before late fusion, whereas MCCLK shares one LightGCN-style graph convolution for both; create user_mlp (L Linear+Tanh) and keep item path inside cc_units only.\n\n3. Prediction head differs: MKR optionally applies an **H-layer MLP on concatenated [u_L; v_L]** or inner-product, controlled by use_inner_product flag; MCCLK simply does u^T·e_j. Add predict_mlp ModuleList and a branch in forward_rs.\n\n4. KGE scoring in MKR is **semantic matching** (normalised inner product between predicted tail vector and true tail embedding), not translational; implement forward_kg that (a) averages multiple associated items for head entity (Eq. 7 expectation), (b) runs relation MLP, (c) K-layer tail_mLP, (d) normalises both predictions and labels before dot-product.\n\n5. Training schedule is **alternating mini-batches** (t RS steps ↔ 1 KG step) with different loss terms (BCE for RS, margin-style for KG); extend MCCLK’s single calculate_loss to a switch that computes L_RS or L_KG depending on the presence of LABEL or HEAD_ENTITY_ID in the current batch, and enforce the t-step loop in the training script.",
    "rank": "rank3"
  },
  {
    "source": "MCCLK_2021",
    "target": "RippleNet_2018",
    "type": "in-domain",
    "similarities": "1. Both models use entity and relation embeddings from the same embedding layers (nn.Embedding) that can be directly reused - MCCLK's entity_embedding and relation_embedding modules are identical to RippleNet's requirements, allowing direct code reuse for embedding initialization and lookup operations.\n\n2. Both employ attention-based aggregation mechanisms for neighborhood information - MCCLK's Aggregator.calculate_sim_hrt() method computes attention weights using entity-relation interactions (entity_emb[head] * relation_emb), which can be adapted for RippleNet's _key_addressing() function that uses user-relation interactions (relation_emb * user_emb_expanded).\n\n3. Both implement multi-hop propagation over knowledge graphs - MCCLK's GraphConv with n_hops parameter and iterative convolutions can guide RippleNet's K-hop ripple set propagation implementation, particularly the loop structure and residual connection patterns.\n\n4. Both use similar loss function structures with BPR loss and embedding regularization - MCCLK's BPRLoss() and EmbLoss() implementations can be directly reused, including the regularization weight parameter (reg_weight) and the pattern of combining recommendation loss with regularization loss.\n\n5. Both models handle knowledge graph data through edge_index and edge_type tensors - MCCLK's get_edges() method and edge sampling techniques can be reused for RippleNet's ripple set construction, particularly for efficient KG traversal and neighbor sampling.",
    "differences": "1. RippleNet requires building user-specific ripple sets through K-hop expansion from historical items - This is completely absent in MCCLK and needs new implementation of _build_ripple_sets() method that performs breadth-first search over KG for each user, stores padded ripple sets per hop, and maintains user-specific data structures.\n\n2. RippleNet uses memory network-style propagation where user representations evolve through hops - Unlike MCCLK's fixed entity aggregation, RippleNet needs iterative user representation updates via transform_matrix() that applies linear transformation to (user_repr + aggregated_emb) at each hop, requiring implementation of the full forward() propagation loop.\n\n3. RippleNet requires batch-wise ripple set retrieval for training - MCCLK processes all entities globally, but RippleNet needs _get_ripple_set_batch() to dynamically fetch user-specific (heads, relations, tails) tensors for each training batch, including proper tensor stacking and device management.\n\n4. RippleNet's attention mechanism operates on user-relation interactions rather than entity-relation - While MCCLK computes attention between head entity and relation (entity_emb[head] * relation_emb), RippleNet needs user-relation attention (relation_emb * user_emb_expanded), requiring modification of the attention computation logic in _key_addressing().\n\n5. RippleNet requires special handling for full sort prediction - Unlike MCCLK's matrix multiplication approach, RippleNet's full_sort_predict() needs to compute scores user-by-user and item-by-item due to the user-specific nature of ripple sets, requiring efficient batching strategies to avoid memory issues with large item sets.",
    "rank": "rank1"
  },
  {
    "source": "MCCLK_2021",
    "target": "KTUP_2020",
    "type": "in-domain",
    "similarities": "1. Both models adopt joint user/item/entity/relation embeddings initialized with Xavier and L2-normalized; MCCLK’s Embedding + normalization blocks (lines 264-281) can be reused directly for KTUP’s six embedding tables (user, item, pref, pref_norm, entity, relation, relation_norm).\n2. Both exploit a pairwise ranking loss (BPR) for recommendation; MCCLK’s BPRLoss (line 288) and the sampling loop in calculate_loss() (lines 492-495) can be copied to KTUP’s rec_loss with minimal renaming.\n3. Both inject KG structural knowledge via relation-specific transformations: MCCLK’s Aggregator performs relation-weighted neighbor aggregation (lines 40-48) and can be adapted to produce the entity embeddings that KTUP later fuses with items.\n4. Both use multi-task regularization: MCCLK’s EmbLoss + contrastive terms (lines 501-505) map to KTUP’s EmbMarginLoss + orthogonalLoss; the reg_loss helper (lines 18-30) is already implemented in KTUP’s repo and can be kept.",
    "differences": "1. KTUP requires TransH-style projection – a hyperplane norm vector per relation/preference and the _transH_projection() routine (lines 147-152) – which is completely absent in MCCLK; this operator must be newly coded and integrated into the forward pass.\n2. KTUP explicitly aligns item and entity vectors via alignLoss (lines 232-234, 259-261) and the additive fusion î = i + e; MCCLK never fuses item & entity vectors additively, so an alignment loss module and the fusion step need fresh implementation.\n3. KTUP uses a gumbel-softmax preference selector (_get_preferences, lines 154-171) to obtain a weighted relation vector per (user,item) pair; MCCLK’s attention only works at graph-aggregation time, so the gumbel path, st-gumbel helper functions (lines 94-125) and temperature scheduling must be built from scratch.\n4. KTUP’s KG loss is a margin-based TransH ranking loss (calculate_kg_loss, lines 236-266) with negative tail sampling, whereas MCCLK has no explicit KG triple loss; thus MarginRankingLoss, orthogonalLoss for norms, and triple batch generation logic are new components to implement.",
    "rank": "rank2"
  },
  {
    "source": "KGIN_2021",
    "target": "MCCLK_2021",
    "type": "in-domain",
    "similarities": "1. Both parse kg_graph into edge_index/edge_type and use entity/relation embeddings with scatter_mean to aggregate neighbor messages.\n\n2. Both build a normalized user-item adjacency and use sparse matrix multiplication for user aggregation on the interaction graph.\n\n3. Both optimize pairwise recommendation with BPRLoss plus EmbLoss regularization.",
    "differences": "1. MCCLK adds relation-aware attention in Aggregator.calculate_sim_hrt, while KGIN uses plain mean aggregation and separate intent-factor attention over users.\n\n2. MCCLK introduces three view encoders (structural CKG, collaborative LightGCN, semantic kNN item graph), whereas KGIN has a single KG GCN with intent factors.\n\n3. MCCLK uses multi-level contrastive losses with projection heads fc1/fc2/fc3 and a temperature hyperparameter; KGIN has no contrastive learning module.\n\n4. MCCLK constructs and updates a kNN item-item semantic graph via cosine+topk and lambda_coeff; KGIN does not build an item-item graph.\n\n5. MCCLK averages hop embeddings across layers, while KGIN maintains factorized relation weights with n_factors and disen_weight_att.",
    "rank": "rank1"
  },
  {
    "source": "KGAT_2019",
    "target": "MCCLK_2021",
    "type": "in-domain",
    "similarities": "1. Both build a collaborative knowledge graph by merging user-item interactions with KG triplets, and use user-as-entity mapping when forming the joint graph.\n\n2. Both maintain entity/relation embedding tables and parse kg_graph into edge_index/edge_type for message passing.\n\n3. Both apply relation-aware neighbor aggregation with scatter-based reduction; MCCLK's attention weights from calculate_sim_hrt can be adapted to KGAT-style edge weighting.\n\n4. Both use pairwise ranking with BPRLoss plus embedding regularization, and the final score is an inner product between user/item representations.",
    "differences": "1. KGAT uses TransR projection (relation-specific matrices) and an explicit KG triple loss with alternating KG/RS training, while MCCLK has no TransR and no calculate_kg_loss branch.\n\n2. KGAT concatenates layer-wise embeddings for final representations, whereas MCCLK stacks and averages hop embeddings in GraphConv and LightGCN.\n\n3. MCCLK builds a kNN item-item semantic graph via build_adj and runs item_agg_layer propagation; KGAT has no item-item semantic view.\n\n4. MCCLK adds a LightGCN collaborative view plus multi-level contrastive losses with fc1/fc2/fc3, temperature, and alpha/beta; KGAT does not include contrastive objectives.\n\n5. MCCLK supports build_graph_separately and uses node/message dropout in GraphConv; KGAT instead switches among GCN/GraphSage/Bi-Interaction aggregators with different linear layers.",
    "rank": "rank2"
  },
  {
    "source": "KGCN_2019",
    "target": "MCCLK_2021",
    "type": "in-domain",
    "similarities": "1. Both keep user/entity/relation embedding tables with Xavier-style initialization and use EmbLoss for L2 regularization.\n\n2. Both rely on relation-aware neighbor aggregation over KG edges and feed aggregated entity representations into scoring.\n\n3. Both use pairwise training and compute recommendation scores via user-item inner product.",
    "differences": "1. KGCN samples a fixed-size receptive field (adj_entity/adj_relation) and expands hop-wise tensors per batch, whereas MCCLK uses global KG edges with scatter_mean and no per-user receptive-field expansion.\n\n2. MCCLK constructs a CKG and a normalized user-item graph, then adds a LightGCN collaborative view; KGCN does not propagate on the interaction graph.\n\n3. MCCLK builds a kNN item-item semantic graph (build_adj) and item_agg_layer propagation; KGCN has no item-item semantic view.\n\n4. MCCLK adds multi-level contrastive losses with fc1/fc2/fc3, temperature, alpha/beta; KGCN only uses BPR/BCE plus L2.\n\n5. MCCLK applies node/message dropout and optionally build_graph_separately, while KGCN uses aggregator types (sum/concat/neighbor) with linear layers and a BCEWithLogitsLoss head.",
    "rank": "rank3"
  },
  {
    "source": "RippleNet_2018",
    "target": "MCCLK_2021",
    "type": "in-domain",
    "similarities": "1. Both use entity and relation embedding tables initialized with Xavier-style methods and reuse kg_graph edges for neighborhood information.\n\n2. Both perform multi-hop propagation over KG structure and apply attention-style weighting when aggregating neighbors.\n\n3. Both train with pairwise ranking loss (BPR) plus embedding regularization and score with user-item dot products.",
    "differences": "1. RippleNet builds user-specific ripple sets and updates user representations via memory-network hops, while MCCLK uses global GCN propagation with scatter_mean on the full KG.\n\n2. MCCLK uses three parallel views (structural, collaborative LightGCN, semantic kNN) and fuses them, whereas RippleNet has a single view.\n\n3. MCCLK adds local/global contrastive objectives with projection heads and temperature; RippleNet does not include contrastive learning.\n\n4. MCCLK constructs a kNN item-item semantic graph and updates it with lambda_coeff; RippleNet never builds an item-item graph.\n\n5. MCCLK caches embeddings for full_sort_predict and scores with concatenated user/item vectors; RippleNet computes scores from user-specific memories without such caching.",
    "rank": "rank4"
  }
]