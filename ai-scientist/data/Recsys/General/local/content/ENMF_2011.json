{
  "id": "ENMF_2011",
  "paper_title": "Efficient Neural Matrix Factorization without Sampling for Recommendation",
  "alias": "ENMF",
  "year": 2011,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "",
  "introduction": "# Introduction\nRecommender systems provide essential web services on the Internet to alleviate the information overload problem. An effective recommender system not only can facilitate the information seeking process of users but also can help providers display products accurately to the target population. With such an important role, recommendation has become a hot research topic and attracted increasing attention in information retrieval and data-mining communities.\n\nThe key to personalized recommender systems is in modelling users’ preference on items based on their past interactions (e.g., ratings and clicks), known as collaborative filtering. Among various collaborative filtering methods, Matrix Factorization (MF) is the most popular one and has been widely adopted in many real-world applications. MF maps users and items to a shared latent factor space, so that user-item relationships can be captured by their latent factors’ dot product. Early work on MF algorithms mainly focused on explicit feedback, where users’ ratings that directly reflect their preference on items are utilized. However, later researchers found that this way of modelling only the observed positive feedbacks leads to poor performance of the real top-N recommendation system. Moreover, explicit ratings are not always available in many applications. More commonly, users interact with items through implicit feedback, e.g., users’ viewing records and purchase history.\n\nCompared to explicit ratings, implicit feedback is easier to collect but more challenging to utilize, since it is binary and only has positive examples. To solve the problem of lacking negative feedback, two learning strategies have been proposed: (1) negative sampling strategy that randomly samples negative instances from the missing data; (2) whole-data-based strategy that treats all the missing data as negative. Both solutions have pros and cons: Negative sampling has controllable efficiency, but its effectiveness may suffer from the low quality of negative examples and slow convergence, while modeling all missing data is costly, it can be more effective.\n\nRecently, deep learning has made massive strides in many research areas and achieved great performance. The successful integration of deep-learning methods in recommendation systems has demonstrated the advantages of complex network structures over traditional models. However, existing studies have largely focused on exploring newly proposed deep-learning architectures for recommendation task, such as attention mechanisms, memory networks, Convolutional Neural Network (CNN), Generative Adversarial Networks (GAN), Graph Neural Networks (GNN), and so on. While for model learning, these works typically rely on the negative sampling strategy for efficient training. Despite effectiveness, we argue that existing deep-learning-based recommendation methods suffer from two important limitations: First, the methods with complex network structures have a substantial number of parameters, and require expensive computations even with a sampling-based learning strategy; second, the performance of negative sampling is not robust as it is highly sensitive to the sampling distribution and the number of negative samples. Essentially, sampling is biased, making it difficult to converge to the same loss with all training examples, regardless of how many update steps have been taken.\n\nTo address the limitation caused by negative sampling, we propose to perform whole-data-based learning for neural recommendation models. In contrast to sampling, whole-data-based learning does not involve any sampling procedure and computes the gradient over all training data (including all missing data). As such, it can easily converge to a better optimum in a more stable way. Unfortunately, the difficulty in applying whole-data-based learning lies in the expensive computational cost for large-scale data, which makes the straightforward method less applicable to neural models.\n\nMotivated by the above observations, in this work, we enhance (1) the effectiveness of neural recommendation models by performing whole-data-based learning strategy, and (2) the practicability of learning from the whole training data by developing three efficient optimization algorithms. The two significant enhancements of our methods make it easy to address large-scale scenarios with a more expressive modeling on implicit data. To ensure training efficiency, we accelerate the optimization method by reformulating a commonly used square loss function with rigorous mathematical reasoning. Specifically, we perform the optimization on each element of user and item latent vectors, rather than the traditional vector-wise manner. By leveraging the sparsity of implicit data, we successfully update each parameter in a manageable time complexity without sampling. Moreover, based on a simple Neural Matrix Factorization architecture, we present a general framework named ENMF (short for Efficient Neural Matrix Factorization), and propose three instantiations-ENMF-U, ENMF-I, and ENMF-A based on the newly derived optimization approaches.\n\nTo evaluate the recommendation performance and training efficiency of our proposed methods, we apply ENMF on three real-world datasets with extensive experiments. The results indicate that our ENMF methods consistently and significantly outperform the state-of-the-art methods on Top-K personalized recommendation task, while maintaining the favorable properties of not having compositional parameters. Furthermore, ENMF also shows significant advantages in training efficiency, which makes it more practical in practical E-commerce scenarios. Our main contributions are outlined as follows:\n• We propose to learn neural recommendation models without sampling, which is more effective and stable due to the consideration of all samples in each parameter update. Three efficient optimization methods are derived: user-based, item-based, and alternating-based, which solve the challenging problem of learning neural models from the whole data with a controllable time complexity.\n• A generic Efficient Neural Matrix Factorization framework (ENMF) is proposed based on the derived learning methods. It complements the mainstream sampling-based neural models for recommendation, providing a new approach to improve recommendation models.\n• Extensive experiments are performed on three real-world datasets. The results show that ENMF significantly outperforms the state-of-the-art methods by more than 5.90%, 4.08%, 6.30%, on the three datasets, respectively, while maintaining the favorable properties of not having compositional parameters. Furthermore, ENMF also shows significant advantages in training efficiency, which makes it more applicable to real-world large-scale systems.",
  "method": "# Method\n## 1. Preliminaries\n### 1.1 Notations\nLet \\(U\\) be the set of users, \\(V\\) be the set of items. The user-item interaction matrix is \\(R=[R_{uv}]_{M×N} \\in \\{0,1\\}\\), where \\(R_{uv}=1\\) indicates an observed interaction between user \\(u\\) and item \\(v\\), and \\(R_{uv}=0\\) otherwise. \\(p_u\\) denotes the latent vector of user \\(u\\), \\(q_v\\) denotes the latent vector of item \\(v\\), \\(c_{uv}\\) is the weight of entry \\(R_{uv}\\), \\(d\\) is the number of latent factors, and \\(\\Theta\\) is the set of neural parameters.\n\n### 1.2 MF Method for Implicit Data\nMatrix Factorization maps users and items into a joint latent feature space, with the interaction estimated as:\n\\[ \\hat{R}_{uv} = \\langle p_u, q_v \\rangle = p_u^T q_v \\]\nFor implicit data, the weighted regression loss function is:\n\\[ \\mathcal{L}(\\Theta) = \\sum_{u \\in U} \\sum_{v \\in V} c_{uv} (R_{uv} - \\hat{R}_{uv})^2 \\]\nwhere \\(c_{uv}\\) assigns weights to observed and missing entries to alleviate data imbalance.\n\n### 1.3 Weighting Strategies for Missing Data\nCommon weighting strategies include:\n1. Zero weight on missing entries (ignores missing data, unsuitable for top-N recommendation).\n2. Uniform weight on all entries (dominated by missing data in sparse scenarios).\n3. Uniform weight on missing entries: \\(c_{uv}=c_1\\) if \\(R_{uv}=1\\), \\(c_{uv}=c_0\\) (smaller than \\(c_1\\)) if \\(R_{uv}=0\\).\n4. Frequency-based weight on missing entries: \\(c_{uv}=c_1\\) if \\(R_{uv}=1\\), \\(c_{uv}=c_v^-\\) (depends on item popularity) if \\(R_{uv}=0\\).\n\n## 2. Efficient Neural Matrix Factorization (ENMF)\n### 2.1 General Framework\nENMF follows the Neural Collaborative Filtering (NCF) architecture with two key differences: (1) Inputs are user-item interaction sets (not individual pairs) to enable whole-data learning; (2) Efficient optimization methods replace negative sampling.\n- **Embedding Layer**: Converts users/items to dense latent vectors \\(p_u \\in \\mathbb{R}^d\\) and \\(q_v \\in \\mathbb{R}^d\\).\n- **Element-wise Product Layer**: Computes interaction between latent vectors: \\(\\phi_1(p_u, q_v) = p_u \\odot q_v\\).\n- **Prediction Layer**: Projects the product to a score: \\(\\hat{R}_{uv} = h^T (p_u \\odot q_v)\\), where \\(h\\) is the prediction layer weight vector.\n\n### 2.2 User-based Efficient Learning\nTraining batches are generated based on users. The loss function is reformulated to avoid iterating over all missing data:\n\\[ \\tilde{\\mathcal{L}}_1(\\Theta) = \\sum_{u \\in B} \\sum_{v \\in V^+} \\left( (c_v^+ - c_v^-) \\hat{R}_{uv}^2 - 2c_v^+ \\hat{R}_{uv} \\right) + \\sum_{i=1}^d \\sum_{j=1}^d \\left( (h_i h_j) \\left( \\sum_{u \\in B} p_{u,i} p_{u,j} \\right) \\left( \\sum_{v \\in V} c_v^- q_{v,i} q_{v,j} \\right) \\right) \\]\nwhere \\(B\\) is a user batch, \\(V^+\\) is the set of items with observed interactions, \\(c_v^+\\) and \\(c_v^-\\) are weights for observed and missing entries. Time complexity is reduced from \\(O(|B||V|d)\\) to \\(O((|B|+|V|)d^2 + |R_B|d)\\) ( \\(R_B\\) is positive interactions in the batch).\n\n### 2.3 Item-based Efficient Learning\nTraining batches are generated based on items. The loss function is:\n\\[ \\tilde{\\mathcal{L}}_2(\\Theta) = \\sum_{u \\in U^+} \\sum_{v \\in B} \\left( (c_v^+ - c_v^-) \\hat{R}_{uv}^2 - 2c_v^+ \\hat{R}_{uv} \\right) + \\sum_{i=1}^d \\sum_{j=1}^d \\left( (h_i h_j) \\left( \\sum_{u \\in U} p_{u,i} p_{u,j} \\right) \\left( \\sum_{v \\in B} c_v^- q_{v,i} q_{v,j} \\right) \\right) \\]\nwhere \\(B\\) is an item batch, \\(U^+\\) is the set of users with observed interactions.\n\n### 2.4 Alternating-based Efficient Learning\nAlternates between user-based and item-based training to balance user/item parameter updates:\n1. First step: Train with user-based loss.\n2. Second step: Train with item-based loss.\nGuarantees loss reduction in each epoch and accelerates convergence.\n\n### 2.5 Training Details\n- **Optimizer**: Mini-batch Adagrad (adaptive learning rate to alleviate imbalance).\n- **Regularization**: Dropout (randomly drops latent factors after element-wise product to prevent overfitting).\n- **Implementation**: Compatible with TensorFlow/PyTorch, leveraging matrix operations for efficiency.\n\n## 3. Complexity Analysis\n- **User-based**: \\(O((|U| + \\frac{|U||V|}{|B|})d^2 + |R|d)\\) per epoch.\n- **Item-based**: \\(O((|V| + \\frac{|U||V|}{|B|})d^2 + |R|d)\\) per epoch.\n- **Alternating-based**: Sum of user-based and item-based complexity, but requires fewer epochs for convergence.\nAll methods avoid sampling and achieve efficient whole-data learning by leveraging mathematical decoupling and sparsity.",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nThree public datasets (filtered to retain items with ≥5 interactions):\n| Dataset | #User | #Item | #Interaction | Density |\n|---------|-------|-------|--------------|---------|\n| Ciao | 7,267 | 11,211 | 157,995 | 0.19% |\n| Epinion | 20,608 | 23,585 | 454,002 | 0.09% |\n| Movielens | 6,940 | 3,706 | 1,000,209 | 4.47% |\n\n### 1.2 Baselines\n- **MostPopular (MP)**: Non-personalized, ranks items by popularity.\n- **ItemKNN**: Item-based collaborative filtering with similarity metrics.\n- **BPR**: Bayesian Personalized Ranking (sampling-based MF).\n- **WMF**: Weighted MF (whole-data-based, uniform weights for missing data).\n- **ExpoMF**: Whole-data-based MF with popularity-weighted missing data.\n- **GMF**: Neural MF (sampling-based, same network structure as ENMF).\n- **NCF**: Neural Collaborative Filtering (sampling-based, MLP + MF).\n- **ConvNCF**: CNN-enhanced NCF (sampling-based).\n\n### 1.3 Evaluation Protocol\n- **Leave-one-out split**: Last user interaction as test, second last as validation, rest as training.\n- **Metrics**: Hit Ratio (HR@N) and Normalized Discounted Cumulative Gain (NDCG@N) for N={50,100,200}.\n- **Implementation**: ENMF implemented with TensorFlow; hyperparameters tuned via validation set.\n\n### 1.4 Parameter Settings\n- Learning rate: 0.05.\n- Batch size: 512.\n- Latent dimension \\(d\\): 64.\n- Dropout ratio: 0.3 (Ciao), 0.5 (Epinion), 0.7 (Movielens).\n- Weight of missing data \\(c_0\\): 0.05 (Ciao/Epinion), 0.5 (Movielens).\n\n## 2. Main Results\n### 2.1 Performance Comparison\nTable 4 (core result table) shows ENMF outperforms all baselines on all datasets:\n- **ENMF-A (alternating-based)** achieves the best performance, with average relative improvements of 5.90% (Ciao), 4.87% (Epinion), 6.30% (Movielens) over ConvNCF.\n- Whole-data-based methods (WMF, ExpoMF, ENMF) outperform sampling-based methods (BPR, GMF, NCF, ConvNCF) due to unbiased training.\n- ENMF-U and ENMF-I perform similarly; ENMF-A surpasses them by balancing user/item training.\n\n### 2.2 Efficiency Analysis\n- **Per iteration speed**: ENMF methods are 10–30x faster than baselines. For example, on Epinion, ENMF-U takes 8s/iteration vs. GMF’s 216s/iteration.\n- **Total training time**: ENMF-A takes 20–70 minutes vs. baselines’ 7–70 hours.\n- **Convergence**: ENMF-A converges in 50–150 epochs, while baselines require 300–500 epochs.\n\n### 2.3 Hyper-parameter Analysis\n- **Embedding size**: Performance improves with \\(d\\) (up to 64), ENMF outperforms baselines even with smaller \\(d\\).\n- **Negative weight \\(c_0\\)**: Optimal \\(c_0\\) is 0.05 (sparse datasets) and 0.5 (dense datasets); ENMF is more robust to weight variations than WMF.\n- **Dropout ratio**: Proper dropout (0.3–0.7) improves generalization; ENMF maintains stability across ratios.\n\n## 3. Key Observations\n1. Whole-data-based learning without sampling is more effective than negative sampling for neural recommendation.\n2. ENMF’s simple architecture (element-wise product + prediction layer) outperforms complex deep models (NCF, ConvNCF) due to efficient optimization.\n3. Alternating-based training balances user/item parameter updates, accelerating convergence and improving performance.\n4. ENMF achieves both superior performance and training efficiency, making it suitable for large-scale real-world systems.",
  "hyperparameter": ""
}