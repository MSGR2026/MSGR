{
  "id": "MultiDAE_2018",
  "paper_title": "Variational Autoencoders for Collaborative Filtering (MultiDAE variant)",
  "alias": "MultiDAE",
  "year": 2018,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "The paper proposes Mult-VAE^PR, a variational autoencoder for collaborative filtering that uses multinomial likelihood to model user-item interactions and introduces partial regularization (β-VAE with β∈[0,1]) to trade off between reconstruction quality and prior constraint. The key innovation is treating the KL divergence term as a tunable regularization parameter rather than strictly maintaining the ELBO bound, allowing the model to prioritize recommendation performance over generative modeling capabilities. The multinomial likelihood naturally handles implicit feedback by making items compete for limited probability mass, better aligning with top-N ranking objectives.",
  "introduction": "# 1 INTRODUCTION\n\nRecommender systems are an integral component of the web. In a typical recommendation system, we observe how a set of users interacts with a set of items. Using this data, we seek to show users a set of previously unseen items they will like. As the web grows in size, good recommendation systems will play an important part in helping users interact more effectively with larger amounts of content.\n\nCollaborative filtering is among the most widely applied approaches in recommender systems. Collaborative filtering predicts what items a user will prefer by discovering and exploiting the similarity patterns across users and items. Latent factor models [13, 19, 38] still largely dominate the collaborative filtering research literature due to their simplicity and effectiveness. However, these models are inherently linear, which limits their modeling capacity. Previous work [27] has demonstrated that adding carefully crafted non-linear features into the linear latent factor models can significantly boost recommendation performance. Recently, a growing body of work involves applying neural networks to the collaborative filtering setting with promising results [14, 41, 51, 54].\n\nHere, we extend variational autoencoders (VAEs) [24, 37] to collaborative filtering for implicit feedback. VAEs generalize linear latent-factor models and enable us to explore non-linear probabilistic latent-variable models, powered by neural networks, on large-scale recommendation datasets. We propose a neural generative model with multinomial conditional likelihood. Despite being widely used in language modeling and economics [5, 30], multinomial likelihoods appear less studied in the collaborative filtering literature, particularly within the context of latent-factor models. Recommender systems are often evaluated using ranking-based measures, such as mean average precision and normalized discounted cumulative gain [21]. Top-  $N$  ranking loss is difficult to optimize directly and previous work on direct ranking loss minimization resorts to relaxations and approximations [49, 50]. Here, we show that the multinomial likelihoods are well-suited for modeling implicit feedback data, and are a closer proxy to the ranking loss relative to more popular likelihood functions such as Gaussian and logistic.\n\nThough recommendation is often considered a big-data problem (due to the huge numbers of users and items typically present in a recommender system), we argue that, in contrast, it represents a uniquely challenging \"small-data\" problem: most users only interact with a tiny proportion of the items and our goal is to collectively\n\nmake informed inference about each user's preference. To make use of the sparse signals from users and avoid overfitting, we build a probabilistic latent-variable model that shares statistical strength among users and items. Empirically, we show that employing a principled Bayesian approach is more robust regardless of the scarcity of the data.\n\nAlthough VAEs have been extensively studied for image modeling and generation, there is surprisingly little work applying VAEs to recommender systems. We find that two adjustments are essential to getting state-of-the-art results with VAEs on this task:\n\n- First, we use a multinomial likelihood for the data distribution. We show that this simple choice realizes models that outperform the more commonly used Gaussian and logistic likelihoods.  \n- Second, we reinterpret and adjust the standard vAE objective, which we argue is over-regularized. We draw connections between the learning algorithm resulting from our proposed regularization and the information-bottleneck principle and maximum-entropy discrimination.\n\nThe result is a recipe that makes VAEs practical solutions to this important problem. Empirically, our methods significantly outperform state-of-the-art baselines on several real-world datasets, including two recently proposed neural-network approaches.\n",
  "method": "# 2 METHOD\n\nWe use  $u \\in \\{1, \\dots, U\\}$  to index users and  $i \\in \\{1, \\dots, I\\}$  to index items. In this work, we consider learning with implicit feedback [19, 34]. The user-by-item interaction matrix is the click matrix  $\\mathbf{X} \\in \\mathbb{N}^{U \\times I}$ . The lower case  $\\mathbf{x}_u = [x_{u1}, \\dots, x_{uI}]^\\top \\in \\mathbb{N}^I$  is a bag-of-words vector with the number of clicks for each item from user  $u$ . For simplicity, we binarize the click matrix. It is straightforward to extend it to general count data.\n\n# 2.1 Model\n\nThe generative process we consider in this paper is similar to the deep latent Gaussian model [37]. For each user  $u$ , the model starts by sampling a  $K$ -dimensional latent representation  $\\mathbf{z}_u$  from a standard Gaussian prior. The latent representation  $\\mathbf{z}_u$  is transformed via a non-linear function  $f_{\\theta}(\\cdot) \\in \\mathbb{R}^{I}$  to produce a probability distribution over  $I$  items  $\\pi(\\mathbf{z}_u)$  from which the click history  $\\mathbf{x}_u$  is assumed to have been drawn:\n\n$$\n\\begin{array}{l} \\mathbf {z} _ {u} \\sim \\mathcal {N} \\left(0, \\mathrm {I} _ {K}\\right), \\quad \\pi \\left(\\mathbf {z} _ {u}\\right) \\propto \\exp \\left\\{f _ {\\theta} \\left(\\mathbf {z} _ {u}\\right) \\right\\}, \\tag {1} \\\\ \\mathbf {x} _ {u} \\sim \\operatorname {M u l t} \\left(N _ {u}, \\pi (\\mathbf {z} _ {u})\\right). \\\\ \\end{array}\n$$\n\nThe non-linear function  $f_{\\theta}(\\cdot)$  is a multilayer perceptron with parameters  $\\theta$ . The output of this transformation is normalized via a softmax function to produce a probability vector  $\\pi(\\mathbf{z}_u) \\in \\mathbb{S}^{I-1}$  (an  $(I-1)$ -simplex) over the entire item set. Given the total number of clicks  $N_u = \\sum_i x_{ui}$  from user  $u$ , the observed bag-of-words vector  $\\mathbf{x}_u$  is assumed to be sampled from a multinomial distribution with probability  $\\pi(\\mathbf{z}_u)$ . This generative model generalizes the latent-factor model - we can recover classical matrix factorization [38] by setting  $f_{\\theta}(\\cdot)$  to be linear and using a Gaussian likelihood.\n\nThe log-likelihood for user  $u$  (conditioned on the latent representation) is:\n\n$$\n\\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid \\mathbf {z} _ {u}\\right) \\stackrel {{c}} {{=}} \\sum_ {i} x _ {u i} \\log \\pi_ {i} (\\mathbf {z} _ {u}). \\tag {2}\n$$\n\nThis multinomial likelihood is commonly used in language models, e.g., latent Dirichlet allocation [5], and economics, e.g., multinomial logit choice model [30]. It is also used in the cross-entropy loss² for multi-class classification. For example, it has been used in recurrent neural networks for session-based sequential recommendation [8, 15, 16, 42, 45] and in feedforward neural networks applied to Youtube recommendation [9]. The multinomial likelihood is less well studied in the context of latent-factor models such as matrix factorization and autoencoders. A notable exception is the collaborative competitive filtering (CCF) model [53] and its successors, which take advantage of more fine-grained information about what options were presented to which users. (If such information is available, it can also be incorporated into our VAE-based approach.)\n\nWe believe the multinomial distribution is well suited to modeling click data. The likelihood of the click matrix (Eq. 2) rewards the model for putting probability mass on the non-zero entries in  $\\mathbf{x}_u$ . But the model has a limited budget of probability mass, since  $\\pi (\\mathbf{z}_u)$  must sum to 1; the items must compete for this limited budget [53]. The model should therefore assign more probability mass to items that are more likely to be clicked. To the extent that it can, it will perform well under the top- $N$  ranking loss that recommender systems are commonly evaluated on.\n\nBy way of comparison, we present two popular choices of likelihood functions used in latent-factor collaborative filtering: Gaussian and logistic likelihoods. Define  $f_{\\theta}(\\mathbf{z}_u) \\equiv [f_{u1}, \\dots, f_{uI}]^\\top$  as the output of the generative function  $f_{\\theta}(\\cdot)$ . The Gaussian log-likelihood for user  $u$  is\n\n$$\n\\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid \\mathbf {z} _ {u}\\right) \\stackrel {{c}} {{=}} - \\sum_ {i} \\frac {c _ {u i}}{2} \\left(x _ {u i} - f _ {u i}\\right) ^ {2}. \\tag {3}\n$$\n\nWe adopt the convention in Hu et al. [19] and introduce a \"confidence\" weight  $c_{x_{ui}} \\equiv c_{ui}$  where  $c_1 > c_0$  to balance the unobserved 0's which far outnumber the observed 1's in most click data. This is also equivalent to training the model with unweighted Gaussian likelihood and negative sampling. The logistic log-likelihood<sup>3</sup> for user  $u$  is\n\n$$\n\\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid \\mathbf {z} _ {u}\\right) = \\sum_ {i} x _ {u i} \\log \\sigma \\left(f _ {u i}\\right) + \\left(1 - x _ {u i}\\right) \\log \\left(1 - \\sigma \\left(f _ {u i}\\right)\\right), \\tag {4}\n$$\n\nwhere  $\\sigma(x) = 1 / (1 + \\exp(-x))$  is the logistic function. We compare multinomial likelihood with Gaussian and logistic in Section 4.\n\n# 2.2 Variational inference\n\nTo learn the generative model in Eq. 1, we are interested in estimating  $\\theta$  (the parameters of  $f_{\\theta}(\\cdot)$ ). To do so, for each data point we need to approximate the intractable posterior distribution  $p(\\mathbf{z}_u \\mid \\mathbf{x}_u)$ . We resort to variational inference [22]. Variational inference approximates the true intractable posterior with a simpler variational\n\ndistribution  $q(\\mathbf{z}_u)$ . We set  $q(\\mathbf{z}_u)$  to be a fully factorized (diagonal) Gaussian distribution:\n\n$$\nq (\\mathbf {z} _ {u}) = \\mathcal {N} \\left(\\boldsymbol {\\mu} _ {u}, \\operatorname {d i a g} \\{\\sigma_ {u} ^ {2} \\}\\right).\n$$\n\nThe objective of variational inference is to optimize the free variational parameters  $\\{\\mu_u,\\sigma_u^2\\}$  so that the Kullback-Leibler divergence  $\\mathrm{KL}(q(\\mathbf{z}_u)\\| p(\\mathbf{z}_u|\\mathbf{x}_u))$  is minimized.\n\n2.2.1 Amortized inference and the variational autoencoder: With variational inference the number of parameters to optimize  $\\{\\mu_u,\\sigma_u^2\\}$  grows with the number of users and items in the dataset. This can become a bottleneck for commercial recommender systems with millions of users and items. The variational autoencoder (VAE) [24, 37] replaces individual variational parameters with a data-dependent function (commonly called an inference model):\n\n$$\ng _ {\\phi} (\\mathbf {x} _ {u}) \\equiv [ \\mu_ {\\phi} (\\mathbf {x} _ {u}), \\sigma_ {\\phi} (\\mathbf {x} _ {u}) ] \\in \\mathbb {R} ^ {2 K}\n$$\n\nparametrized by  $\\phi$  with both  $\\mu_{\\phi}(\\mathbf{x}_u)$  and  $\\sigma_{\\phi}(\\mathbf{x}_u)$  being  $K$ -vectors and sets the variational distribution as follows:\n\n$$\nq _ {\\phi} (\\mathbf {z} _ {u} \\mid \\mathbf {x} _ {u}) = \\mathcal {N} (\\mu_ {\\phi} (\\mathbf {x} _ {u}), \\mathrm {d i a g} \\{\\sigma_ {\\phi} ^ {2} (\\mathbf {x} _ {u}) \\}).\n$$\n\nThat is, using the observed data  $\\mathbf{x}_u$  as input, the inference model outputs the corresponding variational parameters of variational distribution  $q_{\\phi}(\\mathbf{z}_u\\mid \\mathbf{x}_u)$ , which, when optimized, approximates the intractable posterior  $p(\\mathbf{z}_u\\mid \\mathbf{x}_u)$ .<sup>4</sup> Putting  $q_{\\phi}(\\mathbf{z}_u\\mid \\mathbf{x}_u)$  and the generative model  $p_{\\theta}(\\mathbf{x}_u\\mid \\mathbf{z}_u)$  together in Figure 2c, we end up with a neural architecture that resembles an autoencoder - hence the name variational autoencoder.\n\nVAEs make use of amortized inference [12]: they flexibly reuse inferences to answer related new problems. This is well aligned with the ethos of collaborative filtering: analyze user preferences by exploiting the similarity patterns inferred from past experiences. In Section 2.4, we discuss how this enables us to perform prediction efficiently.\n\nLearning VAEs: As is standard when learning latent-variable models with variational inference [4], we can lower-bound the log marginal likelihood of the data. This forms the objective we seek to maximize for user  $u$  (the objective function of the dataset is obtained by averaging the objective function over all the users):\n\n$$\n\\begin{array}{l} \\log p \\left(\\mathbf {x} _ {u}; \\theta\\right) \\geq \\mathbb {E} _ {q _ {\\phi} \\left(\\mathbf {z} _ {u} \\mid \\mathbf {x} _ {u}\\right)} \\left[ \\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid \\mathbf {z} _ {u}\\right) \\right] - \\mathrm {K L} \\left(q _ {\\phi} \\left(\\mathbf {z} _ {u} \\mid \\mathbf {x} _ {u}\\right) \\| p (\\mathbf {z} _ {u})\\right) \\\\ \\equiv \\mathcal {L} (\\mathbf {x} _ {u}; \\theta , \\phi) \\tag {5} \\\\ \\end{array}\n$$\n\nThis is commonly known as the evidence lower bound (ELBO). Note that the ELBO is a function of both  $\\theta$  and  $\\phi$ . We can obtain an unbiased estimate of ELBO by sampling  $\\mathbf{z}_u \\sim q_\\phi$  and perform stochastic gradient ascent to optimize it. However, the challenge is that we cannot trivially take gradients with respect to  $\\phi$  through this sampling process. The reparametrization trick [24, 37] sidesteps this issue: we sample  $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}_K)$  and reparametrize  $\\mathbf{z}_u = \\mu_\\phi(\\mathbf{x}_u) + \\epsilon \\odot \\sigma_\\phi(\\mathbf{x}_u)$ . By doing so, the stochasticity in the sampling process is isolated and the gradient with respect to  $\\phi$  can be back-propagated through the sampled  $\\mathbf{z}_u$ . The VAE training procedure is summarized in Algorithm 1.\n\nAlgorithm 1: VAE-SGD Training collaborative filtering vAE with stochastic gradient descent.\n\nInput: Click matrix  $\\mathbf{X}\\in \\mathbb{R}^{U\\times I}$    \nRandomly initialize  $\\theta ,\\phi$    \nwhile not converged do Sample a batch of users U forall  $u\\in \\mathcal{U}$  do Sample  $\\epsilon \\sim \\mathcal{N}(0,\\mathbf{I}_K)$  and compute  $\\mathbf{z}_u$  via reparametrization trick Compute noisy gradient  $\\nabla_{\\theta}\\mathcal{L}$  and  $\\nabla_{\\phi}\\mathcal{L}$  with  $\\mathbf{z}_u$  end Average noisy gradients from batch Update  $\\theta$  and  $\\phi$  by taking stochastic gradient steps   \nend   \nreturn  $\\theta ,\\phi$\n\n2.2.2 Alternative interpretation of ELBO. We can view ELBO defined in Eq. 5 from a different perspective: the first term can be interpreted as (negative) reconstruction error, while the second KL term can be viewed as regularization. It is this perspective we work with because it allows us to make a trade-off that forms the crux of our method. From this perspective, it is natural to extend the ELBO by introducing a parameter  $\\beta$  to control the strength of regularization:\n\n$$\n\\begin{array}{l} \\mathcal {L} _ {\\beta} \\left(\\mathbf {x} _ {u}; \\theta , \\phi\\right) \\equiv \\mathbb {E} _ {q _ {\\phi} \\left(\\mathbf {z} _ {u} \\mid \\mathbf {x} _ {u}\\right)} \\left[ \\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid \\mathbf {z} _ {u}\\right) \\right] \\tag {6} \\\\ - \\beta \\cdot \\operatorname {K L} \\left(q _ {\\phi} \\left(\\mathbf {z} _ {u} \\mid \\mathbf {x} _ {u}\\right) \\| p (\\mathbf {z} _ {u})\\right). \\\\ \\end{array}\n$$\n\nWhile the original VAE (trained with ELBO in Eq. 5) is a powerful generative model; we might ask whether we need all the statistical properties of a generative model for tackling problems in recommender systems. In particular, if we are willing to sacrifice the ability to perform ancestral sampling, can we improve our performance? The regularization view of the ELBO (Eq. 6) introduces a trade-off between how well we can fit the data and how close the approximate posterior stays to the prior during learning.\n\nWe propose using  $\\beta \\neq 1$ . This means we are no longer optimizing a lower bound on the log marginal likelihood. If  $\\beta < 1$ , then we are also weakening the influence of the prior constraint  $\\frac{1}{U}\\sum_{u}q(\\mathbf{z}|\\mathbf{x}_u)\\approx p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};0,\\mathbf{I}_K)$  [18]; this means that the model is less able to generate novel user histories by ancestral sampling.\n\nBut ultimately our goal is to make good recommendations, not to maximize likelihood or generate imagined user histories. Treating  $\\beta$  as a free regularization parameter therefore costs us nothing, and, as we will see, yields significant improvements in performance.\n\nSelecting  $\\beta$ : We propose a simple heuristic for setting  $\\beta$ : we start training with  $\\beta = 0$ , and gradually increase  $\\beta$  to 1. We linearly anneal the KL term slowly over a large number of gradient updates to  $\\theta, \\phi$  and record the best  $\\beta$  when its performance reaches the peak. We found this method to work well and it does not require the need for training multiple models with different values of  $\\beta$ , which can be time-consuming. Our procedure is inspired by KL annealing [7], a common heuristic used for training VAEs when there is concern that the model is being underutilized.\n\nFigure 1 illustrates the basic idea (we observe the same trend consistently across datasets). Here we plot the validation ranking metric without KL annealing (blue solid) and with KL annealing all the way to  $\\beta = 1$  (green dashed,  $\\beta$  reaches 1 at around 80 epochs). As we can see, the performance is poor without any KL annealing. With annealing, the validation performance first increases as the training proceeds and then drops as  $\\beta$  gets close to 1 to a value that is only slightly better than doing no annealing at all.\n\nHaving identified the best  $\\beta$  based on the peak validation metric, we can retrain the model with the same annealing schedule, but stop increasing  $\\beta$  after reaching that value (shown as red dot-dashed in Figure 1). This might be sub-optimal compared to a thorough grid search. However, it is much more efficient, and gives us competitive empirical performance. If the computational budget is scarce, then within a single run, we can stop increasing  $\\beta$  when we notice the validation metric dropping. Such a procedure incurs no additional runtime to learning a standard vAE. We denote this partially regularized vAE with multinomial likelihood as Mult-VAE $^{\\mathrm{PR}}$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/dc958a59-7ec1-4c2f-ab4f-346792e6b2e9/1c9706576871603c5b4812d44de4cac39d5ebaef4be99942aee20234e0175ef0.jpg)  \nFigure 1: Validation ranking metrics with different annealing configurations. For the green dashed curve,  $\\beta$  reaches 1 at around 80 epochs.\n\n2.2.3 Computational Burden. Previous collaborative filtering models with neural networks [14, 51] are trained with stochastic gradient descent where in each step a single (user, item) entry from the click matrix is randomly sampled to perform a gradient update. In Algorithm 1 we subsample users and take their entire click history (complete rows of the click matrix) to update model parameters. This eliminates the necessity of negative sampling (and consequently the hyperparameter tuning for picking the number of negative examples), commonly used in the (user, item) entry subsampling scheme.\n\nA computational challenge that comes with our approach, however, is that when the number of items is huge, computing the multinomial probability  $\\pi (\\mathbf{z}_u)$  could be computationally expensive, since it requires computing the predictions for all the items for normalization. This is a common challenge for language modeling where the size of the vocabulary is in the order of millions or more [32]. In our experiments on some medium-to-large datasets with less than 50K items (Section 4.1), this has not yet come up as a computational bottleneck. If this becomes a bottleneck when working with larger item sets, one can easily apply the simple and\n\neffective method proposed by Botev et al. [6] to approximate the normalization factor for  $\\pi (\\mathbf{z}_u)$ .\n\n# 2.3 A taxonomy of autoencoders\n\nIn Section 2.2, we introduced maximum marginal likelihood estimation of VAEs using approximate Bayesian inference under a non-linear generative model (Eq. 1). We now describe our work from the perspective of learning autoencoders. Maximum-likelihood estimation in a regular autoencoder takes the following form:\n\n$$\n\\begin{array}{l} \\theta^ {\\mathrm {A E}}, \\phi^ {\\mathrm {A E}} = \\arg \\max  _ {\\theta , \\phi} \\sum_ {u} \\mathbb {E} _ {\\delta \\left(\\mathbf {z} _ {u} - g _ {\\phi} (\\mathbf {x} _ {u})\\right)} \\left[ \\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid \\mathbf {z} _ {u}\\right) \\right] \\\\ = \\arg \\max  _ {\\theta , \\phi} \\sum_ {u} \\log p _ {\\theta} \\left(\\mathbf {x} _ {u} \\mid g _ {\\phi} \\left(\\mathbf {x} _ {u}\\right)\\right) \\tag {7} \\\\ \\end{array}\n$$\n\nThere are two key distinctions of note: (1) The autoencoder (and denoising autoencoder) effectively optimizes the first term in the vAE objective (Eq. 5 and Eq. 6) using a delta variational distribution  $q_{\\phi}(\\mathbf{z}_u \\mid \\mathbf{x}_u) = \\delta (\\mathbf{z}_u - g_{\\phi}(\\mathbf{x}_u)) -$  it does not regularize  $q_{\\phi}(\\mathbf{z}_u \\mid \\mathbf{x}_u)$  towards any prior distribution as the vAE does. (2) the  $\\delta (\\mathbf{z}_u - g_{\\phi}(\\mathbf{x}_u))$  is a  $\\delta$  distribution with mass only at the output of  $g_{\\phi}(\\mathbf{x}_u)$ . Contrast this to the vAE, where the learning is done using a variational distribution, i.e.,  $g_{\\phi}(\\mathbf{x}_u)$  outputs the parameters (mean and variance) of a Gaussian distribution. This means that vAE has the ability to capture per-data-point variances in the latent state  $\\mathbf{z}_u$ .\n\nIn practice, we find that learning autoencoders is extremely prone to overfitting as the network learns to put all the probability mass to the non-zero entries in  $\\mathbf{x}_u$ . By introducing dropout [43] at the input layer, the denoising autoencoder (DAE) is less prone to overfitting and we find that it also gives competitive empirical results. In addition to the Mult-VAEPR, we also study a denoising autoencoder with a multinomial likelihood. We denote this model Mult-DAE. In Section 4 we characterize the tradeoffs in what is gained and lost by explicitly parameterizing the per-user variance with Mult-VAEPR versus using a point-estimation in Mult-DAE.\n\nTo provide a unified view of different variants of autoencoders and clarify where our work stands, we depict variants of autoencoders commonly found in the literature in Figure 2. For each one, we specify the model (dotted arrows denote a sampling operation) and describe the training objective used in parameter estimation.\n\nIn Figure 2a we have autoencoder. It is trained to reconstruct input with the same objective as in Eq. 7. Adding noise to the input (or the intermediate hidden representation) of an autoencoder yields the denoising autoencoder in Figure 2b. The training objective is the same as that of an autoencoder. Mult-DAE belongs to this model class. Collaborative denoising autoencoder [51] is a variant of this model class. The vAE is depicted in Figure 2c. Rather than using a delta variational distribution, it uses an inference model parametrized by  $\\phi$  to produce the mean and variance of the approximating variational distribution. The training objective of the vAE is given in Eq. 6. Setting  $\\beta$  to 1 recovers the original vAE formulation [24, 37]. Higgins et al. [17] study the case where  $\\beta > 1$ . Our model, Mult-VAEPR corresponds to learning VAES with  $\\beta \\in [0,1]$ .\n\n# 2.4 Prediction\n\nWe now describe how we make predictions given a trained generative model of the form Eq. 1. For both, Mult-VAE $^{\\mathrm{PR}}$  (Section 2.2)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/dc958a59-7ec1-4c2f-ab4f-346792e6b2e9/00d9361e1908bd1c181a749830b602efde327557a24dd0625fedda7bebdfd0e0.jpg)  \nFigure 2: A taxonomy of autoencoders. The dotted arrows denote a sampling operation.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/dc958a59-7ec1-4c2f-ab4f-346792e6b2e9/b9882c2dc338ceeb54d2bd3cf7411f41629848812cb9a6dcb2e4080be75a09a0.jpg)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/dc958a59-7ec1-4c2f-ab4f-346792e6b2e9/998d4108672d668a0309c950812596920a06bcc7d5e172173168820e48ea0e96.jpg)\n\nor Mult-DAE (Section 2.3), we make predictions in the same way. Given a user's click history  $\\mathbf{x}$ , we rank all the items based on the un-normalized predicted multinomial probability  $f_{\\theta}(\\mathbf{z})$ . The latent representation  $\\mathbf{z}$  for  $\\mathbf{x}$  is constructed as follows: For Mult-VAEPR, we simply take the mean of the variational distribution  $\\mathbf{z} = \\mu_{\\phi}(\\mathbf{x})$ ; for Mult-DAE, we take the output  $\\mathbf{z} = g_{\\phi}(x)$ .\n\nIt is easy to see the advantage of using autoencoders. We can effectively make predictions for users by evaluating two functions - the inference model (encoder)  $g_{\\phi}(\\cdot)$  and the generative model (decoder)  $f_{\\theta}(\\cdot)$ . For most of the latent factor collaborative filtering model, e.g., matrix factorization [13, 19], when given the click history of a user that is not present in the training data, normally we need to perform some form of optimization to obtain the latent factor for this user. This makes the use of autoencoders particularly attractive in industrial applications, where it is important that predictions be made cheaply and with low latency.\n",
  "experiments": "# 4 EMPIRICAL STUDY\n\nWe evaluate the performance of Mult-VAE $^{\\mathrm{PR}}$  and Mult-DAE. We provide insights into their performance by exploring the resulting fits. We highlight the following results:\n\n- Mult-VAE $^{\\mathrm{PR}}$  achieves state-of-the-art results on three real-world datasets when compared with various baselines, including recently proposed neural-network-based collaborative filtering models.  \n- For the denoising and variational autoencoder, the multinomial likelihood compares favorably over the more common Gaussian and logistic likelihoods.  \n- Both Mult-VAE $^{\\text{PR}}$  and Mult-DAE produce competitive empirical results. We identify when parameterizing the uncertainty explicitly as in Mult-VAE $^{\\text{PR}}$  does better/worse than the point estimate used by Mult-DAE and list pros and cons for both approaches.\n\nThe source code to reproduce the experimental results is available on GitHub<sup>6</sup>.\n\n# 4.1 Datasets\n\nWe study three medium- to large-scale user-item consumption datasets from various domains:\n\nMovieLens-20M (ML-20M): These are user-movie ratings collected from a movie recommendation service. We binarize the explicit data by keeping ratings of four or higher and interpret them as implicit feedback. We only keep users who have watched at least five movies.\n\nNetflix Prize (Netflix): This is the user-movie ratings data from the Netflix Prize<sup>7</sup>. Similar to ML-20M, we binarize explicit data by keeping ratings of four or higher. We only keep users who have watched at least five movies.\n\nMillion Song Dataset (MSD): This data contains the user-song play counts released as part of the Million Song Dataset [3]. We binarize play counts and interpret them as implicit preference data. We only keep users with at least 20 songs in their listening history and songs that are listened to by at least 200 users.\n\nTable 1 summarizes the dimensions of all the datasets after preprocessing.\n\n# 4.2 Metrics\n\nWe use two ranking-based metrics: Recall@R and the truncated normalized discounted cumulative gain (NDCG@R). For each user, both metrics compare the predicted rank of the held-out items with their true rank. For both Mult-VAE $^{\\text{PR}}$  and Mult-DAE, we get the predicted rank by sorting the un-normalized multinomial probability  $f_{\\theta}(\\mathbf{z})$ . While Recall@R considers all items ranked within the first R to be equally important, NDCG@R uses a monotonically increasing discount to emphasize the importance of higher ranks versus lower ones. Formally, define  $\\omega(r)$  as the item at rank  $r$ ,  $\\mathbb{I}[\\cdot]$  is the indicator function, and  $I_u$  is the set of held-out items that user  $u$  clicked on.\n\nRecall@R for user  $u$  is\n\n$$\n\\operatorname {R e c a l l} @ R (u, \\omega) := \\frac {\\sum_ {r = 1} ^ {R} \\mathbb {I} [ \\omega (r) \\in I _ {u} ]}{\\min  (M , | I _ {u} |)}.\n$$\n\nThe expression in the denominator is the minimum of  $R$  and the number of items clicked on by user u. This normalizes Recall@R to have a maximum of 1, which corresponds to ranking all relevant items in the top  $R$  positions.\n\nTruncated discounted cumulative gain (DCG@R) is\n\n$$\n\\mathrm {D C G} @ R (u, \\omega) := \\sum_ {r = 1} ^ {R} \\frac {2 ^ {\\mathbb {I} [ \\omega (r) \\in I _ {u} ]} - 1}{\\log (r + 1)}.\n$$\n\nNDCG@R is the DCG@R linearly normalized to [0, 1] after dividing by the best possible DCG@R, where all the held-out items are ranked at the top.\n\n# 4.3 Experimental setup\n\nWe study the performance of various models under strong generalization [29]: We split all users into training/validation/test sets. We train models using the entire click history of the training users. To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users.\n\nThis is relatively more difficult than weak generalization where the user's click history can appear during both training and evaluation. We consider it more realistic and robust as well. In the last row of Table 1, we list the number of held-out users (we use the same number of users for validation and test). For each held-out user, we randomly choose  $80\\%$  of the click history as the \"fold-in\" set to learn the necessary user-level representation and report metrics on the remaining  $20\\%$  of the click history.\n\nWe select model hyperparameters and architectures by evaluating NDCG@100 on the validation users. For both Mult-VAE $^{\\mathrm{PR}}$  and Mult-DAE, we keep the architecture for the generative model  $f_{\\theta}(\\cdot)$  and the inference model  $g_{\\phi}(\\cdot)$  symmetrical and explore multilayer perceptron (MLP) with 0, 1, and 2 hidden layers. We set the dimension of the latent representation  $K$  to 200 and any hidden layer to 600. As a concrete example, recall  $I$  is the total number of items, the overall architecture for a Mult-VAE $^{\\mathrm{PR}}$ /Mult-DAE with 1-hidden-layer MLP generative model would be  $[I \\to 600 \\to 200 \\to 600 \\to I]$ . We find that going deeper does not improve performance. The best performing architectures are MLPs with either 0 or 1 hidden layers. We use a tanh non-linearity as the activation function between layers.\n\nNote that for  $\\mathrm{Mult - VA E}^{\\mathrm{PR}}$ , since the output of  $g_{\\phi}(\\cdot)$  is used as the mean and variance of a Gaussian random variable, we do not apply an activation function to it. Thus, the  $\\mathrm{Mult - VA E}^{\\mathrm{PR}}$  with 0-hidden-layer MLP is effectively a log-linear model. We tune the regularization parameter  $\\beta$  for  $\\mathrm{Mult - VA E}^{\\mathrm{PR}}$  following the procedure described in Section 2.2.2. We anneal the Kullback-Leibler term linearly for 200,000 gradient updates. For both  $\\mathrm{Mult - VA E}^{\\mathrm{PR}}$  and Mult-DAE, we apply dropout at the input layer with probability 0.5. We apply a weight decay of 0.01 for Mult-DAE. We do not apply weight decay for any VAE models. We train both  $\\mathrm{Mult - VA E}^{\\mathrm{PR}}$  and Mult-DAE using Adam [23] with batch size of 500 users. For ML-20M, we train for 200 epochs. We train for 100 epochs on the other two datasets. We keep the model with the best validation NDCG@100 and report test set metrics with it.\n\n# 4.4Baselines\n\nWe compare results with the following standard state-of-the-art collaborative filtering models, both linear and non-linear:\n\nWeighted matrix factorization (wMF) [19]: a linear low-rank factorization model. We train wMF with alternating least squares; this generally leads to better performance than with SGD. We set the weights on all the 0's to 1 and tune the weights on all the 1's in the click matrix among  $\\{2,5,10,30,50,100\\}$ , as well as the latent representation dimension  $K \\in \\{100,200\\}$  by evaluating NDCG@100 on validation users.\n\nSLIM [33]: a linear model which learns a sparse item-to-item similarity matrix by solving a constrained  $\\ell_1$ -regularized optimization problem. We grid-search both of the regularization parameters over  $\\{0.1, 0.5, 1, 5\\}$  and report the setting with the best NDCG@100 on validation users. We did not evaluate SLIM on MSD because the dataset is too large for it to finish in a reasonable amount of time (for the Netflix dataset, the parallelized grid search took about two weeks). We also found that the faster approximation of SLIM [26] did not yield competitive performance.\n\nCollaborative denoising autoencoder (CDAE) [51]: augments the standard denoising autoencoder by adding a per-user latent factor to the input. We change the (user, item) entry subsampling strategy in SGD training in the original paper to the user-level subsampling as we did with Mult-VAE and Mult-DAE. We generally find that this leads to more stable convergence and better performance. We set the dimension of the bottleneck layer to 200, and use a weighted square loss, equivalent to what the square loss with negative sampling used in the original paper. We apply tanh activation at both the bottleneck layer as well as the output layer. We use Adam with a batch size of 500 users. As mentioned in Section 3, the number of parameters for CDAE grows linearly with the number of users and items. Thus, it is crucial to control overfitting by applying weight decay. We select the weight decay parameter over  $\\{0.01, 0.1, \\dots, 100\\}$  by examining the validation NDCG@100.\n\nNeural collaborative filtering (NCF) [14]: explores non-linear interactions (via a neural network) between the user and item latent factors. Similar to CDAE, the number of parameters for NCF grows linearly with the number of users and items. We use the publicly available source code provided by the authors, yet cannot obtain\n\ncompetitive performance on the datasets used in this paper - the validation metrics drop within the first few epochs over a wide range of regularization parameters. The authors kindly provided the two datasets (ML-1M and Pinterest) used in the original paper, as well as the training/test split, therefore we separately compare with NCF on these two relatively smaller datasets in the empirical study. In particular, we compare with the hybrid NeuCF model which gives the best performance in He et al. [14], both with and without pre-training.\n\nWe also experiment with Bayesian personalized ranking (BPR) [36]. However, the performance is not on par with the other baselines above. This is consistent with some other studies with similar baselines [40]. Therefore, we do not include BPR in the following results and analysis.\n\n# 4.5 Experimental results and analysis\n\nIn this section, we quantitatively compare our proposed methods with various baselines. In addition, we aim to answer the following two questions:\n\n1. How does multinomial likelihood compare with other commonly used likelihood models for collaborative filtering?  \n2. When does Mult-VAEPR perform better/worse than Mult-DAE?\n\nQuantitative results. Table 2 summarizes the results between our proposed methods and various baselines. Each metric is averaged across all test users. Both Mult-VAE $^{\\text{PR}}$  and Mult-DAE significantly outperform the baselines across datasets and metrics. Mult-VAE $^{\\text{PR}}$  significantly outperforms Mult-DAE on ML-20M and Netflix data-sets. In most of the cases, non-linear models (Mult-VAE $^{\\text{PR}}$ , Mult-DAE, and CDAE) prove to be more powerful collaborative filtering models than state-of-the-art linear models. The inferior results of CDAE on MSD are possibly due to overfitting with the huge number of users and items, as validation metrics drop within the first few epochs even though the training objective continues improving.\n\nWe compare with NCF on the two relatively smaller datasets used in Hu et al. [19]: ML-1M (6,040 users, 3,704 items,  $4.47\\%$  density) and Pinterest (55,187 users, 9,916 items,  $0.27\\%$  density). Because of the size of these two datasets, we use Mult-DAE with a 0-hidden-layer MLP generative model - the overall architecture is  $[I \\to 200 \\to I]$ . (Recall Mult-VAE $^{\\text{PR}}$  with a 0-hidden-layer MLP generative model is effectively a log-linear model with limited modeling capacity.) Table 3 summarizes the results between Mult-DAE and NCF. Mult-DAE significantly outperforms NCF without pre-training on both datasets. On the larger Pinterest dataset, Mult-DAE even improves over the pre-trained NCF model by a big margin.\n\nHow well does multinomial likelihood perform? Despite being commonly used in language models, multinomial likelihoods have typically received less attention in the collaborative filtering literature, especially with latent-factor models. Most previous work builds on Gaussian likelihoods (square loss, Eq. 3) [19, 33, 51] or logistic likelihood (log loss, Eq. 4) [14, 51] instead. We argue in Section 2.1 that multinomial likelihood is in fact a good proxy for the top- $N$  ranking loss and is well-suited for implicit feedback data. To demonstrate the effectiveness of multinomial likelihood, we take the best-performing Mult-VAE $^{PR}$  and Mult-DAE model on each dataset and swap the likelihood distribution model for the data while keeping everything else exactly the same.\n\nTable 2: Comparison between various baselines and our proposed methods. Standard errors are around 0.002 for ML-20M and 0.001 for Netflix and MSD. Both Mult-VAE $^{PR}$  and Mult-DAE significantly outperform the baselines across datasets and metrics. We could not finish SLIM within a reasonable amount of time on MSD.  \n(a) ML-20M  \n\n<table><tr><td></td><td>Recall@20</td><td>Recall@50</td><td>NDCG@100</td></tr><tr><td>Mult-VAEPR</td><td>0.395</td><td>0.537</td><td>0.426</td></tr><tr><td>Mult-DAE</td><td>0.387</td><td>0.524</td><td>0.419</td></tr><tr><td>WMF</td><td>0.360</td><td>0.498</td><td>0.386</td></tr><tr><td>SLIM</td><td>0.370</td><td>0.495</td><td>0.401</td></tr><tr><td>CDAE</td><td>0.391</td><td>0.523</td><td>0.418</td></tr></table>\n\n(b)Netflix  \n\n<table><tr><td></td><td>Recall@20</td><td>Recall@50</td><td>NDCG@100</td></tr><tr><td>Mult-VAEPR</td><td>0.351</td><td>0.444</td><td>0.386</td></tr><tr><td>Mult-DAE</td><td>0.344</td><td>0.438</td><td>0.380</td></tr><tr><td>WMF</td><td>0.316</td><td>0.404</td><td>0.351</td></tr><tr><td>SLIM</td><td>0.347</td><td>0.428</td><td>0.379</td></tr><tr><td>CDAE</td><td>0.343</td><td>0.428</td><td>0.376</td></tr></table>\n\n(c) MSD  \n\n<table><tr><td></td><td>Recall@20</td><td>Recall@50</td><td>NDCG@100</td></tr><tr><td>Mult-VAEPR</td><td>0.266</td><td>0.364</td><td>0.316</td></tr><tr><td>Mult-DAE</td><td>0.266</td><td>0.363</td><td>0.313</td></tr><tr><td>WMF</td><td>0.211</td><td>0.312</td><td>0.257</td></tr><tr><td>SLIM</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CDAE</td><td>0.188</td><td>0.283</td><td>0.237</td></tr></table>\n\nTable 3: Comparison between NCF and Mult-DAE with  $[I\\to$ $200\\rightarrow I]$  architecture. We take the results of NCF from He et al. [14]. Mult-DAE model significantly outperforms NCF without pre-training on both datasets and further improves on Pinterest even comparing with pre-trained NCF.  \n(a) ML-1M  \n\n<table><tr><td></td><td>NCF</td><td>NCF (pre-train)</td><td>Mult-DAE</td></tr><tr><td>Recall@10</td><td>0.705</td><td>0.730</td><td>0.722</td></tr><tr><td>NDCG@10</td><td>0.426</td><td>0.447</td><td>0.446</td></tr></table>\n\n(b) Pinterest  \n\n<table><tr><td></td><td>NCF</td><td>NCF (pre-train)</td><td>Mult-DAE</td></tr><tr><td>Recall@10</td><td>0.872</td><td>0.880</td><td>0.886</td></tr><tr><td>NDCG@10</td><td>0.551</td><td>0.558</td><td>0.580</td></tr></table>\n\nTable 4: Comparison of Mult-VAE $^{\\text{PR}}$  and Mult-DAE with different likelihood functions at the output layer on ML-20M. The standard error is around 0.002 (the results on the other two datasets are similar.) The multinomial likelihood performs better than the other two commonly-used likelihoods from the collaborative filtering literature.  \n\n<table><tr><td></td><td>Recall@20</td><td>Recall@50</td><td>NDCG@100</td></tr><tr><td>Mult-VAEPR</td><td>0.395</td><td>0.537</td><td>0.426</td></tr><tr><td>Gaussian-VAEPR</td><td>0.383</td><td>0.523</td><td>0.415</td></tr><tr><td>Logistic-VAEPR</td><td>0.388</td><td>0.523</td><td>0.419</td></tr><tr><td>Mult-DAE</td><td>0.387</td><td>0.524</td><td>0.419</td></tr><tr><td>Gaussian-DAE</td><td>0.376</td><td>0.515</td><td>0.409</td></tr><tr><td>Logistic-DAE</td><td>0.381</td><td>0.516</td><td>0.414</td></tr></table>\n\nTable 4 summarizes the results of different likelihoods on ML-20M (the results on the other two datasets are similar.) We tune the hyperparameters for each likelihood separately. The multinomial likelihood performs better than the other likelihoods. The gap between logistic and multinomial likelihood is closer - this is understandable since multinomial likelihood can be approximated by individual binary logistic likelihood, a strategy commonly adopted in language modeling [32, 52].\n\nWe wish to emphasize that the choice of likelihood remains data-dependent. For the task of collaborative filtering, the multinomial likelihood achieves excellent empirical results. The methodology behind the partial regularization in Mult-VAE $^{\\mathrm{PR}}$ , however, is a technique we hypothesize will generalize to other domains.\n\nWhen does Mult-VAEPR perform better/worse than Mult-DAE? In Table 2 we can see that both Mult-VAEPR and Mult-DAE produce competitive empirical results with Mult-VAEPR being comparably better. It is natural to wonder when a variational Bayesian inference approach (Mult-VAEPR) will win over using a point estimate (Mult-DAE) and vice versa.\n\nIntuitively,  $\\mathrm{Mult - VA E^{PR}}$  imposes stronger modeling assumptions and therefore could be more robust when user-item interaction data is scarce. To study this, we considered two datasets: ML-20M where  $\\mathrm{Mult - VA E^{PR}}$  has the largest margin over Mult-DAE and MSD where  $\\mathrm{Mult - VA E^{PR}}$  and Mult-DAE have roughly similar performance. The results on the Netflix dataset are similar to ML-20M. We break down test users into quintiles based on their activity level in the fold-in set which is provided as input to the inference model  $g_{\\phi}(\\cdot)$  to make prediction. The activity level is simply the number of items each user has clicked on. We compute NDCG@100 for each group of users using both  $\\mathrm{Mult - VA E^{PR}}$  and Mult-DAE and plot results in Figure 3. This summarizes how performance differs across users with various levels of activity.\n\nIn Figure 3, we show performance across increasing user activity. Error bars represent one standard error. For each subplot, a paired t-test is performed and statistical significance is marked. Although there are slight variations across datasets, Mult-VAE $^{\\text{PR}}$  consistently improves recommendation performance for users who have only clicked on a small number of items. This is particularly prominent for ML-20M (Figure 3a). Interestingly, Mult-DAE actually\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/dc958a59-7ec1-4c2f-ab4f-346792e6b2e9/c6fee837b077f782e2b3301eb22bc7b140cb7ac472587f7be4d07e8d4bedc59a.jpg)  \n(b) MSD  \nFigure 3: NDCG@100 breakdown for users with increasing levels of activity (starting from  $0\\%$ ), measured by how many items a user clicked on in the fold-in set. The error bars represent one standard error. For each subplot, a paired t-test is performed and * indicates statistical significance at  $\\alpha = 0.05$  level, ** at  $\\alpha = 0.01$  level, and *** at  $\\alpha = 0.001$  level. Although details vary across datasets, Mult-VAE $^{\\text{PR}}$  consistently improves recommendation performance for users who have only clicked on a small number of items.\n\noutperforms Mult-VAE $^{\\text{PR}}$  on the most active users. This indicates the stronger prior assumption could potentially hurt the performance when a lot of data is available for a user. For MSD (Figure 3b), the least-active users have similar performance under both Mult-VAE $^{\\text{PR}}$  and Mult-DAE. However, as we described in Section 4.1, MSD is pre-processed so that a user has at least listened to 20 songs. Meanwhile for ML-20M, each user has to watch at least 5 movies. This means that the first bin of ML-20M has much lower user activity than the first bin of MSD.\n\nOverall, we find that  $\\mathrm{Mult - VAE^{PR}}$ , which may be viewed under the lens of a principled Bayesian inference approach, is more robust than the point estimate approach of Mult-DAE, regardless of the scarcity of the data. More importantly, the  $\\mathrm{Mult - VAE^{PR}}$  is less sensitive to the choice of hyperparameters - weight decay is important for Mult-DAE to achieve competitive performance, yet it is not required for  $\\mathrm{Mult - VAE^{PR}}$ . On the other hand, Mult-DAE also has advantages: it requires fewer parameters in the bottleneck layer -  $\\mathrm{Mult - VAE^{PR}}$  requires two sets of parameters to obtain the latent representation  $\\mathbf{z}$ : one set for the variational mean  $\\mu_{\\phi}(\\cdot)$  and another for the variational variance  $\\sigma_{\\phi}(\\cdot)$  - and Mult-DAE is conceptually simpler for practitioners.\n",
  "hyperparameter": "Latent dimension K=200; Hidden layer dimension=600; Architecture: 0 or 1 hidden layers (e.g., [I→600→200→600→I]); Dropout at input=0.5; Weight decay for Mult-DAE=0.01 (no weight decay for VAE); Optimizer: Adam with batch size=500 users; Training epochs: 200 for ML-20M, 100 for Netflix and MSD; KL annealing: linear annealing over 200,000 gradient updates; β tuned via validation (typically β<1 for best performance); Activation function: tanh between layers; No activation at encoder output for VAE (outputs mean and variance directly)"
}