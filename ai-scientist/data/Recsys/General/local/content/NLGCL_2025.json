{
    "id": "NLGCL_2025",
    "paper_title": "NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for Recommendation",
    "alias": "NLGCL",
    "year": 2025,
    "domain": "Recsys",
    "task": "GeneralRecommendation",
    "idea": "NLGCL proposes a graph contrastive learning paradigm that exploits naturally existing contrastive views between adjacent GCN layers (l and l+1) instead of manually generating views. For each node it treats all its neighbors at layer l+1 as multiple positives and remaining nodes as negatives, alleviating the semantic-gap problem of traditional CL and eliminating expensive view-construction overhead.",
    "introduction": "# 1 Introduction\n\nThe proliferation of the internet has led to an explosion of information, making recommender systems indispensable in modern society, including domains such as social media [9, 15, 49], e-commerce [8, 33, 47], and short video platforms [11, 43]. Traditional recommender systems typically model user preferences based on historical user-item interactions [17, 46]. With advances in graph representation learning [20, 39], Graph Neural Networks (GNNs) have been introduced to extract collaborative signals from high-order neighbors, enhancing representation learning in recommender systems [17, 23, 37, 56]. Despite their popularity and effectiveness, learning high-quality user and item representations remains challenging due to the data sparsity problem and label dependency. To mitigate data sparsity, self-supervised learning (SSL) offers a promising avenue by generating supervised signals from unlabeled data [24, 25]. Among SSL paradigms, contrastive learning (CL) [18] has shown potential by maximizing mutual information between positive pairs in contrastive views, thereby leveraging self-supervised signals to enhance learning. Recently, graph contrastive learning (GCL) methods [40, 50, 51] have garnered significant attention in the recommender system field. Typically, GCL methods construct contrastive views using data augmentation techniques such as node deletion, edge perturbation, feature masking, and noise addition [1, 28]. For example, SGL [40] applies stochastic perturbations to nodes and edges, while SimGCL [51] and LightGCL [2] manipulate graph structures via noise addition and singular value decomposition, respectively. Other methods like NCL [22] use clustering algorithms, and DCCF\n\n[29] and BIGCF [54] focus on disentangled representations through SSL.\n\nHowever, existing GCL methods have significant limitations regarding both effectiveness and efficiency, which are discussed and validated in Section 3 by comprehensive investigation. Effectiveness: While augmentation techniques enable models to learn robust and invariant features, they can introduce semantically irrelevant noise [40, 51] that distorts critical structural and feature information. Random alterations may not preserve the underlying semantics of the graph, potentially hindering model performance [22, 29]. Efficiency: Constructing and storing augmented contrastive views increases computational and storage overhead, leading to higher time complexity and reduced scalability [2, 54]. In practical scenarios where quick responses and resource efficiency are crucial, these additional costs limit the applicability of existing methods.\n\nTo this end, we propose a simple yet effective paradigm called Neighbor Layers Graph Contrastive Learning (NLGCL). Unlike existing GCL methods relying on data augmentation, NLGCL leverages naturally existing contrastive views within GNNs. Specifically, NLGCL treats each node and its neighbors in the next layer as positive pairs, and each node and other nodes as negative pairs. In the recommendation field, graphs are constructed from historical user-item interactions. Users and items with similar interaction histories share similar semantic information after neighbor aggregation in GNNs. By utilizing the naturally existing contrastive views between neighbor layers, NLGCL avoids introducing irrelevant noise and eliminates the need for additional augmented views, reducing computational costs. We conduct extensive experiments to validate the superiority of our NLGCL over various state-of-the-art baselines in terms of effectiveness and efficiency.",
    "method": "# 4 Methodology\n\nIn this section, we introduce a novel CL-based recommendation paradigm called Neighbor Layers Graph Contrastive Learning (NLGCL). Specifically, we first introduce the naturally existing contrastive views in neighbor layers. Then, we detailed our tailored loss function. Finally, a deeper analysis of our NLGCL is further proposed.\n\n# 4.1 Contrastive Views\n\nWe use LightGCN as our graph convolution backbone, obtaining node embeddings  $\\mathbf{e}_u^{(0)} / \\mathbf{e}_i^{(0)},\\mathbf{e}_u^{(1)} / \\mathbf{e}_i^{(1)},\\dots,\\mathbf{e}_u^{(L)} / \\mathbf{e}_i^{(L)}$ , at each layer, as in Eq (4). We introduce two scopes of contrastive views: (a) heterogeneous and (b) entire. In the heterogeneous scope, based on the analysis in Section 3.3, all item embeddings in layer  $(l - 1)$  and all user embeddings in layer  $l$  form one pair of naturally contrastive views; conversely, all user embeddings in layer  $(l - 1)$  and all item embeddings in layer  $l$  form another pair. In the entire scope, we consider all embeddings in layer  $(l - 1)$  and all embeddings in layer  $l$  as naturally contrastive views. Next, we define the positive and negative pairs within these contrastive views.\n\n4.1.1 Positive Pairs. Two scopes have the same positive pairs. Given a user  $u$ , the neighbor set for  $u$  is  $\\mathcal{N}_u$ . For user  $u$ , the  $(l)$ -th layer embeddings of  $u$ 's neighbors construct the positive pairs with the  $(l-1)$ -th layer embedding  $\\mathbf{e}_u^{(l-1)}$  of user  $u$ . Specifically,  $\\mathbf{e}_u^{(l-1)}$  constructs positive pairs with  $\\mathbf{e}_{\\bar{i}}^{(l)}$ , where  $\\bar{i} \\in \\mathcal{N}_u$ . The reason for this because  $\\mathbf{e}_{\\bar{i}}^{(l)}$  are weighted aggregated by  $\\mathbf{e}_{\\bar{u}}^{(l-1)}$ , where  $\\bar{u} \\in \\mathcal{N}_{\\bar{i}}$  (as Eq (5)). Similar positive pairs are defined for item  $i$ .\n\n4.1.2 Negative Pairs. In the heterogeneous scope, the  $(l)$ -th layer embeddings of all items, excluding items  $\\tilde{i}$ , construct negative pairs\n\nwith the  $(l - 1)$ -th layer embedding  $\\mathbf{e}_u^{(l - 1)}$  of user  $u$ , where  $\\tilde{i} \\in \\mathcal{N}_u$ . Similar negative pairs are defined for item  $i$ . In the entire scope, the  $(l)$ -th layer embeddings of all users additionally construct negative pairs with the  $(l - 1)$ -th layer embedding  $\\mathbf{e}_u^{(l - 1)}$  of user  $u$ , while still maintains all negative pairs in the heterogeneous scope. Similar negative pairs are defined for item  $i$ . We categorize the positive and negative pairs in Table 1.\n\n\nTable 1: Positive and Negative pairs in Contrastive Views.\n\n\n<table><tr><td>Node</td><td>Scope</td><td>Positive Pairs</td><td>Negative Pairs</td></tr><tr><td>u: e(u(l-1))</td><td>Heterogeneous</td><td>e(i)l|i∈Nu</td><td>e(i)l|i∉Nu</td></tr><tr><td>u: e(u(l-1))</td><td>Entire</td><td>e(i)l|i∈Nu</td><td>e(i)l∪e(u(l)l|i∉Nu,u∈U</td></tr><tr><td>i: e(i(l-1))</td><td>Heterogeneous</td><td>e(i)l|i∈Ni</td><td>e(i)l|i∉Ni</td></tr><tr><td>i: e(i(l-1))</td><td>Entire</td><td>e(i)l|i∈Ni</td><td>e(i)l∪e(i)l|i∉Ni,i∈I</td></tr></table>\n\nAnalysis: Unlike traditional contrastive learning, our approach associates each node with multiple positive samples, where noise among positives arises from semantically similar nodes. This enhances the alignment of positive samples while reducing the impact of random noise. Additionally, it addresses the limitation of traditional CL, which arbitrarily treats semantically similar nodes as negatives. Both scopes use the same positive sample selection, but the entire scope provides a broader range of negatives. While a larger negative sample scope can improve positive feature learning, it may hinder semantic alignment between users and items and increase computational costs. We empirically validate the effectiveness and efficiency of both scopes in Sections 5.3 and 5.4.\n\n# 4.2 Contrastive Learning Loss\n\nAs discussed in Section 3.2, traditional CL considers a node and its counterpart in another view as positive pairs, while treating the node and all other nodes in different views as negative pairs. However, having a small number of positive pairs and arbitrarily defined negative pairs can irrationally push nodes with similar semantics farther away. In contrast, our tailored CL method assigns each node a set of positive pairs, enhancing alignment and reducing the impact of random noise. In an  $L$ -layer GNN, up to  $L$  groups of contrastive views can be constructed, but this incurs additional computational overhead. Let  $G$  denote the number of contrastive view groups, where  $G \\leq L$ . Selecting these  $G$  groups is crucial, and\n\nwe theoretically prove in Theorem 1 (Appendix A.1) that the first  $G$  groups are optimal.\n\nTHEOREM 1. In GCNs, contrastive views that naturally exist between layer  $(l)$  and layer  $(l + 1)$  are more effective for contrastive learning when  $(l)$  is smaller.\n\nBased on Theorem 1, selecting the first  $G$  layers and their neighbor layers as contrastive views achieves optimal results. We introduce our tailored CL loss for two scopes of contrastive views, respectively.\n\n4.2.1 Heterogeneous Scope. For the heterogeneous scope, our neighbor layer CL loss can be formulated as:\n\n$$\n\\mathcal {L} _ {n l _ {u}} = - \\frac {1}{G | \\mathcal {U} |} \\sum_ {g = 0} ^ {G - 1} \\sum_ {u \\in \\mathcal {U}} \\frac {1}{| \\mathcal {N} _ {u} |} \\log \\frac {\\prod_ {i ^ {+} \\in \\mathcal {N} _ {u}} \\exp \\left(\\mathbf {e} _ {u} ^ {(g) ^ {\\top}} \\mathbf {e} _ {i ^ {+}} ^ {(g + 1)} / \\tau\\right)}{\\sum_ {\\hat {i} \\in \\mathcal {I}} \\exp \\left(\\mathbf {e} _ {u} ^ {(g) ^ {\\top}} \\mathbf {e} _ {\\hat {i}} ^ {(g + 1)} / \\tau\\right)}, \\tag {6}\n$$\n\n$$\n\\mathcal {L} _ {n l _ {i}} = - \\frac {1}{G | \\mathcal {I} |} \\sum_ {g = 0} ^ {G - 1} \\sum_ {i \\in \\mathcal {I}} \\frac {1}{| \\mathcal {N} _ {i} |} \\log \\frac {\\prod_ {u ^ {+} \\in \\mathcal {N} _ {i}} \\exp \\left(\\mathbf {e} _ {i} ^ {(g) ^ {\\top}} \\mathbf {e} _ {u ^ {+}} ^ {(g + 1)} / \\tau\\right)}{\\sum_ {\\hat {u} \\in \\mathcal {U}} \\exp \\left(\\mathbf {e} _ {i} ^ {(g) ^ {\\top}} \\mathbf {e} _ {\\hat {u}} ^ {(g + 1)} / \\tau\\right)}, \\tag {7}\n$$\n\nwhere positive samples  $i^{+} / u^{+}$  refer to all neighbors of each  $u / i$ , while negative samples  $\\hat{i} / \\hat{u}$  are drawn from the entire set of  $\\mathcal{I}$  or  $\\mathcal{U}$ . We compute the average over the first  $G$  layers for all users/items.\n\n4.2.2 Entire Scope. For the entire scope, our neighbor layer CL loss can be formulated as:\n\n$$\n\\mathcal {L} _ {n l _ {u}} = - \\frac {1}{G | \\mathcal {U} |} \\sum_ {g = 0} ^ {G - 1} \\sum_ {u \\in \\mathcal {U}} \\frac {1}{| \\mathcal {N} _ {u} |} \\log \\frac {\\prod_ {i ^ {+} \\in \\mathcal {N} _ {u}} \\exp \\left(\\mathrm {e} _ {u} ^ {(g) ^ {\\top}} \\mathrm {e} _ {i ^ {+}} ^ {(g + 1)} / \\tau\\right)}{\\sum_ {\\hat {x} \\in \\mathcal {V}} \\exp \\left(\\mathrm {e} _ {u} ^ {(g) ^ {\\top}} \\mathrm {e} _ {\\hat {x}} ^ {(g + 1)} / \\tau\\right)}, \\tag {8}\n$$\n\n$$\n\\mathcal {L} _ {n l _ {i}} = - \\frac {1}{G | \\mathcal {I} |} \\sum_ {g = 0} ^ {G - 1} \\sum_ {i \\in \\mathcal {I}} \\frac {1}{| \\mathcal {N} _ {i} |} \\log \\frac {\\prod_ {u ^ {+} \\in \\mathcal {N} _ {i}} \\exp \\left(\\mathbf {e} _ {i} ^ {(g) ^ {\\top}} \\mathbf {e} _ {u ^ {+}} ^ {(g + 1)} / \\tau\\right)}{\\sum_ {\\hat {x} \\in \\mathcal {V}} \\exp \\left(\\mathbf {e} _ {i} ^ {(g) ^ {\\top}} \\mathbf {e} _ {\\hat {x}} ^ {(g + 1)} / \\tau\\right)}, \\tag {9}\n$$\n\nwhere positive samples  $i^{+} / u^{+}$  refer to all neighbors of each  $u / i$ , while negative samples  $\\hat{x}$  are drawn from the entire set  $\\mathcal{V} = \\mathcal{I} \\cup \\mathcal{U}$ . We compute the average over the first  $G$  layers for all users/items.\n\nFor any scope, we get the final loss  $\\mathcal{L}_{nl}$  by summing both userside loss  $\\mathcal{L}_{n l_{u}}$  and item-side loss  $\\mathcal{L}_{n l_{i}}$ , formally:  $\\mathcal{L}_{nl} = \\mathcal{L}_{n l_u} + \\mathcal{L}_{n l_i}$ .\n\nEfficiency: Although our NLGCL introduces a computational complexity proportional to  $G$ , the cost of constructing contrastive views in traditional CL is significantly higher than the cost of computing the CL loss itself. By leveraging the naturally existing contrastive views within GCNs, our NLGCL eliminates the substantial overhead of view construction. Detailed analysis of time efficiency is provided in Appendices A.2.\n\n# 4.3 Model Optimization\n\nTo extract node representations in NLGCL, we adopt a multi-task training strategy that jointly optimizes the traditional BPR loss (Eq (2)) and our neighbor layers graph contrastive learning loss:\n\n$$\n\\mathcal {L} = \\mathcal {L} _ {b p r} + \\lambda_ {1} \\mathcal {L} _ {n l} + \\lambda_ {2} \\| \\boldsymbol {\\Theta} \\| ^ {2}, \\tag {10}\n$$\n\nwhere  $\\lambda_{1}$  and  $\\lambda_{2}$  are balancing hyper-parameters of CL and  $L_{2}$  regularization term, respectively.  $\\Theta$  denotes model parameters.",
    "experiments": "# 5 Experiments\n\nIn this section, we briefly describe our experimental settings and then conduct extensive experiments on four public datasets to evaluate our proposed NLGCL by answering the following research questions: RQ1: How does our proposed NLGCL perform in comparison to various state-of-the-art recommender systems? RQ2: How do the different scopes of our NLGCL impact its performance? RQ3: How efficient is our NLGCL compared with various state-of-the-art recommender systems? RQ4: Can our NLGCL learn high-quality representations with uniform distribution? RQ5: How do different hyper-parameters influence?\n\n# 5.1 Experimental Settings\n\n5.1.1 Datasets. To validate the effectiveness of NLGCL, we conduct experiments on four public datasets, namely Yelp [22], Pinterest [13], QB-Video [52], and Alibaba [6]. These datasets originate from diverse domains, exhibiting variances in both scale and sparsity. To ensure the quality of the data, we employ the 15-core setting for the Yelp and Alibaba datasets, which ensures a minimum of 15 interactions between users and items. For the Pinterest and QB-Video datasets, users and items with less than 5 interactions are filtered out. Table 2 summarizes statistical information for all datasets. We use the user-based split approach to divide the data for experimentation. Specifically, we divide training, validation, and test sets into a ratio of 8:1:1, respectively. Moreover, we employ pair-wise sampling, where a negative interaction is randomly chosen for each interaction in the training set to construct the training sample.\n\n5.1.2 Metrics. To evaluate the top- $K$  recommendation task performance fairly, we adopt two widely-used metrics: Recall and Normalized Discounted Cumulative Gain (NDCG). The values of  $K$  are set to 10, 20, 50. Following previous work [22], we adopt the all-rank protocol which ranks all candidate items that users have not interacted with during evaluation.\n\n5.1.3 Implementation Details. To ensure a fair comparison, we implement our NLGCL² and all baselines using the RecBole and RecBole-GNN [55], unified frameworks for traditional recommendation models and graph-based traditional recommendation models, respectively. We use the Adam optimizer [19] and Xavier initialization [14] with default parameters, and conduct extensive hyper-parameter tuning for all baselines. For our NLGCL, we fix the batch size to 4096, set  $\\lambda_{2}$  for  $L_{2}$  regularization to  $10^{-4}$ , and use an embedding size of 64. Early stopping is applied with an epoch limit of 20, monitored by NDCG@10. For our NLGCL, we tune hyper-parameters  $\\lambda_{1} \\in \\{10^{-6}, 10^{-5}, 10^{-4}\\}$ , temperature  $\\tau \\in$\n\n\nTable 3: Performance comparison of baselines and our NLGCL in terms of Recall@K(R@K) and NDCG@K(N@K). The superscript * indicates the improvement is statistically significant where the  $p$ -value is less than 0.01.\n\n\n<table><tr><td rowspan=\"2\"></td><td>Model</td><td>MF-BPR</td><td>NGCF</td><td>LightGCN</td><td>IMP-GCN</td><td>LayerGCN</td><td>SGL</td><td>NCL</td><td>SimGCL</td><td>LightGCL</td><td>DCCF</td><td>BIGCF</td><td>NLGCL</td><td rowspan=\"2\">Improv.</td></tr><tr><td>Metric</td><td>[30]</td><td>[37]</td><td>[17]</td><td>[23]</td><td>[56]</td><td>[40]</td><td>[22]</td><td>[51]</td><td>[2]</td><td>[29]</td><td>[54]</td><td>Our</td></tr><tr><td rowspan=\"6\">Yelp</td><td>R@10</td><td>0.0643</td><td>0.0630</td><td>0.0730</td><td>0.0751</td><td>0.0771</td><td>0.0833</td><td>0.0902</td><td>0.0908</td><td>0.0766</td><td>0.0818</td><td>0.0880</td><td>0.0952*</td><td>4.85%</td></tr><tr><td>R@20</td><td>0.1043</td><td>0.1026</td><td>0.1163</td><td>0.1182</td><td>0.1208</td><td>0.1288</td><td>0.1325</td><td>0.1331</td><td>0.1188</td><td>0.1268</td><td>0.1311</td><td>0.1414*</td><td>6.24%</td></tr><tr><td>R@50</td><td>0.1862</td><td>0.1864</td><td>0.2016</td><td>0.2017</td><td>0.2041</td><td>0.2100</td><td>0.2127</td><td>0.2130</td><td>0.2008</td><td>0.2107</td><td>0.2119</td><td>0.2327*</td><td>9.25%</td></tr><tr><td>N@10</td><td>0.0458</td><td>0.0446</td><td>0.0520</td><td>0.0539</td><td>0.0560</td><td>0.0601</td><td>0.0673</td><td>0.0682</td><td>0.0551</td><td>0.0596</td><td>0.0630</td><td>0.0713*</td><td>4.55%</td></tr><tr><td>N@20</td><td>0.0580</td><td>0.0567</td><td>0.0652</td><td>0.0662</td><td>0.0684</td><td>0.0739</td><td>0.0811</td><td>0.0823</td><td>0.0681</td><td>0.0741</td><td>0.0779</td><td>0.0859*</td><td>4.37%</td></tr><tr><td>N@50</td><td>0.0793</td><td>0.0784</td><td>0.0875</td><td>0.0885</td><td>0.0901</td><td>0.0964</td><td>0.1033</td><td>0.1039</td><td>0.0899</td><td>0.0974</td><td>0.1005</td><td>0.1094*</td><td>5.29%</td></tr><tr><td rowspan=\"6\">Pinterest</td><td>R@10</td><td>0.0855</td><td>0.0870</td><td>0.1000</td><td>0.0985</td><td>0.1004</td><td>0.1080</td><td>0.1033</td><td>0.1051</td><td>0.0881</td><td>0.1040</td><td>0.1040</td><td>0.1148*</td><td>6.30%</td></tr><tr><td>R@20</td><td>0.1409</td><td>0.1428</td><td>0.1621</td><td>0.1603</td><td>0.1620</td><td>0.1704</td><td>0.1609</td><td>0.1576</td><td>0.1322</td><td>0.1613</td><td>0.1619</td><td>0.1793*</td><td>5.22%</td></tr><tr><td>R@50</td><td>0.2599</td><td>0.2633</td><td>0.2862</td><td>0.2845</td><td>0.2880</td><td>0.2963</td><td>0.2887</td><td>0.2442</td><td>0.2383</td><td>0.2871</td><td>0.2894</td><td>0.3089*</td><td>4.25%</td></tr><tr><td>N@10</td><td>0.0537</td><td>0.0545</td><td>0.0635</td><td>0.0624</td><td>0.0635</td><td>0.0701</td><td>0.0666</td><td>0.0705</td><td>0.0534</td><td>0.0661</td><td>0.0680</td><td>0.0760*</td><td>7.80%</td></tr><tr><td>N@20</td><td>0.0708</td><td>0.0721</td><td>0.0830</td><td>0.0814</td><td>0.0826</td><td>0.0897</td><td>0.0833</td><td>0.0871</td><td>0.0673</td><td>0.0828</td><td>0.0864</td><td>0.0948*</td><td>5.69%</td></tr><tr><td>N@50</td><td>0.1001</td><td>0.1018</td><td>0.1136</td><td>0.1113</td><td>0.1121</td><td>0.1209</td><td>0.1150</td><td>0.1086</td><td>0.0981</td><td>0.1157</td><td>0.1155</td><td>0.1267*</td><td>4.80%</td></tr><tr><td rowspan=\"6\">QB-Video</td><td>R@10</td><td>0.1120</td><td>0.1261</td><td>0.1275</td><td>0.1278</td><td>0.1291</td><td>0.1341</td><td>0.1372</td><td>0.1337</td><td>0.1330</td><td>0.1350</td><td>0.1341</td><td>0.1427*</td><td>4.01%</td></tr><tr><td>R@20</td><td>0.1741</td><td>0.1943</td><td>0.1969</td><td>0.1966</td><td>0.1990</td><td>0.2030</td><td>0.2089</td><td>0.2030</td><td>0.2018</td><td>0.2044</td><td>0.2056</td><td>0.2167*</td><td>3.73%</td></tr><tr><td>R@50</td><td>0.2969</td><td>0.3237</td><td>0.3251</td><td>0.3259</td><td>0.3271</td><td>0.3342</td><td>0.3414</td><td>0.3356</td><td>0.3258</td><td>0.3281</td><td>0.3423</td><td>0.3501*</td><td>2.28%</td></tr><tr><td>N@10</td><td>0.0782</td><td>0.0888</td><td>0.0900</td><td>0.0899</td><td>0.0904</td><td>0.0932</td><td>0.0964</td><td>0.0945</td><td>0.0921</td><td>0.0951</td><td>0.0928</td><td>0.0996*</td><td>3.32%</td></tr><tr><td>N@20</td><td>0.0977</td><td>0.1099</td><td>0.1109</td><td>0.1105</td><td>0.1123</td><td>0.1141</td><td>0.1189</td><td>0.1151</td><td>0.1130</td><td>0.1150</td><td>0.1144</td><td>0.1219*</td><td>2.52%</td></tr><tr><td>N@50</td><td>0.1303</td><td>0.1438</td><td>0.1448</td><td>0.1447</td><td>0.1461</td><td>0.1482</td><td>0.1530</td><td>0.1501</td><td>0.1459</td><td>0.1508</td><td>0.1503</td><td>0.1573*</td><td>2.81%</td></tr><tr><td rowspan=\"6\">Alibaba</td><td>R@10</td><td>0.0303</td><td>0.0382</td><td>0.0457</td><td>0.0400</td><td>0.0448</td><td>0.0461</td><td>0.0477</td><td>0.0474</td><td>0.0459</td><td>0.0490</td><td>0.0502</td><td>0.0531*</td><td>5.78%</td></tr><tr><td>R@20</td><td>0.0467</td><td>0.0615</td><td>0.0692</td><td>0.0635</td><td>0.0680</td><td>0.0692</td><td>0.0713</td><td>0.0691</td><td>0.0716</td><td>0.0729</td><td>0.0744</td><td>0.0773*</td><td>3.90%</td></tr><tr><td>R@50</td><td>0.0799</td><td>0.1081</td><td>0.1144</td><td>0.1110</td><td>0.1138</td><td>0.1141</td><td>0.1165</td><td>0.1092</td><td>0.1204</td><td>0.1199</td><td>0.1205</td><td>0.1238*</td><td>2.74%</td></tr><tr><td>N@10</td><td>0.0161</td><td>0.0198</td><td>0.0246</td><td>0.0221</td><td>0.0238</td><td>0.0248</td><td>0.0259</td><td>0.0262</td><td>0.0239</td><td>0.0257</td><td>0.0266</td><td>0.0299*</td><td>12.41%</td></tr><tr><td>N@20</td><td>0.0203</td><td>0.0257</td><td>0.0299</td><td>0.0271</td><td>0.0285</td><td>0.0307</td><td>0.0319</td><td>0.0317</td><td>0.0305</td><td>0.0311</td><td>0.0322</td><td>0.0361*</td><td>12.11%</td></tr><tr><td>N@50</td><td>0.0269</td><td>0.0349</td><td>0.0396</td><td>0.0369</td><td>0.0393</td><td>0.0396</td><td>0.0409</td><td>0.0397</td><td>0.0410</td><td>0.0404</td><td>0.0415</td><td>0.0450*</td><td>8.43%</td></tr></table>\n\n\nTable 4: Efficiency comparison of different methods across four datasets, including average training time per epoch, number of epochs to converge, total training time (T/E: Time/Epoch, #E: #Epoch, TT: Total Time, N@10: NDCG@10; s: second, m: minute, h: hour). We highlight the optimal and suboptimal models for the TT metric in bold and underline, respectively.\n\n\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">Yelp</td><td colspan=\"4\">Pinterest</td><td colspan=\"4\">QB-Video</td><td colspan=\"4\">Alibaba</td></tr><tr><td>T/E↓</td><td>#E↓</td><td>TT↓</td><td>N@10↑</td><td>T/E↓</td><td>#E↓</td><td>TT↓</td><td>N@10↑</td><td>T/E↓</td><td>#E↓</td><td>TT↓</td><td>N@10↑</td><td>T/E↓</td><td>#E↓</td><td>TT↓</td><td>N@10↑</td></tr><tr><td>LightGCN</td><td>22.42s</td><td>332</td><td>2h9m</td><td>0.0520</td><td>8.40s</td><td>194</td><td>27m</td><td>0.0635</td><td>25.32s</td><td>283</td><td>1h59m</td><td>0.0900</td><td>21.55s</td><td>371</td><td>2h13m</td><td>0.0246</td></tr><tr><td>SGL</td><td>104.19s</td><td>55</td><td>1h36m</td><td>0.0601</td><td>42.71s</td><td>43</td><td>31m</td><td>0.0701</td><td>124.44s</td><td>61</td><td>2h7m</td><td>0.0932</td><td>95.37s</td><td>83</td><td>2h12m</td><td>0.0248</td></tr><tr><td>NCL</td><td>94.42s</td><td>102</td><td>2h41m</td><td>0.0673</td><td>35.10s</td><td>83</td><td>49m</td><td>0.0666</td><td>108.98s</td><td>90</td><td>2h43m</td><td>0.0964</td><td>85.79s</td><td>88</td><td>2h6m</td><td>0.0477</td></tr><tr><td>SimGCL</td><td>98.33s</td><td>155</td><td>4h14m</td><td>0.0682</td><td>37.31s</td><td>121</td><td>1h15m</td><td>0.0705</td><td>110.92s</td><td>167</td><td>5h9m</td><td>0.0945</td><td>87.31s</td><td>241</td><td>5h51m</td><td>0.0262</td></tr><tr><td>LightGCL</td><td>100.51s</td><td>64</td><td>1h47m</td><td>0.0551</td><td>38.87s</td><td>51</td><td>33m</td><td>0.0534</td><td>119.18s</td><td>57</td><td>1h53m</td><td>0.0921</td><td>92.24s</td><td>69</td><td>1h46m</td><td>0.0459</td></tr><tr><td>DCCF</td><td>221.31s</td><td>38</td><td>2h20m</td><td>0.0596</td><td>76.39s</td><td>32</td><td>41m</td><td>0.0661</td><td>228.21s</td><td>38</td><td>2h25m</td><td>0.0951</td><td>180.08s</td><td>49</td><td>2h27m</td><td>0.0257</td></tr><tr><td>BIGCF</td><td>208.19s</td><td>41</td><td>2h22m</td><td>0.0630</td><td>70.11s</td><td>35</td><td>41m</td><td>0.0680</td><td>206.60s</td><td>38</td><td>2h11m</td><td>0.0928</td><td>167.12s</td><td>42</td><td>1h57m</td><td>0.0266</td></tr><tr><td>NLGCL-H</td><td>46.93s</td><td>63</td><td>49m</td><td>0.0713</td><td>19.55s</td><td>40</td><td>13m</td><td>0.0760</td><td>53.19s</td><td>55</td><td>49m</td><td>0.0996</td><td>44.73s</td><td>74</td><td>55m</td><td>0.0299</td></tr><tr><td>NLGCL-E</td><td>61.32s</td><td>82</td><td>1h28m</td><td>0.0689</td><td>28.81s</td><td>51</td><td>24m</td><td>0.0729</td><td>72.99s</td><td>62</td><td>1h15m</td><td>0.0969</td><td>59.67s</td><td>79</td><td>1h19m</td><td>0.0279</td></tr></table>\n\n$\\{0.1, 0.2, 0.3, 0.4\\}$ , the number of GCN layers  $L \\in \\{1, 2, 3, 4\\}$ , and the number of contrastive view groups  $G \\in \\{1, \\dots, L\\}$ .\n\n5.1.4 Baselines. To assess the effectiveness of NLGCL, we compare it against two categories of baselines: (1) CF-based models (MF-BPR [30], NGCF [37], LightGCN [17], IMP-GCN [23], and LayerGCN [56]) and (2) CL-based models (SGL [40], NCL [22], SimGCL [51], LightGCL [2], DCCF [29], and BIGCF [54]).\n\n# 5.2 Overall Performance (RQ1)\n\nTable 3 reports the recommendation performance of all baselines on four public datasets. From the results, we observed that:\n\nObservation1: Our proposed NLGCL achieves the best performance, outperforming all baselines and demonstrating its effectiveness in recommendation tasks. Specifically, NLGCL improves\n\nNDCG@10 by  $4.55\\%$ ,  $5.69\\%$ ,  $2.52\\%$ , and  $12.11\\%$  on Yelp, Pinterest, QB-Video, and Alibaba, respectively, compared to the strongest baseline. These results highlight NLGCL's strong capability in providing personalized recommendations.\n\n**Observation2:** Across all datasets, CL-based methods generally outperform CF-based methods, highlighting the benefits of integrating contrastive learning with collaborative filtering. However, existing CL-based methods often lose crucial information and introduce noise during view construction. In contrast, NLGCL mitigates this issue by associating multiple positive samples with each node, where noise among these samples reflects other nodes with similar preferences.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/e5c6e3e5e4836592e63a62d5c7809315ecea4193063d5618d35ebcb893c6286e.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/981d4c23928279fa25b8154b60e8a454788ff5352de86145549fc56789baf787.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/0707ba77b30cd4470b9841e93561a2022d504b77626e5b318f6070f269775c75.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/a5d107b7a5acf40660f3a145198025903d1306a4192eab97cef449216deec389.jpg)\n\n\n\nFigure 2: The influence of the scope of contrastive views.\n\n\n# 5.3 The Influence of Different Scopes in Contrastive Views (RQ2)\n\nTo validate the effectiveness of different scopes of contrastive views, we conduct experiments on all datasets. We design the following variants: 1) NLGCL-H, which adopts the heterogeneous scope. 2) NLGCL-E, which adopts the entire scope. In Figure 2, we observed that NLGCL-H consistently outperforms NLGCL-E across all datasets. This advantage is attributable to the scope of NLGCL-H being more accurate than NLGCL-E. Specifically, there is an inevitable semantic gap between users and items. To this end, arbitrarily treating all nodes as negative samples will aggravate semantic noise in user/item representation and hurt recommendation accuracy. Moreover, NLGCL-E also requires higher computational resources than NLGCL-H, which we detail in Section 5.4.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/693c69bbceb7084a45d822482b069dac69a057022298317a6509cd3271be2664.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/4bd933ca7916594d2769ca82a02c5051bc360f586fa6f0f59fc8c8b86342e99f.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/d71bf2345a3917f5b3a5fe1609cda705169184b16226faeb13a0b177c9325c2f.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-07/25a7f31b-2eb8-44c4-be77-fd016f78402b/6d4469ab6947e564be5ec42fae85a20c61a28cdd2eef070fd3f78518892b2817.jpg)\n\n\n\nHeterogeneous with  $G = 1$\n\n\n\nHeterogeneous with  $G = 2$\n\n\n\nHeterogeneous with  $G = 3$\n\n\n\nEntire with  $G = 1$\n\n\n\nEntire with  $G = 2$\n\n\n\nEntire with  $G = 3$\n\n\n\nFigure 3: Efficiency study in terms of NDCG@10.\n\n\n# 5.4 Efficiency Study (RQ3)\n\nWe provide theoretical analysis for the efficiency of our proposed NLGCL in Appendix A.2. In this section, we further validate it with an empirical study. We conduct experiments across all CL-based baselines using four datasets. In Table 4, we present the efficiency\n\nof NLGCL and CL-based baselines in terms of average training time per epoch, the number of epochs to converge, and total training time. Our results show that NLGCL achieves competitive per-epoch training efficiency and the fastest convergence among all models. This indicates that NLGCL not only reduces computational time but also reaches optimal performance swiftly, making it ideal for scenarios with limited time or computational resources, such as rapid deployment and frequent updates. As analyzed in Appendix A.2, our NLGCL-H and NLGCL-E further reduce training time by adjusting hyper-parameter  $G$ . Figure 3 illustrates the trade-off between performance and efficiency across different values of  $G$ . By tuning  $G$ , NLGCL adapts to diverse computational environments, balancing efficiency and effectiveness. This flexibility ensures its suitability for real-world applications, even in resource-limited scenarios, while maintaining strong performance.\n\n# 5.5 Visualization Analysis (RQ4)\n\nWe validate the effectiveness of NLGCL in learning high-quality, uniformly distributed representations, thereby maximizing the benefits of CL. Specifically, we randomly sample 3,000 users from the Yelp and Pinterest datasets and project their final embeddings from the optimal model onto 2-dimensional unit vectors using t-SNE [34]. As shown in Figure 4, we plot the feature distributions and estimate their angular densities using Gaussian Kernel Density Estimation (KDE) [7]. Figure 4 demonstrates that the representations learned by NLGCL are more uniformly distributed. Consistent with prior studies [35, 36, 51, 54], such uniform distribution enhances the intrinsic quality of representations and maximizes information retention, leading to a better ability to preserve unique user preferences.\n\n# 5.6 Hyper-parameter Study (RQ5)\n\nThis section investigates the sensitivity of hyper-parameters on the recommendation performance of NLGCL. The evaluation results in terms of NDCG@10 are reported in Figure 5 and Figure 6.\n\nPerformance Comparison w.r.t.  $\\lambda_{1}$  and  $\\tau$ : We analyze the hyperparameter sensitivity for contrastive learning by varying the balancing hyper-parameter  $\\lambda_{1}$  from  $10^{-6}$  to  $10^{-4}$  and temperature parameter  $\\tau$  from 0.1 to 0.4. As shown in Figure 5, NLGCL consistently achieves the best performance with  $\\lambda_{1} = 10^{-5}$  and  $\\tau = 0.2$  across all datasets, aligning with hyper-parameter settings in traditional CL.\n\nPerformance Comparison w.r.t.  $L$  and  $G$ : We analyze the influence of GCN layer number  $L$  and contrastive view group number  $G$  across all datasets. Figure 6 shows that  $G = 2$  is optimal across all datasets, while the optimal  $L$  is 2 for Yelp and Alibaba datasets, and 3 for Pinterest and QB-Video datasets. Notably, even with  $G = 1$ , NLGCL maintains strong performance, consistent with the efficiency analysis in Section 5.4.",
    "hyperparameter": "embedding_size=64, batch_size=4096, L2_reg(λ₂)=1e-4, CL_weight(λ₁)=1e-5, temperature(τ)=0.2, GCN_layers(L)∈{2,3} (2 for Yelp/Alibaba, 3 for Pinterest/QB-Video), contrastive_groups(G)=2, early-stop_patience=20 epochs, optimizer=Adam with Xavier init."
  }