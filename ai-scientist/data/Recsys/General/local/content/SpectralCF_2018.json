{
  "id": "SpectralCF_2018",
  "paper_title": "Spectral Collaborative Filtering",
  "alias": "SpectralCF",
  "year": 2018,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "SpectralCF performs collaborative filtering directly in the spectral domain of user-item bipartite graphs by applying graph Fourier transforms and learnable spectral convolution filters. The core innovation is using a polynomial-approximated spectral convolution operation (limited to order P=1) that dynamically adjusts the importance of different frequency components to capture rich graph structural information, enabling better learning of user-item relationships compared to traditional spatial domain methods. The model stacks multiple spectral convolution layers and concatenates features from all layers to form final user and item representations.",
  "introduction": "# 1 INTRODUCTION\n\nThe effectiveness of recommender systems (RS) often relies on how well users' interests or preferences can be understood and interactions between users and items can be modeled. Collaborative Filtering (CF) [19] is one of the widely used and prominent techniques for RS. The underlying assumption of the CF approach is that if a user  $u_{1}$  shares a common item with another user  $u_{2}$ ,  $u_{1}$  is also likely to be interested in other items liked by  $u_{2}$ . Although CF has been successfully applied to many recommendation applications, the cold-start problem is considered as one of its major challenges [19]. The problem arises when a user interacted with a very small number of items. Consequently, the user shares few items with other users, and effectively recommending for the user becomes a challenging task for RS.\n\nIf we formulate the relationships between users and items as a bipartite graph $^1$ , we argue that the connectivity information of the graph can play an important role for tackling the cold-start problem. For example, let us see a bipartite graph  $\\mathcal{B}$  in Figure 1. A cold-start user  $u_{1}$  only interacts with item  $i_{1}$ . Since  $u_{1}$  shares  $i_{1}$  with\n\nuser  $u_{2}$  and user  $u_{3}$ , as a result, three items  $(i_{2}, i_{3}$  and  $i_{4})$  connected with  $u_{2}$  or  $u_{3}$  can all be recommended to  $u_{1}$  by a CF-based model. However, a natural and important question arises: which one in the three items is the most reliable recommendation for  $u_{1}$ ? The key to answer the question lies in the user-item connectivity information. In fact, if we take a look at the connections of the graph, it is clear that there is only one path existing between  $u_{1}$  and  $i_{2}$  (or  $i_{3}$ ), while two paths connect  $u_{1}$  to  $i_{4}$ . Thus, compared with  $i_{2}$  and  $i_{3}$ , obviously,  $i_{4}$  is a more reliable recommendation for  $u_{1}$ .\n\nHowever, existing CF-based methods, including model-based and memory-based approaches, often suffer from the difficulty of modeling the connectivity information. Previous model-based approaches, such as Matrix Factorization (MF) [19], are usually designed to approximate the direct connections (or proximities). However, indirect connectivity information hidden in the graph structures is rarely captured by traditional model-based approaches. For instance, it is formidable for them to model the number of paths between  $u_{1}$  and  $i_{4}$  in Figure 1. Whereas a number of memory-based approaches [14, 25] is introduced to model the connectivity information, these methods often rely on pre-defined similarity functions. However, in the real world, defining an appropriate similarity function suitable for diverse application cases is never an easy task.\n\nSpectral graph theory [27] studies connections between combinatorial properties of a graph and the eigenvalues of matrices associated to the graph, such as the laplacian matrix (see Definition 2.4 in Section 2). In general, the spectrum of a graph focuses on the connectivity of the graph, instead of the geometrical proximity. To see how does the spectral domain come to help for recommendations and better understand the advantages of viewing a user-item bipartite graph in the spectral perspective, let us revisit the toy example shown in Figure 1. For the bipartite graph  $\\mathcal{B}$ , we visually plot its vertices in one specific frequency domain. Although vertices do not come with coordinates, a popular way to draw them in a space is to use eigenvectors of a laplacian matrix associated with the graph to supply coordinates [28]. Figure 2 shows that, compared with  $i_2$  and  $i_3$ ,  $i_4$  becomes closer to  $u_1$  in the space<sup>2</sup>. Thus, when transformed into the frequency domain,  $i_4$  is revealed to be a more suitable choice than  $i_2$  or  $i_3$  for  $u_1$ . The underlying reason is that the connectivity information of the graph has been uncovered in the frequency domain, where the relationships between vertices depend on not only their proximity but also connectivity. Thus, exploiting the spectrum of a graph can help better explore and identify the items to be recommended.\n\nInspired by the recent progress [6, 17] in node/graph classification methods, we propose a spectral graph theory based method to leverage the broad information existing in the spectral domain to overcome the aforementioned drawbacks and challenges. Specifically, to conquer the difficulties (see Section 3.3) of directly learning from the spectral domain for recommendations, we first present a new spectral convolution operation (see Eq. (10)), which is approximated by a polynomial to dynamically amplify or attenuate each frequency domain. Then, we introduce a deep recommendation model, named Spectral Collaborative Filtering (SpectralCF), built by\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/a337393af48e35487518b03fc5c7bf927c0d961ee97169252f76b8e020a23ef9.jpg)  \nFigure 2: Vertices of the bipartite graph in Figure 1 are plotted in a frequency domain. Note that the vertices not shown above are omitted for simplicity.\n\nmultiple proposed spectral convolution layers. SpectralCF directly performs collaborative filtering in the spectral domain.\n\nThe key contributions of this work can be summarized as follows:\n\n- Novelty: To the best of our knowledge, it is the first CF-based method directly learning from the spectral domains of user-item bipartite graphs.  \n- A deep recommendation model: We propose a new spectral convolution operation performing in the spectral domain. Stacked by multiple layers of the proposed spectral convolution operation, a deep recommendation model, named Spectral Collaborative Filtering (SpectralCF), is introduced.  \n- **Strong Performance:** In the experiments, SpectralCF outperforms state-of-the-art comparative models. It is shown that SpectralCF effectively utilizes the rich information of connectivity existing in the spectral domain to ease the cold-start problem.\n\nThe rest of the paper is organized as follows. In Section 2, we provide preliminary concepts. Section 3 describes SpectralCF in detail. Experiments are presented in Section 4 to analyze SpectralCF and demonstrate its effectiveness compared with state-of-the-art techniques for RS. In Section 5, we give a short review of the works related to our study. Finally, conclusions are presented in Section 6.",
  "method": "# 3 PROPOSED MODEL\n\nIn this section, we first describe the process of performing a graph fourier transform on a bipartite graph  $\\mathcal{B}$  for recommendations. Then we propose to place a novel spectral convolution filter on vertices (users and items) of the bipartite graph to dynamically filter the contributions of each frequency component in the spectral domain. Later, a polynomial approximation is employed to overcome the shortcomings of the proposed convolution operation. Finally, with the approximate convolution operation, we introduce our final recommender system, named Spectral Collaborative Filtering, stacked by multiple spectral convolution layers.\n\n# 3.1 Graph Fourier Transform\n\nDefinition 3.1. (Graph Signal). Given any graph  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{V}$  and  $\\mathcal{E}$  are a vertex and an edge set, respectively, a graph signal is defined as a state vector  $\\pmb{x}\\in \\mathcal{R}^{|\\mathcal{V}|\\times 1}$  over all vertices in the graph, where  $x_{j}$  is the  $j_{th}$  value of  $\\pmb{x}$  observed at the  $j_{th}$  vertex of  $\\mathcal{G}$ .\n\nThe classical fourier transform is defined as an expansion of a function  $f$  in terms of the complex exponentials as:\n\n$$\n\\hat {f} (\\xi) = \\int_ {- \\infty} ^ {+ \\infty} f (x) e ^ {- 2 \\pi i \\xi} d x, \\tag {3}\n$$\n\nwhere  $i$  is an imaginary number, and the complex exponentials  $(e^{-2\\pi i\\xi})$  form an orthonormal basis.\n\nAnalogously, the graph fourier transform is defined as an expansion of an observed graph signal in terms of the eigenvectors of the graph laplacian  $L$ , and the eigenvectors serve as a basis in the spectral domain. Let us assume that a graph signal  $(x \\in \\mathcal{R}^{|\\mathcal{V}| \\times 1})$  is\n\nobserved on a graph  $\\mathcal{G}$ , we define the graph fourier transform and its inverse on  $\\mathcal{G}$  as:\n\n$$\n\\hat {x} (l) = \\sum_ {j = 0} ^ {N - 1} x (j) \\mu_ {l} (j) \\quad \\text {a n d} \\quad x (j) = \\sum_ {l = 0} ^ {N - 1} \\hat {x} (l) \\mu_ {l} (j), \\tag {4}\n$$\n\nwhere  $x(j),\\hat{x} (l)$  and  $\\mu_l(j)$  denote the  $j_{\\mathrm{th}}$ $l_{\\mathrm{th}}$  and  $j_{\\mathrm{th}}$  value of  $\\pmb {x}$ $\\hat{\\pmb{x}}$  and  $\\pmb {\\mu}_l$  , respectively;  $\\pmb {\\mu}_l$  denotes the  $l_{\\mathrm{th}}$  eigenvector of  $\\pmb {L}$  .  $\\hat{\\pmb{x}}$  represents a graph signal which has been transformed into the spectral domain. For simplicity, we rewrite Eq. (4) in the matrix form as  $\\hat{\\pmb{x}} = \\pmb{U}^{\\top}\\pmb{x}$  and  $\\pmb {x} = \\pmb {U}\\hat{\\pmb{x}}$  , respectively, where  $\\pmb {U} = \\{\\pmb {\\mu}_0,\\pmb {\\mu}_1,\\dots,\\pmb {\\mu}_l,\\dots,\\pmb {\\mu}_{N - 1}\\}$  are eigenvectors of  $\\pmb{L}$\n\nIn particular, for a bipartite graph  $\\mathcal{B}$ , assume that there are two types of graph signals:  $\\pmb{x}^u \\in \\mathcal{R}^{|\\mathcal{U}| \\times 1}$  and  $\\pmb{x}^i \\in \\mathcal{R}^{|\\mathcal{T}| \\times 1}$ , associated with user and item vertices, respectively. We transform them into the spectral domain and vice versa as:\n\n$$\n\\left[ \\begin{array}{l} \\hat {\\boldsymbol {x}} ^ {u} \\\\ \\hat {\\boldsymbol {x}} ^ {i} \\end{array} \\right] = \\boldsymbol {U} ^ {\\top} \\left[ \\begin{array}{l} \\boldsymbol {x} ^ {u} \\\\ \\boldsymbol {x} ^ {i} \\end{array} \\right] \\quad \\text {a n d} \\quad \\left[ \\begin{array}{l} \\boldsymbol {x} ^ {u} \\\\ \\boldsymbol {x} ^ {i} \\end{array} \\right] = \\boldsymbol {U} \\left[ \\begin{array}{l} \\hat {\\boldsymbol {x}} ^ {u} \\\\ \\hat {\\boldsymbol {x}} ^ {i} \\end{array} \\right]. \\tag {5}\n$$\n\n# 3.2 Spectral Convolution Filtering\n\nThe broad information of graph structures exists in the spectral domain, and different types of connectivity information between users and items can be uncovered in different frequency domains. It is desirable to dynamically adjust the importance of each frequency domain for RS.\n\nTo this end, we propose a convolution filter, parameterized by  $\\theta \\in \\mathcal{R}^N$ , as  $g_{\\theta}(\\Lambda) = \\text{diag}([\\theta_0 \\lambda_0, \\theta_1 \\lambda_1, \\dots, \\theta_{N-1} \\lambda_{N-1}])$  into the spectral domain as:\n\n$$\n\\left[ \\begin{array}{l} \\boldsymbol {x} _ {n e w} ^ {u} \\\\ \\boldsymbol {x} _ {n e w} ^ {i} \\end{array} \\right] = U g _ {\\boldsymbol {\\theta}} (\\boldsymbol {\\Lambda}) \\left[ \\begin{array}{c} \\hat {\\boldsymbol {x}} ^ {u} \\\\ \\hat {\\boldsymbol {x}} ^ {i} \\end{array} \\right] = U g _ {\\boldsymbol {\\theta}} (\\boldsymbol {\\Lambda}) U ^ {\\top} \\left[ \\begin{array}{c} \\boldsymbol {x} ^ {u} \\\\ \\boldsymbol {x} ^ {i} \\end{array} \\right], \\tag {6}\n$$\n\nwhere  $\\pmb{x}_{new}^{u}$  and  $\\pmb{x}_{new}^{i}$  are new graph signals on  $\\mathcal{B}$  learned by the filter  $g_{\\theta}(\\Lambda)$ , and  $\\Lambda = \\{\\lambda_0, \\lambda_1, \\dots, \\lambda_{N-1}\\}$  denotes eigenvalues of the graph laplacian matrix  $\\mathbf{L}$ .\n\nIn Eq. (6), a convolution filter  $g_{\\theta}(\\Lambda)$  is placed on a spectral graph signal  $\\left[ \\begin{array}{c}\\hat{\\boldsymbol{x}}^u\\\\ \\hat{\\boldsymbol{x}}^i \\end{array} \\right]$ , and each value of  $\\theta$  is responsible for boosting or diminishing each corresponding frequency component. The eigenvector matrix  $U$  in Eq. (6) is used to perform an inverse graph fourier transform.\n\n# 3.3 Polynomial Approximation\n\nRecall that we proposed a convolution operation, as shown in Eq. (6), to directly perform in the spectral domain. Although the filter is able to dynamically measure contributions of each frequency component for the purpose of recommendations, there are two limitations. First, as shown in Eq. (6), the learning complexity of the filter is  $\\mathcal{O}(N)$ , where  $N$  is the number of vertices. That is, unlike classical Convolutional Neural Networks (CNNs), the number of parameters of the filter is linear to the dimensionality of data. It constrains the scalability of the proposed filter. Second, the learned graph signals  $(\\pmb{x}_{new}^{u} \\in \\mathcal{R}^{|U| \\times 1}$  and  $\\pmb{x}_{new}^{i} \\in \\mathcal{R}^{|I| \\times 1})$  are vectors. It means that each vertex of users or items is represented by a scalar feature. However, a vector for every user and item is necessary to model the deep and complex connections between users and items.\n\nThe first limitation can be overcome by using a polynomial approximation. We first demonstrate that the set of all convolution\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/35a90fec5c432d0d301648925fa2c1c63640d09479f378ff378dc95f4c50868a.jpg)  \nFigure 3: The feed-forward procedure of SpectralCF. The function  $sp(:, U, \\Lambda, \\Theta)$  denotes the spectral convolution operation shown in Eq. (10).\n\nfilters  $S_{g} = \\{g_{\\theta}(\\Lambda) = diag([\\theta_{0}\\lambda_{0},\\theta_{1}\\lambda_{1},\\dots,\\theta_{N - 1}\\lambda_{N - 1}]),\\theta \\in \\mathcal{R}^{N}\\}$  is equal to the set of finite-order polynomials  $S_{h} = \\{h_{\\theta^{\\prime}}(\\Lambda) = \\sum_{p = 0}^{N - 1}\\theta_{p}^{\\prime}\\Lambda^{p},\\theta^{\\prime}\\in \\mathcal{R}^{N}\\}$\n\n# Proposition 3.1.  $S_h$  is equal to  $S_g$ .\n\nPROOF. Let us consider an instance  $h_{\\theta'}(\\Lambda) \\in S_h$ . Then,  $h_{\\theta'}(\\Lambda) = \\sum_{p=0}^{N-1} \\theta_p' \\Lambda^p = \\text{diag}([\\sum_{p=0}^{N-1} \\theta_p' \\lambda_0^{p-1} \\cdot \\lambda_0, \\sum_{p=0}^{N-1} \\theta_p' \\lambda_1^{p-1} \\cdot \\lambda_1, \\dots, \\sum_{p=0}^{N-1} \\theta_p' \\lambda_{N-1}^{p-1} \\cdot \\lambda_{N-1}])$ . So,  $h_{\\theta'}(\\Lambda) \\in S_g$ . Now, consider a convolution filter  $g_\\theta(\\Lambda) \\in S_g$ . Then, there must exist a polynomial function  $\\phi(\\lambda) = \\sum_{p=0}^{N-1} a_p \\lambda^p$  that interpolates through all pairs  $(\\lambda_i, \\theta_i \\lambda_i)$  for  $i \\in \\{0, 1, \\dots, N-1\\}$ . The maximum degree of such a polynomial is at most  $N-1$  as there are maximum  $N$  points to interpolate. Therefore,  $g_\\theta(\\Lambda) = \\sum_{p=0}^{N-1} a_p \\Lambda^p = h_a(\\Lambda) \\in S_h$ .\n\nNow, we can approximate the convolution filters by using first  $P$  polynomials as the following:\n\n$$\ng _ {\\boldsymbol {\\theta}} (\\boldsymbol {\\Lambda}) \\approx \\sum_ {p = 0} ^ {P} \\theta_ {p} ^ {\\prime} \\boldsymbol {\\Lambda} ^ {p}. \\tag {7}\n$$\n\nIn this way, the learning complexity of the filter becomes  $O(P)$ , where  $P$  is a hyper-parameter, and independent from the number of vertices. Specially, we limit the order of the polynomial,  $P$ , to 1 in order to avoid over-fitting. By substituting Eq. (7) into Eq. (6), we have:\n\n$$\n\\left[ \\begin{array}{l} \\boldsymbol {x} _ {n e w} ^ {u} \\\\ \\boldsymbol {x} _ {n e w} ^ {i} \\end{array} \\right] = \\left(\\theta_ {0} ^ {\\prime} \\boldsymbol {U} \\boldsymbol {U} ^ {\\top} + \\theta_ {1} ^ {\\prime} \\boldsymbol {U} \\boldsymbol {\\Lambda} \\boldsymbol {U} ^ {\\top}\\right) \\left[ \\begin{array}{l} \\boldsymbol {x} ^ {u} \\\\ \\boldsymbol {x} ^ {i} \\end{array} \\right]. \\tag {8}\n$$\n\nFurthermore, it is beneficial to further decrease the number of parameters by setting  $\\theta' = \\theta_0' = \\theta_1'$ . As a result, Eq. (8) becomes:\n\n$$\n\\left[ \\begin{array}{l} \\boldsymbol {x} _ {n e w} ^ {u} \\\\ \\boldsymbol {x} _ {n e w} ^ {i} \\end{array} \\right] = \\theta^ {\\prime} (\\boldsymbol {U} \\boldsymbol {U} ^ {\\intercal} + \\boldsymbol {U} \\boldsymbol {\\Lambda} \\boldsymbol {U} ^ {\\intercal}) \\left[ \\begin{array}{l} \\boldsymbol {x} ^ {u} \\\\ \\boldsymbol {x} ^ {i} \\end{array} \\right], \\tag {9}\n$$\n\nwhere  $\\theta^\\prime$  is a scalar.\n\nFor the second limitation, one can generalize the graph signals  $(\\pmb{x}^u\\in \\mathcal{R}^{|\\mathcal{U}|\\times 1}$  and  $\\pmb {x}^i\\in \\mathcal{R}^{|\\mathcal{I}|\\times 1})$  to  $C$  -dimensional graph signals:  $X^{u}\\in \\mathcal{R}^{|\\mathcal{U}|\\times C}$  and  $X^{i}\\in \\mathcal{R}^{|\\mathcal{I}|\\times C}$ . Hence, Eq. (9) becomes  $\\left[ \\begin{array}{l}X_{new}^{u}\\\\ X_{new}^{i} \\end{array} \\right] = (UU^{\\top} + U\\Lambda U^{\\top})\\left[ \\begin{array}{l}X^{u}\\\\ X^{i} \\end{array} \\right]\\theta^{\\prime}$ . To take one step further, we generalize the filter parameter  $\\theta^\\prime$  to a matrix of filter parameters  $\\Theta^{\\prime}\\in \\mathcal{R}^{C\\times F}$  with  $C$  input channels and  $F$  filters. As a result, our final spectral convolution operation is shown as the following:\n\n$$\n\\left[ \\begin{array}{l} X _ {n e w} ^ {u} \\\\ X _ {n e w} ^ {i} \\end{array} \\right] = \\sigma \\left(\\left(U U ^ {\\top} + U \\Lambda U ^ {\\top}\\right) \\left[ \\begin{array}{l} X ^ {u} \\\\ X ^ {i} \\end{array} \\right] \\Theta^ {\\prime}\\right), \\tag {10}\n$$\n\nwhere  $X_{new}^{u}\\in \\mathcal{R}^{|U|\\times F}$  and  $X_{new}^{i}\\in \\mathcal{R}^{|I|\\times F}$  denote convolution results learned with  $F$  filters from the spectral domain for users and items, respectively;  $\\sigma$  denotes the logistic sigmoid function.\n\nIn fact, Eq. (10) is a general version of Eq. (9) as it is equivalent to perform Eq. (9) in  $C$  input channels with  $F$  filters. Hereafter, the proposed convolution operation as shown in Eq. (10) is denoted as a function  $sp(:,U,\\Lambda ,\\Theta ')$ , which is parameterized by  $U,\\Lambda$  and  $\\Theta^{\\prime}$ .\n\n# 3.4 Multi-layer Model\n\nGiven user vectors  $X^u$  and item vectors  $X^i$ , new graph singals  $(X_{new}^u$  and  $X_{new}^i$ ) in Eq. (10) are convolution results learned from the spectral domain with a parameter matrix  $\\Theta' \\in \\mathcal{R}^{C \\times F}$ . As in classical CNNs, one can regard Eq. (10) as a propagation rule to build a deep neural feed-forward network based model, which we refer as Spectral Collaborative Filtering (SpectralCF).\n\nSimilar to word embedding techniques, we first randomly initialize user vectors  $X_0^u$  and item vectors  $X_0^i$ . Taking  $X_0^u$  and  $X_0^i$  as inputs, a  $K$  layered deep spectralCF can be formulated as:\n\n$$\n\\left[ \\begin{array}{c} X _ {K} ^ {u} \\\\ X _ {K} ^ {t} \\end{array} \\right] = \\underbrace {s p \\left(\\dots s p \\left(\\begin{array}{l} X _ {0} ^ {u} \\\\ X _ {0} ^ {t} \\end{array} \\right) ; U , \\Lambda , \\Theta_ {0} ^ {\\prime}\\right)} _ {K}, \\tag {11}\n$$\n\nwhere  $\\Theta_{K - 1}^{\\prime}\\in \\mathcal{R}^{F\\times F}$  is a matrix of filter parameters for the  $k_{\\mathrm{th}}$  layer;  $X_{k}^{u}$  and  $X_{k}^{i}$  denote the convolution filtering results of the  $k_{\\mathrm{th}}$  layer.\n\nIn order to utilize features from all layers of SpectralCF, we further concatenate them into our final latent factors of users and items as:\n\n$$\n\\mathbf {V} ^ {u} = \\left[ \\mathbf {X} _ {0} ^ {u}, \\mathbf {X} _ {1} ^ {u}, \\dots , \\mathbf {X} _ {K} ^ {u} \\right] \\quad \\mathrm {a n d} \\quad \\mathbf {V} ^ {i} = \\left[ \\mathbf {X} _ {0} ^ {i}, \\mathbf {X} _ {1} ^ {i}, \\dots , \\mathbf {X} _ {K} ^ {i} \\right], \\quad (1 2)\n$$\n\nwhere  $\\mathbf{V}^u\\in \\mathcal{R}^{|\\mathcal{U}|\\times (C + KF)}$  and  $\\mathbf{V}^i\\in \\mathcal{R}^{|\\mathcal{T}|\\times (C + KF)}$\n\nIn terms of the loss function, the conventional BPR loss suggested in [23] is employed. BPR is a pair-wise loss to address the implicit data for recommendations. Unlike point-wise based methods [18], BPR learns a triple  $(r,j,j')$ , where item  $j$  is liked/ clicked/viewed by user  $r$  and item  $j'$  is not. By maximizing the preference difference between  $j$  and  $j'$ , BPR assumes that the user  $i$  prefers item  $j$  over the unobserved item  $j'$ . In particular, given a user matrix  $\\mathbf{V}^u$  and an item matrix  $\\mathbf{V}^i$  as shown in Eq. (12), the loss function of SpectralCF is given as:\n\n$$\n\\begin{array}{l} \\mathcal {L} = \\arg \\min  _ {\\boldsymbol {V} ^ {u}, \\boldsymbol {V} ^ {i}} \\sum_ {(r, j, j ^ {\\prime}) \\in \\mathcal {D}} - \\ln \\sigma \\left(\\boldsymbol {v} _ {r} ^ {u \\top} \\boldsymbol {v} _ {j} ^ {i} - \\boldsymbol {v} _ {r} ^ {u \\top} \\boldsymbol {v} _ {j ^ {\\prime}} ^ {i}\\right) \\tag {13} \\\\ + \\lambda_ {r e g} (\\| V ^ {u} \\| _ {2} ^ {2} + \\| V ^ {i} \\| _ {2} ^ {2}), \\\\ \\end{array}\n$$\n\nwhere  $\\pmb{v}_r^u$  and  $\\pmb{v}_j^i$  denote  $r_{\\mathrm{th}}$  and  $j_{\\mathrm{th}}$  column of  $\\pmb{V}^u$  and  $\\pmb{V}^i$ , respectively;  $\\lambda_{reg}$  represents the weight on the regularization terms. The\n\n# Algorithm 1: SpectralCF\n\nInput: Training set:  $\\mathcal{D} := \\{(r,j,j') | r \\in \\mathcal{U} \\land j \\in \\mathcal{I}_i^+ \\land j' \\subseteq \\mathcal{I}_i^-\\}$ , number of epochs  $E$ , batch size  $B$ , number of layers  $K$ , dimension of latent factors  $C$ , number of filters  $F$ , regularization term  $\\lambda_{reg}$ , learning rate  $\\lambda$ , laplacian matrix  $L$  and its corresponding eigenvectors  $U$  and eigenvalues  $\\Lambda$ . Output: Model's parameter set:  $\\Psi = \\{\\Theta_0', \\Theta_1', \\dots, \\Theta_{K-1}', X_0^u, X_0^i\\}$ . Randomly initialize  $X_0^u$  and  $X_0^i$  from a Gaussian distribution  $\\mathcal{N}(0.01, 0.02)$ ; for  $e = 1, 2, \\dots, E$  do  \nGenerate the  $e_{th}$  batch of size  $B$  by uniformly sampling from  $\\mathcal{U}$ ,  $\\mathcal{I}_i^+$  and  $\\mathcal{I}_i^-$ ;  \nfor  $k = 0, 1, \\dots, K-1$  do  \nCalculate  $X_{k+1}^u$  and  $X_{k+1}^i$  by using Eq. (10); end  \nConcatenate  $[X_0^u, X_1^u, \\dots, X_K^u]$  into  $V^u$  and  $[X_0^i, X_1^i, \\dots, X_K^i]$  into  $V^i$ ;  \nEstimate gradients  $\\frac{\\partial \\mathcal{L}}{\\partial \\Psi_e}$  by back propagation;  \nUpdate  $\\Psi_{e+1}$  according to the procedure of RMSprop optimization [29];  \nend  \nreturn  $\\Psi_E$ .\n\ntraining data  $\\mathcal{D}$  is generated as:\n\n$$\n\\mathcal {D} = \\left\\{\\left(r, j, j ^ {\\prime}\\right) \\mid r \\in \\mathcal {U} \\wedge j \\in \\mathcal {I} _ {i} ^ {+} \\wedge j ^ {\\prime} \\in \\mathcal {I} _ {i} ^ {-} \\right\\}. \\tag {14}\n$$\n\n# 3.5 Optimization and Prediction\n\nAt last, RMSprop [29] is used to minimize the loss function. The RMSprop is an adaptive version of gradient descent which adaptively controls the step size with respect to the absolute value of the gradient. It is done by scaling the updated value of each weight by a running average of its gradient norm.\n\nAs shown in Algorithm 1, for a batch of randomly sampled triple  $(r,j,j')$ , we update parameters in each epoch using the gradients of the loss function. After the training process, with optimized  $\\Theta$ ,  $X_0^u$  and  $X_0^i$ , we derive the user  $r$ 's preference over item  $j$  as  $\\pmb{v}_r^{u\\top}\\pmb{v}_j^i$ . The final item recommendation for a user  $r$  is given according to the ranking criterion as Eq. (15).\n\n$$\nr: j _ {1} \\succcurlyeq j _ {2} \\succcurlyeq \\dots \\succcurlyeq j _ {n} \\Rightarrow \\boldsymbol {v} _ {r} ^ {u \\top} \\boldsymbol {v} _ {j _ {1}} ^ {i} > \\boldsymbol {v} _ {r} ^ {u \\top} \\boldsymbol {v} _ {j _ {2}} ^ {i} > \\dots > \\boldsymbol {v} _ {r} ^ {u \\top} \\boldsymbol {v} _ {j _ {n}} ^ {i}. \\tag {15}\n$$\n",
  "experiments": "# 4 EXPERIMENTS\n\nAs discussed in the introduction section, leveraging the connectivity information in a user-item bipartite graph is essentially important for an effective recommendation model. In this section, we argue that, directly learning from the spectral domain, the proposed SpectraCF can reveal the rich information of graph structures existing in the spectral domain for making better recommendations. One may ask the following research questions:\n\nRQ1: How much does SpectralCF benefit from the connectivity information learned from the spectral domain?  \nRQ2: Does SpectralCF learn from the spectral domain in an effective way?  \nRQ3: Compared with traditional methods, can SpectralCF better counter the cold-start problem?\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/5cdea36fa465298f541a36a7bab7b817ed957b3a66e3a8996e26ab1ef0ef91cc.jpg)  \nFigure 4: Effects of hyper-parameter  $K$  in terms of Recall@20 and MAP@20 in the dataset of MovieLens-1M.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/d927e82b2745299fa788fd8ccd9e3e0f691761ac63d90b2be8af4b56f2af34c8.jpg)\n\nIn this section, in order to answer the questions above, we conduct experiments to compare SpectralCF with state-of-the-art models.\n\n# 4.1 Comparative Methods\n\nTo validate the effectiveness of SpectralCF, we compare it with six state-of-the-art models. The comparative models can be categorized into two groups: (1) CF-based Models: To answer RQ1, we compare SpectralCF with four state-of-the-art CF-based methods (ItemKNN, BPR, eALS and NCF) which ignore the information in the spectral domain; (2) Graph-based Models: For RQ2, we are interested in how effectively does SpectralCF learn the connectivity information from the spectral domain. We therefore compare SpectralCF with two graph-based models: GNMF and GCMC. Although the two models are also CF-based, we term them as graph-based models since they learn the structural information from a bipartite graph. These two groups of comparative models are summarized below:\n\n- ItemKNN [25]: ItemKNN is a standard neighbor-based collaborative filtering method. The model finds similar items for a user based on their similarities.  \n- BPR [23]: We use Bayesian Personalized Ranking based Matrix Factorization. BPR introduces a pair-wise loss into the Matrix Factorization to be optimized for ranking [8].  \n- eALS [12]: This is a state-of-the-art matrix factorization based method for item recommendation. This model takes all unobserved interactions as negative instances and weighting them non-uniformly by the item popularity.  \n- NCF [11]: Neural Collaborative Filtering fuses matrix factorization and Multi-Layer Perceptron (MLP) to learn from user-item interactions. The MLP endows NCF with the ability of modelling non-linearities between users and items.  \n- GNMF [3]: Graph regularized Non-negative Matrix Factorization considers the graph structures by seeking a matrix factorization with a graph-based regularization.  \n- GCMC [2]: Graph Convolutional Matrix Completion utilizes a graph auto-encoder to learn the connectivity information of a bipartite interaction graph for latent factors of users and items.\n\nPlease note that, GNMF and GCMC are originally designed for explicit datasets. For a fair comparison, we follow the setting of [13] to adapt them for implicit data.\n\nTable 1: The hyper-parameter setting of SpectralCF.  \n\n<table><tr><td>Hyper-parameters</td><td>K</td><td>C</td><td>F</td><td>位reg</td><td>B</td><td>E</td><td>位</td></tr><tr><td>Values</td><td>3</td><td>16</td><td>16</td><td>0.001</td><td>1,024</td><td>200</td><td>0.001</td></tr></table>\n\n# 4.2 Datasets\n\nWe test our method as well as comparative models on three publicly available datasets<sup>3</sup>:\n\n- MovieLens-1M [10]: This movie rating dataset has been widely used to evaluate collaborative filtering algorithms. We used the version containing 1,000,209 ratings from 6,040 users for 3,900 movies. While it is a dataset with explicit feedbacks, we follow the convention [11] that transforms it into implicit data, where each entry is marked as 0 or 1 indicating whether the user has rated the item. After transforming, we retain a dataset of  $1.0\\%$  density.  \n- HetRec [4]: This dataset has been released by the Second International Workshop on Information Heterogeneity and Fusion in Recommender Systems<sup>4</sup>. It is an extension of MovieLens-10M dataset and contains 855,598 ratings, 2,113 users and 10,197 movies. After converting it into implicit data as MovieLens-1M, we obtain a dataset of  $0.3\\%$  density.  \n- Amazon Instant Video [20]: The dataset consists of 426,922 users, 23,965 videos and 583,933 ratings from Amazon.com. Similarly, we transformed it into implicit data and removed users with less than 5 interactions. As a result, a dataset of  $0.12\\%$  density is obtained.\n\n# 4.3 Experimental Setting\n\nIdeally, a recommendation model should not only be able to retrieve all relevant items out of all items but also provide a rank for each user where relevant items are expected to be ranked in the top. Therefore, in our experiments, we use Recall@M and MAP@M to evaluate the performance of the top-M recommendations. Recall@M is employed to measure the fraction of relevant items retrieved out of all relevant items. MAP@M is used for evaluating the ranking performance of RS. The Recall@M for each user is then defined as:\n\n$$\n\\operatorname {R e c a l l} @ \\mathrm {M} = \\frac {\\text {i g i t e s t h e u s e r l i k e s a m o n g t h e t o p M}}{\\text {t o t a l n u m b e r o f i t e m s t h e u s e r l i k e s}}. \\tag {16}\n$$\n\nThe final results reported are average recall over all users.\n\nFor each dataset, we randomly select  $80\\%$  items associated with each user to constitute the training set and use all the remaining as the test set. For each evaluation scenario, we repeat the evaluation five times with different randomly selected training sets and the average performance is reported in the following sections.\n\nWe use a validation set from the training set of each dataset to find the optimal hyper-parameters of comparative methods introduced in the Section 4.1. For ItemKNN, we employ the cosine distance to measure item similarities. The dimensions of latent factors for BPR, eALS and GNMF are searched from  $\\{8,16,32,64,128\\}$  via the validation set. The hyperparameter  $\\lambda$  of eALS is selected from 0.001 to 0.04. Since the architecture of a multi-layer perceptron (MLP) is difficult to optimize, we follow the suggestion from the\n\noriginal paper [11] to employ a three-layer MLP with the shape of (32, 16, 8) for NCF. The dropout rate of nodes for GCMC is searched from  $\\{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\\}$ . Our SpectralCF has one essential hyperparameter:  $K$ . Figure 4 shows how the performances of SpectralCF vary as  $K$  is set from 1 to 5 on the validation set of MovieLens-1M. As we can see, in terms of Recall@20 and MAP@20, SpectralCF reaches its best performances when  $K$  is fixed as 3. Other hyperparameters of SpectralCF are empirically set and summarized in Table 1, where  $\\lambda$  denotes the learning rate of RMSprop. Our models are implemented in TensorFlow [1].\n\n# 4.4 Experimental Results (RQ1 and RQ2)\n\nIn Figure 5, we compare SpectralCF with four CF-based models and two graph-based models in terms of Recall@M on all three datasets. Overall, when  $M$  is varied from 20 to 100, SpectralCF consistently yields the best performance across all cases. Among CF-based comparative models, ItemKNN gives the worst performances in all three datasets, indicating the necessity of modeling users' personalized preferences rather than just recommending similar items to users. For graph-based models (GNMF and GCMC), they generally underperform CF-based models such as BPR and NCF. The unsatisfying performance of GNMF shows that adding a graph-based regularization is not sufficient to capture complex structures of graphs. Though GCMC directly performs on a user-item bipartite graph, each vertex in the graph is only allowed to learn from its neighbors. This constrains its ability of capturing global structures in the graph. Among all comparative models, benefiting from its capability of modeling non-linear relationships between users and items, NCF beats all other models and becomes the strongest one. However, none of models above are able to directly perform in the spectral domain. They lose the rich information in the domain and as a result, SpectralCF greatly outperforms NCF by  $16.1\\%$ ,  $16.2\\%$  and  $28.0\\%$  in the dataset of MovieLen-1M, HetRec and Amazon Instant Video, respectively.\n\nIn Figure 6, we compare SpectralCF with all comparative models in terms of MAP@M. Again, when  $M$  is in a range from 20 to 100, SpectralCF always yields the best performance. Neighborbased ItemKNN performs the worst among all models. It further shows the advantages of modeling users' personalized preferences. Compared with NCF and BPR, graph-based models (GNMF and GCMC) again fail to show convincing ranking performances measured by MAP@M. For CF-based models, while NCF beats other CF-based models in the dataset of HetRec, BPR shows itself as a strong model for ranking, owing to its pairwise ranking loss. It slightly outperforms NCF on average in the datasets of MovieLens-1M and Amazon Instant Video. However, SpectralCF improves BPR by  $15.9\\%$ ,  $64.9\\%$  and  $47.5\\%$  in the dataset of MovieLen-1M, HetRec and Amazon Instant Video, respectively.\n\nOverall, as shown in Figure 5 and 6, not surprisingly, the performances of all models decline as the dataset becomes sparse. However, SpectralCF always outperforms all comparative models regardless of the sparsities of the datasets. By comparing spectralCF with traditional CF-based models, we demonstrate that the rich information of connectivity existing in the spectral domain assists SpectralCF in learning better latent factors of users and items. By\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/b0aa874de167c30c54d7d151b6081978187de5dc40e17a4b4868fe13feb0759f.jpg)  \n(a) MovieLens-1M\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/1134b94c9bec36a4dbc2c4967956e4284941f949e109b41e214373c712af1ea7.jpg)  \n(b) HetRec\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/86a56512eac0fcef1755156203a3cda42466dee61c1052c7f1c73f638ac59919.jpg)  \n(c) Amazon Instant Video\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/6ee93bd6c9c1a7cd5996ffb67b289ade2121b26568b06b428f61bed268f7342d.jpg)  \nFigure 5: Performance comparison in terms of recall@M with M varied from 20 to 100. Errors bars are 1-standard deviation.  \n(a) MovieLens-1M  \nFigure 6: Performance comparison in terms of MAP@M with M varied from 20 to 100. Errors bars are 1-standard deviation.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/6976a9898781e6b83ed678f546e90d29b5fe163ac492a79ba3c84ff46ba28a2a.jpg)  \n(b) HetRec\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/784c4566-2f54-4c5a-9369-18b942e8911d/c86f517bc74f8aed17c0ac0d2d0586ecf38d0531b35e2c76335746fc4b6304df.jpg)  \n(c) Amazon Instant Video\n\ncomparing SpectralCF with graph-based models, we show that SpectralCF can effectively learn from the spectral domain.\n\n# 4.5 Quality of Recommendations for Cold-start Users (RQ3)\n\nTo answer RQ3, in this section, we conduct an experiment to investigate the quality of recommendations made by SpectralCF for cold-start users. To this end, in the dataset of MovieLens-1M, we build training sets with different degrees of sparsity by varying the number of items associated with each user, denoted as  $P$ , from one to five. All the remaining items associated with users are used as the test set. We compare SpectralCF with BPR, which is widely known and also shown as a strong ranking performer in Figure 6. The test results are reported in the Table 2.\n\nIn Table 2, it is shown that, suffering from the cold-start problem, the performances of BPR and SpectralCF inevitably degrade. However, regardless of the number of items associated with users, SpectralCF consistently outperforms BPR in terms of Recall@20 and MAP@20. On average, SpectralCF improves BPR by  $36.8\\%$  and  $33.8\\%$  in Recall@20 and MAP@20, respectively. Hence, it is demonstrated that compared with BPR, spectralCF can better handle cold-start users and provide more reliable recommendations.\n",
  "hyperparameter": "Number of layers K=3, dimension of initial latent factors C=16, number of filters per layer F=16, regularization weight 位_reg=0.001, batch size B=1024, number of epochs E=200, learning rate 位=0.001 (using RMSprop optimizer), polynomial order P=1 (fixed to avoid overfitting), initial embeddings sampled from Gaussian distribution N(0.01, 0.02)"
}