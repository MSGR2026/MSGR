{
  "id": "FISM_2013",
  "paper_title": "FISM: Factored Item Similarity Models for Top-N Recommender Systems",
  "alias": "FISM",
  "year": 2013,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "FISM (Factored Item Similarity Models) learns item-item similarity as the product of two low-dimensional latent factor matrices P and Q, computing recommendations as a weighted sum of similarities between items a user has interacted with and candidate items. The key innovation is excluding the target item from its own estimation to avoid trivial solutions, combined with a neighborhood agreement parameter α that balances between average and aggregate similarity. This approach effectively captures transitive item relationships and performs especially well on sparse data.",
  "introduction": "# Introduction\nTop-N recommender systems have been widely used in E-commerce applications to recommend ranked lists of items so as to help the users in identifying the items that best fit their personal tastes. Over the years, many algorithms have been developed to address the top-N recommender problem. These algorithms make use of the user feedback (purchase, rating or review) to compute the recommendations. Typically these algorithms represent the feedback information as a user-purchase matrix and act on it.\n\nThe existing methods can be broadly classified into two classes: collaborative filtering (CF) based methods and content based methods. User/Item co-rating information is utilized in collaborative methods to build models. One class of CF methods, referred to as nearest-neighborhood-based methods, compute the similarities between the users/items using the co-rating information and new items are recommended based on these similarity values. Another class of CF methods, referred to as model-based methods, employ a machine learning algorithm to build a model (in terms of similarities or latent factors), which is then used to perform the recommendation task. The state-of-the-art methods for rating prediction and top-N recommendation problem learn the relationship between items in the form of an item similarity matrix. In content based methods, the features associated with users/items are used to build models.\n\nRecently, a novel top-N recommendation method has been developed, called SLIM, which improves upon the traditional item-based nearest neighbor collaborative filtering approaches by learning directly from the data, a sparse matrix of aggregation coefficients that are analogous to the traditional item-item similarities. SLIM has been shown to achieve good performance on a wide variety of datasets and to outperform other state-of-the-art approaches. However, an inherent limitation of SLIM is that it can only model relations between items that have been co-purchased/co-rated by at least some users. As a result, it cannot capture transitive relations between items that are essential for good performance of item-based approaches in sparse datasets.\n\nIn this paper we propose a method, called FISM, which learns the item-item similarity matrix as a product of two low-dimensional latent factor matrices. This factored representation of the item-item similarity matrix allows FISM to capture and model relations between items even on very sparse datasets. Our experimental evaluation on multiple datasets and at different sparsity levels confirms that and shows that FISM performs better than SLIM and other state-of-the-art methods. Moreover, the relative performance gains increase with the sparsity of the datasets.\n\nThe key contributions of the work presented in this paper are the following:\n(i) extends the factored item-based methods to the top-N problem, which allow them to effectively handle sparse datasets;\n(ii) estimates the factored item-based top-N models using a structural equation modeling approach;\n(iii) estimates the factored item-based top-N models using both squared error and a ranking loss; and\n(iv) investigates the impact of various parameters as they relate to biases, neighborhood agreement, and model’s induced sparsity.",
  "method": "# Method\n## 1. Notations\n- Let \\( \\mathcal{C} \\) and \\( \\mathcal{D} \\) denote the sets of users and items, with cardinalities \\( n \\) and \\( m \\) respectively.\n- Matrix \\( R \\in \\{0,1\\}^{n×m} \\) represents user-item implicit feedback, where \\( r_{ui}=1 \\) if user \\( u \\) has provided feedback for item \\( i \\), and 0 otherwise.\n- \\( \\mathcal{R}_u^+ \\) denotes the set of items rated by user \\( u \\), \\( n_u^+ = |\\mathcal{R}_u^+| \\).\n- Vectors are represented by bold lowercase letters (e.g., \\( p, q \\)), matrices by bold uppercase letters (e.g., \\( P, Q \\)).\n- Predicted values are marked with \\( \\tilde{} \\) (e.g., \\( \\tilde{r}_{ui} \\)), estimated values with \\( \\hat{} \\) (e.g., \\( \\hat{r}_{ui} \\)).\n\n## 2. Core Model: FISM\nFISM learns the item-item similarity matrix as the product of two low-dimensional latent factor matrices \\( P \\in \\mathbb{R}^{m×k} \\) and \\( Q \\in \\mathbb{R}^{m×k} \\) ( \\( k \\ll m \\) ). The recommendation score for user \\( u \\) on item \\( i \\) is computed as:\n\\[ \\overline{r}_{ui} = b_u + b_i + (n_u^+)^{-\\alpha} \\sum_{j \\in \\mathcal{R}_u^+} p_j q_i^\\top \\]\nwhere:\n- \\( b_u \\) and \\( b_i \\) are user and item biases.\n- \\( \\alpha \\in [0,1] \\) controls the neighborhood agreement (balances average vs. aggregate similarity).\n- \\( p_j \\) (row of \\( P \\)) and \\( q_i \\) (row of \\( Q \\)) are latent vectors of items \\( j \\) and \\( i \\).\n\n### 2.1 FISMrmse (Squared Error Loss)\nUses squared error loss and conforms to structural equation modeling (excludes \\( i \\) from its own estimation):\n\\[ \\hat{r}_{ui} = b_u + b_i + (n_u^+ - 1)^{-\\alpha} \\sum_{j \\in \\mathcal{R}_u^+ \\backslash \\{i\\}} p_j q_i^\\top \\]\nOptimization objective (regularized):\n\\[ \\underset{P, Q}{minimize} \\frac{1}{2} \\sum_{u,i \\in R} (r_{ui} - \\hat{r}_{ui})^2 + \\frac{\\beta}{2}(\\|P\\|_F^2 + \\|Q\\|_F^2) + \\frac{\\lambda}{2}\\|b_u\\|_2^2 + \\frac{\\gamma}{2}\\|b_i\\|_2^2 \\]\n- Training: Stochastic Gradient Descent (SGD) with negative sampling (samples \\( \\rho \\cdot nnz(R) \\) zeros per iteration to reduce computation).\n\n### 2.2 FISMauc (Ranking Loss)\nUses ranking loss based on Bayesian Personalized Ranking (BPR) to optimize AUC:\n\\[ \\mathcal{L} = \\sum_{u \\in \\mathcal{C}} \\sum_{i \\in \\mathcal{R}_u^+, j \\in \\mathcal{R}_u^-} \\left( (r_{ui} - r_{uj}) - (\\hat{r}_{ui} - \\hat{r}_{uj}) \\right)^2 \\]\nOptimization objective (no user bias, cancels out in pairwise difference):\n\\[ \\underset{P, Q}{minimize} \\frac{1}{2} \\sum_{u,i,j} \\left( (r_{ui} - r_{uj}) - (\\hat{r}_{ui} - \\hat{r}_{uj}) \\right)^2 + \\frac{\\beta}{2}(\\|P\\|_F^2 + \\|Q\\|_F^2) + \\frac{\\gamma}{2}\\|b_i\\|_2^2 \\]\n- Training: SGD with sampling of negative items \\( j \\in \\mathcal{R}_u^- \\) per user \\( u \\).\n\n### 2.3 Scalability & Sparsification\n- **Training**: SGD supports parallelization (distributed SGD or cluster-based implementation).\n- **Inference**: Induce sparsity in \\( S = PQ^\\top \\) by zeroing small entries, reducing prediction time with minimal performance loss.\n\n## 3. Key Design Choices\n- **Estimation Approach**: Excludes item \\( i \\) from its own estimation to avoid trivial solutions (unlike NSVD/SVD++).\n- **Non-negativity**: No explicit constraint; model naturally learns mostly non-negative similarities during training.\n- **Neighborhood Agreement**: \\( \\alpha \\in [0.4, 0.5] \\) works best (requires substantial neighborhood similarity for high ratings).",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nThree datasets with three sparsity levels each (converted to implicit feedback, \\( r_{ui}=1 \\) for ratings):\n| Dataset | #Users | #Items | #Ratings | Density |\n|---------|--------|--------|----------|---------|\n| ML100K-1 | 943 | 1,178 | 59,763 | 5.43% |\n| ML100K-2 | 943 | 1,178 | 39,763 | 3.61% |\n| ML100K-3 | 943 | 1,178 | 19,763 | 1.80% |\n| Netflix-1 | 6,079 | 5,641 | 429,339 | 1.25% |\n| Netflix-2 | 6,079 | 5,641 | 221,304 | 0.65% |\n| Netflix-3 | 6,079 | 5,641 | 110,000 | 0.32% |\n| Yahoo-1 | 7,558 | 3,951 | 282,075 | 0.94% |\n| Yahoo-2 | 7,558 | 3,951 | 149,050 | 0.50% |\n| Yahoo-3 | 7,558 | 3,951 | 75,000 | 0.25% |\n\n### 1.2 Evaluation Protocol\n- **5-fold Leave-One-Out Cross-Validation (LOOCV)**: One item per user as test set, rest as training set.\n- **Metrics**: Hit Rate (HR@10) and Average Reciprocal Hit Rank (ARHR@10).\n  - \\( HR = \\frac{\\#hits}{\\#users} \\) (test item in top-10).\n  - \\( ARHR = \\frac{1}{\\#users} \\sum_{i=1}^{\\#hits} \\frac{1}{pos_i} \\) (position-weighted hit rate).\n\n### 1.3 Baselines\n- **ItemKNN Variants**: ItemKNN (cos), ItemKNN (cprob), ItemKNN (log) (neighborhood-based).\n- **Model-Based**: PureSVD (matrix factorization), BPRkNN (BPR-optimized kNN), BPRMF (BPR-optimized MF).\n- **State-of-the-Art**: SLIM (sparse linear item similarity).\n\n## 2. Main Results\n### 2.1 Core Result Table (Table 3)\nFISM outperforms all baselines across all datasets, with larger gains on sparser data:\n- **ML100K-3 (sparsest)**: FISMrmse achieves HR=0.1260, outperforming SLIM (HR=0.0919) by 24%.\n- **Netflix-3**: FISMrmse (HR=0.0578) outperforms BPRMF (HR=0.0454) by 27%.\n- **Yahoo-3**: FISMrmse (HR=0.0740) outperforms SLIM (HR=0.0491) by 51%.\n\n### 2.2 Key Observations\n- **Sparsity Impact**: FISM’s relative performance improves as data becomes sparser (captures transitive item relations).\n- **Loss Function**: FISMrmse (squared error) outperforms FISMauc (ranking loss) unexpectedly (contrary to prior studies).\n- **Estimation Approach**: FISM (excluding self-estimation) outperforms FISM(F) (including self-estimation) for large \\( k \\) (avoids trivial solutions).\n- **Bias Effect**: Item bias contributes the most to performance gains; combining user and item bias may degrade results.\n- **Sparsification**: Reducing \\( S = PQ^\\top \\) density to 0.1–0.15 cuts prediction time drastically with minimal HR loss.\n\n### 2.3 Parameter Sensitivity\n- **Neighborhood Agreement (\\( \\alpha \\))**: Optimal range is 0.4–0.5 (substantial neighborhood similarity required for high ratings).\n- **Latent Factors (\\( k \\))**: Performance improves with \\( k \\) up to 128–192; FISM maintains gains over baselines for large \\( k \\).",
  "hyperparameter": "{\n  \"latent_factors_k\": \"128-192 (performance improves up to this range)\",\n  \"neighborhood_agreement_alpha\": \"0.4-0.5 (optimal range)\",\n  \"regularization_beta\": \"regularization for P and Q matrices (specific values not provided)\",\n  \"regularization_lambda\": \"regularization for user bias (specific values not provided)\",\n  \"regularization_gamma\": \"regularization for item bias (specific values not provided)\",\n  \"negative_sampling_ratio_rho\": \"ρ · nnz(R) negative samples per iteration for FISMrmse\",\n  \"sparsification_density\": \"0.1-0.15 (for S=PQ^T to reduce prediction time with minimal performance loss)\",\n  \"learning_algorithm\": \"Stochastic Gradient Descent (SGD)\"\n}"
}