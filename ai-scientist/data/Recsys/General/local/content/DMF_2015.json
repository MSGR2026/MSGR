{
  "id": "DMF_2015",
  "paper_title": "Deep Matrix Factorization Models for Recommender Systems",
  "alias": "DMF",
  "year": 2015,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "The paper proposes Deep Matrix Factorization (DMF), which uses two separate deep neural networks to project users and items into a common latent space. The key innovations are: (1) using the user-item interaction matrix directly as input (rather than random initialization) where users are represented by their rating vectors across all items and items by rating vectors from all users, (2) introducing a normalized cross entropy loss (nce) that integrates both explicit ratings and implicit feedback by normalizing ratings with max(R) to handle different rating scales, and (3) using cosine similarity in the latent space for final prediction.",
  "introduction": "# Introduction\nIn the era of information explosion, information overload is one of the dilemmas we are confronted with. Recommender systems (RSs) are instrumental to address this problem as they help determine which information to offer to individual consumers and allow online users to quickly find the personalized information that fits their needs. RSs are nowadays ubiquitous in e-commerce platforms, such as recommendation of books at Amazon, music at Last.com, movie at Netflix and reference at CiteULike.\n\nCollaborative filtering (CF) recommender approaches are extensively investigated in research community and widely used in industry. They are based on the simple intuition that if users rate items similarly in the past, they are likely to rate other items similarly in the future. As the most popular approach among various collaborative filtering techniques, matrix factorization (MF) which learns a latent space to represent a user or an item becomes a standard model for recommendation due to its scalability, simplicity, and flexibility. In the latent space, the recommender system predicts a personalized ranking over a set of items for each individual user with the similarities among the users and items.\n\nRatings in the user-item interaction matrix are explicit knowledge which have been deeply exploited in early recommendation methods. Because of the variation in rating values associated with users on items, biased matrix factorization are used to enhance the rating prediction. To overcome the sparseness of the ratings, additional extra data are integrated into MF, such as social matrix factorization with social relations, topic matrix factorization with item contents or reviews text, and so on.\n\nHowever, modeling only observed ratings is insufficient to make good top-N recommendations. Implicit feedback, such as purchase history and unobserved ratings, is applied in recommender systems. The SVD++ model firstly factorizes the rating matrix with the implicit feedback, and is followed by many techniques for recommender systems.\n\nRecently, due to the powerful representation learning abilities, deep learning methods have been successfully applied including various areas of Computer Vision, Audio Recognition and Natural Language Processing. A few efforts have also been made to apply deep learning models in recommender systems. Restricted Boltzmann Machines was firstly proposed to model users’ explicit ratings on items. Autoencoders and the denoising autoencoders have also been applied for recommendation. The key idea of these methods is to reconstruct the users’ ratings through learning hidden structures with the explicit historical ratings. Implicit feedback is also applied in this research line of deep learning for recommendation. An extended work presented a collaborative denoising autoencoder (CDAE) to model user’s preference with implicit feedback. Another work of neural collaborative filtering (NCF) was proposed to model the user-item interactions with a multi-layer feedforward neural network. Two recent works above exploit only implicit feedback for item recommendations instead of explicit rating feedback.\n\nIn this paper, to make use of both explicit ratings and implicit feedback, we propose a new neural matrix factorization model for top-N recommendation. We firstly construct a user-item matrix with both explicit ratings and nonpreference implicit feedback, which is different from other related methods using either only explicit ratings or only implicit ratings. With this full matrix (explicit ratings and zero of implicit feedback) as input, a neural network architecture is proposed to learn a common latent low dimensional space to represent the users and items. This architecture is inspired by the deep structured semantic models which have been proved to be useful for web search, where it can map the query and document in a latent space through multiple layers of non-linear projections. In addition, we design a new loss function based on cross entropy, which includes the considerations of both explicit ratings and implicit feedback.\n\nIn sum, our main contributions are outlined as follows:\n1. We propose novel deep matrix factorization models with a neural network that map the users and items into a common low-dimensional space with non-linear projections. We use a matrix including both explicit ratings and non-preference implicit feedback as the input of our models.\n2. We design a new loss function to consider both explicit ratings and implicit feedback for better optimization.\n3. The experimental results show the effectiveness of our proposed models which outperform other state-of-the-art methods in top-N recommendation.",
  "method": "# Method\n## 1. Problem Statement\nSuppose there are M users \\(U=\\{u_{1}, ..., u_{M}\\}\\), N items \\(V=\\{v_{1}, ..., v_{N}\\}\\). Let \\(R \\in \\mathbb{R}^{M ×N}\\) denote the rating matrix, where \\(R_{i j}\\) is the rating of user i on item j, and we mark unk if it is unknown. We construct the user-item interaction matrix \\(Y \\in \\mathbb{R}^{M ×N}\\) from R as follows:\n\\[Y_{ij}=\\left\\{ \\begin{array} {ll}{0, }&{if R_{ij}=unk}\\\\ {R_{ij}, }&{otherwise}\\end{array} \\right.\\]\nHere, explicit ratings are reserved in Y to indicate the user’s preference degree, and unknown ratings are marked as 0 (non-preference implicit feedback).\n\nThe goal of recommender systems is to estimate the unobserved entries in Y for item ranking. Model-based approaches assume an underlying model to generate all ratings:\n\\[\\hat{Y}_{i j}=F\\left(u_{i}, v_{j} | \\Theta\\right)\\]\nwhere \\(\\hat{Y}_{i j}\\) is the predicted score, Θ is the model parameter, and F is the mapping function. We follow the Latent Factor Model (LFM) to use inner product for user-item interactions, and achieve non-linear connections through deep representation learning.\n\nWe use the following notations: \\(Y^{+}\\) (observed interactions), \\(Y^{-}\\) (zero elements in Y), \\(Y_{sampled }^{-}\\) (sampled negative instances), \\(Y_{i *}\\) (i-th row of Y), \\(Y_{* j}\\) (j-th column of Y).\n\n## 2. Deep Matrix Factorization Models (DMF)\n### 2.1 Model Architecture\nInspired by Deep Structured Semantic Models (DSSM), we propose a deep neural network architecture to project users and items into a common latent space. The architecture includes:\n- **Input Layer**: Takes the interaction matrix Y as input. Each user \\(u_i\\) is represented by \\(Y_{i *}\\) (ratings across all items), and each item \\(v_j\\) by \\(Y_{* j}\\) (ratings from all users).\n- **Hidden Layers**: Multiple non-linear projection layers with ReLU activation. We use two separate multi-layer networks for users and items to learn their latent representations.\n- **Latent Space**: The final output of the hidden layers are low-dimensional vectors \\(p_i\\) (user) and \\(q_j\\) (item), computed as:\n\\[p_{i}=f_{\\theta_{N}^{U}}\\left(... f_{\\theta_{3}^{U}}\\left(W_{U 2} f_{\\theta_{2}^{U}}\\left(Y_{i *} W_{U 1}\\right)\\right) ...\\right)\\]\n\\[q_{j}=f_{\\theta_{N}^{I}}\\left(... f_{\\theta_{3}^{I}}\\left(W_{V 2} f_{\\theta_{2}^{I}}\\left(Y_{* j}^{T} W_{V 1}\\right)\\right) ...\\right)\\]\nwhere \\(W_{U k}\\) and \\(W_{V k}\\) are weight matrices for user and item networks, respectively.\n- **Similarity Calculation**: The predicted score is the cosine similarity between \\(p_i\\) and \\(q_j\\):\n\\[\\hat{Y}_{i j}=cosine\\left(p_{i}, q_{j}\\right)=\\frac{p_{i}^{T} q_{j}}{\\left\\| p_{i}\\right\\| \\left\\| q_{j}\\right\\| }\\]\n\n### 2.2 Loss Function\nWe design a **normalized cross entropy loss (nce)** to integrate explicit ratings and implicit feedback:\n\\[L=-\\sum_{(i, j) \\in Y^{+} \\cup Y^{-}}\\left(\\frac{Y_{i j}}{max (R)} log \\hat{Y}_{i j}^{o}+\\left(1-\\frac{Y_{i j}}{max (R)}\\right) log \\left(1-\\hat{Y}_{i j}^{o}\\right)\\right)\\]\nwhere \\(max(R)\\) is the maximum rating (5 for 5-star systems) for normalization, and \\(\\hat{Y}_{i j}^{o}=max(\\mu, \\hat{Y}_{i j})\\) (μ=1e-6) to avoid negative predicted scores.\n\nWe also compare with cross entropy loss (ce) for ablation study:\n\\[L=-\\sum_{(i, j) \\in Y^{+} \\cup Y^{-}} Y_{i j} log \\hat{Y}_{i j}^{o}+\\left(1-Y_{i j}\\right) log \\left(1-\\hat{Y}_{i j}^{o}\\right)\\]\n\n### 2.3 Training Algorithm\n1. Initialize weight matrices \\(W_U\\) and \\(W_V\\) with Gaussian distribution (mean=0, std=0.01).\n2. Construct Y from R, and sample \\(Y_{sampled }^{-}\\) (7 negative instances per positive instance in experiments).\n3. For each iteration:\n   - For each (i,j) in \\(Y^{+} \\cup Y_{sampled }^{-}\\), compute \\(p_i\\) and \\(q_j\\) via forward propagation.\n   - Calculate predicted score \\(\\hat{Y}_{i j}^{o}\\) and loss L.\n   - Update parameters via back propagation with mini-batch Adam optimizer (batch size=256, learning rate=0.0001).",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nWe evaluate on four benchmark datasets, filtered to retain users with ≥20 interactions and items with ≥5 interactions:\n| Dataset | #users | #items | #ratings | Rating Density |\n|---------|--------|--------|----------|----------------|\n| ML100k | 944 | 1,683 | 100,000 | 0.06294 |\n| ML1m | 6,040 | 3,706 | 1,000,209 | 0.04468 |\n| Amusic | 844 | 18,813 | 46,468 | 0.00292 |\n| Amovie | 9,582 | 92,221 | 766,759 | 0.00087 |\n\n### 1.2 Evaluation Protocol\n- **Leave-one-out split**: Hold out the latest user interaction as test item, use the rest for training.\n- **Negative sampling**: For each test item, sample 100 non-interacted items to form the test set.\n- **Metrics**: Hit Ratio (HR@10) and Normalized Discounted Cumulative Gain (NDCG@10). HR measures if the test item is in top-10, NDCG measures ranking quality.\n\n### 1.3 Implementation Details\n- Framework: TensorFlow.\n- Hyperparameter tuning: Use validation set (one random interaction per user) to tune parameters.\n- Optimization: Mini-batch Adam (batch size=256, learning rate=0.0001).\n- Negative sampling ratio: 7 negative instances per positive instance.\n\n## 2. Baseline Methods\n- **ItemPop**: Non-personalized baseline, ranks items by popularity.\n- **ItemKNN**: Item-based collaborative filtering (Amazon’s commercial method).\n- **eALS**: State-of-the-art MF with square loss, weights unobserved interactions by item popularity.\n- **NeuMF-p**: State-of-the-art neural MF with cross entropy loss, initializes representations randomly (best-performing variant with pre-training).\n- **DMF-2-ce**: Proposed DMF with 2 hidden layers and cross entropy loss.\n- **DMF-2-nce**: Proposed DMF with 2 hidden layers and normalized cross entropy loss.\n\n## 3. Main Results\n### 3.1 Performance Comparison\nTable 2 shows the comparison results of all methods. Key findings:\n- Both DMF variants outperform state-of-the-art baselines on all datasets.\n- DMF-2-nce achieves 2.5-7.4% (average 5.1%) relative improvement in NDCG@10 and 1.4-6.8% (average 3.8%) in HR@10 over NeuMF-p.\n- DMF-2-nce outperforms DMF-2-ce in most cases, verifying the effectiveness of the normalized cross entropy loss.\n\n### 3.2 Impact of Input Matrix\nWe compare DMF-1-nce (1 hidden layer, Y as input) with LFM-nce (1 hidden layer, random initialization):\n- DMF-1-nce outperforms LFM-nce on all datasets, proving that using the interaction matrix Y as input is more effective than random initialization.\n\n### 3.3 Sensitivity Analysis\n- **Negative sampling ratio**: Optimal ratio is around 5, consistent with previous work.\n- **Number of hidden layers**: 2 layers achieve the best performance; deeper layers (3+) decrease performance due to overfitting.\n- **Latent space dimension**: 64 factors are optimal for most datasets; 128 factors work better for sparse small datasets (Amusic).",
  "hyperparameter": "Learning rate: 0.0001; Batch size: 256; Optimizer: mini-batch Adam; Negative sampling ratio: 7 negative instances per positive instance (optimal around 5); Number of hidden layers: 2 layers (best performance); Latent space dimension: 64 factors (optimal for most datasets, 128 for sparse datasets); Weight initialization: Gaussian distribution with mean=0 and std=0.01; μ (minimum predicted score): 1e-6; Activation function: ReLU; Evaluation metrics: HR@10 and NDCG@10 with 100 negative samples per test item"
}