{
  "id": "LINE_2015",
  "paper_title": "LINE: Large-scale Information Network Embedding",
  "alias": "LINE",
  "year": 2015,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "LINE (Large-scale Information Network Embedding) proposes a scalable network embedding method that preserves both first-order proximity (local pairwise similarity between directly connected nodes) and second-order proximity (similarity of neighborhood structures). The core innovation is decomposing these two types of proximity into separate objective functions optimized via negative sampling and edge sampling, enabling linear time complexity O(dK|E|) that scales to billion-edge networks. The method treats each vertex with dual roles (vertex and context) for second-order proximity and combines both proximities through concatenation for comprehensive network representation.",
  "introduction": "# Introduction\nInformation networks are ubiquitous in the real world, including airline networks, social networks, citation networks, and the World Wide Web. These networks range from hundreds to billions of nodes, and analyzing large-scale information networks has attracted increasing attention in academia and industry. Network embedding—representing each vertex as a low-dimensional vector—plays a crucial role in tasks like visualization, node classification, link prediction, and recommendation.\n\nExisting graph embedding methods (e.g., MDS, IsoMap, Laplacian eigenmap) perform well on small networks but fail to scale to millions of nodes, as their time complexity is at least quadratic to the number of vertices. Recent attempts to handle large networks either use indirect approaches not tailored for networks or lack clear objective functions for preserving network structures.\n\nTo address these limitations, we propose a novel network embedding method called \"LINE,\" which is suitable for arbitrary types of information networks (undirected, directed, weighted) and scales to millions of nodes. The key insights are:\n1. **Dual Proximity Preservation**: Existing methods mostly preserve first-order proximity (local pairwise relationships via observed edges), but real-world networks have sparse observed links. We complement this with second-order proximity (similarity of neighborhood structures), which captures global network patterns (e.g., nodes with shared neighbors are similar).\n2. **Efficient Optimization**: Direct stochastic gradient descent (SGD) struggles with weighted networks due to high variance in edge weights, leading to gradient explosion. We propose an edge-sampling algorithm that samples edges proportional to their weights, treating them as binary edges for stable and efficient training.\n\nLINE’s contributions are:\n1. A scalable network embedding model that preserves both first-order and second-order proximities, adapting to any network type.\n2. An edge-sampling optimization technique that addresses SGD’s limitations on weighted edges, improving effectiveness and efficiency.\n3. Extensive experiments on real-world networks (language, social, citation) showing LINE outperforms state-of-the-art methods in tasks like word analogy, node classification, and visualization, while processing millions of nodes and billions of edges in hours on a single machine.",
  "method": "# Method\n## 1. Problem Definition\n### 1.1 Key Concepts\n- **Information Network**: \\( G=(V, E) \\), where \\( V \\) is the set of vertices, \\( E \\) is the set of edges. Each edge \\( (u, v) \\in E \\) has a weight \\( w_{uv} > 0 \\) (undirected networks satisfy \\( w_{uv}=w_{vu} \\); directed networks do not).\n- **First-order Proximity**: Local pairwise proximity between linked vertices, measured by edge weight \\( w_{uv} \\); zero for unlinked vertices.\n- **Second-order Proximity**: Similarity of neighborhood structures between vertices. For vertex \\( u \\), let \\( p_u=(w_{u1}, w_{u2}, ..., w_{u|V|}) \\) denote its first-order proximity to all vertices; second-order proximity between \\( u \\) and \\( v \\) is the similarity between \\( p_u \\) and \\( p_v \\).\n- **Network Embedding**: Learn a function \\( f_G: V \\to \\mathbb{R}^d \\) ( \\( d \\ll |V| \\) ) that preserves both first-order and second-order proximities.\n\n## 2. LINE Model\n### 2.1 Preserving First-order Proximity (Undirected Networks Only)\nFor an undirected edge \\( (i, j) \\), define the joint probability of vertices \\( i \\) and \\( j \\) in the embedded space:\n\\[ p_1(v_i, v_j) = \\frac{1}{1 + \\exp(-\\vec{u}_i^T \\cdot \\vec{u}_j)} \\]\nwhere \\( \\vec{u}_i \\in \\mathbb{R}^d \\) is the embedding vector of vertex \\( i \\).\n\nThe empirical probability is \\( \\hat{p}_1(i, j) = \\frac{w_{ij}}{W} \\) ( \\( W = \\sum_{(i,j) \\in E} w_{ij} \\) ). The objective is to minimize the KL-divergence between \\( p_1 \\) and \\( \\hat{p}_1 \\):\n\\[ O_1 = -\\sum_{(i,j) \\in E} w_{ij} \\log p_1(v_i, v_j) \\]\n\n### 2.2 Preserving Second-order Proximity (All Network Types)\nEach vertex plays two roles: \"vertex\" and \"context.\" We introduce two vectors for each vertex \\( i \\): \\( \\vec{u}_i \\) (as vertex) and \\( \\vec{u}_i' \\) (as context). For a directed edge \\( (i, j) \\), the conditional probability of context \\( j \\) generated by vertex \\( i \\) is:\n\\[ p_2(v_j | v_i) = \\frac{\\exp(\\vec{u}_j'^T \\cdot \\vec{u}_i)}{\\sum_{k=1}^{|V|} \\exp(\\vec{u}_k'^T \\cdot \\vec{u}_i)} \\]\n\nThe empirical probability is \\( \\hat{p}_2(v_j | v_i) = \\frac{w_{ij}}{d_i} \\) ( \\( d_i = \\sum_{k \\in N(i)} w_{ik} \\), \\( N(i) \\) is the out-neighbors of \\( i \\) ). The objective (weighted by vertex degree \\( \\lambda_i = d_i \\)) is:\n\\[ O_2 = -\\sum_{(i,j) \\in E} w_{ij} \\log p_2(v_j | v_i) \\]\n\n### 2.3 Combining Proximities\nTrain first-order and second-order models separately, then concatenate their embedding vectors for final representation (supervised tasks) or use either individually (unsupervised tasks).\n\n## 3. Model Optimization\n### 3.1 Negative Sampling\nTo reduce computational cost of \\( p_2 \\) (summing over all vertices), use negative sampling. For each edge \\( (i, j) \\), optimize:\n\\[ \\log \\sigma(\\vec{u}_j'^T \\cdot \\vec{u}_i) + K \\cdot \\mathbb{E}_{v_k \\sim P_n(v)} \\left[ \\log \\sigma(-\\vec{u}_k'^T \\cdot \\vec{u}_i) \\right] \\]\nwhere \\( \\sigma(x) = 1/(1+\\exp(-x)) \\), \\( K=5 \\) (number of negative samples), and \\( P_n(v) \\propto d_v^{3/4} \\) (noise distribution based on out-degree).\n\nFor first-order proximity, replace \\( \\vec{u}_j' \\) with \\( \\vec{u}_j \\) in the above objective.\n\n### 3.2 Edge Sampling\nTo handle weighted edges (high variance leads to gradient explosion), sample edges proportional to their weights using the alias table method ( \\( O(1) \\) per sample). Treat sampled edges as binary edges for model updating, preserving the original objective while stabilizing gradients.\n\n### 3.3 Practical Considerations\n- **Low-Degree Vertices**: Expand neighbors by adding second-order neighbors (neighbors of neighbors) to improve embedding accuracy.\n- **New Vertices**: If connections to existing vertices are known, optimize \\( O_1 \\) or \\( O_2 \\) to learn the new vertex’s embedding (fix existing embeddings).\n\n## 4. Complexity Analysis\nOverall time complexity is \\( O(dK|E|) \\), linear in the number of edges \\( |E| \\) and independent of \\( |V| \\), enabling scalability to large networks.",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nFive real-world networks of different types (directed/undirected, weighted/binary):\n| Dataset                | #Users/Vertices | #Edges | Type          | Key Task                  |\n|------------------------|-----------------|--------|---------------|---------------------------|\n| Wikipedia Language Network | 2M              | 1B     | Undirected, Weighted | Word analogy, Document classification |\n| Flickr Social Network  | 80K             | 5.8M   | Undirected, Binary | Multi-label node classification |\n| Youtube Social Network | 1.1M            | 2.9M   | Directed, Binary | Multi-label node classification |\n| DBLP Author Citation  | 28K             | 571K   | Directed, Weighted | Multi-label node classification |\n| DBLP Paper Citation    | 31K             | 423K   | Directed, Weighted | Multi-label node classification |\n\n### 1.2 Baselines\n- **Graph Factorization (GF)**: Matrix factorization for undirected networks, optimized via SGD.\n- **DeepWalk**: Random walk-based embedding for unweighted networks (preserves second-order proximity).\n- **LINE-SGD**: LINE model optimized with direct SGD (no edge sampling).\n- **SkipGram**: State-of-the-art word embedding (for language network comparison).\n\n### 1.3 Parameter Settings\n- Embedding dimension: 200 (language network), 128 (others); normalized to \\( \\|\\vec{w}\\|_2=1 \\).\n- Learning rate: \\( \\rho_0=0.025 \\), \\( \\rho_t=\\rho_0(1-t/T) \\) ( \\( T \\) = total samples).\n- Negative samples \\( K=5 \\); total samples \\( T=10 \\) billion (LINE), \\( T=20 \\) billion (GF).\n- Evaluation metrics: Accuracy (word analogy), Micro-F1/Macro-F1 (classification), visualization (t-SNE).\n\n## 2. Main Results\n### 2.1 Language Network\n#### 2.1.1 Word Analogy\nLINE(2nd) outperforms all baselines (including SkipGram) with 66.10% overall accuracy, as second-order proximity better captures semantic/syntactic relationships. LINE-SGD performs poorly due to gradient explosion from weighted edges.\n\n#### 2.1.2 Document Classification\nLINE(1st+2nd) achieves the highest Micro-F1 (83.74%) and Macro-F1 (83.66%), confirming first-order and second-order proximities are complementary.\n\n### 2.2 Social Networks\n#### 2.2.1 Flickr (Dense)\nLINE(1st+2nd) outperforms DeepWalk (64.75% vs. 61.29% Micro-F1), with LINE(1st) slightly better than LINE(2nd) (dense networks favor local proximity).\n\n#### 2.2.2 Youtube (Sparse)\nLINE(2nd) underperforms DeepWalk on the original sparse network but surpasses it after expanding low-degree vertices’ neighbors. LINE(1st+2nd) achieves 46.43% Micro-F1, outperforming DeepWalk (45.81%).\n\n### 2.3 Citation Networks\n#### 2.3.1 Author Citation (Sparse)\nLINE(2nd) outperforms DeepWalk after network reconstruction (expanding neighbors), as random walks fail to capture citation contexts.\n\n#### 2.3.2 Paper Citation (Directed)\nLINE(2nd) significantly outperforms DeepWalk (62.80% vs. 55.90% Micro-F1), as it directly models reference relationships instead of random walks.\n\n### 2.4 Visualization\nLINE(2nd) produces meaningful network layouts, clustering nodes of the same community (data mining, machine learning, computer vision) more effectively than GF and DeepWalk.\n\n### 2.5 Scalability\nLINE processes 2M nodes and 1B edges in <3 hours on a single machine, 5x faster than DeepWalk. It achieves near-linear speedup with multiple threads (up to 16 threads).\n\n## 3. Key Observations\n- **Proximity Complementarity**: Combining first-order and second-order proximities consistently improves performance.\n- **Edge Sampling**: Critical for weighted networks; LINE outperforms LINE-SGD by a large margin.\n- **Network Sparsity**: Second-order proximity struggles with extreme sparsity but improves after expanding neighbors; first-order proximity is more robust for sparse networks.",
  "hyperparameter": "Embedding dimension d: 200 (language networks), 128 (social/citation networks); Learning rate: initial ρ₀=0.025 with linear decay ρₜ=ρ₀(1-t/T); Number of negative samples K=5; Noise distribution for negative sampling: P_n(v) ∝ d_v^(3/4) (proportional to vertex out-degree to power 0.75); Total training samples T: 10 billion (LINE), 20 billion (baseline GF); Edge sampling: proportional to edge weights using alias table method; Embedding vectors normalized to unit length ||w||₂=1"
}