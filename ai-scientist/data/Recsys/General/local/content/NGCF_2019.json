{
  "id": "NGCF_2019",
  "paper_title": "Neural Graph Collaborative Filtering",
  "alias": "NGCF",
  "year": 2019,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "NGCF (Neural Graph Collaborative Filtering) introduces embedding propagation layers that explicitly inject collaborative signals into user and item representations by propagating embeddings on the user-item interaction graph. The core innovation is using graph neural networks to capture high-order connectivity (multi-hop neighbors) in collaborative filtering, where message construction incorporates both linear transformation and feature interaction (element-wise product) between connected nodes. The model aggregates representations from multiple propagation layers via concatenation to capture collaborative signals at different orders, going beyond traditional methods that only use direct user-item interactions.",
  "introduction": "# 1 INTRODUCTION\n\nPersonalized recommendation is ubiquitous, having been applied to many online services such as E-commerce, advertising, and social media. At its core is estimating how likely a user will adopt an item based on the historical interactions like purchases and clicks. Collaborative filtering (CF) addresses it by assuming that behaviorally similar users would exhibit similar preference on items. To implement the assumption, a common paradigm is to parameterize users and items for reconstructing historical interactions, and predict user preference based on the parameters [1, 14].\n\nGenerally speaking, there are two key components in learnable CF models - 1) embedding, which transforms users and items to vectorized representations, and 2) interaction modeling, which reconstructs historical interactions based on the embeddings. For example, matrix factorization (MF) directly embeds user/item ID as an vector and models user-item interaction with inner product [20]; collaborative deep learning extends the MF embedding function by integrating the deep representations learned from rich side information of items [30]; neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks [14]; and translation-based CF models instead use Euclidean distance metric as the interaction function [28], among others.\n\nDespite their effectiveness, we argue that these methods are not sufficient to yield satisfactory embeddings for CF. The key reason is that the embedding function lacks an explicit encoding of the crucial collaborative signal, which is latent in user-item interactions to reveal the behavioral similarity between users (or items). To be more specific, most existing methods build the embedding function with the descriptive features only (e.g., ID and attributes), without considering the user-item interactions - which are only used to define the objective function for model training [26, 28]. As a result,\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/c21bea7eb5696d90d1b6e0c09b0c486bbb1dd9fd987138c6abdeef12eb6853b7.jpg)  \nFigure 1: An illustration of the user-item interaction graph and the high-order connectivity. The node  $u_{1}$  is the target user to provide recommendations for.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/2ca0c4978e6de8b276c3767f6eab7c3c2b5fca0ce0826d7ef7218d480038a146.jpg)\n\nwhen the embeddings are insufficient in capturing CF, the methods have to rely on the interaction function to make up for the deficiency of suboptimal embeddings [14].\n\nWhile intuitively useful to integrate user-item interactions into the embedding function, it is non-trivial to do it well. In particular, the scale of interactions can easily reach millions or even larger in real applications, making it difficult to distill the desired collaborative signal. In this work, we tackle the challenge by exploiting the high-order connectivity from user-item interactions, a natural way that encodes collaborative signal in the interaction graph structure.\n\nRunning Example. Figure 1 illustrates the concept of high-order connectivity. The user of interest for recommendation is  $u_{1}$ , labeled with the double circle in the left subfigure of user-item interaction graph. The right subfigure shows the tree structure that is expanded from  $u_{1}$ . The high-order connectivity denotes the path that reaches  $u_{1}$  from any node with the path length  $l$  larger than 1. Such high-order connectivity contains rich semantics that carry collaborative signal. For example, the path  $u_{1} \\gets i_{2} \\gets u_{2}$  indicates the behavior similarity between  $u_{1}$  and  $u_{2}$ , as both users have interacted with  $i_{2}$ ; the longer path  $u_{1} \\gets i_{2} \\gets u_{2} \\gets i_{4}$  suggests that  $u_{1}$  is likely to adopt  $i_{4}$ , since her similar user  $u_{2}$  has consumed  $i_{4}$  before. Moreover, from the holistic view of  $l = 3$ , item  $i_{4}$  is more likely to be of interest to  $u_{1}$  than item  $i_{5}$ , since there are two paths connecting  $< i_{4}, u_{1}>$ , while only one path connects  $< i_{5}, u_{1}>$ .\n\nPresent Work. We propose to model the high-order connectivity information in the embedding function. Instead of expanding the interaction graph as a tree which is complex to implement, we design a neural network method to propagate embeddings recursively on the graph. This is inspired by the recent developments of graph neural networks [8, 32, 38], which can be seen as constructing information flows in the embedding space. Specifically, we devise an embedding propagation layer, which refines a user's (or an item's) embedding by aggregating the embeddings of the interacted items (or users). By stacking multiple embedding propagation layers, we can enforce the embeddings to capture the collaborative signal in high-order connectivities. Taking Figure 1 as an example, stacking two layers captures the behavior similarity of  $u_{1} \\gets i_{2} \\gets u_{2}$ , stacking three layers captures the potential recommendations of  $u_{1} \\gets i_{2} \\gets u_{2} \\gets i_{4}$ , and the strength of the information flow (which is estimated by the trainable weights between layers) determines the recommendation priority of  $i_{4}$  and  $i_{5}$ . We conduct extensive experiments on three public\n\nbenchmarks to verify the rationality and effectiveness of our Neural Graph Collaborative Filtering (NGCF) method.\n\nLastly, it is worth mentioning that although the high-order connectivity information has been considered in a very recent method named HOP-Rec [40], it is only exploited to enrich the training data. Specifically, the prediction model of HOP-Rec remains to be MF, while it is trained by optimizing a loss that is augmented with high-order connectivities. Distinct from HOP-Rec, we contribute a new technique to integrate high-order connectivities into the prediction model, which empirically yields better embeddings than HOP-Rec for CF.\n\nTo summarize, this work makes the following main contributions:\n\n- We highlight the critical importance of explicitly exploiting the collaborative signal in the embedding function of model-based CF methods.  \n- We propose NGCF, a new recommendation framework based on graph neural network, which explicitly encodes the collaborative signal in the form of high-order connectivities by performing embedding propagation.  \n- We conduct empirical studies on three million-size datasets. Extensive results demonstrate the state-of-the-art performance of NGCF and its effectiveness in improving the embedding quality with neural embedding propagation.\n",
  "method": "# 2 METHODOLOGY\n\nWe now present the proposed NGCF model, the architecture of which is illustrated in Figure 2. There are three components in the framework: (1) an embedding layer that offers and initialization of user embeddings and item embeddings; (2) multiple embedding propagation layers that refine the embeddings by injecting high-order connectivity relations; and (3) the prediction layer that aggregates the refined embeddings from different propagation layers and outputs the affinity score of a user-item pair. Finally, we discuss the time complexity of NGCF and the connections with existing methods.\n\n# 2.1 Embedding Layer\n\nFollowing mainstream recommender models [1, 14, 26], we describe a user  $u$  (an item  $i$ ) with an embedding vector  $\\mathbf{e}_u \\in \\mathbb{R}^d$  ( $\\mathbf{e}_i \\in \\mathbb{R}^d$ ), where  $d$  denotes the embedding size. This can be seen as building a parameter matrix as an embedding look-up table:\n\n$$\n\\mathbf {E} = [ \\underbrace {\\mathbf {e} _ {u _ {1}} , \\cdots , \\mathbf {e} _ {u _ {N}}} _ {,}, \\underbrace {\\mathbf {e} _ {i _ {1}} , \\cdots , \\mathbf {e} _ {i _ {M}}} ] \\tag {1}\n$$\n\nusers embeddings item embeddings\n\nIt is worth noting that this embedding table serves as an initial state for user embeddings and item embeddings, to be optimized in an end-to-end fashion. In traditional recommender models like MF and neural collaborative filtering [14], these ID embeddings are directly fed into an interaction layer (or operator) to achieve the prediction score. In contrast, in our NGCF framework, we refine the embeddings by propagating them on the user-item interaction graph. This leads to more effective embeddings for recommendation, since the embedding refinement step explicitly injects collaborative signal into embeddings.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/19fedc304748eb9b5cf00f3b912d6c5946f32b2cf3b94a8da6dd58220783c4df.jpg)  \nFigure 2: An illustration of NGCF model architecture (the arrowed lines present the flow of information). The representations of user  $u_{1}$  (left) and item  $i_{4}$  (right) are refined with multiple embedding propagation layers, whose outputs are concatenated to make the final prediction.\n\n# 2.2 Embedding Propagation Layers\n\nNext we build upon the message-passing architecture of GNNs [8, 38] in order to capture CF signal along the graph structure and refine the embeddings of users and items. We first illustrate the design of one-layer propagation, and then generalize it to multiple successive layers.\n\n2.2.1 First-order Propagation. Intuitively, the interacted items provide direct evidence on a user's preference [16, 39]; analogously, the users that consume an item can be treated as the item's features and used to measure the collaborative similarity of two items. We build upon this basis to perform embedding propagation between the connected users and items, formulating the process with two major operations: message construction and message aggregation.\n\nMessage Construction. For a connected user-item pair  $(u,i)$ , we define the message from  $i$  to  $u$  as:\n\n$$\n\\mathbf {m} _ {u \\leftarrow i} = f \\left(\\mathbf {e} _ {i}, \\mathbf {e} _ {u}, p _ {u i}\\right), \\tag {2}\n$$\n\nwhere  $\\mathbf{m}_{u\\leftarrow i}$  is the message embedding (i.e., the information to be propagated).  $f(\\cdot)$  is the message encoding function, which takes embeddings  $\\mathbf{e}_i$  and  $\\mathbf{e}_u$  as input, and uses the coefficient  $p_{ui}$  to control the decay factor on each propagation on edge  $(u,i)$ .\n\nIn this work, we implement  $f(\\cdot)$  as:\n\n$$\n\\mathbf {m} _ {u \\leftarrow i} = \\frac {1}{\\sqrt {\\left| \\mathcal {N} _ {u} \\right| \\left| \\mathcal {N} _ {i} \\right|}} \\left(\\mathbf {W} _ {1} \\mathbf {e} _ {i} + \\mathbf {W} _ {2} \\left(\\mathbf {e} _ {i} \\odot \\mathbf {e} _ {u}\\right)\\right), \\tag {3}\n$$\n\nwhere  $\\mathbf{W}_1, \\mathbf{W}_2 \\in \\mathbb{R}^{d' \\times d}$  are the trainable weight matrices to distill useful information for propagation, and  $d'$  is the transformation size. Distinct from conventional graph convolution networks [4, 18, 29, 42] that consider the contribution of  $\\mathbf{e}_i$  only, here we additionally encode the interaction between  $\\mathbf{e}_i$  and  $\\mathbf{e}_u$  into the message being passed via  $\\mathbf{e}_i \\odot \\mathbf{e}_u$ , where  $\\odot$  denotes the element-wise product. This makes the message dependent on the affinity between  $\\mathbf{e}_i$  and  $\\mathbf{e}_u$ , e.g., passing more messages from the similar items. This not\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/03a45512d086f2cb077434eb4c4064597f1ff595f836db87beab35f5de2731af.jpg)  \nFigure 3: Illustration of third-order embedding propagation for user  $u_{1}$ . Best view in color.\n\nonly increases the model representation ability, but also boosts the performance for recommendation (evidences in our experiments Section 4.4.2).\n\nFollowing the graph convolutional network [18], we set  $p_{ui}$  as the graph Laplacian norm  $1 / \\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}$ , where  $\\mathcal{N}_u$  and  $\\mathcal{N}_i$  denote the first-hop neighbors of user  $u$  and item  $i$ . From the viewpoint of representation learning,  $p_{ui}$  reflects how much the historical item contributes the user preference. From the viewpoint of message passing,  $p_{ui}$  can be interpreted as a discount factor, considering the messages being propagated should decay with the path length.\n\nMessage Aggregation. In this stage, we aggregate the messages propagated from  $u$ 's neighborhood to refine  $u$ 's representation. Specifically, we define the aggregation function as:\n\n$$\n\\mathbf {e} _ {u} ^ {(1)} = \\text {L e a k y R e L U} \\left(\\mathbf {m} _ {u \\leftarrow u} + \\sum_ {i \\in \\mathcal {N} _ {u}} \\mathbf {m} _ {u \\leftarrow i}\\right), \\tag {4}\n$$\n\nwhere  $\\mathbf{e}_u^{(1)}$  denotes the representation of user  $u$  obtained after the first embedding propagation layer. The activation function of LeakyReLU [23] allows messages to encode both positive and small negative signals. Note that in addition to the messages propagated from neighbors  $\\mathcal{N}_u$ , we take the self-connection of  $u$  into consideration:  $\\mathbf{m}_{u\\leftarrow u} = \\mathbf{W}_1\\mathbf{e}_u$ , which retains the information of original features ( $\\mathbf{W}_1$  is the weight matrix shared with the one used in Equation (3)). Analogously, we can obtain the representation  $\\mathbf{e}_i^{(1)}$  for item  $i$  by propagating information from its connected users. To summarize, the advantage of the embedding propagation layer lies in explicitly exploiting the first-order connectivity information to relate user and item representations.\n\n2.2.2 High-order Propagation. With the representations augmented by first-order connectivity modeling, we can stack more embedding propagation layers to explore the high-order connectivity information. Such high-order connectivities are crucial to encode the collaborative signal to estimate the relevance score between a user and item.\n\nBy stacking  $l$  embedding propagation layers, a user (and an item) is capable of receiving the messages propagated from its  $l$ -hop neighbors. As Figure 2 displays, in the  $l$ -th step, the representation of user  $u$  is recursively formulated as:\n\n$$\n\\mathbf {e} _ {u} ^ {(l)} = \\text {L e a k y R e L U} \\left(\\mathbf {m} _ {u \\leftarrow u} ^ {(l)} + \\sum_ {i \\in \\mathcal {N} _ {u}} \\mathbf {m} _ {u \\leftarrow i} ^ {(l)}\\right), \\tag {5}\n$$\n\nwherein the messages being propagated are defined as follows,\n\n$$\n\\left\\{ \\begin{array}{l} \\mathbf {m} _ {u \\leftarrow i} ^ {(l)} = p _ {u i} \\left(\\mathbf {W} _ {1} ^ {(l)} \\mathbf {e} _ {i} ^ {(l - 1)} + \\mathbf {W} _ {2} ^ {(l)} \\left(\\mathbf {e} _ {i} ^ {(l - 1)} \\odot \\mathbf {e} _ {u} ^ {(l - 1)}\\right)\\right), \\\\ \\mathbf {m} _ {u \\leftarrow u} ^ {(l)} = \\mathbf {W} _ {1} ^ {(l)} \\mathbf {e} _ {u} ^ {(l - 1)}, \\end{array} \\right. \\tag {6}\n$$\n\nwhere  $\\mathbf{W}_1^{(l)},\\mathbf{W}_2^{(l)},\\in \\mathbb{R}^{d_l\\times d_{l - 1}}$  are the trainable transformation matrices, and  $d_{l}$  is the transformation size;  $\\mathbf{e}_i^{(l - 1)}$  is the item representation generated from the previous message-passing steps, memorizing the messages from its (l-1)-hop neighbors. It further contributes to the representation of user  $u$  at layer  $l$ . Analogously, we can obtain the representation for item  $i$  at the layer  $l$ .\n\nAs Figure 3 shows, the collaborative signal like  $u_{1} \\gets i_{2} \\gets u_{2} \\gets i_{4}$  can be captured in the embedding propagation process. Furthermore, the message from  $i_{4}$  is explicitly encoded in  $\\mathbf{e}_{u_{1}}^{(3)}$  (indicated by the red line). As such, stacking multiple embedding propagation layers seamlessly injects collaborative signal into the representation learning process.\n\nPropagation Rule in Matrix Form. To offer a holistic view of embedding propagation and facilitate batch implementation, we provide the matrix form of the layer-wise propagation rule (equivalent to Equations (5) and (6)):\n\n$$\n\\mathbf {E} ^ {(l)} = \\text {L e a k y R e L U} \\left(\\left(\\mathcal {L} + \\mathbf {I}\\right) \\mathbf {E} ^ {(l - 1)} \\mathbf {W} _ {1} ^ {(l)} + \\mathcal {L} \\mathbf {E} ^ {(l - 1)} \\odot \\mathbf {E} ^ {(l - 1)} \\mathbf {W} _ {2} ^ {(l)}\\right), \\tag {7}\n$$\n\nwhere  $\\mathbf{E}^{(l)}\\in \\mathbb{R}^{(N + M)\\times d_l}$  are the representations for users and items obtained after  $l$  steps of embedding propagation.  $\\mathbf{E}^{(0)}$  is set as  $\\mathbf{E}$  at the initial message-passing iteration, that is  $\\mathbf{e}_u^{(0)} = \\mathbf{e}_u$  and  $\\mathbf{e}_i^{(0)} = \\mathbf{e}_i$ ; and I denote an identity matrix.  $\\mathcal{L}$  represents the Laplacian matrix for the user-item graph, which is formulated as:\n\n$$\n\\mathcal {L} = \\mathbf {D} ^ {- \\frac {1}{2}} \\mathbf {A} \\mathbf {D} ^ {- \\frac {1}{2}} \\text {a n d} \\mathbf {A} = \\left[ \\begin{array}{c c} \\mathbf {0} & \\mathbf {R} \\\\ \\mathbf {R} ^ {\\top} & \\mathbf {0} \\end{array} \\right], \\tag {8}\n$$\n\nwhere  $\\mathbf{R} \\in R^{N \\times M}$  is the user-item interaction matrix, and  $\\mathbf{0}$  is all-zero matrix;  $\\mathbf{A}$  is the adjacency matrix and  $\\mathbf{D}$  is the diagonal degree matrix, where the  $t$ -th diagonal element  $D_{tt} = |\\mathcal{N}_t|$ ; as such, the nonzero off-diagonal entry  $\\mathcal{L}_{ui} = 1 / \\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}$ , which is equal to  $p_{ui}$  used in Equation (3).\n\nBy implementing the matrix-form propagation rule, we can simultaneously update the representations for all users and items in a rather efficient way. It allows us to discard the node sampling procedure, which is commonly used to make graph convolution network runnable on large-scale graph [25]. We will analyze the complexity in Section 2.5.2.\n\n# 2.3 Model Prediction\n\nAfter propagating with  $L$  layers, we obtain multiple representations for user  $u$ , namely  $\\{\\mathbf{e}_u^{(1)}, \\dots, \\mathbf{e}_u^{(L)}\\}$ . Since the representations obtained in different layers emphasize the messages passed over different connections, they have different contributions in reflecting user preference. As such, we concatenate them to constitute the final embedding for a user; we do the same operation on items, concatenating the item representations  $\\{\\mathbf{e}_i^{(1)}, \\dots, \\mathbf{e}_i^{(L)}\\}$  learned by different layers to get the final item embedding:\n\n$$\n\\mathbf {e} _ {u} ^ {*} = \\mathbf {e} _ {u} ^ {(0)} \\| \\dots \\| \\mathbf {e} _ {u} ^ {(L)}, \\quad \\mathbf {e} _ {i} ^ {*} = \\mathbf {e} _ {i} ^ {(0)} \\| \\dots \\| \\mathbf {e} _ {i} ^ {(L)}, \\tag {9}\n$$\n\nwhere  $\\parallel$  is the concatenation operation. By doing so, we not only enrich the initial embeddings with embedding propagation layers, but also allow controlling the range of propagation by adjusting  $L$ . Note that besides concatenation, other aggregators can also be applied, such as weighted average, max pooling, LSTM, etc., which imply different assumptions in combining the connectivities of different orders. The advantage of using concatenation lies in its simplicity, since it involves no additional parameters to learn, and it has been shown quite effectively in a recent work of graph neural networks [38], which refers to layer-aggregation mechanism.\n\nFinally, we conduct the inner product to estimate the user's preference towards the target item:\n\n$$\n\\hat {y} _ {\\mathrm {N G C F}} (u, i) = \\mathbf {e} _ {u} ^ {* \\top} \\mathbf {e} _ {i} ^ {*}. \\tag {10}\n$$\n\nIn this work, we emphasize the embedding function learning thus only employs the simple interaction function of inner product. Other more complicated choices, such as neural network-based interaction functions [14], are left to explore in the future work.\n\n# 2.4 Optimization\n\nTo learn model parameters, we optimize the pairwise BPR loss [26], which has been intensively used in recommender systems [2, 13]. It considers the relative order between observed and unobserved user-item interactions. Specifically, BPR assumes that the observed interactions, which are more reflective of a user's preferences, should be assigned higher prediction values than unobserved ones. The objective function is as follows,\n\n$$\n\\operatorname {L o s s} = \\sum_ {(u, i, j) \\in O} - \\ln \\sigma \\left(\\hat {y} _ {u i} - \\hat {y} _ {u j}\\right) + \\lambda \\| \\Theta \\| _ {2} ^ {2}, \\tag {11}\n$$\n\nwhere  $O = \\{(u,i,j)|(u,i)\\in \\mathcal{R}^{+},(u,j)\\in \\mathcal{R}^{-}\\}$  denotes the pairwise training data,  $\\mathcal{R}^+$  indicates the observed interactions, and  $\\mathcal{R}^{-}$  is the unobserved interactions;  $\\sigma (\\cdot)$  is the sigmoid function;  $\\Theta =$ $\\{\\mathbf{E},\\{\\mathbf{W}_1^{(l)},\\mathbf{W}_2^{(l)}\\}_{l = 1}^L\\}$  denotes all trainable model parameters, and  $\\lambda$  controls the  $L_{2}$  regularization strength to prevent overfitting. We adopt mini-batch Adam [17] to optimize the prediction model and update the model parameters. In particular, for a batch of randomly sampled triples  $(u,i,j)\\in O$ , we establish their representations  $[\\mathbf{e}^{(0)},\\dots ,\\mathbf{e}^{(L)}]$  after  $L$  steps of propagation, and then update model parameters by using the gradients of the loss function.\n\n2.4.1 Model Size. It is worth pointing out that although NGCF obtains an embedding matrix  $(\\mathbf{E}^{(l)})$  at each propagation layer  $l$ , it only introduces very few parameters - two weight matrices of size  $d_{l} \\times d_{l-1}$ . Specifically, these embedding matrices are derived from the embedding look-up table  $\\mathbf{E}^{(0)}$ , with the transformation based on the user-item graph structure and weight matrices. As such, compared to MF - the most concise embedding-based recommender model, our NGCF uses only  $2Ld_{l}d_{l-1}$  more parameters. Such additional cost on model parameters is almost negligible, considering that  $L$  is usually a number smaller than 5, and  $d_{l}$  is typically set as the embedding size, which is much smaller than the number of users and items. For example, on our experimented Gowalla dataset (20K users and 40K items), when the embedding size is 64 and we use 3 propagation layers of size  $64 \\times 64$ , MF has 4.5 million parameters, while our NGCF uses only 0.024 million additional parameters. To summarize, NGCF uses very few\n\nadditional model parameters to achieve the high-order connectivity modeling.\n\n2.4.2 Message and Node Dropout. Although deep learning models have strong representation ability, they usually suffer from overfitting. Dropout is an effective solution to prevent neural networks from overfitting. Following the prior work on graph convolutional network [29], we propose to adopt two dropout techniques in NGCF: message dropout and node dropout. Message dropout randomly drops out the outgoing messages. Specifically, we drop out the messages being propagated in Equation (6), with a probability  $p_1$ . As such, in the  $l$ -th propagation layer, only partial messages contribute to the refined representations. We also conduct node dropout to randomly block a particular node and discard all its outgoing messages. For the  $l$ -th propagation layer, we randomly drop  $(M + N)p_2$  nodes of the Laplacian matrix, where  $p_2$  is the dropout ratio.\n\nNote that dropout is only used in training, and must be disabled during testing. The message dropout endows the representations more robustness against the presence or absence of single connections between users and items, and the node dropout focuses on reducing the influences of particular users or items. We perform experiments to investigate the impact of message dropout and node dropout on NGCF in Section 4.4.3.\n\n# 2.5 Discussions\n\nIn the subsection, we first show how NGCF generalizes SVD++ [19]. In what follows, we analyze the time complexity of NGCF.\n\n2.5.1 NGCF Generalizes SVD++. SVD++ can be viewed as a special case of NGCF with no high-order propagation layer. In particular, we set  $L$  to one. Within the propagation layer, we disable the transformation matrix and nonlinear activation function. Thereafter,  $\\mathbf{e}_u^{(1)}$  and  $\\mathbf{e}_i^{(1)}$  are treated as the final representations for user  $u$  and item  $i$ , respectively. We term this simplified model as NGCF-SVD, which can be formulated as:\n\n$$\n\\hat {y} _ {\\text {N G C F - S V D}} = \\left(\\mathbf {e} _ {u} + \\sum_ {i ^ {\\prime} \\in \\mathcal {N} _ {u}} p _ {u i ^ {\\prime}} \\mathbf {e} _ {i ^ {\\prime}}\\right) ^ {\\top} \\left(\\mathbf {e} _ {i} + \\sum_ {u ^ {\\prime} \\in \\mathcal {N} _ {i}} p _ {u u ^ {\\prime}} \\mathbf {e} _ {i}\\right). \\tag {12}\n$$\n\nClearly, by setting  $p_{ui'}$  and  $p_{u'i}$  as  $1 / \\sqrt{|\\mathcal{N}_u|}$  and 0 separately, we can exactly recover  $\\mathrm{SVD}++$  model. Moreover, another widely-used item-based CF model, FISM [16], can be also seen as a special case of NGCF, wherein  $p_{iu'}$  in Equation (12) is set as 0.\n\n2.5.2 Time Complexity Analysis. As we can see, the layer-wise propagation rule is the main operation. For the  $l$ -th propagation layer, the matrix multiplication has computational complexity  $O(|\\mathcal{R}^{+}|d_{l}d_{l - 1})$ , where  $|\\mathcal{R}^{+}|$  denotes the number of nonzero entries in the Laplacian matrix; and  $d_{l}$  and  $d_{l - 1}$  are the current and previous transformation size. For the prediction layer, only the inner product is involved, for which the time complexity of the whole training epoch is  $O(\\sum_{l = 1}^{L}|\\mathcal{R}^{+}|d_{l})$ . Therefore, the overall complexity for evaluating NGCF is  $O(\\sum_{l = 1}^{L}|\\mathcal{R}^{+}|d_{l}d_{l - 1} + \\sum_{l = 1}^{L}|\\mathcal{R}^{+}|d_{l})$ . Empirically, under the same experimental settings (as explained in Section 4), MF and NGCF cost around 20s and 80s per epoch on Gowalla dataset for training, respectively; during inference, the time costs of MF and NGCF are 80s and 260s for all testing instances, respectively.\n",
  "experiments": "# 4 EXPERIMENTS\n\nWe perform experiments on three real-world datasets to evaluate our proposed method, especially the embedding propagation layer. We aim to answer the following research questions:\n\n- RQ1: How does NGCF perform as compared with state-of-the-art CF methods?  \n- RQ2: How do different hyper-parameter settings (e.g., depth of layer, embedding propagation layer, layer-aggregation mechanism, message dropout, and node dropout) affect NGCF?  \n- RQ3: How do the representations benefit from the high-order connectivity?\n\n# 4.1 Dataset Description\n\nTo evaluate the effectiveness of NGCF, we conduct experiments on three benchmark datasets: Gowalla, Yelp2018 $^{2}$ , and Amazon-book,\n\nwhich are publicly accessible and vary in terms of domain, size, and sparsity. We summarize the statistics of three datasets in Table 1.\n\nGowalla: This is the check-in dataset [21] obtained from Gowalla, where users share their locations by checking-in. To ensure the quality of the dataset, we use the 10-core setting [10], i.e., retaining users and items with at least ten interactions.\n\nYelp2018*: This dataset is adopted from the 2018 edition of the Yelp challenge. Wherein, the local businesses like restaurants and bars are viewed as the items. We use the same 10-core setting in order to ensure data quality.\n\nAmazon-book: Amazon-review is a widely used dataset for product recommendation [9]. We select Amazon-book from the collection. Similarly, we use the 10-core setting to ensure that each user and item have at least ten interactions.\n\nFor each dataset, we randomly select  $80\\%$  of historical interactions of each user to constitute the training set, and treat the remaining as the test set. From the training set, we randomly select  $10\\%$  of interactions as validation set to tune hyper-parameters. For each observed user-item interaction, we treat it as a positive instance, and then conduct the negative sampling strategy to pair it with one negative item that the user did not consume before.\n\n# 4.2 Experimental Settings\n\n4.2.1 Evaluation Metrics. For each user in the test set, we treat all the items that the user has not interacted with as the negative items. Then each method outputs the user's preference scores over all the items, except the positive ones used in the training set. To evaluate the effectiveness of top-  $K$  recommendation and preference ranking, we adopt two widely-used evaluation protocols [14, 40]: recall@  $K$  and ndcg@  $K^3$ . By default, we set  $K = 20$ . We report the average metrics for all users in the test set.\n\n4.2.2 Baselines. To demonstrate the effectiveness, we compare our proposed NGCF with the following methods:\n\n- MF [26]: This is matrix factorization optimized by the Bayesian personalized ranking (BPR) loss, which exploits the user-item direct interactions only as the target value of interaction function.  \n- NeuMF [14]: The method is a state-of-the-art neural CF model which uses multiple hidden layers above the element-wise and concatenation of user and item embeddings to capture their nonlinear feature interactions. Especially, we employ two-layered plain architecture, where the dimension of each hidden layer keeps the same.  \n- CMN [5]: It is a state-of-the-art memory-based model, where the user representation attentively combines the memory slots of neighboring users via the memory layers. Note that the first-order connections are used to find similar users who interacted with the same items.  \n- HOP-Rec [40]: This is a state-of-the-art graph-based model, where the high-order neighbors derived from random walks are exploited to enrich the user-item interaction data.  \n- PinSage [42]: PinSage is designed to employ GraphSAGE [8] on item-item graph. In this work, we apply it on user-item\n\nTable 2: Overall Performance Comparison.  \n\n<table><tr><td></td><td colspan=\"2\">Gowalla</td><td colspan=\"2\">Yelp2018*</td><td colspan=\"2\">Amazon-Book</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>MF</td><td>0.1291</td><td>0.1109</td><td>0.0433</td><td>0.0354</td><td>0.0250</td><td>0.0196</td></tr><tr><td>NeuMF</td><td>0.1399</td><td>0.1212</td><td>0.0451</td><td>0.0363</td><td>0.0258</td><td>0.0200</td></tr><tr><td>CMN</td><td>0.1405</td><td>0.1221</td><td>0.0457</td><td>0.0369</td><td>0.0267</td><td>0.0218</td></tr><tr><td>HOP-Rec</td><td>0.1399</td><td>0.1214</td><td>0.0517</td><td>0.0428</td><td>0.0309</td><td>0.0232</td></tr><tr><td>GC-MC</td><td>0.1395</td><td>0.1204</td><td>0.0462</td><td>0.0379</td><td>0.0288</td><td>0.0224</td></tr><tr><td>PinSage</td><td>0.1380</td><td>0.1196</td><td>0.0471</td><td>0.0393</td><td>0.0282</td><td>0.0219</td></tr><tr><td>NGCF-3</td><td>0.1569*</td><td>0.1327*</td><td>0.0579*</td><td>0.0477*</td><td>0.0337*</td><td>0.0261*</td></tr><tr><td>%Improv.</td><td>11.68%</td><td>8.64%</td><td>11.97%</td><td>11.29%</td><td>9.61%</td><td>12.50%</td></tr><tr><td>p-value</td><td>2.01e-7</td><td>3.03e-3</td><td>5.34e-3</td><td>4.62e-4</td><td>3.48e-5</td><td>1.26e-4</td></tr></table>\n\ninteraction graph. Especially, we employ two graph convolution layers as suggested in [42], and the hidden dimension is set equal to the embedding size.\n\n- GC-MC [29]: This model adopts GCN [18] encoder to generate the representations for users and items, where only the first-order neighbors are considered. Hence one graph convolution layer, where the hidden dimension is set as the embedding size, is used as suggested in [29].\n\nWe also tried SpectralCF [43] but found that the eigen-decomposition leads to high time cost and resource cost, especially when the number of users and items is large. Hence, although it achieved promising performance in small datasets, we did not select it for comparison. For fair comparison, all methods optimize the BPR loss as shown in Equation (11).\n\n4.2.3 Parameter Settings. We implement our NGCF model in Tensorflow. The embedding size is fixed to 64 for all models. For HOP-Rec, we search the steps of random walks in  $\\{1,2,3\\}$  and tune the learning rate in  $\\{0.025,0.020,0.015,0.010\\}$ . We optimize all models except HOP-Rec with the Adam optimizer, where the batch size is fixed at 1024. In terms of hyperparameters, we apply a grid search for hyperparameters: the learning rate is tuned amongst  $\\{0.0001,0.0005,0.001,0.005\\}$ , the coefficient of  $L_{2}$  normalization is searched in  $\\{10^{-5},10^{-4},\\dots ,10^{1},10^{2}\\}$ , and the dropout ratio in  $\\{0.0,0.1,\\dots ,0.8\\}$ . Besides, we employ the node dropout technique for GC-MC and NGCF, where the ratio is tuned in  $\\{0.0,0.1,\\dots ,0.8\\}$ . We use the Xavier initializer [6] to initialize the model parameters<sup>4</sup>. Moreover, early stopping strategy is performed, i.e., premature stopping if recall@20 on the validation data does not increase for 50 successive epochs. To model the CF signal encoded in third-order connectivity, we set the depth of NGCF  $L$  as three. Without specification, we show the results of three embedding propagation layers, node dropout ratio of 0.0, and message dropout ratio of 0.1.\n\n# 4.3 Performance Comparison (RQ1)\n\nWe start by comparing the performance of all the methods, and then explore how the modeling of high-order connectivity improves under the sparse settings.\n\n4.3.1 Overall Comparison. Table 2 reports the performance comparison results. We have the following observations:\n\n- MF achieves poor performance on three datasets. This indicates that the inner product is insufficient to capture the complex\n\nrelations between users and items, further limiting the performance. NeuMF consistently outperforms MF across all cases, demonstrating the importance of nonlinear feature interactions between user and item embeddings. However, neither MF nor NeuMF explicitly models the connectivity in the embedding learning process, which could easily lead to suboptimal representations.\n\n- Compared to MF and NeuMF, the performance of GC-MC verifies that incorporating the first-order neighbors can improve the representation learning. However, in Yelp2018*, GC-MC underperforms NeuMF w.r.t. ndcg@20. The reason might be that GC-MC fails to fully explore the nonlinear feature interactions between users and items.  \n- CMN generally achieves better performance than GC-MC in most cases. Such improvement might be attributed to the neural attention mechanism, which can specify the attentive weight of each neighboring user, rather than the equal or heuristic weight used in GC-MC.  \n- PinSage slightly underperforms CMN in Gowalla and Amazon-Book, while performing much better in Yelp2018*; meanwhile, HOP-Rec generally achieves remarkable improvements in most cases. It makes sense since PinSage introduces high-order connectivity in the embedding function, and HOP-Rec exploits high-order neighbors to enrich the training data, while CMN considers the similar users only. It therefore points to the positive effect of modeling the high-order connectivity or neighbors.  \n- NGCF consistently yields the best performance on all the datasets. In particular, NGCF improves over the strongest baselines w.r.t. recall@20 by  $11.68\\%$ ,  $11.97\\%$ , and  $9.61\\%$  in Gowalla, Yelp2018*, and Amazon-Book, respectively. By stacking multiple embedding propagation layers, NGCF is capable of exploring the high-order connectivity in an explicit way, while CMN and GC-MC only utilize the first-order neighbors to guide the representation learning. This verifies the importance of capturing collaborative signal in the embedding function. Moreover, compared with PinSage, NGCF considers multi-grained representations to infer user preference, while PinSage only uses the output of the last layer. This demonstrates that different propagation layers encode different information in the representations. And the improvements over HOP-Rec indicate that explicit encoding CF in the embedding function can achieve better representations. We conduct one-sample t-tests and  $p$ -value  $< 0.05$  indicates that the improvements of NGCF over the strongest baseline (underlined) are statistically significant.\n\n4.3.2 Performance Comparison w.r.t. Interaction Sparsity Levels. The sparsity issue usually limits the expressiveness of recommender systems, since few interactions of inactive users are insufficient to generate high-quality representations. We investigate whether exploiting connectivity information helps to alleviate this issue.\n\nTowards this end, we perform experiments over user groups of different sparsity levels. In particular, based on interaction number per user, we divide the test set into four groups, each of which has the same total interactions. Taking Gowalla dataset as an example, the interaction numbers per user are less than 24, 50, 117, and 1014 respectively. Figure 4 illustrates the results w.r.t. ndcg@20 on\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/bfa06f0104d2cff3351fefffbfa59d09a1f756932a1632d8fda13ac3ea0140e9.jpg)  \n(a) ndcg on Gowalla\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/71a8f6e07e8d8fe36ddf0513b6669b22873fbc9e3b284db53d2b40529e53849d.jpg)  \n(b) ndcg on Yelp2018*  \nFigure 4: Performance comparison over the sparsity distribution of user groups on different datasets. Wherein, the background histograms indicate the number of users involved in each group, and the lines demonstrate the performance w.r.t. ndcg@20.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/c9296275e1e2b2bd0f5d4617d939a0ecfbb678dbd165cb02d3268b518da4b519.jpg)  \n(c) ndcg on Amazon-book\n\nTable 3: Effect of embedding propagation layer numbers  $(L)$  \n\n<table><tr><td></td><td colspan=\"2\">Gowalla</td><td colspan=\"2\">Yelp2018*</td><td colspan=\"2\">Amazon-Book</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>NGCF-1</td><td>0.1556</td><td>0.1315</td><td>0.0543</td><td>0.0442</td><td>0.0313</td><td>0.0241</td></tr><tr><td>NGCF-2</td><td>0.1547</td><td>0.1307</td><td>0.0566</td><td>0.0465</td><td>0.0330</td><td>0.0254</td></tr><tr><td>NGCF-3</td><td>0.1569</td><td>0.1327</td><td>0.0579</td><td>0.0477</td><td>0.0337</td><td>0.0261</td></tr><tr><td>NGCF-4</td><td>0.1570</td><td>0.1327</td><td>0.0566</td><td>0.0461</td><td>0.0344</td><td>0.0263</td></tr></table>\n\ndifferent user groups in Gowalla, Yelp2018*, and Amazon-Book; we see a similar trend for performance w.r.t. recall@20 and omit the part due to the space limitation. We find that:\n\n- NGCF and HOP-Rec consistently outperform all other baselines on all user groups. It demonstrates that exploiting high-order connectivity greatly facilitates the representation learning for inactive users, as the collaborative signal can be effectively captured. Hence, it might be promising to solve the sparsity issue in recommender systems, and we leave it in future work.  \n- Jointly analyzing Figures 4(a), 4(b), and 4(c), we observe that the improvements achieved in the first two groups (e.g.,  $8.49\\%$  and  $7.79\\%$  over the best baselines separately for  $< 24$  and  $< 50$  in Gowalla) are more significant than that of the others (e.g.,  $1.29\\%$  for  $< 1014$  Gowalla groups). It verifies that the embedding propagation is beneficial to the relatively inactive users.\n\n# 4.4 Study of NGCF (RQ2)\n\nAs the embedding propagation layer plays a pivotal role in NGCF, we investigate its impact on the performance. We start by exploring the influence of layer numbers. We then study how the Laplacian matrix (i.e., discounting factor  $p_{ui}$  between user  $u$  and item  $i$ ) affects the performance. Moreover, we analyze the influences of key factors, such as node dropout and message dropout ratios. We also study the training process of NGCF.\n\n4.4.1 Effect of Layer Numbers. To investigate whether NGCF can benefit from multiple embedding propagation layers, we vary the model depth. In particular, we search the layer numbers in the range of  $\\{1,2,3,4\\}$ . Table 3 summarizes the experimental results, wherein NGCF-3 indicates the model with three embedding propagation layers, and similar notations for others. Jointly analyzing Tables 2 and 3, we have the following observations:\n\n- Increasing the depth of NGCF substantially enhances the recommendation cases. Clearly, NGCF-2 and NGCF-3 achieve consistent improvement over NGCF-1 across all the board, which considers the first-order neighbors only. We attribute the\n\nimprovement to the effective modeling of CF effect: collaborative user similarity and collaborative signal are carried by the second- and third-order connectivities, respectively.\n\n- When further stacking propagation layer on the top of NGCF-3, we find that NGCF-4 leads to overfitting on Yelp2018* dataset. This might be caused by applying a too deep architecture might introduce noises to the representation learning. The marginal improvements on the other two datasets verifies that conducting three propagation layers is sufficient to capture the CF signal.  \n- When varying the number of propagation layers, NGCF is consistently superior to other methods across three datasets. It again verifies the effectiveness of NGCF, empirically showing that explicit modeling of high-order connectivity can greatly facilitate the recommendation task.\n\n4.4.2 Effect of Embedding Propagation Layer and Layer-Aggregation Mechanism. To investigate how the embedding propagation (i.e., graph convolution) layer affects the performance, we consider the variants of NGCF-1 that use different layers. In particular, we remove the representation interaction between a node and its neighbor from the message passing function (cf. Equation (3)) and set it as that of PinSage and GC-MC, termed NGCF-1PinSage and NGCF-1GC-MC respectively. Moreover, following SVD++, we obtain one variant based on Equations (12), termed NGCF-1SVD++. We show the results in Table 4 and have the following findings:\n\n- NGCF-1 is consistently superior to all variants. We attribute the improvements to the representation interactions (i.e.,  $\\mathbf{e}_u\\odot \\mathbf{e}_i$ ), which makes messages being propagated dependent on the affinity between  $\\mathbf{e}_i$  and  $\\mathbf{e}_u$  and functions like the attention mechanism [2]. Whereas, all variants only take linear transformation into consideration. It hence verifies the rationality and effectiveness of our embedding propagation function.  \n- In most cases, NGCF-1 $_{\\mathrm{SVD}++}$  underperforms NGCF-1 $_{\\mathrm{PinSage}}$  and NGCF-1 $_{\\mathrm{GC-MC}}$ . It illustrates the importance of messages passed by the nodes themselves and the nonlinear transformation.  \n- Jointly analyzing Tables 2 and 4, we find that, when concatenating all layers' outputs together, NGCF-1PinSage and NGCF-1GC-MC achieve better performance than PinSage and GC-MC, respectively. This emphasizes the significance of layer-aggregation mechanism, which is consistent with [38].\n\n4.4.3 Effect of Dropout. Following the prior work [29], we employ node dropout and message dropout techniques to prevent NGCF from overfitting. Figure 5 plots the effect of message dropout\n\nTable 4: Effect of graph convolution layers.  \n\n<table><tr><td></td><td colspan=\"2\">Gowalla</td><td colspan=\"2\">Yelp2018*</td><td colspan=\"2\">Amazon-Book</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>NGCF-1</td><td>0.1556</td><td>0.1315</td><td>0.0543</td><td>0.0442</td><td>0.0313</td><td>0.0241</td></tr><tr><td>NGCF-1SVD++</td><td>0.1517</td><td>0.1265</td><td>0.0504</td><td>0.0414</td><td>0.0297</td><td>0.0232</td></tr><tr><td>NGCF-1GC-MC</td><td>0.1523</td><td>0.1307</td><td>0.0518</td><td>0.0420</td><td>0.0305</td><td>0.0234</td></tr><tr><td>NGCF-1PinSage</td><td>0.1534</td><td>0.1308</td><td>0.0516</td><td>0.0420</td><td>0.0293</td><td>0.0231</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/cbaa1ccb942287afa3176161d58ca67e536c1808805d3ea440e55661ca1308b5.jpg)  \n(a) Gowalla\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/cfe8e7512a47762383514d1238adf09c88b368ae9f495566f85fa69e613e034f.jpg)  \n(b) Yelp2018*\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/b1e222688db1f9afc921e56ab2133dae6b5297a626d654434fb97ec6d3c856b0.jpg)  \n(c) Amazon-Book  \nFigure 7: Visualization of the learned t-SNE transformed representations derived from MF and NGCF-3. Each star represents a user from Gowalla dataset, while the points with the same color denote the relevant items. Best view in color.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/8abcba5332cedd4934f13c3028fc24a42b70f6d33037d71e78acd0df800d939f.jpg)  \nFigure 5: Effect of node dropout and message dropout ratios.  \n(a) Gowalla  \nFigure 6: Test performance of each epoch of MF and NGCF.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/68eb482bd77d6b6e44e69df4764418bffc0583a70faeeaf1ffb3a4dadddccfff.jpg)  \n(b) Yelp2018*\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/7cb0d4c50c44cac03e0f04a777e852ce8dd3c676243be8dad7ba138af266e7a1.jpg)  \n(c) Amazon-Book\n\nratio  $p_1$  and node dropout ratio  $p_2$  against different evaluation protocols on different datasets.\n\nBetween the two dropout strategies, node dropout offers better performance. Taking Gowalla as an example, setting  $p_2$  as 0.2 leads to the highest recall@20 of 0.1514, which is better than that of message dropout 0.1506. One reason might be that dropping out all outgoing messages from particular users and items makes the representations robust against not only the influence of particular edges, but also the effect of nodes. Hence, node dropout is more effective than message dropout, which is consistent with the findings of prior effort [29]. We believe this is an interesting finding, which means that node dropout can be an effective strategy to address overfitting of graph neural networks.\n\n4.4.4 Test Performance w.r.t. Epoch. Figure 6 shows the test performance w.r.t. recall of each epoch of MF and NGCF. Due to the space limitation, we omit the performance w.r.t. ndcg which has the similar trend. We can see that, NGCF exhibits fast convergence than MF on three datasets. It is reasonable since indirectly connected users and items are involved when optimizing the interaction pairs in mini-batch. Such an observation demonstrates the better model capacity of NGCF and the effectiveness of performing embedding propagation in the embedding space.\n\n# 4.5 Effect of High-order Connectivity (RQ3)\n\nIn this section, we attempt to understand how the embedding propagation layer facilitates the representation learning in the embedding space. Towards this end, we randomly selected six users from Gowalla dataset, as well as their relevant items. We observe how their representations are influenced w.r.t. the depth of NGCF.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/3ba35d1587db51ea12ea909d6391c4e8652f0b99291f2afe2b032f7d8b3f5fe5.jpg)  \n(a) MF (NGCF-0)\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/65c67eba-9556-4f14-ae11-58e34a5fa758/c9b778353f595d3b7ff7d38cb703b83f31725d12c9255f0047da78f5aed553f4.jpg)  \n(b) NGCF-3\n\nFigures 7(a) and 7(b) show the visualization of the representations derived from MF (i.e., NGCF-0) and NGCF-3, respectively. Note that the items are from the test set, which are not paired with users in the training phase. There are two key observations:\n\n- The connectivities of users and items are well reflected in the embedding space, that is, they are embedded into the near part of the space. In particular, the representations of NGCF-3 exhibit discernible clustering, meaning that the points with the same colors (i.e., the items consumed by the same users) tend to form the clusters.  \n- Jointly analyzing the same users (e.g., 12201 and 6880) across Figures 7(a) and 7(b), we find that, when stacking three embedding propagation layers, the embeddings of their historical items tend to be closer. It qualitatively verifies that the proposed embedding propagation layer is capable of injecting the explicit collaborative signal (via NGCF-3) into the representations.\n",
  "hyperparameter": "Embedding size d=64; Number of propagation layers L=3 (searching in {1,2,3,4}); Transformation size d_l typically set equal to embedding size (6464); Learning rate searched in {0.0001, 0.0005, 0.001, 0.005}; L2 regularization coefficient  searched in {10^-5, 10^-4, ..., 10^1, 10^2}; Message dropout ratio p1=0.1 (searched in {0.0, 0.1, ..., 0.8}); Node dropout ratio p2=0.0 (searched in {0.0, 0.1, ..., 0.8}); Batch size=1024; Optimizer: Adam with Xavier initialization; Early stopping patience: 50 epochs; Train/validation/test split: 80%/10%/10%"
}