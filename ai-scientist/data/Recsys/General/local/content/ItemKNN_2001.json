{
  "id": "ItemKNN_2001",
  "paper_title": "Efficient Top-N Recommendation for Very Large Scale Binary Rated Datasets",
  "alias": "ItemKNN",
  "year": 2001,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "",
  "introduction": "# Introduction\nRecommender systems often rely on implicit feedback (e.g., listening history) in large-scale binary-rated datasets. The Million Song Dataset (MSD) challenge exemplifies this—predicting 500 songs for 100k users from 380k+ songs, using 1M users’ full history and 110k users’ partial history.\n\nTraditional collaborative filtering (CF) has two main limitations for such tasks:\n1. Model-based CF (e.g., Matrix Factorization) is computationally expensive for large datasets, ill-suited for implicit feedback, and converges slowly to local minima.\n2. Memory-based CF (neighborhood methods) requires computing full similarity matrices, which is infeasible for large-scale data.\n\nThis paper proposes an efficient memory-based CF approach for top-N recommendation with implicit feedback. Key contributions:\n1. An asymmetric cosine similarity function (parameterized for domain adaptation) that avoids full similarity matrix computation.\n2. A flexible recommendation pipeline with locality adjustment, calibration, and ranking aggregation (user-based + item-based views).\n3. Scalability to very large datasets (MSD’s 1.2M users, 380k+ songs) and state-of-the-art performance (won the MSD challenge).",
  "method": "# Method\n## 1. Core Framework: Memory-Based CF for Implicit Feedback\n### 1.1 Problem Formulation\n- **Binary Implicit Feedback**: \\( r_{ui} = 1 \\) if user \\( u \\) interacted with item \\( i \\) (e.g., listened to a song), \\( 0 \\) otherwise.\n- **Task**: Top-N recommendation—rank unseen items for user \\( u \\) based on interaction history.\n\n### 2. Asymmetric Cosine Similarity\nGeneralizes cosine similarity to handle asymmetric domain characteristics (e.g., popular items vs. niche items):\n\\[\nS_\\alpha(a, b) = \\frac{|R(a) \\cap R(b)|}{|R(a)|^\\alpha |R(b)|^{1-\\alpha}}\n\\]\n- \\( R(a) \\): Set of entities (users/items) related to \\( a \\) (e.g., items listened to by user \\( a \\)).\n- \\( \\alpha \\in [0,1] \\): Parameter balancing conditional probabilities \\( P(a|b) \\) and \\( P(b|a) \\).\n- Special cases: \\( \\alpha=0.5 \\) (standard cosine similarity), \\( \\alpha=1 \\) (conditional probability \\( P(b|a) \\)).\n\n#### 2.1 User/Item Similarity\n- **User Similarity**: \\( w_{uv} = S_\\alpha(u, v) = \\frac{|I(u) \\cap I(v)|}{|I(u)|^\\alpha |I(v)|^{1-\\alpha}} \\) ( \\( I(u) \\): items of user \\( u \\) ).\n- **Item Similarity**: \\( w_{ij} = S_\\alpha(i, j) = \\frac{|U(i) \\cap U(j)|}{|U(i)|^\\alpha |U(j)|^{1-\\alpha}} \\) ( \\( U(i) \\): users of item \\( i \\) ).\n\n### 3. Scoring Functions\n#### 3.1 User-Based Scoring\nScore item \\( i \\) for user \\( u \\) using similar users’ interactions:\n\\[\n\\hat{r}_{ui}^U = \\frac{\\sum_{v \\in U(i)} w_{uv}}{|U(i)|^{1-\\beta}}\n\\]\n- \\( U(i) \\): Users who interacted with \\( i \\), \\( \\beta \\in [0,1] \\): asymmetric prediction parameter.\n\n#### 3.2 Item-Based Scoring\nScore item \\( i \\) for user \\( u \\) using similar items’ interactions:\n\\[\n\\hat{r}_{ui}^I = \\frac{\\sum_{j \\in I(u)} w_{ij}}{\\|w_i\\|^{2(1-\\beta)}}\n\\]\n- \\( I(u) \\): Items interacted with by \\( u \\), \\( \\|w_i\\| \\): norm of item \\( i \\)’s similarity weights.\n\n### 4. Locality Adjustment\nEmphasize high-similarity contributions via \\( f(w) = w^q \\) ( \\( q \\in \\mathbb{N} \\) ):\n- High \\( q \\): Focus on nearest neighbors (locality), low \\( q \\): Aggregate more contributions.\n\n### 5. Calibration\nAdjust scores using item-specific statistics (mean/max scores of positive feedback):\n\\[\n\\hat{r}_{ui}' = C(\\hat{r}_{ui}) = \\begin{cases} \n\\frac{\\hat{r}_{ui}}{m_i} \\theta & \\hat{r}_{ui} \\leq m_i \\\\\n\\theta + \\frac{\\hat{r}_{ui}-m_i}{M_i-m_i}(1-\\theta) & m_i < \\hat{r}_{ui} \\leq M_i \\\\\n1 & \\hat{r}_{ui} > M_i \n\\end{cases}\n\\]\n- \\( m_i \\): Mean score of item \\( i \\) for positive feedback, \\( M_i \\): Max score, \\( \\theta=0.5 \\).\n\n### 6. Ranking Aggregation\nCombine user-based and item-based rankings (diversity boost):\n- **Stochastic Aggregation**: Randomly select rankers by probability \\( p_k \\), pick top-unused item.\n- **Linear Aggregation**: Weighted average of normalized scores.\n- **Borda Aggregation**: Score items by weighted rank positions.\n\n### 7. Optimizations\n- User-based CF: Batch compute user similarities and accumulate item contributions (12x speedup on MSD).\n- Item-based CF: Estimate similarity norms via item subsampling (avoids full computation).",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\n- **MSD Taste Profile Subset**: 1.2M users, 380k+ songs, 48M (user-song) interactions (density=0.01%).\n  - Split: 900k training users (full history), 100k validation/test users (split into visible/hidden halves).\n- **MovieLens1M**: 6k users, 3.8k movies, 226k 5-star ratings (treated as implicit positive feedback, density=0.01%).\n  - Split: 90% training users, 10% test users (visible/hidden halves).\n\n### 1.2 Baselines\n- **Popularity**: Recommend most popular unseen items (simple baseline).\n- **User-based kNN**: Traditional neighborhood method (k=100, cosine similarity).\n\n### 1.3 Evaluation Metric\n- **Truncated mAP@500**: Mean Average Precision up to 500 recommendations (MSD challenge standard).\n\n## 2. Main Results\n### 2.1 MSD Challenge Performance (Core Result Tables)\nTable 1–4 summarize key results on MSD validation data:\n| Method                                  | Parameters                  | mAP@500 |\n|-----------------------------------------|-----------------------------|---------|\n| Baseline (Popularity)                    | -                           | 0.02262 |\n| Item-based CF (AsymC, α=0.15)           | q=3, β=1                    | 0.17732 |\n| Item-based CF + Calibration             | α=0.15, q=3, β=1            | 0.18108 |\n| User-based CF (AsymC, α=0.5)            | q=4, β=0.7                  | 0.16229 |\n| Stochastic Aggregation (Item+User)       | p_item=0.8, p_user=0.2      | 0.17896 |\n| Linear Aggregation (Item+User)           | p_item=0.8, p_user=0.2      | 0.18092 |\n\n### 2.2 Key Findings\n- **Asymmetric Similarity**: Item-based CF with \\( \\alpha=0.15 \\) (bias toward niche items) outperforms symmetric cosine (α=0.5).\n- **Locality Adjustment**: \\( q=3 \\) (item-based) and \\( q=4 \\) (user-based) optimize performance (balances locality and coverage).\n- **Calibration**: Improves item-based CF by 2.1% (mAP@500 from 0.17732 to 0.18108).\n- **Aggregation**: Combining user-based and item-based rankers outperforms individual methods (diversity gain).\n- **Scalability**: Handles MSD’s 1.2M users efficiently (no full similarity matrix computation).\n\n### 2.3 MovieLens1M Results\n| Method                                  | Parameters                  | mAP@500 |\n|-----------------------------------------|-----------------------------|---------|\n| User-based kNN                          | k=100                       | 0.18318 |\n| User-based AsymC + Calibration          | α=0.6, q=6, β=0.8           | 0.18984 |\n| Aggregated (Item+User, Borda)           | p_item=0.2, p_user=0.8      | 0.19554 |\n\n- Asymmetric CF and aggregation outperform traditional kNN, confirming generality across datasets.\n\n### 2.4 MSD Challenge Leaderboard\nThe proposed approach won the MSD challenge, outperforming 150+ teams. Key competitors:\n- 2nd place: Learning-to-rank with 17 user-item features (RankNet).\n- 5th place: Graph-based random walk (YouTube’s Absorption algorithm).\n- Matrix Factorization: Poor performance (computationally expensive, ill-suited for implicit feedback).",
  "hyperparameter": ""
}