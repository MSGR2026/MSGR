{
  "id": "RecVAE_2019",
  "paper_title": "RecVAE: A New Variational Autoencoder for Top-N Recommendations with Implicit Feedback",
  "alias": "RecVAE",
  "year": 2019,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "RecVAE introduces several key innovations to improve VAE-based collaborative filtering: (1) a composite prior that combines standard Gaussian with previous epoch's posterior to prevent catastrophic forgetting during training; (2) user-specific KL divergence rescaling proportional to feedback amount (β'(x) = γ|X_u^o|) instead of fixed β; (3) alternating training strategy that separately optimizes encoder (with denoising) and decoder (without denoising) with different update frequencies (M_enc = 3M_dec); (4) enhanced encoder architecture using dense connections, swish activation, and layer normalization. These modifications address the instability and overfitting issues in sparse high-dimensional implicit feedback scenarios.",
  "introduction": "# 1 INTRODUCTION\n\nMatrix factorization (MF) has become the industry standard as the foundation of recommender systems based on collaborative filtering. However, there are certain general issues that arise with this family of models. First, the number of parameters in any matrix factorization model is huge: it linearly depends on the number of both users and items, which leads to slow model learning and overfitting. Second, to make a prediction for a new user/item based on their ratings, one has to run an optimization procedure in order to find the corresponding user/item embedding. Third, only a small amount of ratings are known for some (often for a majority of) users and items, which could also lead to overfitting. This makes it necessary to heavily regularize matrix factorization models, and standard  $L_{1}$  or  $L_{2}$  regularizers are hard to tune.\n\nRecently proposed models such as the Collaborative Denoising Autoencoder (CDAE) [39] partially solve these issues by using a parameterized function which maps user feedback to user embeddings. It performs regularization in an alternative way and makes it possible to predict item ratings for new users without additional iterative training. The Variational Autoencoder for Collaborative Filtering (Mult-VAE) [22] is a subsequent improvement of CDAE that extends it to multinomial distributions in the likelihood, which are more suitable for recommendations.\n\nIn this work, we propose the Recommender VAE (RecVAE) model for collaborative filtering with implicit feedback based on the variational autoencoder (VAE) and specifically on the Mult-VAE approach. RecVAE presents a number of important novelties that together combine into significantly improved performance. First, we have designed a new architecture for the encoder network. Second, we have introduced a novel composite prior distribution for the latent code  $z$  in the variational autoencoder. The composite prior is a mixture of a standard Gaussian prior and the latent code distribution with parameters fixed from the previous iteration of the model (Section 3.3), an idea originating from reinforcement learning where it is used to stabilize training [13, 30]. In the context of recommendations, we have also found that this prior improves training stability and performance. Third, we have developed a new approach to setting the hyperparameter  $\\beta$  for the Kullback-Leibler term in the objective function (Section 3.4). We have found that\n\n$\\beta$  should be user-specific,  $\\beta = \\beta (\\mathbf{x}_u)$ , and should depend on the amount of data (implicit feedback) available for a given user.\n\nFinally, we introduce a novel approach for training the model. In RecVAE, training is done by alternating updates for the encoder and decoder (see Section 3.5). This approach has two important advantages. First, it allows to perform multiple updates of the encoder (a more complex network) for every update of the decoder (a very simple, single-layer network that contains item embeddings and biases). Second, it allows to use corrupted inputs (following the general idea of denoising autoencoders [16, 31]) only for training the encoder while still training the decoder on clean input data. This is again beneficial for the final model training due to the differing complexities of the encoder and decoder.\n\nAs a result of the above novelties, our model significantly outperforms all autoencoder-based previous works and shows competitive or better results in comparison with other models across a variety of collaborative filtering datasets, including MovieLens-20M (ML-20M), Netflix Prize Dataset, and Million Songs Dataset (MSD).\n\nThe paper is organized as follows. In Section 2 we review the crucial components and approaches we are to employ in the proposed methods as well as other relevant prior art. In Section 3 we describe the basic Mult-VAE approach and our modifications. Section 4 contains the results of a comprehensive experimental study for our model, and Section 5 concludes the paper.\n",
  "method": "# 3 PROPOSED APPROACH\n\n# 3.1 Mult-VAE\n\nWe begin with a description of the Mult-VAE model proposed in [22]. The basic idea of Mult-VAE is similar to VAE but with the multinomial distribution as the likelihood function instead of Gaussian and Bernoulli distributions commonly used in VAE. The generative model samples a  $k$ -dimensional latent representation  $\\mathbf{z}_u$  for a user  $u$ , transforms it with a function  $f_{\\theta}:\\mathbb{R}^{k}\\to \\mathbb{R}^{|I|}$  parameterized by  $\\theta$ , and then the feedback history  $\\mathbf{x}_u$  of user  $u$ , which consists of  $n_u$  interactions ( clicks, purchases etc.), is assumed to be drawn from the multinomial distribution:\n\n$$\nz _ {u} \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I}), \\quad \\pi (z _ {u}) = \\operatorname {s o f t m a x} (f _ {\\theta} (z _ {u})), \\tag {8}\n$$\n\n$$\n\\mathbf {x} _ {u} \\sim \\operatorname {M u l t} \\left(n _ {u}, \\pi \\left(z _ {u}\\right)\\right). \\tag {9}\n$$\n\nNote that classical collaborative filtering models also follow this scheme with a linear  $f_{\\theta}$ ; the additional flexibility of Mult-VAE comes from parameterizing  $f$  with a neural network with parameters  $\\theta$ .\n\nTo estimate  $\\theta$  one has to approximate the intractable posterior  $p(z_{u} \\mid \\mathbf{x}_{u})$ . This, similar to regular VAE, is done by constructing an evidence lower bound for the variational approximation where  $q(z_{u})$  is assumed to be a fully factorized diagonal Gaussian distribution:  $q(z_{u}) = \\mathcal{N}\\left(\\mu_{u}, \\mathrm{diag}(\\sigma_{u}^{2})\\right)$ . The resulting ELBO is\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/764fd283-ab79-424f-910a-287973a77269/940b5c29d71877b0669d0d61ac491f5ca0d169b31adc4cb863cc000489478b62.jpg)  \nFigure 1: RecVAE architecture.\n\n$$\n\\mathcal {L} _ {\\text {M u l t - V A E}} =\n$$\n\n$$\n\\left. \\mathbb {E} _ {q _ {\\phi} \\left(z _ {u} \\mid x _ {u}\\right)} \\left[ \\log p _ {\\theta} \\left(x _ {u} \\mid z _ {u}\\right) - \\beta \\mathrm {K L} \\left(q _ {\\phi} \\left(z _ {u} \\mid x _ {u}\\right) \\| p (z _ {u})\\right) \\right], \\right. \\tag {10}\n$$\n\nwhich follows the general VAE structure with an additional hyperparameter  $\\beta$  that allows to achieve a better balance between latent code independence and reconstruction accuracy, following the  $\\beta$ -VAE framework [11].\n\nThe likelihood  $p_{\\theta}(\\pmb{x}_u|\\pmb{z}_u)$  in the ELBO of Mult-VAE is multinomial distribution. The logarithm of multinomial likelihood for a single user  $u$  in Mult-VAE is now\n\n$$\n\\log \\operatorname {M u l t} \\left(\\boldsymbol {x} _ {u} \\mid n _ {u}, \\mathbf {p} _ {u}\\right) = \\sum_ {i} \\boldsymbol {x} _ {u i} \\log \\mathbf {p} _ {u i} + C _ {u}, \\tag {11}\n$$\n\nwhere  $C_u$  is the logarithm of the normalizing constant which is ignored during training. We treat it as a sum of cross-entropy losses.\n\n# 3.2 Model Architecture\n\nBefore introducing novel regularization techniques, we provide a general description of the proposed model. Our model is inherited from Mult-VAE, but we also suggest some architecture changes. The general architecture is shown on Figure 1; the figure reflects some of the novelties we will discuss below in this section.\n\nThe first change is that we move to a denoising variational autoencoder, that is, move from the ELBO as shown in (10) to\n\n$$\n\\begin{array}{l} \\mathcal {L} _ {\\text {M u l t - V A E}} = \\mathbb {E} _ {q _ {\\phi} \\left(z _ {u} \\mid \\tilde {x} _ {u}\\right)} \\mathbb {E} _ {p \\left(\\tilde {x} _ {u} \\mid x _ {u}\\right)} \\left[ \\log p _ {\\theta} \\left(x _ {u} \\mid z _ {u}\\right) - \\right. \\\\ \\left. \\left. - \\beta \\mathrm {K L} \\left(q _ {\\phi} \\left(z _ {u} \\mid x _ {u}\\right) \\| p \\left(z _ {u}\\right)\\right) \\right]. \\right. \\tag {12} \\\\ \\end{array}\n$$\n\nNote that while the original paper [22] compares Mult-VAE with Mult-DAE, a denoising autoencoder that applies Bernoulli-based noise to the input but does not have the VAE structure (Mult-DAE is a regular denoising autoencoder), in reality the authors used denoising for Mult-VAE as well. This is evidenced both by their code base and by our experiments, where we were able to match the\n\nresults of [22] when we used denoising and got nowhere even close without denoising (we will return to this discussion in Section 3.5). According to our intuition and experiments, the input noise for the denoising autoencoder and latent variable noise of Monte Carlo integration play different roles: the former forces the model not only to reconstruct the input vector but also to predict unobserved feedback, while the latter leads to more robust embedding learning.\n\nSimilar to Mult-VAE, we use the noise distribution  $p(\\tilde{\\boldsymbol{x}}|\\boldsymbol{x})$  defined as elementwise multiplication of the vector  $\\boldsymbol{x}$  by a vector of Bernoulli random variables parameterized by their mean  $\\mu_{\\mathrm{noise}}$ .\n\nWe keep the structure of both likelihood and approximate posterior unchanged:\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {u} \\mid \\boldsymbol {z} _ {u}\\right) = \\operatorname {M u l t} \\left(\\boldsymbol {x} \\mid n _ {u}, \\pi \\left(\\boldsymbol {z} _ {u}\\right)\\right), \\tag {13}\n$$\n\n$$\n\\pi \\left(z _ {u}\\right) = \\operatorname {s o f t m a x} \\left(f _ {\\theta} \\left(z _ {u}\\right)\\right), \\tag {14}\n$$\n\n$$\nf _ {\\boldsymbol {\\theta}} \\left(\\boldsymbol {z} _ {u}\\right) = W \\boldsymbol {z} _ {u} + \\mathbf {b}, \\tag {15}\n$$\n\n$$\nq _ {\\phi} \\left(z _ {u} \\mid \\boldsymbol {x} _ {u}\\right) = \\mathcal {N} \\left(z _ {u} \\mid \\psi_ {\\phi} \\left(\\boldsymbol {x} _ {u}\\right)\\right), \\tag {16}\n$$\n\nwhere  $\\psi_{\\phi}(\\cdot)$  is the inference network, parameterized by  $\\phi$ , that predicts the mean vector and (diagonal) covariance matrix for the latent code  $z_{u}$ . However, we change the underlying neural networks. Our proposed architecture for the inference network is shown on Figure 3; it uses the ideas of densely connected layers from dense CNNs [15], swish activation functions [27], and layer normalization [21]. The decoder network is a simple linear layer with softmax activation, where  $\\theta = \\{W, \\mathbf{b}\\}$ . Here  $W$  and  $\\mathbf{b}$  can be considered as the item embeddings matrix and the item bias vector respectively. In a similar way, we can consider encoder  $\\psi_{\\phi}(\\cdot)$  as a function that maps user feedback to user embeddings.\n\n# 3.3 Composite prior\n\nBoth input and output of Mult-VAE are high-dimensional sparse vectors. Besides, while shared amortized approximate posterior regularizes learning, posterior updates for some parts of the observed data may hurt variational parameters corresponding to other parts of the data. These features may lead to instability during training, an effect similar to a well-known \"forgetting\" effect in reinforcement learning. Previous work on policy-based reinforcement learning showed that it helps to regularize model parameters by bringing them closer to model parameters on the previous epoch [13, 30]; in reinforcement learning, it helps to make the final score grow more smoothly, preventing the model from forgetting good behaviours.\n\nA direct counterpart of these ideas in our setting would be to use a standard Gaussian prior for the latent code  $z$  and add a separate regularization term in the form of the KL divergence between the new parameter distribution  $q_{\\phi}(z|x)$  and the previous one  $q_{\\phi_{old}}(z|x)$ , where  $\\phi_{old}$  are the parameters from the previous epoch of the learning process. However, in our experiments it worked better to unite these two ideas (prior and additional regularizer) by using a composite prior\n\n$$\np (z | \\phi_ {o l d}, \\boldsymbol {x}) = \\alpha \\mathcal {N} (z | 0, \\mathbf {I}) + (1 - \\alpha) q _ {\\phi_ {o l d}} (z | \\boldsymbol {x}), \\tag {17}\n$$\n\ni.e., a convex combination (with  $0 \\leq \\alpha \\leq 1$ ) of a standard normal distribution and an approximate posterior  $q_{\\phi_{old}}(z|\\pmb{x})$  with fixed parameters carried over from the previous epoch. The second term regulates large steps during variational parameters optimization\n\nand can be interpreted as an auxiliary loss function, while the first term prevents overfitting.\n\nNote that this approach is not equivalent mathematically to a Gaussian prior and a separate KL regularizer that pulls current variational parameters to their previous values, and the fact that it works better makes this composite prior into a new meaningful contribution. We also note several works that argue for the benefits of trainable and/or complex prior distributions [35, 40], although in our experiments these approaches have not brought any improvements compared to the prior proposed above.\n\nConditioning the prior on variational parameters from the previous training epoch converts our model to a conditional variational autoencoder where we assume both approximate posterior and likelihood to be conditionally independent of variational parameters from the previous epoch. Comparing our model to VAEAC [17], the latter has a noised conditional prior while in our model we add noise to the approximate posterior during training. Also, unlike VAEAC, we do not train prior parameters.\n\n# 3.4 Rescaling KL divergence\n\nWe have already mentioned the  $\\beta$ -VAE framework [11] which is crucial for the performance of Mult-VAE and, by extension, for RecVAE. However, the question of how to choose or change  $\\beta$  is still not solved conclusively. Some works (see, e.g., [5]) advocate to increase the value of  $\\beta$  from 0 to 1 during training, with 1 yielding the basic VAE model and the actual ELBO. For the training of Mult-VAE, the authors of [22] proposed to increase  $\\beta$  from 0 up to some constant. In our experiments, we have not found any improvements when  $\\beta$  is set to increase over some schedule, so we propose to keep scale factor fixed; this also makes it easier to find the optimal value for this hyperparameter.\n\nMoreover, we propose an alternative view on KL divergence rescaling. Assume that the user feedback data is partially observed. We denote by  $\\mathbf{X}_u^o$  the set of items which user  $u$  has positively interacted with (according to observed data);  $\\mathbf{X}_u^f$  similarly denotes the full set of items which user  $u$  has positively interacted with (together with unobserved items). Items in  $\\mathbf{X}_u^f$  and  $\\mathbf{X}_u^o$  are represented in one-hot encoding, so that  $\\pmb{x}_u = \\sum_{a\\in \\mathbf{X}_u^o}\\pmb{1}_a$ , where  $\\pmb{x}_u$  is the feedback vector for user  $u$  and  $\\mathbf{1}_a$  is a vector with one 1 in the position corresponding to item  $a$ . We denote  $\\pmb{x}_u = \\sum_{a\\in \\mathbf{X}_u^o}\\pmb{1}_a$ ,  $\\pmb{x}_u^f = \\sum_{a\\in \\mathbf{X}_u^f}\\pmb{1}_a$  and abbreviate  $\\mathrm{KL}_u = \\mathrm{KL}\\left(q_\\phi (\\pmb {z}_u|\\pmb {x}_u)\\bigg\\| p(\\pmb {z}_u)\\right)$  and  $\\mathrm{KL}_u^f = \\mathrm{KL}\\left(q_\\phi (\\pmb {z}_u|\\pmb {x}_u^f)\\bigg\\| p(\\pmb {z}_u)\\right)$ .\n\nConsider the evidence lower bound of a variational autoencoder (1) with multinomial likelihood. It can be rewritten as follows:\n\n$$\n\\begin{array}{l} \\mathcal {L} = \\mathbb {E} _ {q _ {\\phi} \\left(\\boldsymbol {z} _ {u} \\mid \\boldsymbol {x} _ {u} ^ {f}\\right)} \\left[ \\log \\operatorname {M u l t} \\left(\\boldsymbol {x} _ {u} ^ {f} \\mid \\boldsymbol {\\pi} \\left(\\boldsymbol {z} _ {u}\\right)\\right) - \\mathrm {K L} _ {u} ^ {f} \\right] = \\\\ \\mathbb {E} _ {q _ {\\phi} \\left(\\boldsymbol {z} _ {u} \\mid \\boldsymbol {x} _ {u} ^ {f}\\right)} \\left[ \\sum_ {a \\in \\mathbf {X} _ {u} ^ {f}} \\log \\operatorname {C a t} \\left(\\mathbf {1} _ {a} \\mid \\boldsymbol {\\pi} \\left(\\boldsymbol {z} _ {u}\\right)\\right) - \\mathrm {K L} _ {u} ^ {f} \\right] + C _ {u} = \\tag {18} \\\\ \\mathbb {E} _ {q _ {\\phi} (z _ {u} | \\boldsymbol {x} _ {u} ^ {f})} \\sum_ {a \\in \\mathbf {X} _ {u} ^ {f}} \\left[ \\log \\mathrm {C a t} (\\mathbf {1} _ {a} | \\boldsymbol {\\pi} (z _ {u})) - \\frac {1}{| \\mathbf {X} _ {u} ^ {f} |} \\mathrm {K L} _ {u} ^ {f} \\right] + C _ {u}, \\\\ \\end{array}\n$$\n\nwhere  $\\mathrm{Cat}(\\mathbf{1}_a|\\mathbf{p}_u) = p_{ua}$  is the categorical distribution, and  $C_u^\\prime$  a constant that depends on normalizing constant of the multinomial distribution  $\\mathrm{Mult}(\\pmb{x}_u^f |\\pmb {\\pi}(\\pmb {z}_u))$ , which does not affect optimization.\n\nWe approximate the ELBO obtained above by summing over observed feedback only and assuming that  $q_{\\phi}(z_u|\\pmb{x}_u) \\approx q_{\\phi}(z_u|\\pmb{x}_u^f)$  and therefore  $\\mathrm{KL}_u \\approx \\mathrm{KL}_u^f$ . In the sequence of equalities below, we first approximate the sum over  $\\mathbf{X}_u^f$  from (18) with a sum over  $\\mathbf{X}_u^o$  with the corresponding rescaling coefficient  $|\\mathbf{X}_u^f| / |\\mathbf{X}_u^o|$ , then approximate  $\\mathrm{KL}_u^f$  with  $\\mathrm{KL}_u$ :\n\n$$\n\\begin{array}{l} \\mathcal {L} \\approx \\frac {| \\mathbf {X} _ {u} ^ {f} |}{| \\mathbf {X} _ {u} ^ {o} |} \\mathbb {E} _ {q _ {\\phi} (\\boldsymbol {z} _ {u} | \\boldsymbol {x} _ {u} ^ {f})} \\sum_ {a \\in \\mathbf {X} _ {u} ^ {o}} \\left[ \\log \\operatorname {C a t} (\\mathbf {1} _ {a} | \\boldsymbol {\\pi} (\\boldsymbol {z} _ {u})) - \\frac {1}{| \\mathbf {X} _ {u} ^ {f} |} \\mathrm {K L} _ {u} ^ {f} \\right] + C _ {u} ^ {\\prime} \\\\ \\approx \\frac {| \\mathbf {X} _ {u} ^ {f} |}{| \\mathbf {X} _ {u} ^ {o} |} \\mathbb {E} _ {q _ {\\phi} (\\boldsymbol {z} _ {u} | \\boldsymbol {x} _ {u})} \\sum_ {a \\in \\mathbf {X} _ {u} ^ {o}} \\left[ \\log \\mathrm {C a t} (\\mathbf {1} _ {a} | \\boldsymbol {\\pi} (\\boldsymbol {z} _ {u})) - \\frac {1}{| \\mathbf {X} _ {u} ^ {f} |} \\mathrm {K L} _ {u} \\right] + C _ {u} ^ {\\prime} \\\\ = \\frac {\\left| \\mathbf {X} _ {u} ^ {f} \\right|}{\\left| \\mathbf {X} _ {u} ^ {o} \\right|} \\mathbb {E} _ {q _ {\\phi} \\left(z _ {u} \\mid \\boldsymbol {x} _ {u}\\right)} \\left[ \\sum_ {a \\in \\mathbf {X} _ {u} ^ {o}} \\log \\operatorname {C a t} \\left(\\mathbf {1} _ {a} \\mid \\boldsymbol {\\pi} \\left(z _ {u}\\right)\\right) - \\frac {\\left| \\mathbf {X} _ {u} ^ {o} \\right|}{\\left| \\mathbf {X} _ {u} ^ {f} \\right|} \\mathrm {K L} _ {u} \\right] + C _ {u} ^ {\\prime} \\\\ = \\frac {| \\mathbf {X} _ {u} ^ {f} |}{| \\mathbf {X} _ {u} ^ {o} |} \\mathbb {E} _ {q _ {\\phi} (\\boldsymbol {z} _ {u} | \\boldsymbol {x} _ {u})} \\left[ \\log \\mathrm {M u l t} (\\boldsymbol {x} _ {u} | \\boldsymbol {\\pi} (\\boldsymbol {z} _ {u})) - \\frac {| \\mathbf {X} _ {u} ^ {o} |}{| \\mathbf {X} _ {u} ^ {f} |} \\mathrm {K L} _ {u} \\right] + C _ {u} ^ {\\prime \\prime}, \\\\ \\end{array}\n$$\n\nwhere  $C_u'$  and  $C_u''$  are constants related to normalizing constants of the multinomial distribution.\n\nFinally, we make the assumption that  $|\\mathbf{X}_u^f |$  is the same for every  $u$  and denote  $\\gamma = 1 / |\\mathbf{X}_u^f |$ . While this assumption might look strange, in effect  $|\\mathbf{X}_u^f |$  is not merely unknown but is actually under our control: it is the number of items  $u$  has feedback about plus the number of items for recommendation. Therefore, we reduce all  $|\\mathbf{X}_u^f |$  into a single hyperparameter  $\\gamma$ :\n\n$$\n\\mathcal {L} \\approx \\frac {1}{\\gamma | \\mathbf {X} _ {u} ^ {o} |} \\mathbb {E} _ {q _ {\\phi} (z _ {u} | \\boldsymbol {x} _ {u})} \\left[ \\log \\operatorname {M u l t} (\\boldsymbol {x} _ {u} | \\pi (z _ {u})) - \\gamma | \\mathbf {X} _ {u} ^ {o} | \\mathrm {K L} _ {u} \\right]. \\tag {19}\n$$\n\nIn practice, we drop the  $1 / \\gamma |\\mathbf{X}_u^o |$  factor in front of the expectation: it does not change the relation between the log likelihood and KL regularizer but rather changes the learning rate individually for each user, which slightly degraded performance in our experiments. The resulting KL divergence scaling factor,\n\n$$\n\\beta^ {\\prime} = \\beta^ {\\prime} \\left(\\mathbf {x} _ {u}\\right) = \\gamma \\left| \\mathbf {X} _ {u} ^ {o} \\right| = \\gamma \\sum_ {i} x _ {u i}, \\tag {20}\n$$\n\nwhere  $\\gamma$  is a constant hyperparameter shared across all users and chosen with cross-validation, works better (see below). In total, we have proposed and motivated an approach where the  $\\beta$  constant in  $\\beta$ -VAE is proportional to the amount of feedback available for the current user,  $|\\mathbf{X}_u^o|$ ; this is an important modification that has led to significant improvements in our experiments.\n\n# 3.5 Alternating Training and Regularization by Denoising\n\nAlternating least squares (ALS) [2] is a popular technique for matrix factorization. We train our model in a similar way, alternating between user and item embeddings. User embeddings are amortized by the inference network, while each item embedding is trained individually. This means that the two groups of parameters,  $\\phi$  in the\n\nALGORITHM 1: Proposed training procedure  \nData:  $\\mathcal{D} = \\{\\pmb {x}_1,\\dots ,\\pmb{x}_{|U|}\\}$    \nResult:  $\\phi ,\\theta$    \nfor  $n\\coloneqq 1,\\ldots ,N$  do for  $m\\coloneqq 1,\\ldots ,M_{\\mathrm{enc}}$  do Sample batch  $\\{\\pmb {x}_1,\\dots ,\\pmb {x}_b\\} \\sim \\mathcal{D};$  Update  $\\phi$  based on  $\\widetilde{\\mathcal{L}}$  end  $\\phi_{old}\\coloneqq \\phi$  . for  $m\\coloneqq 1,\\dots ,M_{\\mathrm{dec}}$  do Sample batch  $\\{\\pmb {x}_1,\\dots ,\\pmb {x}_b\\} \\sim \\mathcal{D};$  Update  $\\theta$  based on  $\\widetilde{\\mathcal{L}}_{\\mathrm{dec}}$  end   \nend\n\nencoder network and  $\\theta$  in the item matrix and bias vector, are of a different nature and it might be best to train them in different ways. We propose to update  $\\phi$  and  $\\theta$  alternately with a different number of iterations: since the encoder is a much more complex network than the decoder, we make multiple updates of  $\\phi$  for each update of  $\\theta$ . This separation of training steps allows for another improvement. Both our experiments and prior art indicate that reconstruction of corrupted input data, i.e., using denoising autoencoders, is necessary in autoencoder-based collaborative filtering, forcing the model to learn to not only reconstruct previously observed feedback but also generalize to unobserved feedback during inference. However, we noticed that performance improves if we do not corrupt input data during the training of  $\\theta$  and leave the denoising purely for  $\\phi$ . Since other types of regularization (such as  $L_{2}$  or moving to a Bayesian decoder) also lead to degraded performance, it appears that decoder parameters are overregularized. Thus, we propose to train the decoder as part of a basic vanilla VAE, with no denoising applied. As for the encoder, however, we train it as part of the denoising variational autoencoder.\n\n# 3.6 Summary\n\nTo summarize the proposed regularizers and changes, we first write down the ELBO for our model:\n\n$$\n\\begin{array}{l} \\mathcal {L} = \\mathbb {E} _ {q _ {\\phi} (z | \\tilde {x})} \\mathbb {E} _ {q (\\tilde {x} | x)} \\left[ \\log p _ {\\theta} (x | z) - \\right. \\\\ \\left. \\left. - \\beta^ {\\prime} (\\boldsymbol {x}) \\operatorname {K L} \\left(q _ {\\phi} (z | \\tilde {\\boldsymbol {x}}) \\| p (z | \\boldsymbol {\\phi} _ {o l d}, \\boldsymbol {x})\\right) \\right], \\right. \\tag {21} \\\\ \\end{array}\n$$\n\nwhere the conditional prior distribution  $p(z|\\phi_{old},\\boldsymbol {x})$  has been defined in (17), and the modified weight  $\\beta^{\\prime}(\\pmb {x})$  of the KL term in (20).\n\nTo keep the input uncorrupted while training the decoder, we introduce a modified objective function for  $\\theta$  updates (we skip the KL term entirely because it does not depend on  $\\theta$ ):\n\n$$\n\\mathcal {L} _ {\\mathrm {d e c}} = \\mathbb {E} _ {q _ {\\phi} (z | x)} \\log p _ {\\theta} (x | z). \\tag {22}\n$$\n\nWe train the model using alternating updates as shown in Algorithm 1; different parameters  $M_{\\mathrm{enc}}$  and  $M_{\\mathrm{dec}}$  reflect the asymmetry between encoder and decoder that we have discussed above. During training, we approximate both inner and outer expectation by single Monte-Carlo samples and use reparametrization similar to\n\nregular VAE. Now we can introduce the empirical lower bound\n\n$$\n\\begin{array}{l} \\widetilde {\\mathcal {L}} (\\pmb {x}, \\pmb {\\theta}, \\pmb {\\phi}, \\pmb {\\phi} _ {o l d}) = \\log p _ {\\pmb {\\theta}} (\\pmb {x} | \\pmb {z} ^ {(*)}) - \\\\ - \\beta^ {\\prime} (\\boldsymbol {x}) \\left(- \\log q _ {\\phi} \\left(\\boldsymbol {z} ^ {(*)} | \\tilde {\\boldsymbol {x}}\\right) + \\log p \\left(\\boldsymbol {z} ^ {(*)} | \\boldsymbol {\\phi} _ {o l d}, \\boldsymbol {x}\\right)\\right), \\tag {23} \\\\ \\end{array}\n$$\n\nwhere  $z^{(*)} = g(\\epsilon, \\mu, \\sigma)$ , for a Gaussian posterior  $g(\\epsilon, \\mu, \\sigma) = \\epsilon \\cdot \\mu + \\sigma$ , where  $[\\mu, \\log \\sigma^2] = \\psi_{\\phi}(\\mathbf{x})$ ,  $\\epsilon \\sim \\mathcal{N}(0, 1)$ , and  $\\tilde{\\mathbf{x}} = \\mathbf{x} \\odot \\mathbf{m}$  is the noised input,  $\\mathbf{m} \\sim \\text{Bernoulli}(\\mu_{\\text{noise}})$ . We use Monte-Carlo sampling for both log likelihood and KL divergence since the KL divergence between a Gaussian and a mixture of Gaussians cannot be calculated analytically. The dropout layer on Figure 3 serves for noising; it is turned off during evaluation, decoder learning phase, and in the composite prior. Since we use the multinomial likelihood, the main component of the loss function is the classification cross-entropy as shown on Figure 1. The empirical lower bound for decoder training  $\\widetilde{\\mathcal{L}}_{\\text{dec}}$  is introduced in a similar way.\n\nSimilar to Mult-VAE, our model is also able to make predictions for users whose feedback was not observed during training. To do that, we predict the user embedding with the inference network  $z = \\psi_{\\phi}(\\mathbf{x})$  based on the feedback  $\\mathbf{x}$  observed at test time and then predict top items using the trained decoder  $p_{\\theta}(\\mathbf{x} \\mid z)$ .\n",
  "experiments": "# 4 EXPERIMENTAL EVALUATION\n\n# 4.1 Metrics\n\nFollowing prior art, we evaluate the models with information retrieval metrics for ranking quality: Recall@k and NDCG@k. Both metrics compares top-k predictions of a model with the test set  $\\mathbf{X}_u^t$  of user feedback for user  $u$ . To obtain recommendations for RecVAE and similar models, we sort the items in descending order of the likelihood predicted by the decoder, exclude items from the training set, and denote the item at the nth place in the resulting list as  $R_u^{(n)}$ . In this notation, evaluation metrics for a user  $u$  are defined as follows:\n\n$$\n\\operatorname {R e c a l l} @ k (u) = \\frac {1}{\\min  \\left(M , \\left| \\mathbf {X} _ {u} ^ {t} \\right|\\right)} \\sum_ {n = 1} ^ {k} \\mathbb {1} \\left[ R _ {u} ^ {(n)} \\in \\mathbf {X} _ {u} ^ {t} \\right], \\tag {24}\n$$\n\nwhere  $\\mathbb{1}[\\cdot ]$  denotes the indicator function,\n\n$$\n\\begin{array}{l} \\operatorname {D C G} @ k (u) = \\sum_ {n = 1} ^ {k} \\frac {2 ^ {\\mathbb {1} \\left[ R _ {u} ^ {(n)} \\in \\mathrm {X} _ {u} ^ {t} \\right]} - 1}{\\log (n + 1)}, \\\\ \\text {N D C G} @ k (u) = \\left(\\sum_ {n = 1} ^ {\\left| X _ {u} ^ {t} \\right|} \\frac {1}{\\log (n + 1)}\\right) ^ {- 1} \\text {D C G} @ k (u), \\tag {25} \\\\ \\end{array}\n$$\n\ni.e., NDCG@k(u) is defined as DCG@k(u) divided by its highest theoretically possible value. Recall@k accounts for all top-  $k$  items equally, while NDCG@k assigns larger weights to top ranked items; thus, it is natural to choose a larger value of  $k$  for NDCG@k.\n\n# 4.2 Datasets\n\nWe have evaluated RecVAE on the MovieLens-20M dataset [9], Netflix Prize Dataset [3], and Million Songs Dataset [4]. We have\n\npreprocessed the datasets in accordance with the Mult-VAE approach [22]. Dataset statistics after preprocessing are as follows: MovieLens-20M contains 9,990,682 ratings on 20,720 movies provided by 136,677 users, Netflix Prize Dataset contains 56,880,037 ratings on 17,769 movies provided by 463,435 users, and Million Songs Dataset contains 33,633,450 ratings on 41,140 songs provided by 571,355 users. In order to evaluate the model on users unavailable during the training, we have held out 10,000 users for validation and testing for MovieLens-20M, 40,000 users for the Netflix Prize, and 50,000 users for the Million Songs Dataset. We used  $80\\%$  of the ratings in the test set in order to compute user embeddings and evaluated the model on the remaining  $20\\%$  of the ratings.\n\n# 4.3 Baselines\n\nWe compare the performance of the proposed model with several baselines, which we divide into three groups. The first group includes linear models from classical collaborative filtering. Weighted Matrix Factorization (WMF) [14] binarizes implicit feedback  $r_{ua}$  (number of times user  $u$  positively interacted with item  $a$ ) as  $p_{ua} = 1$  ( $r_{ua} > 0$ ) and decomposes the matrix of  $p_{ua}$  similar to SVD but with confidence weights that increase with  $r_{ua}$ :\n\n$$\n\\min  _ {\\boldsymbol {w} _ {u}, \\boldsymbol {w} _ {a}} \\sum_ {u, a} (1 + \\alpha r _ {u a}) \\left(p _ {u a} - \\boldsymbol {w} _ {u} ^ {\\top} \\boldsymbol {w} _ {a}\\right) ^ {2} + \\lambda \\left(\\| W _ {u} \\| _ {2} + \\| W _ {a} \\| _ {2}\\right), \\tag {26}\n$$\n\nwhere  $\\| \\cdot \\|_2$  is the  $L_2$ -norm.\n\nThe Sparse Linear Method (SLIM) for top-N recommendation [25] learns a sparse matrix of aggregation coefficients  $W$  that corresponds to the weights of rated items aggregated to produce recommendation scores, i.e., the prediction is  $\\tilde{r}_{ia} = \\boldsymbol{w}_i^\\top \\boldsymbol{w}_a$ , or in matrix form  $\\tilde{R} = RW$ , with the resulting optimization problem\n\n$$\n\\min  _ {W} \\frac {1}{2} \\| R - R W \\| _ {F} ^ {2} + \\frac {\\beta}{2} \\| W \\| _ {F} ^ {2} + \\lambda \\| W \\| _ {1} \\tag {27}\n$$\n\nsubject to  $W \\geq 0$  and  $\\mathrm{diag}(W) = 0$ , where  $\\| \\cdot \\|_F$  is the Frobenius norm and  $\\| \\cdot \\|_1$  is the  $L_1$ -norm. The Embarrassingly Shallow Autoencoder (EASE) [33] is a further improvement on SLIM with a closed-form solution, where the non-negativity constraint and  $L_1$  regularization are dropped. Despite the name, we do not refer to this model as an autoencoder-based one.\n\nThe second is learning to rank methods such as WARP [38] and LambdaNet [6]. LambdaNet is a learning to rank approach that allows to work with objective functions that are either flat or discontinuous; the main idea is to approximate the gradient of each item's score in the ranked list for the corresponding query. The result can be treated as a direction where the item is to \"move\" in the ranked list for the query when sorted with respect to the newly predicted scores. WARP considers every user-item pair  $(u, i)$  corresponding to a positive interaction when training to predict scores for recommendation ranking. For every user  $u$ , other random items  $i'$  are sampled until the first one with the predicted score lower than that of the  $i$  is found. The pair  $(u, i')$  is then treated as a negative sample, and  $(u, i)$  and  $(u, i')$  are employed as positive and a negative contributions respectively for the approximation of the indicator function for the ranking loss similar to those introduced in an earlier work [36]. For more details about WARP and its performance we refer to the original work [38].\n\nThe third group includes autoencoder-based methods that we have already discussed in Sections 2 and 3.1: CDAE [39], Mult-DAE, and Mult-VAE [22]. The proposed RecVAE model can also be considered as a member of this group. We also consider the very recently proposed Ranking-Critical Training (RaCT) [23]. This model adopts an actor-critic framework for collaborative filtering on implicit data. A critic (represented by a neural network) learns to approximate the ranking scores, which in turn improves an MLE-based nonlinear latent variable model (VAE and possibly its variations) with the learned ranking-critical objectives. The critic neural network is feature-based and is using posterior sampling as exploration for better estimates. Both actor and critic are pretrained in this model. Scores for these models have been taken from [22] and [23].\n\n# 4.4 Evaluation setup\n\nRecVAE was trained with the Adam optimizer [18] with learning rate  $= 5 \\cdot 10^{-4}$ , and batch size  $b = 500$ .  $M_{\\mathrm{dec}}$  is selected so that each element in the dataset is selected once per epoch, i.e.,  $M_{\\mathrm{dec}} = \\frac{|U|}{\\mathrm{batch~size}}$ , the Bernoulli noise parameter is  $\\mu_{\\mathrm{noise}} = 0.5$ , and  $M_{\\mathrm{enc}} = 3M_{\\mathrm{dec}}$ . In addition to the standard normal distribution and the old posterior as parts of the composite prior, we also add a normal distribution with zero mean and  $\\log \\sigma^2 = 10$  to the mixture. Weights of these mixture components are  $3/20, 3/4$ , and  $1/10$  respectively. Since the model is sensitive to changes of the parameter  $\\gamma$ , we have picked it individually for each dataset:  $\\gamma = 0.005$  for MovieLens-20M,  $\\gamma = 0.0035$  for the Netflix Prize Dataset, and  $\\gamma = 0.01$  for the Million Songs Dataset. Each model was trained during  $N = 50$  epochs ( $N = 100$  for MSD), choosing the best model by the NDCG@100 score on a validation subset. Initially, we fit the model for MovieLens-20M and then fine-tuned it for the Netflix Prize Dataset and MSD.\n\n# 4.5 Results\n\nPerformance scores for top-N recommendations in the three datasets are presented in Table 1. The results clearly show that in terms of recommendation quality, RecVAE outperforms all previous autoencoder-based models across all datasets in the comparison, in particular, with a big improvement over Mult-VAE. Moreover, new features of RecVAE and RaCT are independent and can be used together for even higher performance. Nevertheless, the proposed model significantly outperforms EASE only on MovieLens-20M datasets, and shows competitive performance on the Netflix Prize Dataset.\n\nFor competing models in the comparison, we have used metrics reported in prior art; for RecVAE we have also indicated confidence intervals, showing that the difference in scores is significant.\n\n# 4.6 Ablation study and negative results\n\nIn order to demonstrate that each of the new features we introduced for RecVAE compared to Mult-VAE indeed helps to improve performance, we have performed a detailed ablation study, comparing various subsets of the features: (1) new encoder architecture, (2) composite prior for the latent codes, (3)  $\\beta$  rescaling, (4) alternating training, and (5) removing denoising for the decoder.\n\nNumerical results of the ablation study are presented in Table 2. We see that each new feature indeed improves the results, with all proposed new features leading to the best NDCG@100 scores on\n\nTable 1: Evaluation scores for RecVAE and baseline models on MovieLens-20M, Netflix Prize Dataset, and MSD. The best results are highlighted in bold. The second best ones are underlined.  \n\n<table><tr><td></td><td>Recall@20</td><td>Recall@50</td><td>NDCG@100</td></tr><tr><td colspan=\"4\">MovieLens-20M Dataset</td></tr><tr><td>WARP [38]</td><td>0.314</td><td>0.466</td><td>0.341</td></tr><tr><td>LambdaNet[6]</td><td>0.395</td><td>0.534</td><td>0.427</td></tr><tr><td>WMF [14]</td><td>0.360</td><td>0.498</td><td>0.386</td></tr><tr><td>SLIM [25]</td><td>0.370</td><td>0.495</td><td>0.401</td></tr><tr><td>CDAE [39]</td><td>0.391</td><td>0.523</td><td>0.418</td></tr><tr><td>Mult-DAE [22]</td><td>0.387</td><td>0.524</td><td>0.419</td></tr><tr><td>Mult-VAE [22]</td><td>0.395</td><td>0.537</td><td>0.426</td></tr><tr><td>RaCT [23]</td><td>0.403</td><td>0.543</td><td>0.434</td></tr><tr><td>EASE [33]</td><td>0.391</td><td>0.521</td><td>0.420</td></tr><tr><td>RecVAE (ours)</td><td>0.414±0.0027</td><td>0.553±0.0028</td><td>0.442±0.0021</td></tr><tr><td colspan=\"4\">Netflix Prize Dataset</td></tr><tr><td>WARP [38]</td><td>0.270</td><td>0.365</td><td>0.306</td></tr><tr><td>LambdaNet[6]</td><td>0.352</td><td>0.441</td><td>0.386</td></tr><tr><td>WMF [14]</td><td>0.316</td><td>0.404</td><td>0.351</td></tr><tr><td>SLIM [25]</td><td>0.347</td><td>0.428</td><td>0.379</td></tr><tr><td>CDAE [39]</td><td>0.343</td><td>0.428</td><td>0.376</td></tr><tr><td>Mult-DAE [22]</td><td>0.344</td><td>0.438</td><td>0.380</td></tr><tr><td>Mult-VAE [22]</td><td>0.351</td><td>0.444</td><td>0.386</td></tr><tr><td>RaCT [23]</td><td>0.357</td><td>0.450</td><td>0.392</td></tr><tr><td>EASE [33]</td><td>0.362</td><td>0.445</td><td>0.393</td></tr><tr><td>RecVAE (ours)</td><td>0.361±0.0013</td><td>0.452±0.0013</td><td>0.394±0.0010</td></tr></table>\n\n<table><tr><td colspan=\"4\">Million Songs Dataset</td></tr><tr><td>WARP [38]</td><td>0.206</td><td>0.302</td><td>0.249</td></tr><tr><td>LambdaNet[6]</td><td>0.259</td><td>0.355</td><td>0.308</td></tr><tr><td>WMF [14]</td><td>0.211</td><td>0.312</td><td>0.257</td></tr><tr><td>SLIM [25]</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CDAE [39]</td><td>0.188</td><td>0.283</td><td>0.237</td></tr><tr><td>Mult-DAE [22]</td><td>0.266</td><td>0.363</td><td>0.313</td></tr><tr><td>Mult-VAE [22]</td><td>0.266</td><td>0.364</td><td>0.316</td></tr><tr><td>RaCT [23]</td><td>0.268</td><td>0.364</td><td>0.319</td></tr><tr><td>EASE [33]</td><td>0.333</td><td>0.428</td><td>0.389</td></tr><tr><td>RecVAE (ours)</td><td>0.276±0.0010</td><td>0.374±0.0011</td><td>0.326±0.0010</td></tr></table>\n\nall three datasets. Some new features are complementary: e.g.,  $\\beta$  rescaling and alternating training degrade the scores when applied individually, but together improve them; the new architecture does not bring much improvement by itself but facilitates other new features;  $\\beta$  rescaling is dataset-sensitive, sometimes improving a lot and sometimes doing virtually nothing.\n\nWe have also performed extended analysis of the composite prior, namely checked how the log  $p(z|\\phi_{old},x)$  regularizer that brings variational parameters closer to old ones affects model stability. This regularizer stabilizes training, as evidenced by the rate of change in the variational parameters. Figure 2 illustrates how the composite prior fixes the \"forgetting\" problem. It shows how NDCG@100 changes for a randomly chosen user as training progresses: each\n\nTable 2: Evaluation of RecVAE with different subsets of new features. The first row corresponds to Mult-VAE.  \n\n<table><tr><td rowspan=\"2\">New architecture</td><td rowspan=\"2\">Composite prior</td><td rowspan=\"2\">β(x) rescaling</td><td rowspan=\"2\">Alternating training</td><td rowspan=\"2\">Decoder w/o denoising</td><td colspan=\"3\">NDCG@100</td></tr><tr><td>ML-20M</td><td>Netflix</td><td>MSD</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.426</td><td>0.386</td><td>0.319</td></tr><tr><td>✓</td><td></td><td></td><td></td><td></td><td>0.428</td><td>0.388</td><td>0.320</td></tr><tr><td>✓</td><td>✓</td><td></td><td></td><td></td><td>0.435</td><td>0.392</td><td>0.325</td></tr><tr><td>✓</td><td></td><td>✓</td><td></td><td></td><td>0.435</td><td>0.390</td><td>0.321</td></tr><tr><td>✓</td><td></td><td></td><td>✓</td><td>✓</td><td>0.427</td><td>0.387</td><td>0.319</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td>0.438</td><td>0.390</td><td>0.325</td></tr><tr><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>0.420</td><td>0.380</td><td>0.308</td></tr><tr><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>0.434</td><td>0.383</td><td>0.321</td></tr><tr><td>✓</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>0.437</td><td>0.392</td><td>0.323</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>0.441</td><td>0.391</td><td>0.322</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>0.442</td><td>0.394</td><td>0.326</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/764fd283-ab79-424f-910a-287973a77269/984f9e03212d7a817599d791c95ec1552d34741b747269ce4813e84640a48d74.jpg)  \nFigure 2: Differences in NDCG@100 for a random user as a function of the training iteration.\n\nvalue is the difference in NDCG@100 for this user after each subsequent training update. Since each update changes the encoder network, it changes all user embeddings, and the changes can be detrimental for some users; note, however, that for the composite prior the changes remain positive almost everywhere while a simple Gaussian prior leads to much more volatile behaviour.\n\nIn addition, we would like to report the negative results of our other experiments. First, autoencoder-based models replace the matrix of user embeddings with a parameterized function, so it was natural to try to do the same for item embeddings. We trained RecVAE with a symmetric autoencoder that predicts top users for a given item, training it alternately with regular RecVAE and regularizing the results of each encoder with embeddings from the other model. The resulting model trained much slower, required more memory, and could not reach the results of RecVAE.\n\nSecond, we have tried to use more complex prior distributions. Mixtures of Gaussians and the variational mixture VampPrior [35] have (nearly) collapsed to a single node in our experiments, an effect previously noted in reinforcement learning [34]. The RealNVP prior [8] has yielded better performance compared to the standard Gaussian prior, but we have not been able to successfully integrate\n\nit into the proposed composite prior: the composite prior with a Gaussian term remained the best throughout our experiments. We note this as a potential direction for further research.\n\nThird, instead of  $\\beta$ -VAE-like weighing of KL divergence, we tried to re-weigh each of the terms in the decomposed KL divergence separately. It appears natural to assume that \"more precise\" regularization could be beneficial for both performance and understanding. However, neither a simple decomposition into entropy and cross-entropy nor the more complex one proposed in [7] has led the model to better results.\n",
  "hyperparameter": "Learning rate: 5×10^-4 (Adam optimizer); Batch size: 500; Bernoulli noise parameter μ_noise: 0.5; Encoder updates per epoch M_enc: 3M_dec where M_dec = |U|/batch_size; Training epochs N: 50 (MovieLens-20M, Netflix), 100 (MSD); KL rescaling parameter γ: 0.005 (MovieLens-20M), 0.0035 (Netflix), 0.01 (MSD); Composite prior mixture weights: [3/20 for N(0,I), 3/4 for old posterior, 1/10 for N(0,10I)]; Latent dimension k: not explicitly specified but inherited from Mult-VAE setup; β parameter: replaced by user-specific β'(x) = γ×sum(x_ui)"
}