{
    "id": "WeightedGCL_2025",
    "paper_title": "Squeeze and Excitation: A Weighted Graph Contrastive Learning for Collaborative Filtering",
    "alias": "WeightedGCL",
    "year": 2025,
    "domain": "Recsys",
    "task": "GeneralRecommendation",
    "idea": "WeightedGCL introduces a robust perturbation strategy that adds noise only to the final GCN layer to avoid destabilizing early representations, and couples it with a Squeeze-and-Excitation network that dynamically learns a feature-wise weight matrix for each perturbed view, enabling the model to emphasize informative features while suppressing noise during graph contrastive learning for collaborative filtering.",
    "introduction": "# 1 Introduction\n\nTo alleviate the problem of information overload on the web, recommender systems have become an important tool [5, 18, 20, 22], which commonly adopt collaborative filtering (CF) to capture the complex relations between users and items. Attribute to the natural bipartite graph structure of user-item interactions, many existing works have achieved advanced performance by effectively leveraging graph-based recommendation models [4, 15]. For example, NGCF [15] integrates a graph convolutional network (GCN) framework into recommender systems, maintaining both feature transformation and non-linear operations. In contrast, LightGCN [4] raises that these components are unnecessary for recommendation tasks, proposing a lightweight GCN model instead. Numerous subsequent graph-based models [16, 24, 28] have pushed the boundaries of graph-based recommendation systems, offering improved scalability, accuracy, and robustness.\n\nIn recent years, contrastive learning (CL) has seen great development in representation learning. In the recommendation field, CL has proven to be a powerful self-supervised learning method for leveraging unlabeled data to alleviate the data sparsity problem.\n\nRecent studies [1, 2, 17, 24] have demonstrated the effectiveness of combining GCN and CL, termed graph contrastive learning (GCL), to improve recommendation performance. GCL typically involves constructing perturbed views of the user-item bipartite graph and applying contrastive learning between these views.\n\nNevertheless, these existing models [17, 25] often apply uniform weights for all features within each perturbed view, neglecting the varying significance of different features. The equal weight strategy will limit the model's ability to effectively leverage crucial features, making it pay too much attention to less relevant information. This raises a crucial question: How to allocate attention to different features dynamically?\n\nTo address this, we propose a tailored and novel framework named Weighted Graph Contrastive Learning (WeightedGCL). Specifically, WeightedGCL applies a robust perturbation strategy, which only perturbs the final GCN layer's view. In addition, WeightedGCL incorporates a squeeze and excitation network (SENet) [6] to dynamically assign weights to the features of these perturbed views. Our WeightedGCL enhances the attention on the crucial features by assigning greater weight to them. At the same time, it reduces the attention to less relevant information, ensuring that the model's performance is not degraded by this information. In a nutshell, the contributions of our work are as follows:\n\n- We identify the limitation in existing GCL-based frameworks, in which they assign equal weight to all features and consequently propose a dynamic feature weighting solution.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/7406485b-d044-46a2-b37a-47e86700a9f3/7fd6dd7d0b6bb94809342a6e68d1a7385b4adcd6467f5e69e52dfb030c68d2b3.jpg)\n\n\n\nFigure 1: Overview Framework of our WeightedGCL.\n\n\n- We propose a Weighted Graph Contrastive Learning framework, which incorporating the robust perturbation to keep views stable and incorporating the SENet to assign weights dynamically.\n\n- Experiment results on three public datasets demonstrate the effectiveness of our WeightedGCL.",
    "method": "# 2 Methodology\n\nIn contrastive learning, the construction of contrastive views is crucial. The previous work [24] on GCN-based perturbation applies perturbations across all layers of the network. However, it introduces noise at earlier layers, which destabilize the representation learning process. To address this, we propose a robust perturbation strategy that only applies perturbations to the final GCN layer, which avoids the negative impact of early-stage noise, thereby maintaining the representations more stable. Due to the inherent randomness in perturbed views, directly learning the weight matrix proves challenging. To address it, we introduce a SENet, which includes two parts: squeeze network and excitation network. The former reduces the dimensions of each perturbed view into a summary statistics matrix, and the latter transforms the matrix back to the original dimensions. Therefore, we propose a dynamic feature weighting strategy by combining SENet and GCL, named WeightedGCL. Our WeightedGCL enhances the focus on crucial features while effectively mitigating the impact of less relevant information. The overview framework of WeightedGCL<sup>1</sup> is depicted in Figure 1.\n\n# 2.1 Preliminary\n\nLet  $\\mathcal{U}$  denotes the set of users and  $\\mathcal{I}$  denotes the set of items. The  $L$  represents the number of layers in the GCN. The set of all nodes, including both users and items, is denoted as  $\\mathcal{N} = \\mathcal{U} \\cup \\mathcal{I}$ . Consider a node  $n$  within the set  $\\mathcal{N}$ , whose representation is  $e_n \\in \\mathbb{R}^{d \\times 1}$  in the view  $\\mathcal{E} \\in \\mathbb{R}^{d \\times |\\mathcal{N}|}$ , where  $d$  denotes the feature dimension.\n\n# 2.2 Feature Encoder\n\nGCNs refine node representations by aggregating messages from their neighboring nodes. This process can be formalized as follows:\n\n$$\n\\begin{array}{l} \\overline {{e}} _ {u} ^ {(l)} = \\operatorname {A g g r} ^ {(l)} (\\{\\left. e _ {i} ^ {(l - 1)}: i \\in \\mathcal {N} _ {u} \\right\\}), \\\\ e _ {i} ^ {(l)} = \\operatorname {A g g r} ^ {(l)} \\left(\\left\\{e _ {u} ^ {(l - 1)}: u \\in \\mathcal {N} _ {i} \\right\\}\\right), \\tag {1} \\\\ \\end{array}\n$$\n\nwhere  $\\mathcal{N}_u$  and  $\\mathcal{N}_i$  denote the neighborhood set of nodes  $u$  and  $i$ , respectively, and  $l$  denotes the  $l$ -th layer of GNNs. The node representation aggregation for the entire embeddings  $\\mathcal{F}$  can be formulated as follows:\n\n$$\n\\mathcal {F} = \\frac {1}{L + 1} \\left(\\mathcal {E} ^ {(0)} + \\mathcal {E} ^ {(1)} + \\dots + \\mathcal {E} ^ {(L - 1)} + \\mathcal {E} ^ {(L)}\\right), \\tag {2}\n$$\n\nwhere  $\\mathcal{E}^{(l)}$  denotes the view of nodes in  $l$ -th layer.\n\n# 2.3 Robust Perturbation\n\nWe initially construct contrastive views by the robust perturbation strategy, which adds a random noise matrix into the view of the final layer in GCN. Formally:\n\n$$\n\\bar {\\mathcal {E}} ^ {(L)} = \\mathcal {E} ^ {(L)} + \\tilde {\\Delta}, \\quad \\tilde {\\mathcal {E}} ^ {(L)} = \\mathcal {E} ^ {(L)} + \\tilde {\\Delta}, \\tag {3}\n$$\n\n$$\n\\bar {\\mathcal {F}} = \\frac {1}{L + 1} \\left(\\mathcal {E} ^ {(0)} + \\mathcal {E} ^ {(1)} + \\dots + \\mathcal {E} ^ {(L - 1)} + \\bar {\\mathcal {E}} ^ {(L)}\\right), \\tag {4}\n$$\n\n$$\n\\tilde {\\mathcal {F}} = \\frac {1}{L + 1} \\left(\\mathcal {E} ^ {(0)} + \\mathcal {E} ^ {(1)} + \\dots + \\mathcal {E} ^ {(L - 1)} + \\tilde {\\mathcal {E}} ^ {(L)}\\right),\n$$\n\nwhere  $\\tilde{\\mathcal{F}}$  and  $\\tilde{\\mathcal{F}}$  denote final representations of two perturbed views. Meanwhile, the  $\\tilde{\\Delta}$  and  $\\tilde{\\Delta} \\in \\mathbb{R}^{d \\times |N|} \\sim U(0,1)$  are random noise matrixs. Only perturbing the final layer will maintain the view stability of other front layers. We will conduct an ablation study in Section 3.3 to demonstrate the effectiveness of our strategy.\n\n# 2.4 Squeeze Network\n\nThe squeeze network adopts average pooling, which is beneficial for retaining feature information, to reduce the dimension of the entire perturbed views  $\\ddot{\\mathcal{F}}$  into a summary statistics matrices  $\\bar{S} / \\bar{S} \\in \\mathbb{R}^{1 \\times |\\mathcal{N}|}$ , formally:\n\n$$\n\\text {S q u e e z e} (\\ddot {\\mathcal {F}}) = \\operatorname {C o n} \\left(\\frac {1}{d} \\sum_ {k = 1} ^ {d} \\ddot {f} _ {n} ^ {k} \\mid n \\in \\mathcal {N}\\right), \\tag {5}\n$$\n\n$$\n\\bar {S} = \\text {S q u e e z e} (\\tilde {\\mathcal {F}}), \\quad \\tilde {S} = \\text {S q u e e z e} (\\tilde {\\mathcal {F}}), \\tag {6}\n$$\n\nwhere  $k$  represents the  $k$ -th feature dimension of perturbed node representation  $\\ddot{f}_n$ , and  $\\frac{1}{d}\\sum_{k=1}^{d}\\ddot{f}_n^k$  is the summary statistics matrix of node  $n$ .\n\n# 2.5 Excitation Network\n\nThe summary statistics matrix is then expanded back to the original dimensions using the excitation network, formally:\n\n$$\n\\operatorname {E x c i t a t i o n} (S) = \\sigma \\left(W _ {K} \\left(\\dots \\left(W _ {1} \\cdot S + b _ {1}\\right) \\dots\\right) + b _ {K}\\right), \\tag {7}\n$$\n\n$$\n\\tilde {\\mathcal {T}} = \\text {E x c i t a t i o n} (\\tilde {\\mathcal {S}}), \\quad \\tilde {\\mathcal {T}} = \\text {E x c i t a t i o n} (\\tilde {\\mathcal {S}}), \\tag {8}\n$$\n\nwhere the resulting matrix  $\\tilde{\\mathcal{T}} / \\tilde{\\mathcal{T}} \\in \\mathbb{R}^{d \\times |N|}$  serves as the weight matrix corresponding to the perturbed views  $\\tilde{\\mathcal{F}} / \\tilde{\\mathcal{F}}$ , and  $\\sigma$  denotes the sigmoid function.  $K$  is the feedforward network layer number in Eq. 7. The excitation network employs a multi-layer architecture, gradually ascending the dimension according to an equal scale  $s = \\sqrt[K]{d}$  until the original dimensions are restored, and the  $W_{1} \\in \\mathbb{R}^{s \\times 1}$ , ...,  $W_{K} \\in \\mathbb{R}^{d \\times s^{(K-1)}}$  are the weight matrices for the linear layers. Maintaining a constant ratio facilitates the simplification of the training process. This design results in a more precise generation of the weight matrix, thereby improving the robustness.\n\n# 2.6 Recalibration\n\nEventually, the weighted views  $\\tilde{\\mathcal{R}} / \\tilde{\\mathcal{R}} \\in \\mathbb{R}^{d \\times |\\mathcal{N}|}$  are obtained by scaling the perturbed views  $\\tilde{\\mathcal{F}} / \\tilde{\\mathcal{F}}$  with dynamic weight matrix  $\\tilde{\\mathcal{T}} / \\tilde{\\mathcal{T}}$ , formally:\n\n$$\n\\tilde {\\mathcal {R}} = \\tilde {\\mathcal {T}} \\odot \\tilde {\\mathcal {F}}, \\quad \\tilde {\\mathcal {R}} = \\tilde {\\mathcal {T}} \\odot \\tilde {\\mathcal {F}}, \\tag {9}\n$$\n\nwhere the  $\\odot$  represents element-wise multiplication.\n\n\nTable 1: Performance comparison of baselines, WeightedGCL and variants of WeightedGCL in terms of Recall@K(R@K) and NDCG@K(N@K). The superscript * indicates the improvement is statistically significant where the  $p$ -value is less than 0.01.\n\n\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">Amazon</td><td colspan=\"4\">Pinterest</td><td colspan=\"4\">Alibaba</td></tr><tr><td>R@10</td><td>R@20</td><td>N@10</td><td>N@20</td><td>R@10</td><td>R@20</td><td>N@10</td><td>N@20</td><td>R@10</td><td>R@20</td><td>N@10</td><td>N@20</td></tr><tr><td>MF-BPR</td><td>0.0607</td><td>0.0956</td><td>0.0430</td><td>0.0537</td><td>0.0855</td><td>0.1409</td><td>0.0537</td><td>0.0708</td><td>0.0303</td><td>0.0467</td><td>0.0161</td><td>0.0203</td></tr><tr><td>NGCF</td><td>0.0617</td><td>0.0978</td><td>0.0427</td><td>0.0537</td><td>0.0870</td><td>0.1428</td><td>0.0545</td><td>0.0721</td><td>0.0382</td><td>0.0615</td><td>0.0198</td><td>0.0257</td></tr><tr><td>LightGCN</td><td>0.0797</td><td>0.1206</td><td>0.0565</td><td>0.0689</td><td>0.1000</td><td>0.1621</td><td>0.0635</td><td>0.0830</td><td>0.0457</td><td>0.0692</td><td>0.0246</td><td>0.0299</td></tr><tr><td>UltraGCN</td><td>0.0760</td><td>0.1155</td><td>0.0540</td><td>0.0643</td><td>0.0967</td><td>0.1588</td><td>0.0613</td><td>0.0808</td><td>0.0411</td><td>0.0644</td><td>0.0227</td><td>0.0276</td></tr><tr><td>LayerGCN</td><td>0.0877</td><td>0.1291</td><td>0.0647</td><td>0.0760</td><td>0.1004</td><td>0.1620</td><td>0.0635</td><td>0.0826</td><td>0.0448</td><td>0.0680</td><td>0.0238</td><td>0.0285</td></tr><tr><td>FKAN-GCF</td><td>0.0838</td><td>0.1265</td><td>0.0602</td><td>0.0732</td><td>0.1003</td><td>0.1614</td><td>0.0633</td><td>0.0827</td><td>0.0441</td><td>0.0681</td><td>0.0240</td><td>0.0290</td></tr><tr><td>SGL</td><td>0.0898</td><td>0.1331</td><td>0.0645</td><td>0.0777</td><td>0.1080</td><td>0.1704</td><td>0.0701</td><td>0.0897</td><td>0.0461</td><td>0.0692</td><td>0.0248</td><td>0.0307</td></tr><tr><td>NCL</td><td>0.0933</td><td>0.1381</td><td>0.0679</td><td>0.0815</td><td>0.1033</td><td>0.1609</td><td>0.0666</td><td>0.0833</td><td>0.0477</td><td>0.0713</td><td>0.0259</td><td>0.0319</td></tr><tr><td>SimGCL</td><td>0.0963</td><td>0.1336</td><td>0.0718</td><td>0.0832</td><td>0.1051</td><td>0.1576</td><td>0.0705</td><td>0.0871</td><td>0.0474</td><td>0.0691</td><td>0.0262</td><td>0.0317</td></tr><tr><td>LightGCL</td><td>0.0820</td><td>0.1278</td><td>0.0589</td><td>0.0724</td><td>0.0881</td><td>0.1322</td><td>0.0534</td><td>0.0673</td><td>0.0459</td><td>0.0716</td><td>0.0239</td><td>0.0305</td></tr><tr><td>DCCF</td><td>0.0903</td><td>0.1307</td><td>0.0655</td><td>0.0781</td><td>0.1040</td><td>0.1613</td><td>0.0661</td><td>0.0828</td><td>0.0490</td><td>0.0729</td><td>0.0257</td><td>0.0311</td></tr><tr><td>RecDCL</td><td>0.0927</td><td>0.1345</td><td>0.0652</td><td>0.0780</td><td>0.1021</td><td>0.1619</td><td>0.0663</td><td>0.0839</td><td>0.0521</td><td>0.0768</td><td>0.0273</td><td>0.0338</td></tr><tr><td>BIGCF</td><td>0.0948</td><td>0.1341</td><td>0.0692</td><td>0.0810</td><td>0.1040</td><td>0.1619</td><td>0.0680</td><td>0.0864</td><td>0.0502</td><td>0.0744</td><td>0.0266</td><td>0.0322</td></tr><tr><td>WGCL-all pert.</td><td>0.0983</td><td>0.1378</td><td>0.0733</td><td>0.0853</td><td>0.1163</td><td>0.1768</td><td>0.0755</td><td>0.0961</td><td>0.0560</td><td>0.0831</td><td>0.0305</td><td>0.0374</td></tr><tr><td>WGCL-w/o pert.</td><td>0.0098</td><td>0.0159</td><td>0.0062</td><td>0.0080</td><td>0.0103</td><td>0.0168</td><td>0.0066</td><td>0.0086</td><td>0.0102</td><td>0.0167</td><td>0.0050</td><td>0.0067</td></tr><tr><td>WeightedGCL</td><td>0.0996*</td><td>0.1396*</td><td>0.0741*</td><td>0.0862*</td><td>0.1167*</td><td>0.1793*</td><td>0.0764*</td><td>0.0961*</td><td>0.0596*</td><td>0.0879*</td><td>0.0326*</td><td>0.0397*</td></tr><tr><td>Improv.</td><td>3.43%</td><td>4.49%</td><td>3.20%</td><td>3.61%</td><td>8.06%</td><td>5.22%</td><td>7.81%</td><td>7.13%</td><td>21.63%</td><td>20.58%</td><td>24.42%</td><td>24.45%</td></tr></table>\n\n# 2.7 Contrastive Learning\n\nWe adopt the InfoNCE [12] loss function to perform contrastive learning between two perturbed views. Formally, the loss function is defined as follows:\n\n$$\n\\mathcal {L} _ {c l} = - \\sum_ {u \\in \\mathcal {U}} \\log \\frac {e ^ {\\left(\\tilde {r} _ {u} ^ {\\top} \\tilde {r} _ {u} / \\tau\\right)}}{\\sum_ {u ^ {\\prime} \\in \\mathcal {U}} e ^ {\\left(\\tilde {r} _ {u} ^ {\\top} \\tilde {r} _ {u ^ {\\prime}} / \\tau\\right)}} - \\sum_ {i \\in \\mathcal {I}} \\log \\frac {e ^ {\\left(\\tilde {r} _ {i} ^ {\\top} \\tilde {r} _ {i} / \\tau\\right)}}{\\sum_ {i ^ {\\prime} \\in \\mathcal {I}} e ^ {\\left(\\tilde {r} _ {i} ^ {\\top} \\tilde {r} _ {i ^ {\\prime}} / \\tau\\right)}}, \\tag {10}\n$$\n\nwhere  $\\bar{r}_u$  and  $\\tilde{r}_{u / u^{\\prime}}$  are representation of user  $u / u^{\\prime}$  in contrastive views  $\\tilde{\\mathcal{R}}$  and  $\\tilde{\\mathcal{R}}$ . Besides,  $\\bar{r}_i$  and  $\\tilde{r}_{i / i'}$  are representation of item  $i / i'$  in contrastive views  $\\tilde{\\mathcal{R}}$  and  $\\tilde{\\mathcal{R}}$ .  $\\tau$  is the temperature hyper-parameter.\n\n# 2.8 Optimization\n\nWe utilize LightGCN [4] as the backbone and adopt the Bayesian Personalized Ranking (BPR) loss [14] as our primary optimization objective. The BPR loss is designed to enhance the distinction between predicted preferences for positive and negative items in each triplet  $(u, p, n) \\in \\mathcal{D}$ , where  $\\mathcal{D}$  is the training dataset. The positive item  $p$  is an item with which user  $u$  has interacted, while the negative item  $n$  is randomly selected from the items not interacted with user  $u$ . The BPR loss function is formally defined as:\n\n$$\n\\mathcal {L} _ {r e c} = \\sum_ {(u, p, n) \\in \\mathcal {D}} - \\log \\left(\\sigma \\left(y _ {u, p} - y _ {u, n}\\right)\\right) + \\lambda \\cdot \\| \\boldsymbol {\\Theta} \\| _ {2} ^ {2}, \\tag {11}\n$$\n\nwhere  $\\sigma$  denotes the sigmoid function,  $\\lambda$  controls the  $L_{2}$  regularization strength, and  $\\Theta$  denotes model parameters. The  $y_{u,p}$  and  $y_{u,n}$  are the ratings of user  $u$  to the positive item  $p$  and negative item  $n$ , which calculated by  $r_u^\\top r_p$  and  $r_u^\\top r_n$ . The final loss function is:\n\n$$\n\\mathcal {L} = \\mathcal {L} _ {r e c} + \\lambda_ {c} \\cdot \\mathcal {L} _ {c l}, \\tag {12}\n$$\n\nwhere  $\\lambda_{c}$  is the balancing hyper-parameter.",
    "experiments": "# 3 Experiment\n\nIn this section, we conduct extensive experiments on three real-world datasets to evaluate WeightedGCL, addressing the following research questions:\n\nRQ1: How does the performance of our WeightedGCL compare to advanced recommender systems?\n\nRQ2: How does our Robust Perturbation component influence the overall performance of our WeightedGCL?\n\nRQ3: How do different hyper-parameters influence the performance of our WeightedGCL?\n\n# 3.1 Experimental Settings\n\n3.1.1 Datasets. To evaluate our WeightedGCL in the recommendation task, we conduct extensive experiments on three widely used datasets: Amazon Books (Amazon) [11], Pinterest [5] and Alibaba. Details can be found in Table 2. These datasets offer a set of user-item interactions, and we use the ratio 8:1:1 for each dataset to randomly split the data for training, validation, and testing.\n\n\nTable 2: Statistics of datasets.\n\n\n<table><tr><td>Datasets</td><td>#Users</td><td>#Items</td><td>#Interactions</td><td>Sparsity</td></tr><tr><td>Amazon</td><td>58,144</td><td>58,051</td><td>2,517,437</td><td>99.925%</td></tr><tr><td>Pinterest</td><td>55,188</td><td>9,912</td><td>1,445,622</td><td>99.736%</td></tr><tr><td>Alibaba</td><td>300,001</td><td>81,615</td><td>1,607,813</td><td>99.993%</td></tr></table>\n\nTo ensure the quality of the data, we employ the 15-core setting for Amazon, which ensures a minimum of 15 interactions between users and items. For the Pinterest datasets, users and items with less than five interactions are filtered out. Due to the high sparsity of the Alibaba dataset, we choose to retain all interaction data.\n\n3.1.2 Baselines. To verify the effectiveness of WeightGCL, we select one matrix factorization model MF-BPR [14], five advanced GCN-based models (NGCF [15], LightGCN [4], UltraGCN [10], LayerGCN [28] and FKAN-GCF [19]), and seven state-of-the-art GCL-based models (SGL[17], NCL [9], SimGCL [25], LightGCL [1], DCCF [13], RecDCL [26] and BIGCF [27]) for comparison.\n\n3.1.3 Hyper-parameters. To ensure a fair comparison, we initially refer to the optimal hyper-parameter settings as reported in the original papers of the baseline models. Subsequently, we fine-tune all hyper-parameters of the baselines using grid search. We apply Xavier initialization [3] to all embeddings. The embedding size is 64, and the batch size is 4096. All models are optimized using the\n\nAdam optimizer [8] with a learning rate of  $1e^{-4}$ . We perform a grid search on the balancing hyper-parameter  $\\lambda_{c}$  in  $\\{1e^{-1}, 1e^{-2}, 1e^{-3}\\}$ , temperature hyper-parameter  $\\tau$  in  $\\{0.2, 0.4, 0.6, 0.8\\}$ , and the layer number  $L$  of GCN in  $\\{2, 3, 4\\}$ . Empirically, we set the  $L_{2}$  regularization parameter to  $1e^{-4}$  for Amazon and Pinterest and  $1e^{-5}$  for Alibaba. To avoid the over-fitting, we set 30 as the early stopping epoch number. Moreover, we set the excitation network boosting granularity as a hyper-parameter, containing 1-4 levels of granularity, and detailed description and analysis are in Section 3.4.\n\n3.1.4 Evaluation Protocols. For fairness, we follow the settings of previous works [5, 21, 23] by adopting two widely-used evaluation protocols for top- $K$  recommendations: Recall@K and NDCG@K[7]. We report the average metrics across all users in the test dataset, evaluating performance at both  $K = 10$  and 20.\n\n# 3.2 Performance Comparison (RQ1)\n\nTable 1 shows detailed results. We highlight the optimal results in bold, and underline the suboptimal results. Based on them, we observed that: our WeightedGCL outperforms the strongest baselines, achieving  $4.49\\% (\\mathrm{R}@\\mathrm{20})$ ,  $3.61\\% (\\mathrm{N}@\\mathrm{20})$  improvement on the Amazon dataset, achieving  $5.22\\% (\\mathrm{R}@\\mathrm{20})$ ,  $7.13\\% (\\mathrm{N}@\\mathrm{20})$  improvement on the Pinterest dataset, and achieving  $20.58\\% (\\mathrm{R}@\\mathrm{20})$ ,  $24.45\\% (\\mathrm{N}@\\mathrm{20})$  improvement on the Alibaba dataset, which demonstrates the effectiveness of WeightedGCL. Moreover, we can identify that most GCL-based models outperform traditional models, which is a common trend. Our WeightedGCL outperforms all baselines in three datasets for various metrics.\n\n# 3.3 Ablation Study (RQ2)\n\nTo validate the effectiveness of our robust perturbation, we design the following variants:\n\nWGCL-w/o pert. For this variant, we directly removed our robust perturbation component.\n\nWGCL-all pert. For this variant, we utilize all layer's perturbation to replace our robust perturbation component.\n\nTable 1 also shows the results of all variants. This ablation study shows that the method's performance without perturbation has a sharp performance degradation, resulting in a completely unusable model, which indicates that the perturbation operation is essential for contrastive learning. Our robust perturbation outperforms other variants, which indicates the robust perturbation component plays a critical role in improving model representation and robustness.\n\n# 3.4 Hyper-parameter Sensitivity Study (RQ3)\n\nTo evaluate the hyper-parameter sensitivity of WeightedGCL, we test its performance on three datasets under varying hyper-parameters. Table 3 highlights the impact of various excitation strategies. We denote different ascending granularity as W-G1, W-G2, W-G3, and W-G4, which correspond to varying numbers of layers in the FFN within the excitation network. The larger the number, the finer the granularity, and the more layers.\n\nAfter analyzing the results in Table 3, we found that our framework performs best at the fourth granularity for the Amazon dataset, and the third granularity is the best for the Pinterest and Alibaba datasets. Although the fourth granularity is not the best choice for the latter two datasets, it is still higher than the performance at\n\n\nTable 3: Excitation strategies analysis.\n\n\n<table><tr><td rowspan=\"2\">Variant</td><td colspan=\"2\">Amazon</td><td colspan=\"2\">Pinterest</td><td colspan=\"2\">Alibaba</td></tr><tr><td>R@20</td><td>N@20</td><td>R@20</td><td>N@20</td><td>R@20</td><td>N@20</td></tr><tr><td>W-G1</td><td>0.1314</td><td>0.0807</td><td>0.1713</td><td>0.0931</td><td>0.0772</td><td>0.0359</td></tr><tr><td>W-G2</td><td>0.1392</td><td>0.0859</td><td>0.1764</td><td>0.0953</td><td>0.0793</td><td>0.0363</td></tr><tr><td>W-G3</td><td>0.1391</td><td>0.0860</td><td>0.1793</td><td>0.0961</td><td>0.0879</td><td>0.0397</td></tr><tr><td>W-G4</td><td>0.1396</td><td>0.0862</td><td>0.1790</td><td>0.0956</td><td>0.0826</td><td>0.0371</td></tr></table>\n\nthe first and second granularities. These observations illustrate two points: from the overall trend, the performance of our incentive network is improving with the increase of granularity; but too high granularity makes it difficult to train due to the increased complexity of the model, which will slightly reduce the performance in some cases. This ablation study shows that the choice of excitation strategies to some extent influences our framework's performance.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/7406485b-d044-46a2-b37a-47e86700a9f3/7e66b640e9992de95c0fde7047226e84bf58d4bc42ba98fe80d04684314a466f.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/7406485b-d044-46a2-b37a-47e86700a9f3/e8ed20f52ffb23292a2b2f994c843b5d5f0c2d2564ff6cc0a4f10de3f8650d6d.jpg)\n\n\n\nFigure 2: Performance of our WeightedGCL with respect to different layer number  $L$  of GCN.\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/7406485b-d044-46a2-b37a-47e86700a9f3/328f4724f4e67702529f14cf81077c068d5593d015263fdd0290a448fa29dd23.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/7406485b-d044-46a2-b37a-47e86700a9f3/85b1168b53ff1858be148f280a56a7efec8e7a41b4af6ab7714104cb523b8ce4.jpg)\n\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-11/7406485b-d044-46a2-b37a-47e86700a9f3/89525ede700d46f15fe9ed61acc01aba4983d821d1330a5aa01a42a19eb3db33.jpg)\n\n\n\nFigure 3: Performance of our WeightedGCL with respect to different hyper-parameter pairs  $(\\lambda_c, \\tau)$  in terms of Recall@20.\n\n\nAs shown in the Figure 2, for the Amazon and Pinterest datasets, the optimal layer number  $L$  is 2, and for the Alibaba dataset, the optimal number is 3. Additionally, Figure 3 reveals that for the Amazon dataset, the optimal  $(\\lambda_c, \\tau)$  pair is (1e-1, 0.2); for Pinterest dataset, the optimal pair is (1e-3, 0.2); and for the Alibaba dataset, (1e-3, 0.8) is the optimal pair. The optimal temperature hyper-parameter  $\\tau$  for the Alibaba dataset differs from the other two datasets, attributed to its sparse user-item interactions and the huge number of users. Note that being flexible in choosing the value of hyper-parameters will allow us to adopt our framework to multiple datasets.",
    "hyperparameter": "embedding size = 64, batch size = 4096, learning rate = 1e-4, GCN layers L ∈ {2,3,4} (best: 2 for Amazon/Pinterest, 3 for Alibaba), contrastive weight λc ∈ {1e-1,1e-2,1e-3} (best: 1e-1 Amazon, 1e-3 Pinterest/Alibaba), temperature τ ∈ {0.2,0.4,0.6,0.8} (best: 0.2 Amazon/Pinterest, 0.8 Alibaba), L2 regularization λ = 1e-4 (Amazon/Pinterest) or 1e-5 (Alibaba), excitation FFN levels K ∈ {1,2,3,4} (best: 4 Amazon, 3 Pinterest/Alibaba), early stopping at 30 epochs."
  }