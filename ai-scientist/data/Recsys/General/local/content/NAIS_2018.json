{
  "id": "NAIS_2018",
  "paper_title": "Neural Attentive Item Similarity Model for Recommendation",
  "alias": "NAIS",
  "year": 2018,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "NAIS (Neural Attentive Item Similarity Model) enhances item-based collaborative filtering by introducing an attention mechanism that assigns variable weights to historical items based on their relevance to the target item. The key innovation is using a neural network to learn attention weights parameterized by item embeddings (p_i and q_j), with a smoothed softmax normalization (exponent β < 1) to handle the large variance in user history lengths. This allows the model to differentiate the importance of historical items when predicting preferences, addressing the limitation of FISM which treats all historical items uniformly.",
  "introduction": "# 1 INTRODUCTION\n\nR ecommender system is a core service for many customer-oriented online services to increase their traffic and make profits, such as E-commerce and social media sites. For example, it was reported that in YouTube, recommendations accounted for about  $60\\%$  video clicks for the homepage [2]; in Netflix, recommender systems contributed about  $80\\%$  of movies watched and placed the business value of over $1 billion per year, as indicated by their Chief Product Officer Neil Hunt [3].\n\nIn modern recommender systems, collaborative filtering (CF) — a technique that predicts users' personalized preference from user-item interactions only — plays a central role especially in the phase of candidate generation [4], [5]. Popularized by the Netflix Prize, matrix factorization (MF) methods have become the most popular recommendation approach in academia and been widely studied in literatures [6], [7]. While MF methods are shown to provide superior accuracy over neighbor-based methods in terms of rating prediction, they have been relatively seldom reported to be used in industrial applications. One possible reason is due to MF's personalization scheme — user-to-item CF that characterizes a user with an ID and associates it with an embedding vector. As a result, to refresh recommendations\n\nfor a user with her new interactions, the user's embedding vector has to be updated. However, re-training a MF model for large-scale data is difficult to achieve in real time and may require complex software stack to support online learning, making the approach less attractive for industrial settings [8].\n\nOn the other hand, item-to-item CF — which characterizes a user with her historically interacted items and recommends items similar to the user's profile — has been heavily used in industrial applications [4], [2], [3], [9]. Not only does item-based CF provide more interpretable prediction suitable for many recommendation scenarios, but it also makes real-time personalization much easier to achieve. Specifically, the major computation that estimates item similarities can be done offline and the online recommendation module only needs to perform a series of lookups on similar items, which can be easily done in real-time.\n\nEarly item-based CF approaches use statistical measures such as Pearson coefficient and cosine similarity to estimate item similarities [10]. Since such heuristic-based approaches lack tailored optimization for recommendation, they typically underperform machine learning-based methods in terms of top-K recommendation accuracy [11], [6]. To tackle this, Ning et al. [12] adopt a machine learning view for item-based CF, which learns item similarity from data by optimizing a recommendation-aware objective function. Although better accuracy can be achieved, directly learning the whole item-item similarity matrix has a quadratic complexity w.r.t. the number of items, making it infeasible for practical recommenders that need to deal with millions or even billions of items.\n\nTo address the inefficiency issue of learning-based item-to-item CF, Kabbur et al. [1] propose a factored item simi\n\nclarity model (FISM), which represents an item as an embedding vector and models the similarity between two items as the inner product of their embedding vectors. Being a germ of representation learning [13], [14], FISM provides state-of-the-art recommendation accuracy and is well suited for online recommendation scenarios. However, we argue that FISM's modeling fidelity can be limited by its assumption that all historical items of a user profile contribute equally in estimating the similarity between the user profile and a target item. Intuitively, a user interacts with multiple items in the past, but it may not be true that these interacted items reflect the user's interest to the same degree. For example, a fan of affectional films might also watch a horror film just because the film was popular during that time. Another example is that user interests may change with time, and as such, recently interacted items should be more reflective of a user's future preference.\n\nIn this work, we propose an enhanced item similarity model by distinguishing the different importance of interacted items in contributing to a user's preference. Our NAIS model is built upon FISM, preserving the same merit with FISM in terms of high efficiency in online prediction, while being more expressive than FISM by learning the varying importance of the interacted items. This is achieved by employing the recent advance in neural representation learning - the attention mechanism [15], [16], [17] - for learning item-to-item interactions. One of our key findings is that the standard attention mechanism fails to learn from users historical data, due to the large variance on the lengths of user histories. To address this, we adjust the attention design by smoothing user histories. We conduct comprehensive experiments on two public benchmarks to evaluate top-K recommendation, demonstrating that our NAIS betters FISM for a  $4.5\\%$  relative improvement in terms of NDCG and achieves competitive performance. To facilitate the research community to validate and make further developments upon NAIS, we have released our implementation codes in: https://github.com/AaronHee/Neural-Attentive-Item-Similarity-Model.\n\nThe remainder of the paper is as follows. After introducing some preliminaries in Section 2, we elaborate our proposed method in Section 3. We then perform experimental evaluation in Section 4. We discuss related work in Section 5, before concluding the whole paper in Section 6.\n",
  "method": "# 3 NEURAL ATTENTIVE ITEM SIMILARITY MODEL\n\nIn this section, we present our proposed NAIS methods. Before introducing the NAIS model, we first discuss several designs of attention mechanism that attempt to address the limitation of FISM. We then elaborate the optimization of model parameters. We focus the discussion of optimizing NAIS with implicit feedback, which is the recent focus of recommendation research since implicit feedback is more prevalent and easy to collect than explicit ratings. Lastly, we discuss several properties of NAIS, including the time complexity, support for online personalization, and options for the attention function.\n\n# 3.1 Model Designs\n\nDesign 1. The original idea of attention is that different parts of a model can contribute (i.e., attend) differently for the final prediction [19]. In the scenario of item-based CF, we can intuitively allow historical items contributing\n\n1. The bias terms in the original paper are omitted for clarity.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/0827499144341040c54e8580f69bc52ac099ce36ada4bf7c9dc27d605f94c708.jpg)  \nFig. 1: The neural collaborative filtering framework of our Neural Attentive Item Similarity (NAIS) model.\n\ndifferently to a user's representation by assigning each item an individualized weight:\n\n$$\n\\hat {y} _ {u i} = \\mathbf {p} _ {i} ^ {T} \\left(\\frac {1}{| \\mathcal {R} _ {u} ^ {+} | ^ {\\alpha}} \\sum_ {j \\in \\mathcal {R} _ {u} ^ {+} \\backslash \\{i \\}} a _ {j} \\mathbf {q} _ {j}\\right), \\tag {4}\n$$\n\nwhere  $a_{j}$  is a trainable parameter that denotes the attention weight of item  $j$  in contributing to user representation. Clearly, this model subsumes the FISM, which can be resumed by fixing  $a_{j}$  to 1 for all items. While this model seems to be capable of differentiating the importance of historical items, it ignores the impact of the target item on a historical item. Particularly, we argue that it is unreasonable to assign a historical item a global weight for all predictions, regardless of which item to predict. For example, when predicting a user's preference on a romantic movie, it is undesirable to consider a horrible movie as equally important as another romantic movie. From the perspective of user representation learning, it assumes that a user has a static vector to represent her interest, which may limit the model's representation ability.\n\nDesign 2. To address the limitation of Design 1, an intuitive solution is to tweak  $a_{j}$  to be aware of the target item, i.e., assigning an individualized weight for each  $(i,j)$  pair:\n\n$$\n\\hat {y} _ {u i} = \\mathbf {p} _ {i} ^ {T} \\left(\\frac {1}{| \\mathcal {R} _ {u} ^ {+} | ^ {\\alpha}} \\sum_ {j \\in \\mathcal {R} _ {u} ^ {+} \\backslash \\{i \\}} a _ {i j} \\mathbf {q} _ {j}\\right), \\tag {5}\n$$\n\nwhere  $a_{ij}$  denotes the attention weight of item  $j$  in contributing to user  $u$ 's representation when predicting  $u$ 's preference on target item  $i$ . Although this solution seems to be technically viable, the problem is that if an item pair  $(i,j)$  has never co-occurred in training data (i.e., no user has interacted with both  $i$  and  $j$ ), its attention weight  $a_{ij}$  cannot be estimated and will be a trivial number.\n\nDesign 3. To solve the generalization issue of Design 2, we consider relating  $a_{ij}$  with the embedding vector  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$ . The rationale is that the embedding vectors are supposed to encode the information of items, thus they can be used to determine the weight of an interaction  $(i,j)$ . Specifically, we parameterize  $a_{ij}$  as a function with  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$  as the input:\n\n$$\na _ {i j} = f \\left(\\mathbf {p} _ {i}, \\mathbf {q} _ {j}\\right). \\tag {6}\n$$\n\nThe advantage of this parameterization is that even though a pair  $(i,j)$  has never co-occurred, as long as  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$  have been reliably learned from data, they can still be used to estimate the attention weight  $a_{ij}$  well. To achieve this goal, we need to ensure the function  $f$  has strong representation power. Inspired by the recent success of using neural networks to model the attention weight [16], [15], we similarly use a Multi-Layer Perception (MLP) to parameterize the attention function  $f$ . Specifically, we consider two ways to define the attention network:\n\n$$\n\\left\\{ \\begin{array}{l} 1. f _ {\\text {c o n c a t}} \\left(\\mathbf {p} _ {i}, \\mathbf {q} _ {j}\\right) = \\mathbf {h} ^ {T} \\operatorname {R e L U} \\left(\\mathbf {W} \\left[ \\begin{array}{l} \\mathbf {p} _ {i} \\\\ \\mathbf {q} _ {j} \\end{array} \\right] + \\mathbf {b}\\right) \\\\ 2. f _ {\\text {p r o d}} \\left(\\mathbf {p} _ {i}, \\mathbf {q} _ {j}\\right) = \\mathbf {h} ^ {T} \\operatorname {R e L U} \\left(\\mathbf {W} \\left(\\mathbf {p} _ {i} \\odot \\mathbf {q} _ {j}\\right) + \\mathbf {b}\\right) \\end{array} \\right. \\tag {7}\n$$\n\nwhere  $\\mathbf{W}$  and  $\\mathbf{b}$  are respectively the weight matrix and bias vector that project the input into a hidden layer, and  $\\mathbf{h}^T$  is the vector that projects the hidden layer into an output attention weight. We term the size of hidden layer as \"attention factor\", for which a larger value brings a stronger representation power for the attention network. We use the Rectified Linear Unit (ReLU) as the activation function for the hidden layer, which has shown to have good performance in neural attention network [15]. In later Section 3.3, we discuss the pros and cons of the two attention functions  $f_{\\text{concat}}$  and  $f_{\\text{prod}}$ .\n\nFollowing the standard setting of neural attention network [20], [16], we can formulate the predictive model of Design 3 as follows:\n\n$$\n\\hat {y} _ {u i} = \\mathbf {p} _ {i} ^ {T} (\\sum_ {j \\in \\mathcal {R} _ {u} ^ {+} \\backslash \\{i \\}} a _ {i j} \\mathbf {q} _ {j}),\n$$\n\n$$\na _ {i j} = \\frac {\\exp \\left(f \\left(\\mathbf {p} _ {i} , \\mathbf {q} _ {j}\\right)\\right)}{\\sum_ {j \\in \\mathcal {R} _ {u} ^ {+} \\backslash \\{i \\}} \\exp \\left(f \\left(\\mathbf {p} _ {i} , \\mathbf {q} _ {j}\\right)\\right)}, \\tag {8}\n$$\n\nwhere the coefficient  $\\frac{1}{|\\mathcal{R}_u^+|^\\alpha}$  is aborted into the attention weight  $a_{ij}$  without affecting the representation power, and the softmax function is used to convert the attention weights to a probabilistic distribution. Note that this is the most natural and straightforward way to employ an attention network on interaction history, which is the same as the history modeling part of the Attentive CF model [16].\n\nUnfortunately, we find such a standard solution of attention does not work well in practice — it underperforms FISM significantly, even though it can generalize FISM in theory. After investigating the attention weights, we unexpectedly find the problem stems from the softmax function, a standard choice in neural attention networks. The rationale is as follows. In conventional usage scenarios of attention such as CV and NLP tasks, the number of attentive components does not vary much, such as words in sentences [21] and regions in images [22], [23]. As such, using softmax can properly normalize attention weights and in turn has a nice probabilistic explanation. However, such a scenario does not exist any more for user historical data, since the history length of users (i.e., number of historical items consumed by users) can vary much. Qualitatively speaking, the softmax function performs  $L_{1}$  normalization on attention weights, which may overly punish the weights of active users with a long history.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/804b02793f7fd32d998d595b10add7b44dc443376b908e4b82e668ea73272c9c.jpg)  \nFig. 2: The distribution of user history length on our experimented MovieLens and Pinterest datasets.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/1f87a814bb7c6c07e2bfae28387c4f1658c50366f24004ee24cfd31c00d07c88.jpg)\n\nTo justify this point, we show the distribution of user history length on our experimented MovieLens and Pinterest datasets in Figure 2. We can see that for both real-world datasets, the history length of users varies a lot; specifically, the (mean, variance) of user history length are (166,37145) and (27,57) for MovieLens and Pinterest, respectively. Taking the left subfigure of MovieLens data as an example, the average length for all users is 166, while the maximum length is 2313. Which means, the average attention weight of the most active user is  $1/2313$ , about 14 times fewer than that of average users (i.e.,  $1/166$ ). Such a large variance on attention weights will cause problems in optimizing the item embeddings of the model.\n\nThe NAIS Model. We now present our final design for the NAIS model. As analyzed above, the weak performance of Design 3 comes from the softmax, which performs  $L_{1}$  normalization on attention weights and results in large variance on attention weights of different users. To address the problem, we propose to smooth the denominator of softmax, so as to lessen the punishment on attention weights of active users and meanwhile decrease the variance of attention weights. Formally, the predictive model of NAIS is as follows:\n\n$$\n\\hat {y} _ {u i} = \\mathbf {p} _ {i} ^ {T} (\\sum a _ {i j} \\mathbf {q} _ {j}),\n$$\n\n$$\n\\begin{array}{l} j \\in \\mathcal {R} _ {u} ^ {+} \\backslash \\{i \\} \\\\ \\operatorname {c o m} (f (\\tau_ {1}, \\dots , \\tau_ {n})) \\end{array} \\tag {9}\n$$\n\n$$\na _ {i j} = \\frac {\\exp \\left(f \\left(\\mathbf {p} _ {i} , \\mathbf {q} _ {j}\\right)\\right)}{\\left[ \\sum_ {j \\in \\mathcal {R} _ {u} ^ {+} \\backslash \\{i \\}} \\exp \\left(f \\left(\\mathbf {p} _ {i} , \\mathbf {q} _ {j}\\right)\\right) \\right] ^ {\\beta}}, \\tag {9}\n$$\n\nwhere  $\\beta$  is the smoothing exponent, a hyperparameter to be set in the range of [0, 1]. Obviously, when  $\\beta$  is set to 1, it recovers the softmax function; when  $\\beta$  is smaller than 1, the value of denominator will be suppressed, as a result the attention weights will not be overly punished for active users. Although the probabilistic explanation of attention network is broken with  $\\beta < 1$ , we empirically find that it leads to a performance much better than using the standard softmax (see Section 4.4 for experiment results). We use the term \"NAIS-concat\" and \"NAIS-prod\" to denote the NAIS model that uses  $f_{\\text{concat}}$  and  $f_{\\text{prod}}$  as the attention function, respectively (cf. Equation (7)).\n\nMoreover, our NAIS model can be viewed under the recently proposed Neural Collaborative Filtering (NCF) framework [5], as illustrated in Figure 1. Differing from the user-based NCF models that use one-hot user ID as the input feature, our NAIS model uses multi-hot interacted items as the input feature for a user. Together with the carefully designed attention network as the hidden layer, our NAIS\n\nmodel can be more intuitively understood as performing item-to-item CF.\n\n# 3.2 Optimization\n\nTo learn a recommender model, we need to specify an objective function to optimize. As we deal with implicit feedback where each entry is a binary value 1 or 0, we can deem the learning of a recommender model as a binary classification task. Similar to the previous work on Neural CF [5], we treat the observed user-item interactions as positive instances, sampling negative instances from the remaining unobserved interactions. Let  $\\mathcal{R}^{+}$  and  $\\mathcal{R}^{-}$  denote the set of positive and negative instances, respectively, we minimize the regularized log loss defined as follows:\n\n$$\nL = - \\frac {1}{N} \\left(\\sum_ {(u, i) \\in \\mathcal {R} ^ {+}} \\log \\sigma \\left(\\hat {y} _ {u i}\\right) + \\sum_ {(u, i) \\in \\mathcal {R} ^ {-}} \\log \\left(1 - \\sigma \\left(\\hat {y} _ {u i}\\right)\\right)\\right) + \\lambda \\| \\Theta \\| ^ {2} \\tag {10}\n$$\n\nwhere  $N$  denotes the number of total training instances, and  $\\sigma$  is a sigmoid function that converts a prediction  $\\hat{y}_{ui}$  to a probability value denoting the likelihood that  $u$  will interact with  $i$ . The hyper-parameter  $\\lambda$  controls the strength of  $L_{2}$  regularization to prevent overfitting, and  $\\Theta = \\{\\{\\mathbf{p}_i\\}, \\{\\mathbf{q}_i\\}, \\mathbf{W}, \\mathbf{b}, \\mathbf{h}\\}$  denotes all trainable parameters. We are aware of other options of objective functions, such as the pointwise regression [6], [24] and pairwise ranking [11], [20] losses, can also be employed to learn NAIS for implicit feedback. As the focus of the work is to show the effectiveness of NAIS, especially on the improvement over FISM to justify the usage of attention, we leave the exploration of other objective functions as future work.\n\nTo optimize the objective function, we adopt Adagrad [25], a variant of Stochastic Gradient Descent (SGD) that applies an adaptive learning rate for each parameter. It draws a stochastic sample from all training instances, updating the related parameters towards the negative direction of their gradients. We use the mini-batch version of Adagrad to speedup the training process, and the generation of a minibatch is detailed in Section 4.1 of experimental settings. In each training epoch, we first generate all negative instances, and then feed them together with positive instances into the training algorithm for parameter updates. This leads to much faster training than sampling the negative instance on-the-fly (as done in Bayesian Personalized Ranking [11]) when training on GPU platforms, since it avoids the unnecessary switch between GPU (for parameter updating) and CPU (for negative sampling). Specifically, for each positive instance  $(u,i)$ , we randomly sample  $X$  items that  $u$  has never interacted before as negative instances. In our experiments we set  $X$  as 4, an empirical number that has shown good performance for neural CF methods [5].\n\nPre-training. Due to the non-linearity of neural network model and non-convexity of the objective function (w.r.t. all parameters), optimization using SGD can be easily trapped to local minimums of poor performance. As such, the initialization of model parameters plays a vital role in the model's final performance. Empirically, when we try to train NAIS from random initialization, we find it converges slowly and leads to a final performance slightly better than FISM. We hypothesize that it is due to the difficulty of optimizing\n\nthe attention network and item embeddings simultaneously. Since the outputs of attention network rescale item embeddings, jointly training them may result in the co-adaption effect, which slows down the convergence. For example, a training epoch may decrease an attention weight  $a_{ij}$  but increase the embedding product  $\\mathbf{p}_i^T\\mathbf{q}_j$ , resulting in only a small progress in updating the prediction score.\n\nTo address the practical issue in training NAIS, we pretrain NAIS with FISM, using the item embeddings learned by FISM to initialize that of NAIS. Since FISM does not have the co-adaption issue, it can learn item embeddings well in encoding item similarity. As such, using FISM embeddings to initialize NAIS can greatly facilitate the learning of the attention network, leading to faster convergence and better performance. With such a meaningful initialization of item embeddings, we can simply initialize the attention network with a random Gaussian distribution.\n\n# 3.3 Discussions\n\nIn this subsection, we discuss three properties of NAIS, namely, its time complexity, ease to support online personalization, and the two options for attention function.\n\nTime Complexity Analysis. We analyze the time complexity of the predictive model of NAIS, i.e., Equation (9). This directly reflects the time cost of NAIS in testing (or recommendation), and the time cost of training should be proportional to that of testing. The time complexity of evaluating a prediction  $\\hat{y}_{ui}$  with FISM (cf. Equation (3)) is  $O(k|\\mathcal{R}_u^+|)$ , where  $k$  denotes the embedding size and  $|\\mathcal{R}_u^+|$  denotes the number of historical interactions of user  $u$ . Compared to FISM, the additional cost of evaluating a prediction with NAIS comes from the attention network. Let  $a$  denote the attention factor, then we can express the time complexity of evaluating  $f(\\mathbf{p}_i,\\mathbf{q}_j)$  as  $O(ak)$ . Since the denominator of softmax (and our proposed smoothed variant of softmax) needs to traverse over all items in  $\\mathcal{R}_u^+$ , the time complexity of evaluating an  $a_{ij}$  is  $O(ak|\\mathcal{R}_u^+|)$ . As such, a direct implementation of NAIS model takes time  $O(ak|\\mathcal{R}_u^+|^2)$ , since we need to evaluate  $a_{ij}$  for each  $j$  in  $|\\mathcal{R}_u^+|$ . However, considering the denominator term is shared across the computation of all items in  $\\mathcal{R}_u^+$ , we only need to compute it once and cache it for all evaluations of  $a_{ij}$  (where  $j$  is in  $\\mathcal{R}_u^+$ ). As such, the overall time complexity of evaluating a NAIS prediction can be reduced to  $O(ak|\\mathcal{R}_u^+|)$ , which is  $a$  times of that of FISM.\n\nSupport for Online Personalization. The offline training of a recommender model provides personalized recommendation based on a user's past history. For online personalization, we consider the practical scenario that a user has new interactions streaming in, and the recommender model needs to refresh the top-K recommendation for the user instantaneously [6], [26]. Since it is prohibitive to perform model re-training in real-time², an alternative solution is to perform local updates on model parameters based on the new feedback only. This is the common strategy used by user-based CF model, such as matrix factorization [6]. However, we argue that even local updates on parameters\n\n2. To the authors' knowledge, the current industrial servings of recommender systems usually perform model re-training on a daily basis.\n\nare difficult to achieve in practice. The key difficulty is that users may have concurrent interactions on an item. As such, separately performing local updates on a per interaction basis will result in collision, and it is non-trivial to resolve the collision in a distributed setting in real-time.\n\nInstead of updating model parameters to adapt new interactions, NAIS can refresh the representation vector of a user without updating any model parameter, reducing the difficulty of provide online personalization services. This is attributed to the item-based CF mechanism that characterizes a user with her interaction history, rather than her ID. Specifically in NAIS, a user's representation vector is aggregated by a weighted sum on item embeddings, which allows a nice decomposable evaluation on a prediction. For example, let's assume user  $u$  has a new interaction on item  $t$ . To refresh the prediction of  $u$  on a candidate item  $i$  (i.e.,  $\\hat{y}_{ui}$ ), instead of computing  $\\hat{y}_{ui}$  from scratch (i.e., following Equation (9)), we only need to evaluate the score of  $a_{it}\\mathbf{p}_i^T\\mathbf{q}_t$ , and then sum it with the old prediction of  $\\hat{y}_{ui}$ . With the cache of the denominator of softmax, the refresh of  $\\hat{y}_{ui}$  can be done in  $O(ak)$  time. This is much more efficient than performing local updates with MF [6] (for which the time complexity is  $O(k^2 + |\\mathcal{R}_u^+|k)$ ), since  $a$  is usually a small number (typically set to be the same as  $k$ ).\n\nOptions for Attention Function. The two choices of attention function differ in the construction of input: the first choice  $f_{\\text{concat}}$  simply concatenates  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$  to learn the attention weight  $w_{ij}$  [19], while second choice  $f_{\\text{prod}}$  feeds the element-wise product of  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$  into the attention network [15]. Analytically speaking, since the attention weight  $w_{ij}$  is to score the interaction  $\\mathbf{p}_i^T \\mathbf{q}_j$ , using the element-wise product  $\\mathbf{p}_i \\odot \\mathbf{q}_j$  as input may facilitate the hidden layer in learning the attention function (since  $\\mathbf{p}_i^T \\mathbf{q}_j = \\mathbf{1}^T (\\mathbf{p}_i \\odot \\mathbf{q}_j)$ ); as a downside, it may also cause some information loss unintentionally, since the original information encoded in  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$  are discarded. In contrast,  $f_{\\text{concat}}$  leverages the original information encoded in  $\\mathbf{p}_i$  and  $\\mathbf{q}_j$  to learn their interaction weight, which has no information loss; however, due to the numerical gap between the concatenation  $[\\mathbf{p}_i, \\mathbf{q}_j]^T$  and element-wise product  $\\mathbf{p}_i \\odot \\mathbf{q}_j$ , it may lead to slower convergence. We will empirically compare the two choices of attention function in the experiments section.\n",
  "experiments": "# 4 EXPERIMENTS\n\nIn this section, we conduct experiments with the aim of answering the following research questions:\n\nRQ1 Are our proposed attention networks useful for providing more accurate recommendations?  \nRQ2 How do our proposed NAIS methods perform compared with state-of-the-art recommendation methods?  \nRQ3 What are the key hyper-parameters for NAIS and how do they impact NAIS's performance?  \nIn what follows, we first present the experimental settings, followed by results answering the above questions.\n\n# 4.1 Experimental Settings\n\nDatasets and Evaluation Protocol. We adopt the same MovieLens and Pinterest datasets as the ones used in\n\nTABLE 1: Statistics of the evaluation datasets.  \n\n<table><tr><td>Dataset</td><td>Interaction#</td><td>Train#</td><td>Item#</td><td>User#</td></tr><tr><td>MovieLens</td><td>1,000,209</td><td>4,970,845</td><td>3,706</td><td>6,040</td></tr><tr><td>Pinterest</td><td>1,500,809</td><td>7,228,110</td><td>9,916</td><td>55,187</td></tr></table>\n\nthe NCF paper [5]. Since both datasets have some preprocessing steps such as removing sparse users and train-test splitting, we directly evaluate on the processed data<sup>3</sup>. Table 1 summarizes the statistics of the two datasets. More details on the generation of the two datasets have been elaborated in [5], so we do not restate them. Note that during training each interaction is paired with 4 negative instances, thus the number of training instances is much more than the number of interactions.\n\nWe adopt the leave-one-out evaluation protocol [11], [5], which holds out the latest interaction of each user as the testing data and uses the remaining interactions for training. Specifically, each testing instance is paired with 99 randomly sampled negative instances; then each method outputs prediction scores for the 100 instances (1 positive plus 99 negatives), and the performance is judged by Hit Ratio (HR) [27] and Normalized Discounted Cumulative Gain (NDCG) [28] at the position 10. Both metrics have been widely used to evaluate top-K recommendation [1] and ranking systems [29] in information retrieval literatures. We report the average scores for all users, where  $HR@10$  can be interpreted as a recall-based measure that indicates the percentage of users are successfully recommended (i.e., the positive instance appears in top-10), and  $NDCG@10$  is a precision-based measure that accounts for the predicted position of the positive instance, the larger the better.\n\nBaselines. We compare NAIS with the following item recommendation methods:\n\nPop. This is a non-personalized method to benchmark the performance of the top-K recommendation task. It ranks items by their popularity, judged by the number of interactions that an item received.\n\nItemKNN [10]. This is the standard item-based CF method as formulated in Equation (1). We use consine similarity to measure  $s_{ij}$ . We experiment with different numbers of nearest item neighbors to consider, finding using all neighbors lead to best results.\n\nFISM [1]. This is a state-of-the-art item-based CF model as formulated in Equation (3). We test  $\\alpha$  from 0 to 1 with a step size of 0.1, finding a value of 0 leads to best result on both datasets (the variance is actually small when  $\\alpha$  is smaller than 0.6).\n\nMF-BPR [11]. MF-BPR learns MF by optimizing the pairwise Bayesian Personalized Ranking (BPR) loss. This method is a popular choice for building a CF recommender from implicit feedback.\n\nMF-eALS [6]. This method also learns a MF model, but optimizes a different pointwise regression loss that treats all missing data as negative feedback with a smaller weight. The optimization is done by an element-wise Alternating Learning Square (eALS) algorithm.\n\nMLP [5]. This method applies a multi-layer perceptron (MLP) above user and item embeddings to learn the scoring\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/27e562e1d68b8ad1c97734353f1ea98be74c6903c161ea2fbc988e1ac32ad475.jpg)  \n(a) MovieLens - HR  \nFig. 3: Testing performance of FISM, NAIS-prod, and NAIS-concat of embedding size 16 in each epoch.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/5880207961493685b3b290a55888a28b17692af68c14eebba6dfe17ffeaf92b9.jpg)  \n(b) MovieLens - NDCG\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/490706f02a8512208db2535a715b0165d5cbba05b4ddea6717b5b02f58a69ff6.jpg)  \n(c) Pinterest - HR\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/2127ffdd2e3cea488191ba0ef4ae96e0d653054ff7e2ab33485ca9e086de91e2.jpg)  \n(d) Pinterest — NDCG\n\nfunction from data. We employ a 3-layer MLP and optimize the same pointwise log loss, which was reported to perform well on the two datasets.\n\nWe have deliberately chosen the above methods to cover a diverse range of recommendation methods: ItemKNN and FISM are representative of item-based CF approaches to validate the utility of our attention-argument modeling, MF-BPR and MF-eALS are competitive user-based CF approaches to evidence the state-of-the-art performance of recommendation from implicit feedback, and MLP is a recently proposed deep neural network-based CF method. Note that we focus on the comparison of single CF models. As such, we do not further compare with NeuMF which achieves the best performance in the NCF paper, since NeuMF is an ensemble method that fuses MF and MLP in the latent space.\n\nParameter Settings. For each method, we first train it without regularization; if overfitting is observed (i.e., training loss keeps decreasing but the performance becomes worse), we then tune the regularization coefficient  $\\lambda$  in the range of  $[10^{-6}, 10^{-5}\\dots, 1]$ . The validation set is consisted of a randomly drew interaction for each user. For the embedding size  $k$ , we test the values of [8, 16, 32, 64], and set the attention factor  $a$  same as the embedding size in each setting. For a fair comparison with FISM, we optimize it with the same pointwise log loss using the same Adagrad learner. We find that using the item embeddings learned by FISM to initialize NAIS (i.e., the pre-training step) leads to slightly better performance but much faster convergence. Without special mention in texts, we report the performance of NAIS with following default settings: 1)  $\\beta = 0.5$ , 2)  $k = a = 16$ , 3)  $\\lambda = 0$ , 4) Adagrad with a learning rate of 0.01, and 5) pre-training with FISM embeddings.\n\nImplementation Details. We implement NAIS using TensorFlow<sup>4</sup>. Since in the input layer an item (user) is represented as a one-hot (multi-hot) vector where most entries are zeros, for efficiency and memory concern, we adopt sparse representation that stores the IDs of non-zero entries only. Here an implementation challenge is that different users have different number of non-zero entries, while TensorFlow requires all training instances of a batch must be of the same length (same as other programming tools for deep learning like Theano). To tackle the challenge, a widely adopted solution is to use the masking trick, which adds masks (i.e., pseudo non-zero entries) to ensure all instances\n\nof a batch have a same length (i.e., the maximum length of instances of the batch). However, we find this solution is very time-consuming on CF datasets, as some active users may have interacted with over thousands of items, making a sampled mini-batch very large. To address the issue, we innovatively form a mini-batch as all training instances of a randomly sampled user, rather than randomly sampling a fixed number of training instances as a mini-batch. This trick of user-based mini-batch has two advantages: 1) no mask is used thus it is much faster (empirically  $3X$  speedup over the masking trick), and 2) no batch size needs to be specified which refrains the pain of tuning the batch size. Moreover, the recommendation performance remains the same according to our experiments.\n\nTABLE 2: Training time per epoch (seconds) of methods that are implemented using TensorFlow.  \n\n<table><tr><td>Methods</td><td>MovieLens</td><td>Pinterest</td></tr><tr><td>MF-BPR</td><td>24.4 s</td><td>17.3 s</td></tr><tr><td>MLP</td><td>125.8 s</td><td>155.8 s</td></tr><tr><td>FISM</td><td>238.3 s</td><td>353.3 s</td></tr><tr><td>NAIS_concat</td><td>455.2 s</td><td>525.6 s</td></tr><tr><td>NAIS_PROD</td><td>428.5 s</td><td>485.2 s</td></tr></table>\n\nTraining Time. Table 2 shows the training time per epoch of NAIS and baselines that are implemented with TensorFlow. A training epoch is defined as training  $5|\\mathcal{R}^{+}|$  instances, since the negative sampling ratio is 4. The running environment is a server with Intel Xeon CPU E5-2630 @ 2.20GHz and 64GB memory. Note that the running time of ItemKNN and MF-eALS are not shown since they are implemented with Java, which are not comparable with other methods. We can see that item-based CF methods (FISM and NAIS) take longer training time than user-based CF methods (MF-BPR and MLP). This is reasonable, since user-based methods use an ID only to represent a user in the input layer, while item-based methods use interacted items to represent a user. MLP uses more time than MF-BPR, since it has three more hidden layers than MF-BPR. Moreover, the two NAIS methods take longer time than FISM, due to the additional use of attention network. The additional time cost is quite acceptable, which is roughly 0.8 times of the training time of FISM. Among the two NAIS methods, NAIS_concat takes slightly longer time than NAIS_prov, since concatenation increases the input dimension while product does not.\n\n# 4.2 Effectiveness of Attention Networks (RQ1)\n\nTechnically speaking, our NAIS model enhances FISM by replacing the constant weight (i.e.,  $1 / |R_u^+|^\\alpha$ ) of an estimated\n\nitem-item similarity (i.e.,  $\\mathbf{p}_i^T\\mathbf{q}_j$ ) with a variable weight learned by an attention network. To demonstrate the efficacy of our designed attention networks, we first run FISM until convergence, and then use FISM embeddings to initialize NAIS for training the attention network.\n\nFigure 3 shows the stable performance of FISM and the scores of our two NAIS methods at embedding size 16 in each epoch. We can clearly see the effectiveness of using attention networks. Specifically, the initialized performance of NAIS are close to FISM, while by training the attention network, the two NAIS methods improve over FISM significantly. Here we show the performance of 50 epochs only, and further training on NAIS can lead to even better performance. Upon convergence (results can be found in Table 5), both NAIS methods achieve a relative improvement of  $6.3\\%$  and  $3.6\\%$  over FISM in terms of NDCG on MovieLens and Pinterest, respectively. We believe the improvements on recommendation accuracy stem from the strong representation power of NAIS. Moreover, we find that NAIS-prod converges faster than NAIS-concat (while their final performance are close). This confirms our analysis in Section 3.3 by providing empirical evidence that feeding  $\\mathbf{p}_i\\odot \\mathbf{q}_j$  into the attention network can facilitate learning the weight of  $\\mathbf{p}_i^T\\mathbf{q}_j$ .\n\n# 4.2.1 Qualitative Analysis\n\nHere we provide some qualitative analysis on the attention weights to show their learnability and interpretability.\n\nFirst, it is interesting to see how do the attention weights evolve during training. However, a prediction of  $\\hat{y}_{ui}$  has  $|\\mathcal{R}_u^+$  attention weights, and it is difficult to plot the attention weights for all predictions. Instead, we record the statistics — mean and variance — of the attention weights of a prediction, and effective learning of the attention network is evidenced by a large variance (note that the variances of FISM are 0). Figure 4 shows the scatter plot of the statistics learned by NAIS-prod at different epochs in Pinterest, where each scatter point denotes the prediction of a testing instance. We can see that in the initial phase of training (Epoch 1), the points are concentrated near x-axis, i.e., variances are close to zero. With more training epochs, the points become more dispersive along the y-axis, and many points start to get a high variance. Together with Figure 3 which shows more training epochs lead to better performance, we can conclude that the attention weights have been properly trained to be more distinguishable for historical items. This reveals the reason of NAIS improving over FISM, justifying our key argument of this work that the historical items of a user do not contribute equally in a prediction.\n\nTABLE 3: Attention weights breakdown of a sampled user on target item #1382 in Pinterest. The user has four historical items which are shown in column 1 to 4, and the last column denotes the prediction score (after sigmoid).  \n\n<table><tr><td>Item ID</td><td>#131</td><td>#894</td><td>#1534</td><td>#3157</td><td>σ(ŷui)</td></tr><tr><td>FISM</td><td>0.25</td><td>0.25</td><td>0.25</td><td>0.25</td><td>0.17</td></tr><tr><td>NAIS-prod</td><td>0.03</td><td>0.52</td><td>0.22</td><td>0.23</td><td>0.81</td></tr></table>\n\nSecond, we show a case study on the attention weights of a prediction of a sampled user in Table 3. The weights have been  $L_{1}$  normalized to make a clear comparison with FISM,\n\nwhich assumes a uniform weight on the historical items. In this example, the target item #1382 is a positive example in the testing set and should be scored larger. We can see that FISM weights all historical items (more precisely, their interactions with the target item) uniformly, which leads to a relatively smaller prediction score. In contrast, NAIS-prod assigns a higher weight on item #894 and a lower weight on item #131, successfully scoring the target item #1382 larger, which is desired. To demonstrate the rationality, we further investigate the content of these items (i.e., Pinterest images). We find that both the target item #1382 and the highest attended item #894 are about natural scenery, while the lowest attended item #131 is a family photo. This is as expected, because when predicting a user's preference on a target item, her historical items of the same category should have a larger impact than other less relevant items. This well justifies our motivating example in introduction, providing evidence on the correlation of the attention weights and the characteristics of items.\n\n# 4.2.2 Effect of Pre-training\n\nTABLE 4: Performance of NAIS methods with (w/) and without (w/o) FISM pre-training at embedding size 16.  \n\n<table><tr><td></td><td colspan=\"2\">MovieLens</td><td colspan=\"2\">Pinterest</td></tr><tr><td>Methods</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td></tr><tr><td>FISM</td><td>66.47</td><td>39.49</td><td>87.40</td><td>55.22</td></tr><tr><td>NAIS-concat w/o pre-training</td><td>67.77</td><td>40.41</td><td>87.90</td><td>56.23</td></tr><tr><td>NAIS-concat w/ pre-training</td><td>69.72</td><td>41.96</td><td>88.44</td><td>57.20</td></tr><tr><td>NAIS-prod w/o pre-training</td><td>68.04</td><td>40.55</td><td>87.90</td><td>56.04</td></tr><tr><td>NAIS-prod w/ pre-training</td><td>69.69</td><td>41.94</td><td>88.44</td><td>57.22</td></tr></table>\n\nTo demonstrate the effect of pre-training (i.e., using the embeddings learned by FISM as model initialization), we show the performance of NAIS with and without pretraining at embedding size 16 in Table 4. Note that the hyper-parameters of NAIS without pre-training have been separately tuned. As can be seen, by pre-training the two NAIS methods with FISM embeddings, both methods are improved significantly. Besides the performance improvements, NAIS methods with pre-training have a faster convergence rate than random initialization. This points to the positive effect of using FISM embeddings to initialize NAIS. Moreover, training NAIS from scratch leads to a performance better than FISM, which further verifies the usefulness of the attention network.\n\n# 4.3 Performance Comparison (RQ2)\n\nWe now compare the performance of NAIS with other item recommendation methods. For these embedding-based methods (MF, MLP, FISM, and NAIS), the embedding size controls their modeling capability; as such, we set it to 16 for all methods for a fair comparison. In later Section 4.4 of hyper-parameter study, we vary the embedding size for each method. Table 5 shows the overall recommendation accuracy. We have the following main observations.\n\n- 1. The two NAIS methods achieve the highest NDCG and HR scores on both datasets. They reach the same performance level, achieving significant improvements over other methods  $(p < 10^{-3}$  judged by the one-sample\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/8ec7572f2d846019076466216e2b9f82f8f4137799511a92eacd0749b6c5e7fa.jpg)  \n(a) Epoch 1\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/1d587290dc28105dcc070a0b746806261adacbe1f9b414ced3c622cf536dd03a.jpg)  \n(b) Epoch 10\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/d61f59bc09c9286eb8142162794322f036fd6c79f838b25c721047af8a9a2770.jpg)  \n(c) Epoch 20\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/569a6ff1b98fbf5ec83cb39a9bcc8bb84874be50db78e4c7b41f9ddd6ee7e7c3.jpg)  \n(d) Epoch 40\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/2171fd140784052952ab70e9cafc72d50a53183ded02bf1092e2deb8c7d1a259.jpg)  \nFig. 4: The scatter plot of mean (x-axis) and variance (y-axis) of attention weights learned by NAIS-prod at different epochs. Each scatter point denotes the prediction of a testing point in Pinterest.  \n(a) MovieLens - HR\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/edfa6b9f1c73e09a63f8c0e6fc906e9c3c568dabdb779ec7304450dd89b837c3.jpg)  \n(b) MovieLens - NDCG\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/27c0785bd1892419726a90d650ca8f2d3268fd2b47e73e62c3c7e95e42320d85.jpg)  \n(c) Pinterest - HR\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/ebe21af00375af4f0acf711f5103456f4b67c8362789bce609707eca8a1a4d25.jpg)  \n(d) Pinterest — NDCG\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/5392090061e7d75d50bdab4251b8cb4f615084950673fce72a86387682714710.jpg)  \nFig. 5: Testing performance of NAIS methods w.r.t. the attention factor  $a$ .  \n(a) MovieLens - HR\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/0c20983204283c0e769e7d649ecb967c5d1be81e3707bf077ffb4441541ea65d.jpg)  \n(b) MovieLens - NDCG  \nFig. 6: Testing performance of NAIS methods w.r.t. the smoothing exponent  $\\beta$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/d252c5401c2473dc50391351606fc5fc5e99c24426be4e1679505b8361259bc0.jpg)  \n(c) Pinterest - HR\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/6ed3aed2-3120-4507-adc3-2f8a1ae2bd1b/db25bc92863990b07fd46da9445d01c9439a3714b958d9b7e071ca6bc35a5bf4.jpg)  \n(d) Pinterest - NDCG\n\nTABLE 5: Recommendation accuracy scores (%) of compared methods at embedding size 16.  \n\n<table><tr><td></td><td colspan=\"2\">MovieLens</td><td colspan=\"2\">Pinterest</td></tr><tr><td>Methods</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td></tr><tr><td>Pop</td><td>45.36</td><td>25.43</td><td>27.39</td><td>14.09</td></tr><tr><td>ItemKNN</td><td>62.27</td><td>35.87</td><td>78.57</td><td>48.32</td></tr><tr><td>MF-BPR</td><td>66.64</td><td>39.73</td><td>86.90</td><td>54.01</td></tr><tr><td>MF-eALS</td><td>67.88</td><td>39.83</td><td>87.13</td><td>52.55</td></tr><tr><td>MLP</td><td>68.41</td><td>41.03</td><td>86.48</td><td>53.85</td></tr><tr><td>FISM</td><td>66.47</td><td>39.49</td><td>87.40</td><td>55.22</td></tr><tr><td>NAIS-concat</td><td>69.72</td><td>41.96</td><td>88.44</td><td>57.20</td></tr><tr><td>NAIS-prod</td><td>69.69</td><td>41.94</td><td>88.44</td><td>57.22</td></tr></table>\n\npaired t-test). We believe the benefits are credited to the effective design of the attention networks in learning item-to-item interactions.\n\n- 2. Learning-based CF approaches perform better than heuristic-based approaches Pop and ItemKNN. In particular, FISM outperforms its counterpart ItemKNN with about  $10\\%$  relative improvements. Considering that both methods use the same prediction model while differ in the way of estimating item similarities, we can clearly\n\nsee the positive effect of tailored optimization for recommendation.\n\n- 3. Among the baselines, there is no obvious winner between user-based CF models (MF, MLP) and item-based CF model (FISM). Specifically, on MovieLens user-based models perform better than FISM, while on Pinterest FISM outperforms user-based models. Since user interactions of the Pinterest data are more sparse, it reveals that item-based CF might be more advantageous for sparse datasets, which is in consistent with the finding in previous work [1].\n\nIt is worth pointing out that the performance of NAIS reported in Table 5 uses the default settings of hyperparameters (reported in Section 4.1). Further improvements can be observed by tuning hyper-parameters, which will be explored in the next subsection.\n\n# 4.4 Hyper-parameter Study (RQ3)\n\nBy introducing an attention network, NAIS has two additional hyper-parameters — the hidden layer size of the attention network (aka. the attention factor  $a$ ) and the smooth-\n\nTABLE 6: Recommendation accuracy scores (%) of embedding-based methods at embedding size 8, 32, and 64. The best performance of each setting is highlighted as bold font.  \n\n<table><tr><td></td><td colspan=\"4\">Embedding size = 8</td><td colspan=\"4\">Embedding size = 32</td><td colspan=\"4\">Embedding size = 64</td></tr><tr><td></td><td colspan=\"2\">MovieLens</td><td colspan=\"2\">Pinterest</td><td colspan=\"2\">MovieLens</td><td colspan=\"2\">Pinterest</td><td colspan=\"2\">MovieLens</td><td colspan=\"2\">Pinterest</td></tr><tr><td>Methods</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td><td>HR</td><td>NDCG</td></tr><tr><td>MF-BPR</td><td>62.86</td><td>36.08</td><td>85.85</td><td>53.26</td><td>68.54</td><td>41.14</td><td>86.34</td><td>54.54</td><td>68.97</td><td>41.91</td><td>85.8</td><td>54.58</td></tr><tr><td>MF-eALS</td><td>62.8</td><td>36.35</td><td>86.26</td><td>51.86</td><td>70.4</td><td>42.16</td><td>86.75</td><td>53.84</td><td>70.35</td><td>43.5</td><td>85.77</td><td>53.77</td></tr><tr><td>MLP</td><td>67.1</td><td>39.98</td><td>85.9</td><td>53.67</td><td>69.24</td><td>42.51</td><td>86.77</td><td>54.2</td><td>70.18</td><td>42.64</td><td>86.9</td><td>54.5</td></tr><tr><td>FISM</td><td>61.71</td><td>35.73</td><td>87.03</td><td>54.82</td><td>69.29</td><td>41.71</td><td>88.43</td><td>57.13</td><td>70.17</td><td>42.82</td><td>88.62</td><td>57.18</td></tr><tr><td>NAIS-concat</td><td>64.17</td><td>37.36</td><td>87.44</td><td>55.27</td><td>70.83</td><td>43.36</td><td>88.56</td><td>57.47</td><td>71.66</td><td>44.15</td><td>88.74</td><td>57.75</td></tr><tr><td>NAIS-prod</td><td>64.5</td><td>37.6</td><td>87.88</td><td>55.75</td><td>70.91</td><td>43.39</td><td>88.67</td><td>57.59</td><td>71.82</td><td>44.18</td><td>88.84</td><td>57.9</td></tr></table>\n\ning exponent  $\\beta$ . In addition, as an embedding-based model, the embedding size is another crucial hyper-parameter for NAIS. This subsection investigates the impact of the three hyper-parameters.\n\nTable 6 shows the performance of embedding-based methods at embedding size 8, 32, and 64. We can see that the performance trends are generally in consistent with the observations at embedding size 16 (elaborated in Section 4.3). Our NAIS methods achieve the best performance in most cases, with the only exception of embedding size 8, where MLP performs the best. This is because when the embedding size is small, linear models are limited by the small embedding size, while non-linear models are easy to express stronger representation ability than linear models.\n\nFigure 5 shows the performance of NAIS w.r.t. attention factor. We can see that regardless of the setting of attention factor, both NAIS methods outperform FISM. Among the two methods, NAIS-prod performs better than NAIS-concat for small attention factors, demonstrating the positive effect of using  $\\mathbf{p}_i\\odot \\mathbf{q}_j$  of as input to the attention network for learning the weight of  $\\mathbf{p}_i^T\\mathbf{q}_j$ . Moreover, using a large attention factor for NAIS-concat can compensate the performance gap between NAIS-prod. This implies the utility of using an expressive model for learning the attention weights.\n\nFigure 6 shows the performance of NAIS w.r.t.  $\\beta$ . It is clear that when  $\\beta$  is smaller than 1, both NAIS methods show good performance and outperform FISM. However, when  $\\beta$  is set to 1, the performances of NAIS degrade significantly and are worse than FISM. Note that setting  $\\beta$  to 1 means using softmax to normalize the attention weights, a standard setting for neural attention networks [19], [15], [16]. Unfortunately, such a standard setting does not work well for CF datasets. We believe the reason is caused by the large variance of the length of user histories. Specifically, on MovieLens and Pinterest, the (mean, variance) of user history's length are (166,37145) and (27,57), respectively. Such a large variance on the number of attentive components seldom happens in NLP and CV tasks that deal with sentences (i.e., attention on words) and images (i.e., attention on regions). This is a key insight of this work for employing attention networks on user behavior data, which to our knowledge has never been investigated before.\n",
  "hyperparameter": "Embedding size k: [8, 16, 32, 64], with 16 as default; Attention factor a: set equal to embedding size k (default 16); Smoothing exponent β: range [0, 1], default 0.5 (β=1 recovers standard softmax but performs poorly); Learning rate: 0.01 with Adagrad optimizer; Negative sampling ratio: 4 negative instances per positive instance; Regularization λ: 0 (no regularization needed in default setting); Pre-training: initialize with FISM embeddings for faster convergence and better performance"
}