{
  "id": "NeuMF_2017",
  "paper_title": "Neural Collaborative Filtering (NeuMF variant)",
  "alias": "NeuMF",
  "year": 2017,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "Neural Collaborative Filtering (NCF) replaces matrix factorization's fixed inner product with learnable neural architectures to model user-item interactions. The NeuMF variant fuses Generalized Matrix Factorization (GMF) for linear interactions and Multi-Layer Perceptron (MLP) for non-linear interactions using separate embeddings, combining their strengths through concatenation before the output layer. The framework uses binary cross-entropy loss with negative sampling to optimize implicit feedback recommendation, and employs pre-training of GMF and MLP components to initialize NeuMF for better convergence.",
  "introduction": "# Introduction\nIn the era of information explosion, recommender systems play a pivotal role in alleviating information overload, being widely adopted by E-commerce, online news, and social media platforms. The core of personalized recommendation is collaborative filtering, which models users’ preferences based on past interactions (e.g., ratings, clicks). Among collaborative filtering techniques, Matrix Factorization (MF) is the most popular, projecting users and items into a shared latent space and modeling interactions via inner product of latent vectors.\n\nDespite MF’s success, its performance is limited by the simple inner product interaction function. The inner product linearly combines latent feature products, failing to capture complex user-item interaction structures. While some deep learning works have been applied to recommendation, they mostly model auxiliary information (e.g., item text, music audio features) and still rely on MF’s inner product for core collaborative filtering interactions.\n\nThis work focuses on implicit feedback (e.g., purchases, clicks), which is easier to collect but noisier and lacks negative feedback. We propose a general framework called Neural Collaborative Filtering (NCF) that uses neural networks to learn arbitrary interaction functions from data, generalizing MF. Key contributions are:\n1. A neural network architecture for modeling user and item latent features, forming the NCF framework for collaborative filtering.\n2. Showing MF is a specialization of NCF, and leveraging Multi-Layer Perceptron (MLP) to introduce non-linearity into NCF.\n3. Extensive experiments on real-world datasets demonstrating NCF’s superiority over state-of-the-art methods, verifying deep learning’s potential for collaborative filtering.",
  "method": "# Method\n## 1. Preliminaries\n### 1.1 Problem Definition\n- Let \\( M \\) (users) and \\( N \\) (items) denote the number of users and items.\n- User-item interaction matrix \\( Y \\in \\mathbb{R}^{M×N} \\), where \\( y_{ui}=1 \\) if user \\( u \\) interacted with item \\( i \\), and \\( 0 \\) otherwise.\n- Goal: Learn a prediction function \\( \\hat{y}_{ui}=f(u, i|\\Theta) \\) to estimate unobserved interactions for item ranking.\n\n### 1.2 Limitation of Matrix Factorization (MF)\nMF models interactions as inner product of latent vectors \\( p_u \\) (user) and \\( q_i \\) (item):\n\\[ \\hat{y}_{ui}=p_u^T q_i = \\sum_{k=1}^K p_{uk} q_{ik} \\]\nThe fixed linear interaction function struggles to capture complex user-item relationships, especially in sparse data.\n\n## 2. Neural Collaborative Filtering (NCF) Framework\nNCF adopts a multi-layer structure to model user-item interactions, consisting of input layer, embedding layer, neural CF layers, and output layer.\n- **Input Layer**: One-hot encoded user/item ID vectors (sparse).\n- **Embedding Layer**: Projects sparse input to dense latent vectors \\( p_u \\) (user) and \\( q_i \\) (item).\n- **Neural CF Layers**: Learns complex interaction functions via customizable layers (e.g., MLP).\n- **Output Layer**: Predicts interaction score \\( \\hat{y}_{ui} \\in [0,1] \\) using sigmoid activation.\n\n### 2.1 Objective Function\nTreat recommendation as binary classification (implicit feedback is binary), optimizing binary cross-entropy loss:\n\\[ L = -\\sum_{(u,i) \\in \\mathcal{Y} \\cup \\mathcal{Y}^-} y_{ui} \\log \\hat{y}_{ui} + (1-y_{ui}) \\log (1-\\hat{y}_{ui}) \\]\nwhere \\( \\mathcal{Y} \\) is observed interactions, \\( \\mathcal{Y}^- \\) is sampled negative instances from unobserved interactions.\n\n## 3. NCF Instantiations\n### 3.1 Generalized Matrix Factorization (GMF)\nNCF’s linear variant, recovering and generalizing MF:\n- Interaction function: Element-wise product of latent vectors \\( p_u \\odot q_i \\).\n- Output: \\( \\hat{y}_{ui} = \\sigma(h^T (p_u \\odot q_i)) \\), where \\( h \\) is learnable weight vector, \\( \\sigma \\) is sigmoid.\n\n### 3.2 Multi-Layer Perceptron (MLP)\nIntroduces non-linearity via MLP to model complex interactions:\n- Input: Concatenated latent vectors \\( [p_u; q_i] \\).\n- Layers: Stacked ReLU-activated hidden layers (tower structure, halving dimension per layer).\n- Output: \\( \\hat{y}_{ui} = \\sigma(h^T \\phi_L(...\\phi_2([p_u; q_i])...)) \\), where \\( \\phi_x \\) is the x-th hidden layer mapping.\n\n### 3.3 Neural Matrix Factorization (NeuMF)\nFuses GMF (linear) and MLP (non-linear) to leverage both strengths:\n- Separate embeddings for GMF (\\( p_u^G, q_i^G \\)) and MLP (\\( p_u^M, q_i^M \\)).\n- Interaction functions: \\( \\phi^{GMF}=p_u^G \\odot q_i^G \\) (GMF), \\( \\phi^{MLP} \\) (MLP’s last hidden layer).\n- Output: \\( \\hat{y}_{ui} = \\sigma(h^T [\\phi^{GMF}; \\phi^{MLP}]) \\).\n- Pre-training: Initializes NeuMF with pre-trained GMF and MLP parameters for better convergence.\n\n## 4. Training Details\n- Optimization: Adam (for GMF/MLP pre-training), SGD (for NeuMF fine-tuning).\n- Negative sampling: 4 negative instances per positive instance (optimal ratio 3–6).\n- Initialization: Gaussian distribution (mean=0, std=0.01) for model parameters.",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nTwo real-world datasets (filtered to retain users with ≥20 interactions):\n| Dataset    | #Interactions | #Items | #Users | Sparsity |\n|------------|---------------|--------|--------|----------|\n| MovieLens  | 1,000,209     | 3,706  | 6,040  | 95.53%   |\n| Pinterest  | 1,500,809     | 9,916  | 55,187 | 99.73%   |\n- MovieLens: Explicit ratings converted to implicit feedback (1=rated, 0=unrated).\n- Pinterest: Implicit feedback (1=pinned, 0=unpinned).\n\n### 1.2 Baselines\n- **ItemPop**: Non-personalized, ranks items by popularity.\n- **ItemKNN**: Standard item-based collaborative filtering (adapted for implicit data).\n- **BPR**: Pairwise-ranked MF, optimized for implicit feedback.\n- **eALS**: State-of-the-art MF with non-uniform weighting of missing data.\n\n### 1.3 Evaluation Protocol\n- **Leave-one-out split**: Latest user interaction as test item, rest as training data.\n- **Negative sampling**: 100 non-interacted items sampled per test item for ranking.\n- **Metrics**: Hit Ratio (HR@10) and Normalized Discounted Cumulative Gain (NDCG@10).\n- **Validation**: Randomly sampled one interaction per user as validation data for hyperparameter tuning.\n\n### 1.4 Parameter Settings\n- Embedding/latent dimension: 8, 16, 32, 64 (optimal varies by model).\n- MLP layers: 3 hidden layers (tower structure).\n- Batch size: 256; learning rate: 0.001 (Adam), 0.01 (SGD).\n- Negative sampling ratio: 4 (per positive instance).\n\n## 2. Main Results\n### 2.1 Performance Comparison (RQ1)\n- **NeuMF** achieves the best performance on both datasets, outperforming state-of-the-art eALS and BPR by 4.5% (average) in HR@10 and NDCG@10.\n- GMF outperforms BPR consistently, verifying the effectiveness of cross-entropy loss for implicit feedback.\n- MLP (3 layers) performs slightly worse than GMF but improves with more layers, confirming non-linearity’s value.\n- All NCF variants outperform ItemKNN and ItemPop, highlighting personalized modeling’s importance.\n\n### 2.2 Utility of Pre-training (NeuMF)\n- NeuMF with pre-trained GMF/MLP parameters outperforms random initialization (2.2% relative improvement on MovieLens, 1.1% on Pinterest).\n- Pre-training mitigates non-convex objective’s local optimum issue, accelerating convergence.\n\n### 2.3 Cross-Entropy Loss & Negative Sampling (RQ2)\n- Training loss decreases with iterations, and recommendation performance improves (saturates after 10 iterations).\n- Negative sampling ratio impacts performance: 3–6 negatives per positive is optimal; excessive sampling degrades performance.\n- GMF with 4 negatives outperforms BPR (1 negative per positive), demonstrating pointwise loss’s flexibility.\n\n### 2.4 Deep Learning Effectiveness (RQ3)\n- MLP performance improves with more hidden layers (e.g., MLP-4 > MLP-3 > MLP-2 > MLP-1).\n- MLP-0 (no hidden layers, direct embedding projection) performs worse than ItemPop, verifying the need for non-linear layers to model interactions.\n- ReLU activation outperforms sigmoid and tanh, avoiding saturation and encouraging sparsity.\n\n## 3. Key Observations\n1. NCF’s neural network-based interaction function outperforms MF’s fixed inner product.\n2. Fusing linear (GMF) and non-linear (MLP) interactions (NeuMF) maximizes expressive power.\n3. Cross-entropy loss is more suitable for implicit feedback than squared loss, paired with flexible negative sampling.\n4. Deeper neural networks enhance recommendation performance by capturing complex interaction patterns.",
  "hyperparameter": "Embedding dimension: {8, 16, 32, 64} (optimal varies by model and dataset); MLP architecture: 3 hidden layers with tower structure (halving dimension per layer); Batch size: 256; Learning rate: 0.001 for Adam optimizer (GMF/MLP pre-training), 0.01 for SGD (NeuMF fine-tuning); Negative sampling ratio: 4 negatives per positive instance (optimal range 3-6); Activation function: ReLU for MLP hidden layers, sigmoid for output layer; Initialization: Gaussian distribution with mean=0 and std=0.01; Training iterations: ~10 epochs until saturation"
}