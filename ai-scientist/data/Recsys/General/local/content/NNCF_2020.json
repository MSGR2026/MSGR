{
  "id": "NNCF_2020",
  "paper_title": "A Neural Collaborative Filtering Model with Interaction-based Neighborhood",
  "alias": "NNCF",
  "year": 2020,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "",
  "introduction": "# 1 INTRODUCTION\n\nDue to the explosive growth of information, recommender systems have become increasingly important in various online services. There are two mainstream approaches to recommender systems, namely memory-based and model-based approaches. As the most typical model-based recommendation approach, Matrix Factorization (MF) represents users and items in a shared latent space, and reconstructs the user-item interaction using their latent vectors. A major problem of MF is that it adopts a simple linear factorization, which may not be sufficient to model the complex user-item interactions. To overcome this limitation, some pioneering studies apply deep learning techniques to recommender systems, including neural rating prediction [7], auto-encoder based recommender [9] and neural collaborative filtering [3]. These works mainly utilize deep learning techniques as a powerful data model, in which complex user-item interactions and auxiliary information can be modeled. However, they also inherit a noteworthy weakness from previous\n\nlatent factor models which directly factorize user-item interactions (e.g., MF): it is poor at identifying strong associations among a small set of closely related items [4], especially when data is highly sparse.\n\nBased on this consideration, the aim of this paper is to retain the capacity of neural network models in learning an arbitrary user-item interaction function, and meanwhile enhance its ability to leverage localized information in complementing the interaction data. To fulfill this purpose, we have to address two challenging issues: (1) what kind of localized information should be used and (2) how to model such localized information in a deep learning recommendation model. For the first issue, previous methods typically utilize the neighborhood information, e.g., the classic ItemKNN algorithms. The neural network model [3] simultaneously captures the two-way information of user-item interaction, while the neighbors derived from previous  $K$  Nearest Neighbors (KNN) algorithms are based on single-way relation, either user-user or item-item relation. Such neighborhood information may not be in the best form to fit into the interaction-based neural model. For solving the first issue, we propose to identify the two-way neighbors based on the interaction network. Unlike KNN algorithms, we characterize the neighborhood information based on user-item interactions. We construct the neighborhood by using a community-based algorithm, which selects direct or community neighbors in a localized group of both users and items. For solving the second issue, we set an integration component by incorporating both interaction and neighborhood information. By combining the two parts, our model jointly characterizes both interaction and neighborhood information in a unified neural network model.\n\nOur contributions are summarized as follows: (1) We propose a model which can integrate neighborhood information into neural collaborative filtering for recommendation; (2) We propose to construct an interaction network which directly employs the interaction information to obtain the interaction-based neighborhood. (3) Extensive experiments on three real-world datasets demonstrate the effectiveness of our model for the implicit recommendation task.\n",
  "method": "# 3 OUR NEIGHBORHOOD-BASED NEURAL COLLABORATIVE FILTERING MODEL\n\nPreviously, deep neural model has been utilized to characterize user-item interactions [3]. However, it cannot utilize the neighborhood information, which has been shown to be effective to improve the recommendation performance. In this paper, we present a unified Neighborhood-based Neural Collaborative Filtering model (NNCF). To give a global picture of our model, we present an illustrative figure\n\nfor the proposed model in Fig. 1. We first introduce the integration component, which describes how to encode the user-item interaction pair together with the neighborhood information, and then introduce the prediction component based on Multi-Layer Perceptron (MLP).\n\n# 3.1 Modeling Interaction and Neighborhood Information\n\nAs the input, our model takes in two kinds of information, namely user-item interaction and the neighborhood information. Now, we describe how to encode these information in the integration component.\n\nEncoding the user-item pair. Formally, given a user-item pair  $\\langle u,i\\rangle$ , we encode the involved user  $u$  and item  $i$  using the one-hot representation, i.e.,  $\\mathbf{x}_u\\in \\mathbb{R}^{|\\mathcal{U}|\\times 1}$  and  $\\mathbf{y}_i\\in \\mathbb{R}^{|\\mathcal{I}|\\times 1}$ . In the one-hot vector  $\\mathbf{x}_u$  (or  $\\mathbf{y}_i$ ), only the  $u$ -th entry (or the  $i$ -th entry) is equal to 1. Similar to matrix factorization, two parameter matrices  $\\mathbf{P}\\in \\mathbb{R}^{K_1\\times |\\mathcal{U}|}$  and  $\\mathbf{Q}\\in \\mathbb{R}^{K_1\\times |\\mathcal{I}|}$  consist of the latent factors for users and items respectively. By applying a lookup layer, the one-hot user or item vector will be transformed into a latent vector as below\n\n$$\n\\mathbf {p} _ {u} = \\mathbf {P} ^ {\\top} \\cdot \\mathbf {x} _ {u}, \\quad \\mathbf {q} _ {i} = \\mathbf {Q} ^ {\\top} \\cdot \\mathbf {y} _ {i}, \\tag {1}\n$$\n\nTo effectively capture the overall structure of user-item interaction, we follow the generalized MF method proposed in [3], and define the interaction function  $\\phi (\\mathbf{p}_u,\\mathbf{q}_i)$  as follows:\n\n$$\n\\mathbf {v} _ {u, i} = \\phi (\\mathbf {p} _ {u}, \\mathbf {q} _ {i}) = \\mathbf {p} _ {u} \\odot \\mathbf {q} _ {i}, \\tag {2}\n$$\n\nwhere “ $\\odot$ ” denotes the element-wise product of vectors. The interaction function  $\\phi(\\mathbf{p}_u, \\mathbf{q}_i)$  applies a linear kernel to model the latent features interactions.\n\nEncoding the neighborhood information. To encode the neighborhood information, we first code the neighbors using the one-hot representation. For a user  $u$ , given the neighborhood set  $\\mathcal{I}^{(u)}$  consisting of item vertices, we represent it using a  $|\\mathcal{I}|$ -dimensional one-hot representation, denoted by  $\\mathbf{n}_u \\in \\mathbb{R}^{|I| \\times 1}$ , only the entries corresponding to  $u$ 's item neighbors will be set to 1. Similarly, the neighborhood set  $\\mathcal{U}^{(i)}$  of an item  $i$  is represented as one-hot vector  $\\mathbf{n}_i \\in \\mathbb{R}^{|U| \\times 1}$ . Then we apply a concatenation-based lookup layer to transform both one-hot vectors into latent vectors\n\n$$\n\\mathbf {p} _ {u} ^ {(N)} = \\text {C O N C A T - L O O K U P} \\left(\\mathbf {P} ^ {\\prime} ^ {\\top}, \\mathbf {n} _ {u}\\right), \\tag {3}\n$$\n\n$$\n\\mathbf {q} _ {i} ^ {(N)} = \\text {C O N C A T - L O O K U P} \\left(\\mathbf {Q} ^ {\\prime} ^ {\\top}, \\mathbf {n} _ {i}\\right), \\tag {4}\n$$\n\nwhere  $\\mathbf{P}' \\in \\mathbb{R}^{K_2 \\times |\\mathcal{I}|}$  and  $\\mathbf{Q}' \\in \\mathbb{R}^{K_2 \\times |\\mathcal{U}|}$  are the transformation matrices for lookup (similar to  $\\mathbf{P}$  and  $\\mathbf{Q}$ ). CONCAT-LOOKUP(\\cdot) is a function which first performs the lookup operation to obtain the corresponding embeddings for the input one-hot representation vector, and then concatenates the multiple embeddings into a single vector as the output. The numbers of neighbors for different vertices are usually varying. To utilize such information, we have to transform the varying-length vector into a fixed-length one. We adopt the convolution operations on the latent vectors for neighbors, i.e.,  $\\mathbf{p}_u^{(N)}$  and  $\\mathbf{q}_i^{(N)}$ . A convolution operation involves a filter which is applied to a fixed window to produce a new feature. Each possible\n\nwindow on the vector space produces a feature map. In order to retain the most important information, we further apply a max-pooling operation on the generated feature maps from convolution. Formally, we obtain the representations for the neighborhood information as follows\n\n$$\n\\mathbf {v} _ {u} ^ {(N)} = \\mathrm {M P} \\left(\\operatorname {C O N V} \\left(\\mathbf {p} _ {u} ^ {(N)}\\right)\\right), \\tag {5}\n$$\n\n$$\n\\mathbf {v} _ {i} ^ {(N)} = \\operatorname {M P} \\left(\\operatorname {C O N V} \\left(\\mathbf {q} _ {i} ^ {(N)}\\right)\\right), \\tag {6}\n$$\n\nwhere  $\\mathbf{v}_u^{(N)}\\in \\mathbb{R}^{K_3\\times 1}$  and  $\\mathbf{v}_i^{(N)}\\in \\mathbb{R}^{K_3\\times 1}$  denote the latent vectors for the neighborhood of user  $u$  and item  $i$  respectively, and  $\\mathrm{MP}(\\cdot)$  and  $\\mathrm{CONV}(\\cdot)$ . denote the max-pooling and convolution operations respectively.\n\nIntegrating interaction with neighborhood information. Once we have obtained the representations for the user-item pair and its neighborhood information, we further integrate these latent vectors into a unified representation as below\n\n$$\n\\widetilde {\\mathbf {v}} _ {u, i} = \\mathbf {v} _ {u, i} \\oplus \\mathbf {v} _ {u} ^ {(N)} \\oplus \\mathbf {v} _ {i} ^ {(N)}, \\tag {7}\n$$\n\nwhere “ $\\oplus$ ” denotes the vector concatenation operation,  $\\mathbf{v}_{u,i}$  (defined in Eq. 2) denotes the latent vector for the user-item pair  $\\langle u,i\\rangle$ ,  $\\mathbf{v}_u^{(N)}$  (defined in Eq. 5) denotes the latent vector for the neighborhood of user  $u$  and  $\\mathbf{v}_i^{(N)}$  (defined in Eq. 6) denotes the latent vector for the neighborhood of item  $i$ .\n\n# 3.2 MLP-based Prediction\n\nThe interaction between a user and an item can be very complex. Previous approaches usually assume a linear relation by decomposing the user-item matrix, e.g., standard matrix factorization. We would like to endow our model a higher level of flexibility and nonlinearity to better characterize the interaction with the incorporation of neighborhood information. Hence, we propose to apply the MLP to model the user-item interactions. Generally, a MLP component can be constructed layer by layer. For  $j = 1,\\dots,L$ , we can have\n\n$$\n\\mathbf {z} _ {j} = f ^ {(j)} \\left(\\mathbf {z} _ {j - 1}\\right), \\tag {8}\n$$\n\n$$\n\\hat {p} _ {u, i} = \\sigma \\left(\\mathbf {w} ^ {\\top} \\cdot \\mathbf {z} _ {L}\\right), \\tag {9}\n$$\n\nwhere  $f^{(j)}(\\cdot)$  is the non-linear activation function for the  $j$ -th layer. We choose Rectifier Linear Unit (ReLU) as the activation function, which performs the best in our experiments. To feed the MLP, we set  $\\mathbf{z}_0 = \\widetilde{\\mathbf{v}}_{u,i}$  (defined in Eq. 7). For the output layer,  $\\mathbf{w}$  is the weights and  $\\sigma(\\cdot)$  is the sigmoid function defined as  $\\sigma(x) = \\frac{1}{1 + \\exp(-x)} \\cdot \\hat{p}_{u,i}$  is the conditional probability of the interaction label being 1.\n\nThe loss function for optimization. Given a training set  $\\mathcal{T} = \\{< u,i,y_{u,i} > \\}$ , we adopt the cross-entropy loss as the optimization objective\n\n$$\n\\mathcal {L} = - \\sum_ {\\left. <   u, i, y _ {u, i} > \\in \\tau \\right.} \\left(y _ {u, i} \\cdot \\log \\hat {p} _ {u, i} + \\left(1 - y _ {u, i}\\right) \\cdot \\log \\left(1 - \\hat {p} _ {u, i}\\right)\\right), \\tag {10}\n$$\n\nwhere  $\\hat{p}_{u,i}$  is the conditional probability defined in Eq. 9. Note that our task is implicit recommendation, and we do not have explicit\n\nTable 1: Statistics of the evaluation datasets.  \n\n<table><tr><td>Datasets</td><td>#Interaction</td><td>#Users</td><td>#Items</td><td>Sparsity</td></tr><tr><td>Delicious</td><td>437,593</td><td>1,867</td><td>69,223</td><td>99.66%</td></tr><tr><td>MovieLens</td><td>1,000,209</td><td>3,706</td><td>6,040</td><td>95.53%</td></tr><tr><td>Rossmann</td><td>1,017,209</td><td>4,086</td><td>1,115</td><td>77.67%</td></tr></table>\n\nnegative cases in training datasets. Following [3], we randomly sample four items that a user does not interact with as negative cases. To optimize the parameters for our model, we adopt the Stochastic Gradient Descent (SGD). The loss function in Eq. 10 is optimized by performing mini-batch Adam with a batch size of 1024, and the learning rate is set to 0.001. We implement our models in PYTHON using the library KERAS. We report the detailed parameter settings of our model below. The embedding size is set to 32 in the input layer, the number of kernels is set to 128 with a kernel size of 5 in the convolutional layer, and the embedding size is set to 128 in the last hidden layer. For all the nearest-neighbor methods, we select at most 50 neighbors for efficiency consideration, i.e.,  $K = 50$ .\n\nOur NNCF retains the capacity of MF to effectively estimate the overall structure of user-item interaction, also utilizes neighborhood information to detect strong associations among a small set of closely related users and items. Moreover, the multi-layer perceptron endows our model with a high level of nonlinearities which can learn an arbitrary interaction function from data.\n",
  "experiments": "# 4 EXPERIMENTS\n\nDataset. We experiment with three real-world datasets from different applications, namely Delicious [1], MovieLens [2], and Rossman [6], which have been commonly adopted in recommendation tasks. The statistics of the three datasets are summarized in Table 1.\n\nEvaluation metrics. We adopt the widely used leave-one-out method to perform the evaluation for recommendation [3, 4]. We randomly sample 100 negative items that a user has not interacted with and further combine the golden item (i.e., the one that the user has interacted with) and the negative items into a randomly shuffled list. A comparison method will rank the list and return the top  $k$  ones as recommendations. Following [3], we adopt Hit ratio at rank  $k$  ( $\\mathrm{HR} @ k$ ) and Normalized Discounted Cumulative Gain at rank  $k$  ( $\\mathrm{NDCG} @ k$ ) to evaluate the performance of a ranked list.\n\nMethods to compare. We consider the following baselines for performance comparisons. (1) ItemPop: It ranks the items according to their popularity measured by the number of interactions [6]; (2) ItemKNN: It makes recommendations according to the similarities of a candidate item to the past items [5]; (3) BPR: It optimizes the MF model with a pairwise ranking loss [6]; (4) NeuMF: It is the recently proposed neural network model for item recommendation [3].\n\nIt is noteworthy that our NNCF model itself is flexible to integrate neighborhood derived from other methods. Here, we consider three variants with different neighborhood construction algorithms, namely  $\\mathbf{NNCF}_{knn}$ ,  $\\mathbf{NNCF}_{direct}$ , and  $\\mathbf{NNCF}_{community}$  (See Section 2).\n\nResults and analysis. We present the results of  $\\mathrm{HR@k}$  and NDCG@k on the recommendation performance in Table 2. We only report the\n\nTable 2: Performance comparisons of different methods on the recommendation task.  \n\n<table><tr><td>Datasets</td><td colspan=\"4\">Delicious</td><td colspan=\"4\">MovieLens</td><td colspan=\"4\">Rossmann</td></tr><tr><td>Models</td><td>HR@5</td><td>HR@10</td><td>NDCG@5</td><td>NDCG@10</td><td>HR@5</td><td>HR@10</td><td>NDCG@5</td><td>NDCG@10</td><td>HR@5</td><td>HR@10</td><td>NDCG@5</td><td>NDCG@10</td></tr><tr><td>ItemPop</td><td>0.0541</td><td>0.1044</td><td>0.0322</td><td>0.0483</td><td>0.3149</td><td>0.4503</td><td>0.2018</td><td>0.2524</td><td>0.0011</td><td>0.0027</td><td>0.0003</td><td>0.0008</td></tr><tr><td>ItemKNN</td><td>0.5969</td><td>0.6869</td><td>0.5590</td><td>0.6881</td><td>0.4501</td><td>0.6252</td><td>0.3014</td><td>0.3570</td><td>0.5836</td><td>0.6509</td><td>0.5150</td><td>0.5866</td></tr><tr><td>BPR</td><td>0.7377</td><td>0.7871</td><td>0.7411</td><td>0.8172</td><td>0.5103</td><td>0.6875</td><td>0.3621</td><td>0.4213</td><td>0.6175</td><td>0.6870</td><td>0.5640</td><td>0.6039</td></tr><tr><td>NeuMF</td><td>0.8553</td><td>0.8628</td><td>0.8068</td><td>0.8243</td><td>0.5655</td><td>0.7322</td><td>0.3830</td><td>0.4507</td><td>0.8496</td><td>0.9326</td><td>0.6566</td><td>0.6837</td></tr><tr><td>NNCFknn</td><td>0.8478</td><td>0.8681</td><td>0.8305</td><td>0.8337</td><td>0.6059</td><td>0.7421</td><td>0.4121</td><td>0.4750</td><td>0.8857</td><td>0.9450</td><td>0.7438</td><td>0.7652</td></tr><tr><td>NNCFdirect</td><td>0.8597</td><td>0.8725</td><td>0.8323</td><td>0.8365</td><td>0.6146</td><td>0.7423</td><td>0.4160</td><td>0.4762</td><td>0.8757</td><td>0.9444</td><td>0.7390</td><td>0.7614</td></tr><tr><td>NNCFcommunity</td><td>0.8731</td><td>0.8832</td><td>0.8458</td><td>0.8490</td><td>0.6200</td><td>0.7441</td><td>0.4221</td><td>0.4766</td><td>0.8815</td><td>0.9503</td><td>0.7465</td><td>0.7691</td></tr></table>\n\nresults at top positions, i.e.,  $k = 5$  and  $k = 10$ . We can have the following observations: (1) ItemPop is the weakest baseline, since it is a non-personalized method. ItemKNN and BPR perform better than ItemPop substantially. NeuMF is the best baseline, which represents the state-of-art performance on the recommendation task in the literature. (2) Our NNCF models perform better than all the baselines consistently on the three datasets, especially with the metrics of NDCG@k. This obversion demonstrates our proposed model is more capable of generating high-quality recommendations at very top positions. (3) By comparing the three NNCF variants, we can observe that the variant NNCF<sub>community</sub> gives the best performance in almost all the cases except HR@5 on the Rossmann dataset. The results indicate that the community-based algorithm is likely to generate better neighborhood for recommendation.\n\nEffect of the number of communities and hidden layers. We examine the effect of different communities and hidden layers on the performance for our model. We only report the tuning results on the Delicious dataset. All the findings are consistent on the other two datasets and are omitted due to the space limitation. We vary the partition parameter  $\\alpha$  in  $\\{0.3, 0.4, 0.5, 0.6, 0.7\\}$ , which corresponds to the number of communities in  $\\{353, 290, 250, 231, 204\\}$ . We vary hidden layer  $L$  from 0 to 5 with a step of 1. As shown in Fig. 2, we can see that (1) when  $\\alpha = 0.5$ , NNCF achieves the best performance, which indicates that the number of communities should be set neither too small nor too large; and (2) NNCF achieves the best performance with four hidden layers, which is substantially better than that without hidden layers by incorporating nonlinear transformation. However, the optimal number of hidden layers should depend on the amount of training data and specific tasks, and using too many hidden layers might hinder the system performance.\n",
  "hyperparameter": ""
}