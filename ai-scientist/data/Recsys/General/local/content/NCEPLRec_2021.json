{
  "id": "NCEPLRec_2021",
  "paper_title": "Noise-Contrastive Estimation for Personalized Label Propagation in Recommendation",
  "alias": "NCEPLRec",
  "year": 2021,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "NCE-PLRec applies Noise-Contrastive Estimation (NCE) to derive improved item embeddings for Personalized Label Propagation Recommendation (PLRec). Instead of explicitly treating unobserved interactions as negative feedback, NCE learns to discriminate between observed interactions and popularity-biased noise samples. The method derives optimal dot products d*_{i,j} = log(Σ|r_{·,j'}|/|r_{·,j}|) for observed interactions, which downweights popular items before performing SVD (NCE-SVD), then applies linear regression on top of these embeddings (NCE-PLRec). This \"depopularization\" approach addresses popularity bias while maintaining scalability through closed-form solutions.",
  "introduction": "# 1 INTRODUCTION\n\nIn an era of virtually unlimited choices, recommender systems are necessary to assist users in finding items they may like. Collaborative filtering (CF) is the de-facto standard approach for making such personalized recommendations based on automated collection of item interaction data from a population of users [23]. However, in many cases, these interactions lack explicit negative signals, e.g., clicks on a website or purchases of a book. In these cases, a lack of interaction should not be construed as implicitly negative; indeed, it could simply be that a user was unaware of the item's existence. This recommendation setting where only positive (and typically very sparse) interactions are observed is known as the One Class Collaborative Filtering (OC-CF) problem [20].\n\nOne approach to tackle OC-CF is to factorize a large sparse implicit matrix into smaller latent matrices of user and item representations [10, 20]. However, matrix factorization requires optimizing a non-convex objective, resulting in local optima and the need for substantial hyperparameter tuning for good practical performance [13]. An alternative scalable solution is to first reduce the dimensionality of the matrix, then learn the importance of different latent projected features using linear regression. Methods such as [24], which we refer to as Projected Linear Recommendation (PLRec) precompute the item embeddings through fast randomized Singular Value Decomposition (SVD) [6] and train separate linear regression models for each user on top of these embeddings. This separation enables parallelization across users and reduces the optimization to a convex objective that is globally optimized in closed-form [14]. However, naive SVD embedding methods exhibit a popularity bias that skews their ability to accurately embed less popular items [21].\n\nIn this paper, we propose a novel projected linear recommendation algorithm called Noise Contrastive Estimation PLRec (NCE-PLRec). Instead of explicitly treating unobserved interactions as negative feedback, we leverage insights from the NCE framework [5] that attempt to discriminate between observed interactions and a noise model; NCE has been previously used extensively in high-quality word embeddings for natural language [15, 18]. Specifically, we first transform the implicit matrix into a de-popularized matrix that optimally re-weights the interactions in closed-form according to the NCE objective. Then we extract item embeddings by projecting items onto the principal components of this de-popularized matrix obtained via SVD. We can then leverage the standard PLRec\n\nframework with these NCE item embeddings in the novel and highly scalable NCE-PLRec method for OC-CF problems.\n\nAn analysis of recommendation popularity distribution demonstrates that NCE-PLRec uniformly distributes its recommendations over the popularity spectrum while other methods exhibit distinct biases towards specific popularity subranges. Overall, our results show that NCE-PLRec is an order of magnitude faster and highly competitive with existing state-of-the-art OC-CF methods.\n",
  "method": "# 3 NCE-PLREC\n\nPLRec stands out as one of the most scalable OC-CF methods (as our runtime results later verify). However, the SVD item embeddings used by PLRec suffer from popularity bias that can significantly affect performance [21]. In this work we leverage ideas from Noise-Contrastive Estimation (NCE) to derive improved item embeddings\n\nfor PLRec. We begin by revisiting recommendation from a probabilistic perspective. Specifically, we fit a model parameterized by the user and item embeddings to maximize the probability of observed feedback. Instead of explicitly treating unobserved interactions as negative feedback, our NCE approach learns to discriminate between observed interactions and unobserved noise.\n\n## 3.1 Noise Contrastive Estimation in Recommendation\n\nNoise-Contrastive Estimation (NCE) [5] learns to discriminate between the observed data and some artificially generated noise. Given a set of observations  \\(X = \\{\\mathbf{x}_1\\cdots \\mathbf{x}_n\\}\\)  and artificially generated noise  \\(Y = \\{\\mathbf{y}_1\\cdots \\mathbf{y}_n\\}\\) , NCE aims to separate observations from noise:\n\n\\[\n\\sum_ {j} \\log (g (\\mathbf {x} _ {j}; \\theta)) + \\log (1 - g (\\mathbf {y} _ {j}; \\theta)), \\tag {2}\n\\]\n\nwhere  \\(g(\\cdot)\\)  is a (possibly unnormalized) probability density function, and  \\(\\theta\\)  are model parameters to estimate. NCE is normally used to estimate the observation probability where the partition function is hard to estimate due to either computational cost or lack of observed negative samples. In implicit feedback recommendation tasks, we only explicitly observe positive observations for each user, which makes NCE an ideal tool to estimate user preferences without explicitly assuming unobserved interactions are negative samples as done in most OC-CF methods. The simple idea driving NCE is that it adversarially trains to maximize prediction probability of the observed user preferences while minimizing the prediction probability of negative samples drawn from a (usually) popularity-biased noise distribution.\n\nTo apply NCE to OC-CF we first note that in the probabilistic formulation the objective of recommendation is to train a model that maximizes the probability of observed interactions  \\(p(r_{i,j} = 1|i,j)\\) . Motivated by the log-odds ratio derived from a Bernoulli Distribution [2], we define this positive interaction probability as the following logistic sigmoid function:\n\n\\[\np (r _ {i, j} = 1 | i, j) = \\sigma (\\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j}) = \\frac {1}{1 + e ^ {- \\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j}}}. \\tag {3}\n\\]\n\nSince the negative feedback is not observed in OC-CF, we could artificially generate negative samples by sampling from some item distribution  \\(q\\) . A natural choice for  \\(q\\)  is the popularity distribution  \\(q(j) = |\\mathbf{r}_{\\cdot,j}| / \\sum_{k}^{n} |\\mathbf{r}_{\\cdot,k}|\\) . Popular items are more likely to be encountered by the user so the absence of an interaction with these items is more likely to be indicative of negative feedback. This leads to the following NCE objective for each user  \\(i\\) , where the goal is to maximize the probability of observed interactions and minimize it for sampled \"noise\" items:\n\n\\[\n\\underset {\\mathbf {u} _ {i}, V} {\\operatorname {a r g m a x}} \\sum_ {j} r _ {i, j} \\left[ \\log \\sigma \\left(\\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j}\\right) + \\log \\sigma \\left(- \\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j ^ {\\prime}}\\right) \\right], \\tag {4}\n\\]\n\nwhere we sample a noise item  \\(j'\\)  according to  \\(q(j')\\)  for each observed user interaction  \\(j\\) . Here we remark that (a) the  \\(r_{i,j}\\)  indicators restrict us to only contrast observed positive interactions with the noise distribution and that (b) observing the logistic sigmoid identity  \\(1 - \\sigma(\\mathbf{u}_i^T \\mathbf{v}_{j'}') = \\sigma(-\\mathbf{u}_i^T \\mathbf{v}_{j'})\\)  allows us to see the equivalence with the general form of NCE in (2).\n\nIf we take the expectation of the above objective w.r.t. distribution  \\(q(j')\\) , we see that we can rewrite the objective in the following expected form by exploiting linearity of expectation:\n\n\\[\n\\underset {\\mathbf {u} _ {i}, V} {\\operatorname {a r g m a x}} \\sum_ {j} r _ {i, j} \\left[ \\log \\sigma \\left(\\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j}\\right) + E _ {q \\left(j ^ {\\prime}\\right)} \\left[ \\log \\sigma \\left(- \\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j ^ {\\prime}}\\right) \\right] \\right]. \\tag {5}\n\\]\n\nIn the multi-user collaborative filtering setting, the full objective function  \\(\\ell\\)  corresponds to a summation over each independent user, where the item embeddings are shared by all users:\n\n\\[\n\\underbrace {\\underset {U , V} {\\operatorname {a r g m a x}} \\sum_ {i} \\sum_ {j} r _ {i , j} \\left[ \\log \\sigma \\left(\\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j}\\right) + E _ {q \\left(j ^ {\\prime}\\right)} \\left[ \\log \\sigma \\left(- \\mathbf {u} _ {i} ^ {T} \\mathbf {v} _ {j ^ {\\prime}}\\right) \\right] \\right]} _ {\\ell}. \\tag {6}\n\\]\n\nGlobally optimizing (6) with respect to both user and item representations in closed-form is intractable due to the shared item embeddings and the nonlinear relationship between user and item embeddings. Therefore, we optimize (6) with respect to the dot product  \\(d_{i,j} = \\mathbf{u}_i^T\\mathbf{v}_j\\)  directly to simplify the objective into a convex optimization problem. Such a procedure is often referred to as lifting in the optimization literature; once we solve for  \\(d_{i,j}\\) , we will then be able to recover a suitable  \\(\\mathbf{u}_i\\)  and  \\(\\mathbf{v}_j\\) . Solving for the optimal  \\(d_{i,j}\\)  for positive observations  \\((r_{i,j} = 1)\\) , we obtain the following:\n\n\\[\n\\frac {\\partial \\ell}{\\partial d _ {i , j}} = \\sigma (- d _ {i, j}) - \\frac {\\left| r _ {i , j} \\right|}{\\sum_ {j ^ {\\prime}} \\left| r _ {i , j ^ {\\prime}} \\right|} \\sigma \\left(d _ {i, j}\\right) \\tag {7}\n\\]\n\n\\[\nd _ {i, j} ^ {*} = \\log \\frac {\\sum_ {j ^ {\\prime}} \\left| r _ {; , j ^ {\\prime}} \\right|}{\\left| r _ {; , j} \\right|} \\quad \\forall r _ {i, j} = 1. \\tag {8}\n\\]\n\nFor the unobserved interactions, the optimal solution is simply zero:\n\n\\[\nd _ {i, j} ^ {*} = 0 \\quad \\forall r _ {i, j} = 0. \\tag {9}\n\\]\n\nThe resulting sparse matrix  \\(D\\)  maintains the same number of non-zero entries and shape from the original implicit matrix. The difference is that the entries are now replaced with the optimal inner product of user and item representations,  \\(D^{*} = U^{*}V^{*T}\\) .\n\nFinally, we recover all  \\(\\mathbf{u}_i\\)  and  \\(\\mathbf{v}_j\\)  embeddings by projecting the sparse  \\(D^{*}\\)  using truncated SVD [6] as it exploits sparsity in the matrix. Then\n\n\\[\nU ^ {*} \\approx U _ {D} \\Sigma_ {D} ^ {\\frac {1}{2}} \\quad V ^ {*} \\approx V _ {D} \\Sigma_ {D} ^ {\\frac {1}{2}}, \\tag {10}\n\\]\n\nwhere  \\(U_{D},V_{D}\\)  and  \\(\\Sigma_D\\)  come from the truncated SVD  \\(D^{*}\\approx U_{D}\\Sigma_{D}V_{D}^{T}\\)\n\nThis concludes our NCE-based derivation of user and item embeddings, where we see that in contrast to the standard SVD method of embeddings, NCE provides a slight variation we call NCE-SVD. That is, instead of taking the SVD of the implicit feedback matrix  \\(R\\)  directly, the NCE embedding instead takes the SVD of the  \\(D\\)  matrix as defined in (8) and (9), which is a logarithmic and popularity renormalized version of  \\(R\\) . In short, a visual inspection of  \\(D\\)  indicates that prior to performing the SVD, NCE suggests that items  \\(j\\)  with more positive observations – i.e., a larger denominator in (8) – should be downweighted prior to deriving the SVD-based user and item embeddings. In this sense, we can view the NCE-SVD embedding as a \"depopularized\" approach that places more emphasis on accurate SVD reconstruction of less popular items.\n\n## 3.2 Relation to the Neural Word Embeddings\n\nNoise Contrastive Estimation was first brought to the attention of the Machine Learning community from the literature on word embeddings [7, 15, 17, 18]. However, to the best of our knowledge, it has not been widely applied in the recommendation setting even though the implicit negative problem of word embeddings is precisely the same as the implicit negative problem in OC-CF.\n\nConceptually, our proposed objective is similar to word embeddings such as Word2Vec [18] (that can be interpreted as a special case of NCE), where we analogize users as word contexts and items as words. One key difference from word embeddings is that we assume the users (contexts) are unique and that the user interactions (words) are independent with a uniform discrete distribution.\n\n### 3.3 An NCE Item Embedding Hyperparameter\n\nThe optimal solution of NCE as shown in Equation (8) penalizes the influence of popular items on the user and item representation. In other words, it is inversely proportional to the popularity of an observed item. However, this relies heavily on a good estimate of the popularity of observed items  \\(p(j')\\) . Since the data is sparse, there is high uncertainty on the popularity estimate and this uncertainty propagates to Equation (8).\n\nTo alleviate this, we introduce a hyperparameter  \\(\\beta\\)  into the denominator, which adjusts the penalty on high frequency items. We rewrite Equation (8) to include  \\(\\beta\\)  as follows:\n\n\\[\nd _ {i, j} = \\max  \\left(\\log \\sum_ {j ^ {\\prime}} \\left| r _ {., j ^ {\\prime}} \\right| - \\beta \\log \\left| \\mathbf {r} _ {., j} \\right|, 0\\right) \\quad \\forall r _ {i, j} = 1, \\tag {11}\n\\]\n\nwhere we add a  \\(d_{i,j} \\geq 0\\)  constraint to guarantee the positive feedback is more significant compared to the unobserved feedback in Equation (9). Empirically, this hyperparameter aids generalization on the test set as shown in Figure 4.\n\n## 3.4 NCE Projected Linear Recommendation\n\nUsing optimal user  \\(U^{*}\\)  and item  \\(V^{*}\\)  embeddings from Equation (10), we can predict unobserved interactions with a simple dot product  \\(U^{*}V^{*T}\\) . Hence, the simple method of NCE-SVD can be used as a recommendation algorithm by itself.\n\nAs noted previously, NCE-SVD effectively de-popularizes the dataset by rescaling entries in  \\(R\\)  inversely proportional to their popularity. However, popularity bias can still be important in terms of ranking performance depending on the dataset [3]. Therefore, following the approach of PLRec, we further perform the Linear Regression of (1) on top of NCE-SVD for the model to learn the importance of different latent features for each user. It also has the side benefit of correcting for approximation error in fast randomized truncated methods used for large-scale SVD [6]. We call this final solution, NCE-PLRec, as it performs NCE followed by PLRec [25].\n\nWe note that the static latent representation of NCE-SVD is unable to capture non-uniform weighting of users and items, such as done for drifting user preferences [12] or to improve OC-CF matrix factorization [10]. For example, in the latter case of [10], the authors propose to use a user-item weighting matrix  \\(C\\)  of the same dimensions  \\(R\\)  with elements  \\(c_{i,j}\\)  defined as follows:\n\n\\[\nc _ {i, j} = 1 + \\alpha r _ {i, j}, \\tag {12}\n\\]\n\nwhere hyperparameter  \\(\\alpha \\geq -1\\)  modulates the loss weighting differential of positive and negative examples (with  \\(\\alpha = 0\\)  leading to all preferences being weighted equally). We will support this form of  \\(C\\)  in the following user-item loss weighted extension of basic NCE-PLRec.\n\nTo leverage the user-item weights of  \\(C\\)  in NCE-PLRec, we project the original implicit matrix  \\(R\\)  onto the learned item representation  \\(V^{*}\\) . This projection produces the user representation,  \\(Q = RV^{*}\\) , which is the sum over all item representations of the user's interaction history. Then, we maximize a user-item reweighted version of the PLRec objective as follows:\n\n\\[\n\\underset {W} {\\operatorname {a r g m i n}} \\sum_ {i, j} c _ {i, j} \\left(r _ {i, j} - \\mathbf {q} _ {i} \\mathbf {w} _ {j}\\right) ^ {2} + \\lambda \\left\\| \\mathbf {w} _ {j} \\right\\| _ {2} ^ {2}, \\tag {13}\n\\]\n\nwhere  \\(\\mathbf{q}_i\\)  is the user representation,  \\(c_{i,j}\\)  is described previously, and  \\(W\\)  is the linear regression coefficient matrix to estimate. This leads to the most general form of NCE-PLRec that we will define in this paper; if specific user-item weights are not needed, one can simply set  \\(\\alpha = 0\\) .\n\n## 3.5 NCE-PLRec for Cold-Start Test Users\n\nReferring to Equation (13), the trained weights  \\(\\mathbf{w}_j\\)  for each item  \\(j\\)  are shared and trained over all training users. Given a cold-start test user  \\(i'\\) , whose ratings  \\(\\mathbf{r}_{i'}\\)  were not observed during training, we can recommend the top-K items from the projection of  \\(\\mathbf{r}_{i'}\\)  onto the item features and weights learned by NCE-PLRec on the training users. Specifically, a vector of NCE-PLRec predictions for cold-start test user  \\(i'\\)  is given by  \\(\\mathbf{r}_{i'} V^* W^T = \\mathbf{q}_{i'} W^T\\) .\n\n## 3.6 Algorithm\n\nWe summarize the general user-item weighted Noise-Contrastive Estimation Projected Linear Recommender (NCE-PLRec) required to optimize Equation (13) in Algorithm 1.\n\n<table><tr><td colspan=\"3\">Algorithm 1 NCE-PLRec</td></tr><tr><td colspan=\"3\">1: procedure TRAIN(R,α=0,β=1)</td></tr><tr><td>2: D*← NCE(R,β)</td><td colspan=\"2\">▷ Construct D matrix</td></tr><tr><td colspan=\"3\">3: U_D,Σ_D, V_D^T← Truncated SVD(D*)</td></tr><tr><td>4: Q← RV_D Σ_D^1/2</td><td colspan=\"2\">▷ Project implicit matrix</td></tr><tr><td>5: for i∈range(1,m) do</td><td colspan=\"2\">▷ Loop over users</td></tr><tr><td colspan=\"3\">6: C^j← diag(1+αr,_j)</td></tr><tr><td colspan=\"3\">7: w_j← (QT^C^jQ + λI)^{-1}QT^Cr,_j</td></tr><tr><td>8: return QWT</td><td colspan=\"2\">▷ Prediction</td></tr></table>\n\nWith user-item loss weighting given by  \\(C\\) , the optimization shown in Algorithm 1 appears to be closed-form only with respect to each user. However, when  \\(\\alpha = 0\\) , we note that the NCE-PLRec algorithm has a globally closed-form solution for this special case, i.e.,  \\(W = (Q^T Q + \\lambda I)^{-1} Q^T R\\) .\n\nSince user-item loss weighting for implicit feedback recommendation has already been well-studied in the literature [8-11], we do not explore tuning  \\(\\alpha\\)  in our experiments, but simply define NCEPLRec as in Algorithm 1 for full generality in cases where varying  \\(\\alpha\\)  is desired. Hence, we will set  \\(\\alpha = 0\\)  in all experiments.\n\nTable 1: Summary of datasets used in evaluation.  \n\n<table><tr><td>Dataset</td><td>m</td><td>n</td><td>|r_i,j&gt;θ|</td><td>Sparsity</td></tr><tr><td>MovieLens-20m</td><td>138,493</td><td>27,278</td><td>12,195,566</td><td>3.47 × 10-3</td></tr><tr><td>Netflix Prize</td><td>2,649,430</td><td>17,771</td><td>56,919,190</td><td>1.2 × 10-3</td></tr><tr><td>YahooR1 Data</td><td>1,948,882</td><td>46110</td><td>48,817,561</td><td>5.43 × 10-4</td></tr></table>\n",
  "experiments": "# 4 EXPERIMENTS AND EVALUATION\n\nIn this section, we evaluate the proposed NCE-PLRec model by comparing to a variety of state-of-the-art OC-CF algorithms on three real-world datasets with at least 10 million interactions. We compare across a variety of performance metrics and settings including an evaluation of top- \\(k\\)  recommendation performance, running time performance, and the distribution of recommended item popularity.\n\nWe ran our experiments on a GPU compute cluster with the Slurm workload manager system. Each computation node has one 4-core CPU, 32GB RAM, and one Nvidia Titan XP GPU. Implementation is done with Python 2.7 and includes Tensorflow 1.4 [1]. Code to reproduce all results is available on Github.<sup>1</sup>\n\n## 4.1 Datasets\n\nWe evaluate the candidate algorithms on three publicly available rating datasets: Movielens-20M, Netflix Prize, and Yahoo R1. Each dataset contains more than 10 million interactions. Thus, we are only able to compare with state-of-the-art models that are able to run on these large-scale datasets. For each dataset, we binarize the rating dataset with a rating threshold,  \\(\\vartheta\\) , defined to be the upper half of the range of the ratings. We do this so that the observed interactions correspond to positive feedback. To be specific, the threshold is  \\(\\vartheta > 3\\)  for Movielens-20M and Netflix Prize, and  \\(\\vartheta > 70\\)  for Yahoo R1. Table 1 summarizes the properties of the binarized matrices.\n\nWe split the data into train, validation and test sets based on timestamps given by the dataset (when available) to provide a recommendation evaluation setting closer to production use [27]. For each user, we use the first  \\(50\\%\\)  of data as the train set,  \\(20\\%\\)  data as validation set and  \\(30\\%\\)  data as the test set. For the Yahoo dataset, we split the dataset randomly as it does not contain timestamps.\n\n## 4.2 Evaluation Metrics\n\nWe evaluate the recommendation performance using five metrics: Precision@K, Recall@K, MAP@K, R-Precision, and B-NDCG, where R-Precision is an order insensitive metrics, NDCG is order sensitive, and Precision@K as well as Recall@K are semi-order sensitive due to the K values given.\n\n## 4.3 Candidate Methods\n\nWe compare the proposed algorithm with nine state-of-the-art models from classical matrix factorization to the latest Collaborative Metric Learning approaches. These models all scale to the large size of our datasets.\n\n- POP: Most popular items - not user personalized but an intuitive baseline to test the claims of this paper.\n\nTable 2: Hyper-parameters tuned on the experiments.  \n\n<table><tr><td>name</td><td>Range</td><td>Functionality</td><td>Algorithms affected</td></tr><tr><td>r</td><td>{50, 100, 200, 500}</td><td>Latent Dimension</td><td>PLRec, PureSVD\nWRMF, AutoRec, CML\nNCE-SVD, NCE-PLRec\nBPR, CDAE, VAE-CF</td></tr><tr><td>α</td><td>{-0.5, -0.4···-0.1} ∪ {0, 0.1, 1, 10, 100}</td><td>Loss Weighting</td><td>WRMF, NCE-PLRec</td></tr><tr><td>β</td><td>{0.7, 0.8···1.3}</td><td>Popularity Sensitivity</td><td>NCE-PLRec</td></tr><tr><td>λ</td><td>{1e-4, 1e-3···1e4}</td><td>Regularization</td><td>PLRec, WRMF, AutoRec\nCML, BPR, NCE-PLRec\nCDAE, VAE-CF</td></tr><tr><td>ρ</td><td>{0.1, 0.2···1}</td><td>Corruption</td><td>CDAE, VAE-CF</td></tr></table>\n\n- PureSVD [4]: A similarity based recommendation method that constructs a similarity matrix through SVD decomposition of implicit matrix  \\(R\\) .  \n- WRMF [10]: Weighted Regularized Matrix Factorization.  \n- AutoRec [26]: A neural Autoencoder based recommendation system with one hidden layer and ReLU activation function.  \n- CDAE [29]: Collaborative Denoising Autoencoder, which is specifically optimized for implicit feedback recommendation tasks.  \n- VAE-CF [16]: Variational Autoencoder for Collaborative Filtering - a state-of-the-art deep learning based recommender system.  \n- BPR [22]: Bayesian Personalized Ranking. One of the first recommendation algorithms that explicitly optimizes pairwise rankings.  \n- CML [9]: Collaborative Metric Learning. A state-of-the-art metric learning based recommender system.  \n- PLRec [24]: Also called Linear-Flow. This is the baseline projected linear recommendation approach. This is one ablation of NCE-PLRec.  \n- NCE-SVD: Inner product of SVD-decomposed item and user representation learned from NCE. This is the second ablation of NCE-PLRec without PLRec's learned linear models.  \n- NCE-PLRec: The full version of the proposed model.\n\nWe tune the hyper-parameters for the candidate algorithms by evaluating on the validation datasets through grid search. The candidate parameter ranges are shown in Table 2. The best hyperparameter settings found for each algorithm and domain are listed in Table 3.\n\n## 4.4 Ranking Performance Evaluation\n\nTables 4, 5 and 6 show the general performance comparison between the proposed model with the nine existing methods on all metrics. From the results, we notice the following observations:\n\n(a) In one of the three domains, the proposed NCE-PLRec model outperforms all candidate methods on all metrics in the experiments. In the other two domains, the NCE-PLRec model also shows to be strongly competitive with the state-of-the-art VAE-CF deep learning model.  \n(b) NCE-PLRec as a projected linear model shows a substantial performance improvement compared to PLRec, which indicates the benefit of NCE optimized embeddings.\n\n![](images/5510e82a039cf15e99d2bedc25f5c4d812ab98f696d8a228e4087c3fddba232c.jpg)\n\n![](images/ea1834436c783723ce11c55ecfff84149c52d0439985aaddcbaa00547b0f0e77.jpg)  \n(a) Movielens20m\n\n![](images/47a6d2b33c7a72c4ea91fe35a06f0fb914978958928c502c0969c1c54abd1097.jpg)  \n(b)Netflix  \nFigure 1: Precision-recall curve based on number of items being recommended. Larger area underneath the curve is better. For all three domains, PLRec overlaps with PureSVD. NCE-PLRec performs strongly across the entire precision-recall spectrum.\n\n![](images/eadb540b5a174e7267512e882a60287d7c85dfb3a58bcbb8fc0ff15401346030.jpg)  \n(c) Yahoo\n\nTable 3: Best hyper-parameter setting for each algorithm.  \n\n<table><tr><td>Domain</td><td>Algorithm</td><td>r</td><td>α</td><td>λ</td><td>Iteration*</td><td>ρ</td><td>β</td></tr><tr><td></td><td>PLRec</td><td>50</td><td>1</td><td>10000</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>BPR</td><td>50</td><td>1</td><td>0.00001</td><td>30</td><td>0</td><td>1</td></tr><tr><td></td><td>NCE-SVD</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>NCE-PLRec</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td>Movielens20m</td><td>CML</td><td>100</td><td>1</td><td>0.01</td><td>30</td><td>0</td><td>1</td></tr><tr><td></td><td>PureSVD</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>CDAE</td><td>50</td><td>1</td><td>0.00001</td><td>300</td><td>0.2</td><td>1</td></tr><tr><td></td><td>WRMF</td><td>200</td><td>1</td><td>100</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>VAE-CF</td><td>200</td><td>1</td><td>0.001</td><td>300</td><td>0.5</td><td>1</td></tr><tr><td></td><td>AutoRec</td><td>50</td><td>1</td><td>0.00001</td><td>300</td><td>0</td><td>1</td></tr><tr><td></td><td>PLRec</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>BPR</td><td>50</td><td>1</td><td>0.00001</td><td>30</td><td>0</td><td>1</td></tr><tr><td></td><td>NCE-SVD</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>NCE-PLRec</td><td>100</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1.1</td></tr><tr><td>Netflix</td><td>CML</td><td>50</td><td>1</td><td>0.00001</td><td>30</td><td>0</td><td>1</td></tr><tr><td></td><td>PureSVD</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>CDAE</td><td>50</td><td>1</td><td>0.00001</td><td>300</td><td>0.2</td><td>1</td></tr><tr><td></td><td>WRMF</td><td>200</td><td>10</td><td>1000</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>VAE-CF</td><td>100</td><td>1</td><td>0.0001</td><td>300</td><td>0.5</td><td>1</td></tr><tr><td></td><td>AutoRec</td><td>50</td><td>1</td><td>0.00001</td><td>300</td><td>0</td><td>1</td></tr><tr><td></td><td>PLRec</td><td>50</td><td>1</td><td>10000</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>BPR</td><td>50</td><td>1</td><td>0.00001</td><td>30</td><td>0</td><td>1</td></tr><tr><td></td><td>NCE-SVD</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td>Yahoo</td><td>NCE-PLRec</td><td>50</td><td>1</td><td>1000</td><td>10</td><td>0</td><td>1.2</td></tr><tr><td></td><td>CML</td><td>100</td><td>1</td><td>0.0001</td><td>30</td><td>0</td><td>1</td></tr><tr><td></td><td>PureSVD</td><td>50</td><td>1</td><td>1</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>CDAE</td><td>50</td><td>1</td><td>0.1</td><td>300</td><td>0.2</td><td>1</td></tr><tr><td></td><td>WRMF</td><td>100</td><td>100</td><td>1000</td><td>10</td><td>0</td><td>1</td></tr><tr><td></td><td>VAE-CF</td><td>200</td><td>1</td><td>0.001</td><td>300</td><td>0.5</td><td>1</td></tr><tr><td></td><td>AutoRec</td><td>50</td><td>1</td><td>0.00001</td><td>300</td><td>0</td><td>1</td></tr></table>\n\n* For PureSVD, PLRec, NCE-SVD, and NCE-PLRec, iterations in this table means number of randomized SVD iterations. For WRMF, iteration shows number of alternative close-form optimizations. For AutoRec, BPR, CDAE, and VAE-CF, iteration shows number of epochs that processed over all users.\n\n(c) NCE-SVD alone is not sufficient to provide good performance, indicating the importance of using the PLRec correction to the NCE-SVD embeddings.\n\n(d) WRMF is another strong competitor in terms of general performance. We notice that WRMF outperforms NCE-PLRec when a large number of items are retrieved. This reflects its wide usage as the first stage algorithm in multi-stage recommendation tasks [28], where the first stage retrieves a high recall candidate list to be refined by other models precise at lower ranks in a second stage.  \n(e) CDAE is inconsistent as it performs well on Movielens-20m and Netflix, but performs poorly on Yahoo R1.  \n(f) PLRec and PureSVD show similar performances across all three datasets. This observation supports our theoretical claim that PLRec should learn a near-optimal weight  \\(W \\approx V\\)  that is close to the SVD decomposition given by PureSVD.\n\nFigure 1 provides further detail of the performance through the precision-recall curve with  \\(K \\in [5, 10, 15, 20, 50]\\) . It shows that NCE-PLRec has consistently good performance over the number of items being retrieved. Especially, in the Netflix dataset, NCE-PLRec shows significantly better performance than the other candidates. We also observe that the performance of AutoRec and CDAE drops considerably when the data becomes sparse with large user-item dimensions. We will discuss reasons for this in Section 4.6, which explores the popularity distribution of recommendations.\n\n## 4.5 Performance vs. User Interaction Level\n\nWe now investigate conditions where the proposed algorithm works better compared to the strongest baselines. We categorize users based on the number of interactions they made in the training set into 4 categories. The categories come from the  \\(25\\%\\) ,  \\(50\\%\\) ,  \\(75\\%\\) , and  \\(100\\%\\)  quantiles of the number of training interactions, which indicate how often the user rated items in the training set.\n\nFigure 2 shows comparative results in regard to the four quantiles for MovieLens-20M. In general, NCE-PLRec shows strong performance across all quantiles of user interaction volume. While VAE-CF shows better performance over the user categories with a lower number of ratings, its performance drops considerably with a\n\nTable 4: Results of Movielens-20M dataset with  \\({95}\\%\\)  confidence interval. Hyper-parameters are chosen from the validation set.  \n\n<table><tr><td>model</td><td>R-Precision</td><td>NDCG</td><td>MAP@5</td><td>MAP@50</td><td>Precision@5</td><td>Precision@50</td><td>Recall@5</td><td>Recall@50</td></tr><tr><td>POP</td><td>0.068±0.0005</td><td>0.1194±0.0007</td><td>0.1011±0.0011</td><td>0.0739±0.0005</td><td>0.0945±0.0009</td><td>0.0585±0.0004</td><td>0.0327±0.0004</td><td>0.167±0.0011</td></tr><tr><td>AutoRec</td><td>0.0931±0.0006</td><td>0.1693±0.0009</td><td>0.1388±0.0012</td><td>0.1018±0.0006</td><td>0.1294±0.001</td><td>0.0821±0.0005</td><td>0.0455±0.0005</td><td>0.2442±0.0013</td></tr><tr><td>BPR</td><td>0.0846±0.0006</td><td>0.154±0.0008</td><td>0.1207±0.0011</td><td>0.0927±0.0006</td><td>0.1149±0.001</td><td>0.0763±0.0004</td><td>0.0397±0.0005</td><td>0.2267±0.0012</td></tr><tr><td>CDAE</td><td>0.084±0.0006</td><td>0.1536±0.0008</td><td>0.1259±0.0012</td><td>0.0932±0.0006</td><td>0.1173±0.001</td><td>0.0756±0.0005</td><td>0.0399±0.0005</td><td>0.2219±0.0012</td></tr><tr><td>CML</td><td>0.0906±0.0006</td><td>0.177±0.0008</td><td>0.1246±0.0011</td><td>0.1024±0.0006</td><td>0.1203±0.001</td><td>0.0869±0.0005</td><td>0.04±0.0005</td><td>0.2766±0.0013</td></tr><tr><td>PLRec</td><td>0.0955±0.0006</td><td>0.1785±0.0008</td><td>0.1376±0.0012</td><td>0.1042±0.0006</td><td>0.1288±0.001</td><td>0.0851±0.0004</td><td>0.0452±0.0005</td><td>0.2652±0.0012</td></tr><tr><td>PureSVD</td><td>0.0954±0.0006</td><td>0.1783±0.0008</td><td>0.1375±0.0012</td><td>0.1041±0.0006</td><td>0.1287±0.001</td><td>0.085±0.0004</td><td>0.0451±0.0005</td><td>0.2647±0.0012</td></tr><tr><td>VAE-CF</td><td>0.1008±0.0006</td><td>0.1973±0.0009</td><td>0.1371±0.0012</td><td>0.1075±0.0006</td><td>0.1302±0.001</td><td>0.0899±0.0004</td><td>0.0503±0.0006</td><td>0.3067±0.0014</td></tr><tr><td>WRMF</td><td>0.1±0.0006</td><td>0.1962±0.0008</td><td>0.1433±0.0012</td><td>0.1106±0.0006</td><td>0.1342±0.001</td><td>0.0918±0.0004</td><td>0.0485±0.0005</td><td>0.3021±0.0014</td></tr><tr><td>NCE-PLRec</td><td>0.102±0.0006</td><td>0.1957±0.0009</td><td>0.1456±0.0012</td><td>0.1113±0.0006</td><td>0.137±0.001</td><td>0.0917±0.0005</td><td>0.0498±0.0005</td><td>0.2971±0.0014</td></tr><tr><td>NCE-SVD</td><td>0.0809±0.0005</td><td>0.1638±0.0008</td><td>0.115±0.0011</td><td>0.0911±0.0005</td><td>0.1083±0.0009</td><td>0.0766±0.0004</td><td>0.0379±0.0005</td><td>0.2594±0.0013</td></tr></table>\n\nTable 5: Results of Netflix dataset with  \\({95}\\%\\)  confidence interval. Hyper-parameters are chosen from the validation set.  \n\n<table><tr><td>model</td><td>R-Precision</td><td>NDCG</td><td>MAP@5</td><td>MAP@50</td><td>Precision@5</td><td>Precision@50</td><td>Recall@5</td><td>Recall@50</td></tr><tr><td>POP</td><td>0.0486±0.0002</td><td>0.0853±0.0003</td><td>0.0747±0.0005</td><td>0.065±0.0003</td><td>0.0711±0.0004</td><td>0.0582±0.0002</td><td>0.0179±0.0002</td><td>0.1215±0.0005</td></tr><tr><td>AutoRec</td><td>0.0876±0.0003</td><td>0.1454±0.0004</td><td>0.14±0.0006</td><td>0.1074±0.0003</td><td>0.1324±0.0005</td><td>0.0894±0.0003</td><td>0.0361±0.0002</td><td>0.1958±0.0006</td></tr><tr><td>BPR</td><td>0.0757±0.0002</td><td>0.1312±0.0003</td><td>0.1197±0.0006</td><td>0.096±0.0003</td><td>0.115±0.0005</td><td>0.0816±0.0002</td><td>0.0291±0.0002</td><td>0.1859±0.0006</td></tr><tr><td>CDAE</td><td>0.0797±0.0003</td><td>0.1316±0.0004</td><td>0.1251±0.0006</td><td>0.0979±0.0003</td><td>0.1198±0.0005</td><td>0.0832±0.0002</td><td>0.0323±0.0002</td><td>0.1788±0.0006</td></tr><tr><td>CML</td><td>0.0878±0.0003</td><td>0.1511±0.0004</td><td>0.1398±0.0006</td><td>0.1091±0.0003</td><td>0.1332±0.0005</td><td>0.0906±0.0003</td><td>0.0365±0.0002</td><td>0.2117±0.0006</td></tr><tr><td>PLRec</td><td>0.0994±0.0003</td><td>0.1645±0.0004</td><td>0.1591±0.0007</td><td>0.118±0.0003</td><td>0.149±0.0006</td><td>0.0953±0.0003</td><td>0.0445±0.0003</td><td>0.2189±0.0006</td></tr><tr><td>PureSVD</td><td>0.0994±0.0003</td><td>0.1644±0.0004</td><td>0.159±0.0007</td><td>0.118±0.0003</td><td>0.149±0.0005</td><td>0.0953±0.0003</td><td>0.0445±0.0003</td><td>0.2188±0.0006</td></tr><tr><td>VAE-CF</td><td>0.1017±0.0003</td><td>0.1705±0.0004</td><td>0.1559±0.0007</td><td>0.1176±0.0003</td><td>0.1465±0.0005</td><td>0.0957±0.0003</td><td>0.0467±0.0003</td><td>0.2309±0.0006</td></tr><tr><td>WRMF</td><td>0.0985±0.0003</td><td>0.1681±0.0004</td><td>0.1531±0.0007</td><td>0.117±0.0003</td><td>0.1447±0.0006</td><td>0.096±0.0003</td><td>0.045±0.0003</td><td>0.2325±0.0007</td></tr><tr><td>NCE-PLRec</td><td>0.1049±0.0003</td><td>0.1764±0.0004</td><td>0.1654±0.0007</td><td>0.1247±0.0003</td><td>0.1552±0.0006</td><td>0.1015±0.0003</td><td>0.0477±0.0003</td><td>0.2392±0.0007</td></tr><tr><td>NCE-SVD</td><td>0.0917±0.0003</td><td>0.158±0.0004</td><td>0.1559±0.0007</td><td>0.1129±0.0003</td><td>0.1446±0.0006</td><td>0.0897±0.0002</td><td>0.0422±0.0003</td><td>0.216±0.0006</td></tr></table>\n\nTable 6: Results of Yahoo dataset with  \\(95\\%\\)  confidence interval. Hyper-parameters are chosen from the validation set.  \n\n<table><tr><td>model</td><td>R-Precision</td><td>NDCG</td><td>MAP@5</td><td>MAP@50</td><td>Precision@5</td><td>Precision@50</td><td>Recall@5</td><td>Recall@50</td></tr><tr><td>POP</td><td>0.0795±0.0004</td><td>0.1843±0.0005</td><td>0.1022±0.0006</td><td>0.0672±0.0003</td><td>0.0897±0.0005</td><td>0.051±0.0002</td><td>0.0652±0.0005</td><td>0.3322±0.0009</td></tr><tr><td>AutoRec</td><td>0.1338±0.0005</td><td>0.2711±0.0007</td><td>0.1845±0.0008</td><td>0.1093±0.0004</td><td>0.1625±0.0007</td><td>0.0723±0.0003</td><td>0.1065±0.0006</td><td>0.437±0.001</td></tr><tr><td>BPR</td><td>0.1377±0.0005</td><td>0.288±0.0006</td><td>0.1822±0.0008</td><td>0.1129±0.0004</td><td>0.1637±0.0007</td><td>0.0784±0.0003</td><td>0.1146±0.0006</td><td>0.4883±0.001</td></tr><tr><td>CDAE</td><td>0.0795±0.0004</td><td>0.1843±0.0005</td><td>0.1022±0.0006</td><td>0.0672±0.0003</td><td>0.0897±0.0005</td><td>0.051±0.0002</td><td>0.0652±0.0005</td><td>0,3323±0.0009</td></tr><tr><td>CML</td><td>0.2101±0.0007</td><td>0.4046±0.0007</td><td>0.2754±0.001</td><td>0.162±0.0005</td><td>0.2484±0.0008</td><td>0.1034±0.0003</td><td>0.1795±0.0007</td><td>0.6401±0.0009</td></tr><tr><td>PLRec</td><td>0.2051±0.0007</td><td>0.3673±0.0008</td><td>0.2838±0.001</td><td>0.1496±0.0005</td><td>0.2456±0.0008</td><td>0.0893±0.0003</td><td>0.1779±0.0007</td><td>0.5363±0.001</td></tr><tr><td>PureSVD</td><td>0.205±0.0007</td><td>0.367±0.0008</td><td>0.2836±0.001</td><td>0.1495±0.0005</td><td>0.2455±0.0008</td><td>0.0893±0.0003</td><td>0.1778±0.0007</td><td>0.5358±0.001</td></tr><tr><td>VAE-CF</td><td>0.2701±0.0008</td><td>0.4698±0.0008</td><td>0.3423±0.001</td><td>0.1849±0.0005</td><td>0.3005±0.0009</td><td>0.1102±0.0003</td><td>0.238±0.0008</td><td>0.6824±0.0009</td></tr><tr><td>WRMF</td><td>0.2612±0.0008</td><td>0.4633±0.0008</td><td>0.3244±0.001</td><td>0.1795±0.0005</td><td>0.2866±0.0008</td><td>0.1094±0.0003</td><td>0.2321±0.0008</td><td>0.6881±0.0009</td></tr><tr><td>NCE-PLRec</td><td>0.2562±0.0008</td><td>0.4595±0.0008</td><td>0.344±0.0011</td><td>0.1841±0.0005</td><td>0.2989±0.0009</td><td>0.1098±0.0004</td><td>0.2249±0.0008</td><td>0.6757±0.0009</td></tr><tr><td>NCE-SVD</td><td>0.2332±0.0007</td><td>0.412±0.0008</td><td>0.313±0.001</td><td>0.1624±0.0005</td><td>0.2713±0.0008</td><td>0.0934±0.0003</td><td>0.2129±0.0008</td><td>0.5959±0.001</td></tr></table>\n\nhigher number of ratings. With more than 71 ratings, VAE-CF performs even worse than the simpler AutoRec model. We conjecture that VAE blindly increases the certainty of prediction given more observed user interactions. While it seems reasonable in the case that the user rates items with coherent properties, this assumption does not hold if there are the thousands of items being rated by the user. WRMF as another strong competitor also shows robust performance where we observe very similar performance for all four rating quantiles comparing to the NCE-PLRec. CML is competitive when the number of observed ratings is higher than 71. This is reasonable because CML learns better distance metrics with more observations.\n\n## 4.6 Popularity Distribution\n\nWe analyze the sensitivity of the candidate methods' recommendations on popular items as shown in Figure 3. In general, most of the candidate learning methods show strong personalization (recommendations of less popular items) except AutoRec and CDAE, which tend to recommend popular items. While recommending popular items in a relatively dense and small dataset achieves acceptable performance, the performance drops quickly with increasing data sparsity and dimensionality as shown in Figure 1. BPR does not show strong personalization in this Figure due to the insufficient pairwise sampling given limited training epochs. While we are able to tune the sampling size for a small dataset such as Movielens\n\n![](images/c421ed61426c5e82c82f5a9529fc0fa04ea48eacd0f224bbbb6850e4e69fc49e.jpg)  \nFigure 2: Performance comparison for different quantiles of user activity (number of ratings) for MovieLens-20M. Error bars show standard derivation. All figures share the legend.\n\n![](images/f6772a945e477bfee5ac9ef220d18c8da1b9b36d829ec163d58c76d0f3932919.jpg)  \nFigure 3: Popularity distribution of items recommended by the candidate algorithms for all the users. Higher value in x-axis indicates less personalization. Algorithms are sorted by the median of their popularity distributions respectively.\n\n1m, it is hard to estimate the pairwise sampling size that maintains a good performance on very large datasets considering the stochasticity of performance contributed by the noise. On the other hand, NCE-SVD learns to only recommend unpopular items since the NCE embedding is de-popularized. Impressively, NCE-PLRec spreads its recommendations over the popularity spectrum compared to other algorithms and this proves to be beneficial in terms of its overall ranking performance previously observed in Tables 4, 5 and 6.\n\n![](images/e05a7555f9fa364364c224061de6d6316254b2dc7e93432ef4f9dcd3b4cf99e8.jpg)  \nFigure 4: The effect of tuning the hyper-parameter  \\(\\beta\\)  on the three datasets.\n\n## 4.7 Hyper-parameter Tuning\n\nFigure 4 shows the effects of tuning hyper-parameter  \\(\\beta\\)  for NCEPLRec defined in Equation (11) on NDCG in all datasets (performance on other metrics was similar). A higher value indicates higher popularity. While only moderate in the Movielens-20M and Netflix datasets, we observe a remarkable performance improvement by adjusting the weighting of the noise contrastive term in the Yahoo dataset. This observation corresponds to our conjecture that this adjustment of the level of depopularization is critical for working with extremely sparse recommendation data as found in this dataset.\n\n## 4.8 Training Time and Scalability\n\nFigure 5 shows the total time taken for training the candidate methods on the Netflix dataset. We compare only the training time since the prediction and evaluation step require similar operations for all algorithms and take approximately the same time. The result shows the significant efficiency improvements from the linear models compared to neural network and alternating least squares training.\n\n![](images/d87f1ee62d8eef3d6190331d4eb7f8cfe1ebf22fb3cf93c10eb7f471d60e7a9e.jpg)  \nFigure 5: Training times in seconds of the various methods on Netflix. AutoRec, CDAE, and VAE-CF are neural networks that optimized through gradient descent. BPR and CML requires pairwise sampling to learn the ranking. Alternating optimization of WRMF requires the inversion of large matrices twice per iteration.\n\n![](images/23b056bb782eb4a4202bc59a951552fa78eb96b647650ad453746afda44a3373.jpg)  \nFigure 6: Cold-start comparison histogram for Recall@50 of NCE-PLRec minus Recall@50 for PLRec. Positive values show NCE-PLRec has higher Recall whereas negative shows PLRec has higher Recall. The significant skew of area to the right side of the dotted 0.0 red line indicates that more cold-start users benefitted from NCE-PLRec.\n\nAll PLRec methods including NCE-PLRec easily scale to these very large datasets.\n\n## 4.9 Cold-Start Test Users Case Study\n\nAmong the recommendation methods, PureSVD, PLRec and NCE-PLRec are able to handle cold-start recommendations without leveraging additional side information. Since PLRec and PureSVD behave similarly, we only compare NCE-PLRec to PLRec for our user cold-start case study.\n\nFigure 6 shows a more comprehensive pairwise comparison between NCE-PLRec and PLRec for the cold-start test users evaluation. In this experiment, we randomly remove  \\(5\\%\\)  of the users from the training dataset and use the remaining users for training. Then, we use the trained model to recommend items to the  \\(5\\%\\)  of held-out cold-start test users and evaluate performance. Clearly, most of the\n\nusers received better cold-start recommendations from NCE-PLRec compared to PLRec in terms of Recall@50.\n",
  "hyperparameter": "Latent dimension r ∈ {50, 100, 200, 500} with best values: r=50 for MovieLens-20m, r=100 for Netflix, r=50 for Yahoo; Regularization λ ∈ {1e-4, 1e-3, ..., 1e4} with best values: λ=1 for MovieLens-20m and Netflix, λ=1000 for Yahoo; Popularity sensitivity β ∈ {0.7, 0.8, ..., 1.3} with best values: β=1.0 for MovieLens-20m, β=1.1 for Netflix, β=1.2 for Yahoo; Loss weighting α ∈ {-0.5, -0.4, ..., -0.1, 0, 0.1, 1, 10, 100} with α=0 used in all experiments (uniform weighting); SVD iterations: 10 iterations for randomized truncated SVD"
}