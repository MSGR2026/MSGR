{
  "id": "CDAE_2015",
  "paper_title": "Collaborative Denoising Auto-Encoders for Top-N Recommender Systems",
  "alias": "CDAE",
  "year": 2015,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "CDAE (Collaborative Denoising Auto-Encoder) introduces a one-hidden-layer neural network for top-N recommendation that incorporates user-specific latent vectors into the denoising auto-encoder framework. The key innovations are: (1) adding a user-specific input node with weight vector V_u to capture user-specific preferences beyond item co-occurrence patterns, (2) applying denoising corruption to implicit feedback vectors to improve robustness and generalization, and (3) using non-tied weights between encoder and decoder layers. This architecture generalizes several classic collaborative filtering models (SVD++, LFM, FSM) while achieving superior performance through flexible non-linear mappings and the corruption mechanism.",
  "introduction": "# Introduction\nIn recent years, recommender systems have become widely utilized by businesses across industries. Given a set of users, items, and observed user-item interactions, these systems can recommend other items that the users might like. Personalized recommendation is one of the key applications of machine learning in e-commerce and beyond.\n\nMany recommendation systems use Collaborative Filtering (CF) methods to make recommendations. In production, recommender systems are often evaluated based on the performance of the top-N recommendations, since typically only a few recommendations are shown to the user each time. Thus, top-N recommendation methods are of particular interest.\n\nIn this paper, we present a new model-based collaborative filtering (CF) method for top-N recommendation called Collaborative Denoising Auto-Encoder (CDAE). CDAE assumes that whatever user-item interactions are observed are a corrupted version of the user’s full preference set. The model learns latent representations of corrupted user-item preferences that can best reconstruct the full input. During training, we feed the model a subset of a user’s item set and train the model to recover the whole item set; at prediction time, the model recommends new items to the user given the existing preference set as input. Training on corrupted data effectively recovers co-preference patterns.\n\nLearning from intentionally corrupted input has been widely studied. For instance, Denoising Auto-Encoders train a one-hidden-layer neural network to reconstruct a data point from the latent representation of its partially corrupted version. However, to our best knowledge, no previous work has explored applying the idea to recommender systems.\n\nCDAE generalizes several previously proposed, state-of-the-art collaborative filtering models. But its structure is much more flexible. For instance, it is easy to incorporate nonlinearities into the model to achieve better top-N recommendation results. We investigate the effects of various choices for model components and compare their performance against prior approaches on three real world datasets. Experimental results show that CDAE consistently outperforms state-of-the-art top-N recommendation methods by a significant margin on a number of common evaluation metrics.\n\nOur contributions can be summarized as follows:\n1. We propose a new model CDAE, which formulates the top-N recommendation problem using the Auto-Encoder framework and learns from corrupted inputs. Compared to related methods, CDAE is novel in both model definition and objective function.\n2. We demonstrate that CDAE is a generalization of several state-of-the-art methods but with a more flexible structure.\n3. We conduct thorough experiments studying the impact of the choices of different components in CDAE, and show that CDAE outperforms state-of-the-art methods on three real world datasets.",
  "method": "# Method\n## 1. Problem Definition\nGiven a set of users \\(U=\\{u=1, ..., U\\}\\), a set of items \\(I=\\{i=1, ..., I\\}\\), and a log of the users’ past preferences of items \\(O=(u, i, y_{ui})\\), our goal is to recommend to each user \\(u\\) a list of items that will maximize her/his satisfaction. In many cases, the log contains only implicit feedback where all the \\(y_{ui}\\) are 1; the rest of the triples are assumed missing. We use \\(\\overline{O}\\) to denote the set of unobserved, missing triples, and \\(O'\\) an augmented user-item pairs dataset that includes some data sampled from \\(\\overline{O}\\). Let \\(O_u\\) denote the set of item preferences in the training set for a particular user \\(u\\), and \\(\\overline{O}_u\\) the unobserved preferences of user \\(u\\). Items in \\(\\overline{O}_u\\) are the candidates to be recommended to user \\(u\\).\n\n## 2. Collaborative Denoising Auto-Encoder (CDAE)\n### 2.1 Model Structure\nCDAE is represented as a one-hidden-layer neural network, consisting of three layers: input layer, hidden layer, and output layer.\n- **Input Layer**: Contains \\(I+1\\) nodes. The first \\(I\\) nodes correspond to items (item input nodes), and the last node is a user-specific node (user input node). The input is a corrupted feedback vector \\(\\overline{y}_u\\) generated from \\(p(\\overline{y}_u | y_u)\\), where \\(y_u\\) is the sparse binary feedback vector of user \\(u\\) ( \\(y_{ui}=1\\) if \\(i \\in O_u\\), else 0).\n- **Hidden Layer**: Has \\(K\\) nodes (predefined latent dimension) and an additional bias node. The weight matrix between item input nodes and hidden layer is \\(W \\in \\mathbb{R}^{I×K}\\), and the weight vector for the user input node is \\(V_u \\in \\mathbb{R}^K\\) (user-specific).\n- **Output Layer**: Contains \\(I\\) nodes representing reconstructions of the input vector \\(y_u\\). The weight matrix between hidden layer and output layer is \\(W' \\in \\mathbb{R}^{I×K}\\), and the bias vector is \\(b' \\in \\mathbb{R}^I\\).\n\n### 2.2 Forward Propagation\n1. **Latent Representation Calculation**: The hidden layer output \\(z_u\\) is computed as:\n\\[ z_u = h\\left(W^{\\top} \\overline{y}_u + V_u + b\\right) \\]\nwhere \\(h(\\cdot)\\) is an element-wise mapping function (e.g., identity, sigmoid, tanh), and \\(b \\in \\mathbb{R}^K\\) is the offset vector.\n2. **Reconstruction Output**: The output layer value \\(\\hat{y}_{ui}\\) (predicted preference of user \\(u\\) for item \\(i\\)) is:\n\\[ \\hat{y}_{ui} = f\\left(W_{i}^{\\prime\\top} z_u + b_i'\\right) \\]\nwhere \\(f(\\cdot)\\) is a mapping function (e.g., identity, sigmoid).\n\n### 2.3 Model Training\nThe objective is to minimize the average reconstruction error with L2 regularization:\n\\[ \\underset{W, W', V, b, b'}{arg min } \\frac{1}{U} \\sum_{u=1}^{U} \\mathbb{E}_{p\\left(\\overline{y}_u | y_u\\right)}\\left[\\ell\\left(\\overline{y}_u, \\hat{y}_u\\right)\\right] + \\frac{\\lambda}{2}\\left(\\| W\\| _{2}^{2}+\\left\\| W'\\right\\| _{2}^{2}+\\| V\\| _{2}^{2}+\\| b\\| _{2}^{2}+\\left\\| b'\\right\\| _{2}^{2}\\right) \\]\nwhere \\(\\ell(\\cdot)\\) is a loss function (e.g., square loss, logistic loss), and \\(\\lambda\\) is the regularization coefficient.\n\nTraining is performed using Stochastic Gradient Descent (SGD) with AdaGrad for adaptive step size. To reduce complexity, negative sampling is used: for each user \\(u\\), sample a subset of negative items \\(S_u \\subset \\overline{O}_u\\) and compute gradients only on \\(O_u \\cup S_u\\).\n\n### 2.4 Recommendation\nAt prediction time, input the user’s existing preference set (without corruption) into CDAE, and recommend items from \\(\\overline{O}_u\\) with the largest \\(\\hat{y}_{ui}\\) values.\n\n### 2.5 Generalization of Other Models\nCDAE generalizes several classic CF models:\n- **LFSM (SVD++)**: When \\(h(x)\\) and \\(f(x)\\) are identity functions and no input corruption (\\(q=0\\)).\n- **LFM**: When corruption level \\(q=1\\) (all non-zero input values dropped out).\n- **FSM**: When removing the user input node and its weights.",
  "experiments": "# Experiment\n## 1. Datasets and Setup\n### 1.1 Datasets\nWe use three public datasets, processed to implicit feedback (ratings ≥4 stars as \\(y_{ui}=1\\), others missing; users/items with <5 ratings removed):\n| Dataset | #users | #items | #dyads | density(%) |\n|---------|--------|--------|--------|------------|\n| MovieLens 10M (ML) | 69K | 8.8K | 5M | 0.82 |\n| Netflix | 37K | 11K | 4.8M | 1.18 |\n| Yelp | 9.6K | 7K | 243K | 0.36 |\n\n### 1.2 Experimental Setup\n- Split: For each user, 20% of ratings as test set, 80% as training set.\n- Hyperparameter tuning: 5-fold cross-validation on training data.\n- Optimization: SGD with AdaGrad (step size \\(\\eta \\in \\{1, 0.1, 0.01, 0.001\\}\\), \\(\\beta=1\\)).\n- Negative sampling: Number of negative samples = 5 × number of observed ratings per user.\n\n## 2. Evaluation Metrics\n- **Precision@N**: Proportion of relevant items in top-N recommendations.\n- **Recall@N**: Proportion of relevant items retrieved in top-N recommendations.\n- **MAP@N (Mean Average Precision)**: Mean of average precision across all users, considering rank positions of relevant items.\n\n## 3. CDAE Component Analysis\nWe study four CDAE variants (different mapping/loss function combinations) under varying corruption levels (\\(q \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}\\)) and latent dimensions \\(K\\):\n\n| Variant | Hidden Layer | Output Layer | Loss Function |\n|---------|--------------|--------------|---------------|\n| M1 | Identity | Identity | Square |\n| M2 | Identity | Sigmoid | Logistic |\n| M3 | Sigmoid | Identity | Square |\n| M4 | Sigmoid | Sigmoid | Logistic |\n\nKey findings:\n- No single variant dominates all datasets; M4 (sigmoid+sigmoid+logistic) performs well on Yelp and MovieLens.\n- Denoising (non-zero \\(q\\)) improves robustness, especially for linear mapping variants.\n- User-specific vectors (\\(V_u\\)) consistently improve performance compared to DAE (without \\(V_u\\)).\n- Tied weights (\\(W=W'\\)) are not recommended, as non-tied weights (NTW) outperform by at least 10% on most datasets.\n- Performance increases with \\(K\\) up to a point, then plateaus or decreases due to overfitting.\n\n## 4. Baseline Comparison\nCompared with state-of-the-art top-N recommendation methods:\n- **POP**: Recommend items by popularity.\n- **ITEMCF**: Item-based CF with Jaccard similarity (k=50).\n- **MF**: Latent Factor Model with negative sampling (best point-wise loss).\n- **BPR**: Bayesian Personalized Ranking (best pair-wise loss).\n- **FISM**: Factored Item Similarity Model (best loss function).\n\nAll baselines use optimal hyperparameters via cross-validation. Latent dimension \\(K=50\\) for all latent factor models (MF, BPR, FISM, CDAE).\n\n## 5. Main Results\n### 5.1 MAP@N Performance\n- **Yelp**: CDAE outperforms other methods by ≥15% on MAP@10; MF is the second best.\n- **MovieLens**: CDAE achieves the highest MAP@N; BPR and FISM perform slightly better than MF.\n- **Netflix**: CDAE is the only model outperforming ITEMCF on MAP@10 (by ~10%); ITEMCF leads on MAP@1.\n\n### 5.2 Recall@N Performance\n- Results are consistent with MAP@N: CDAE consistently outperforms baselines on all datasets.\n- On Yelp, CDAE’s Recall@10 is ≥15% higher than the second best model (MF).\n- On Netflix, CDAE surpasses ITEMCF on Recall@10, while ITEMCF leads on Recall@1.\n\n### 5.3 Key Observations\n- BPR and FISM do not outperform MF on Yelp and Netflix, possibly due to implicit feedback characteristics and top-N metric alignment.\n- CDAE’s flexibility (non-linear mappings, denoising, user-specific vectors) enables superior performance across datasets.",
  "hyperparameter": "Latent dimension K=50 for all experiments; Corruption level q ∈ {0, 0.2, 0.4, 0.6, 0.8, 1.0} with dataset-specific optimal values (non-zero q generally preferred); Learning rate η ∈ {1, 0.1, 0.01, 0.001} selected via cross-validation with AdaGrad optimizer (β=1); L2 regularization coefficient λ tuned via 5-fold cross-validation; Negative sampling ratio = 5× number of observed ratings per user; Training/test split = 80%/20%; Non-tied weights (W ≠ W') recommended; Best variant is dataset-dependent, with M4 (sigmoid hidden layer + sigmoid output layer + logistic loss) performing well on Yelp and MovieLens"
}