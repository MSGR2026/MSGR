{
  "id": "DGCF_2020",
  "paper_title": "Disentangled Graph Collaborative Filtering",
  "alias": "DGCF",
  "year": 2020,
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "idea": "",
  "introduction": "# 1 INTRODUCTION\n\nPersonalized recommendation has become increasingly prevalent in real-world applications, to help users in discovering items of interest. Hence, the ability to accurately capture user preference is the core. As an effective solution, collaborative filtering (CF), which focuses on historical user-item interactions (e.g., purchases, clicks), presumes that behaviorally similar users are likely to have similar preference on items. Extensive studies on CF-based recommenders have been conducted and achieved great success.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/4816838ee6444ef3a2291ab79be0966d55942c1b415f35f5997d029ff9224d73.jpg)  \nFigure 1: An illustration of diverse user-item relationships at the granularity of latent intents.\n\nLearning informative representations of users and items is of crucial importance to improving CF. To this end, the potentials of deepening user-item relationships become more apparent. Early models like matrix factorization (MF) [28] forgo user-item relationships in the embedding function by individually projecting each user/item ID into a vectorized representation (aka. embedding). Some follow-on studies [11, 16, 19, 23] introduce personal history as the pre-existing feature of a user, and integrate embeddings of historical items to enrich her representation. More recent works [10, 36, 40] further organize all historical interactions as a bipartite user-item graph to integrate the multi-hop neighbors into the representations and achieved the state-of-the-art performance. We attribute such remarkable improvements to the modeling of user-item relationships, evolving from using only a single ID, to personal history, and then holistic interaction graph.\n\nDespite effectiveness, we argue that prior manner of modeling user-item relationships is insufficient to discover disentangled user intents. The key reason is that existing embedding functions fail to differentiate user intents on different items - they either treat\n\na user-item interaction as an isolated data instance [12, 28] or uniformly organize it as an edge in the interaction graph [10, 36, 40] (as shown in the left of Figure 1) to train the neural networks. An underlying fact is omitted that: a user generally has multiple intents to adopt certain items; moreover, different intents could motivate different user behaviors [4, 25, 26, 47]. Taking the right of Figure 1 as an example, user  $u$  watches movie  $i_1$  to pass time, while cares less about whether  $i_1$ 's attributes (e.g., director) match with her interests; on the other hand,  $u$ 's interaction with  $i_2$  may be mainly driven by her special interests on its director. Leaving this fact untouched, previous modeling of user-item relationships is coarse-grained, which has several limitations: 1) Without considering the actual user intents could easily lead to suboptimal representations; 2) As noisy behaviors (e.g., random clicks) commonly exist in a user's interaction history, confounding her intents makes the representations less robust to noisy interactions; and 3) User intents will be obscure and highly entangled in representations, which results in poor interpretability.\n\nHaving realized the vital role of user-item relationships and the limitations of prior embedding functions, we focus on exploring the relationships at a more granular level of user intents, to disentangle these factors in the representations. Intuitively, there are multiple intents affecting a user's behavior, such as to pass time, interest matching, or shopping for others like families. We need to learn a distribution over user intents for each user behavior, summarizing the confidence of each intent being the reason why a user adopts an item. Jointly analyzing such distributions of all historical interactions, we can obtain a set of intent-aware interaction graphs, which further distill the signals of user intents. However, this is not trivial due to the following challenges:\n\n- How to explicitly present signals pertinent to each intent in a representation is unclear and remains unexplored;  \n- The quality of disentanglement is influenced by the independence among intents, which requires a tailored modeling.\n\nIn this work, we develop a new model, Disentangled Graph Collaborative Filtering (DGCF), to disentangle representations of users and items at the granularity of user intents. In particular, we first slice each user/item embedding into chunks, coupling each chunk with a latent intent. We then apply an graph disentangling module equipped with neighbor routing [25, 29] and embedding propagation [9, 18, 39, 40] mechanisms. More formally, neighbor routing exploits a node-neighbor affinity to refine the intent-aware graphs, highlighting the importance of influential relationships between users and items. In turn, embedding propagation on such graphs updates a node's intent-aware embedding. By iteratively performing such disentangling operations, we establish a set of intent-aware graphs and chunked representations. Simultaneously, an independence modeling module is introduced to encourage independence of different intents. Specifically, a statistic measure, distance correlation [33, 34], is employed on intent-aware representations. As the end of these steps, we obtain the disentangled representations, as well as explanatory graphs for intents. Empirically, DGCF is able to achieve better performance than the state-of-the-art methods such as NGCF [40], MacridVAE [26], and DisenGCN [25] on three benchmark datasets.\n\nWe further make in-depth analyses on DGCF's disentangled representations w.r.t. disentanglement and interpretability. To be more specific, we find that the discovered intents serve as vitamin to representations - that is, even in small quantities could achieve comparable performance, while the deficiency of any intent would hinder the results severely. Moreover, we use side information (i.e., user reviews) to help interpret what information are being captured in the intent-aware graphs, trying to understand the semantics of intents.\n\nIn a nutshell, this work makes the following main contributions:\n\n- We emphasize the importance of diverse user-item relationships in collaborative filtering, and modeling of such relationships could lead to better representations and interpretability.  \n- We propose a new model DGCF, which considers user-item relationships at the finer granularity of user intents and generates disentangled representations.  \n- We conduct extensive experiments on three benchmark datasets, to demonstrate the advantages of our DGCF on the effectiveness of recommendation, disentanglement of latent user intents, and interpretability of representations.\n",
  "method": "# 3 METHODOLOGY\n\nWe now present disentangled graph collaborative filtering, termed DGCF, which is illustrated in Figure 2. It is composed of two key components to achieve disentanglement: 1) graph disentangling module, which first slices each user/item embedding into chunks, coupling each chunk with an intent, and then incorporates a new neighbor routing mechanism into graph neural network, so as to disentangle interaction graphs and refine intent-aware representations; and 2) independence modeling module, which hires distance correlation as a regularizer to encourage independence of intents. DGCF ultimately yields disentangled representations with intent-aware explanatory graphs.\n\n# 3.1 Graph Disentangling Module\n\nStudies on GNNs [9, 18, 38] have shown that applying embedding-propagation mechanism on graph structure can extract useful information from multi-hop neighbors and enrich representation of the ego node. To be more specific, a node aggregates information from its neighbors and updates its representations. Clearly, the connectivities among nodes provide an explicit channel to guide the information flow. We hence develop a GNN model, termed graph disentangling layer, which incorporates a new neighbor routing mechanism into the embedding propagation, so as to update weights of these graphs. This allows us to differentiate varying importance scores of each user-item connection to refine the interaction graphs, and in turn propagate signals to the intent-aware chunks.\n\n3.1.1 Intent-Aware Embedding Initialization. Distinct from mainstream CF models [12, 19, 28, 40] that parameterize user/item ID as a holistic representation only, we additionally separate the ID embeddings into  $K$  chunks, associating each chunk with a latent intent. More formally, such user embedding is initialized as:\n\n$$\n\\mathbf {u} = \\left(\\mathbf {u} _ {1}, \\mathbf {u} _ {2}, \\dots , \\mathbf {u} _ {K}\\right), \\tag {5}\n$$\n\nwhere  $\\mathbf{u} \\in \\mathbb{R}^d$  is ID embedding to capture intrinsic characteristics of  $u$ ;  $\\mathbf{u}_k \\in \\mathbb{R}^{\\frac{d}{K}}$  is  $u$ 's chunked representation of the  $k$ -th intent. Analogously,  $\\mathbf{i} = (\\mathbf{i}_1, \\mathbf{i}_2, \\dots, \\mathbf{i}_K)$  is established for item  $i$ . Hereafter, we separately adopt random initialization to initialize each chunk representation, to ensure the difference among intents in the beginning of training. It is worth highlighting that, we set the same embedding size (say  $d = 64$ ) with the mainstream CF baselines, instead of doubling model parameters (cf. Section 3.4.1).\n\n3.1.2 Intent-Aware Graph Initialization. We argue that prior works are insufficient to profile rich user intents behind behaviors, since they only utilize one user-item interaction graph [40] or homogeneous rating graphs [36] to exhibit user-item relationships. Hence, we define a set of score matrices  $\\{\\mathbf{S}_k|\\forall k\\in \\{1,\\dots ,K\\} \\}$  for  $K$  latent intents. Focusing on an intent-aware matrix  $\\mathbf{S}_k$ , each entry  $S_{k}(u,i)$  denotes the interaction between user  $u$  and item  $i$ . Furthermore, for each interaction, we can construct a score vector  $\\mathbf{S}(u,i) = (S_1(u,i),\\dots ,S_K(u,i))\\in \\mathbb{R}^K$  over  $K$  latent intents. We uniformly initialize each score vectors as follows:\n\n$$\n\\mathrm {S} (u, i) = (1, \\dots , 1), \\tag {6}\n$$\n\nwhich presumes the equal contributions of intents at the start of modeling. Hence, such score matrix  $\\mathbf{S}_k$  can be seen as the adjacency matrix of intent-aware graph.\n\n3.1.3 Graph Disentangling Layer. Each intent  $k$  now includes a set of chunked representations,  $\\{\\mathbf{u}_k,\\mathbf{i}_k|u\\in \\mathcal{U},i\\in I\\}$ , which specialize its feature space, as well as a specific interaction graph represented by  $S_{k}$ . Within individual intent channels, we aim to distill useful information from high-order connectivity between users and items, going beyond ID embeddings. Towards this end, we devise a new graph disentangling layer, which is equipped with the neighbor routing and embedding propagation mechanisms, with the target of differentiating adaptive roles of each user-item connection when propagating information along it. We define such layer  $g(\\cdot)$  as follows:\n\n$$\n\\mathbf {e} _ {k u} ^ {(1)} = g \\left(\\mathbf {u} _ {k}, \\left\\{\\mathbf {i} _ {k} \\mid i \\in \\mathcal {N} _ {u} \\right\\}\\right), \\tag {7}\n$$\n\nwhere  $\\mathbf{e}_{ku}^{(1)}$  is to collect information that are pertinent to intent  $k$  from  $u$ 's neighbors;  $\\mathcal{N}_u$  is the first-hop neighbors of  $u$  (i.e., the historical items adopted by  $u$ ); and the super-index (1) denotes the first-order neighbors.\n\nIterative Update Rule. Thereafter, as Figure 3 shows, the neighbor routing mechanism is adopted: first, we employ the embedding propagation mechanism to update the intent-aware embeddings,\n\nbased on the intent-aware graphs; then, we in turn utilize the updated embeddings to refine the graphs and output the distributions over intents. In particular, we set  $T$  iterations to achieve such iterative update. In each iteration  $t$ ,  $\\mathbf{S}_k^t$  and  $\\mathbf{u}_k^t$  separately memorize the updated values of adjacency matrix and embeddings, where  $t \\in \\{1, 2, \\dots, T\\}$  and  $T$  is the terminal iteration. It starts by initializing  $\\mathbf{S}_k^0 = \\mathbf{S}_k$  and  $\\mathbf{u}_k^0 = \\mathbf{u}_k$  via Equations (6) and (5).\n\nCross-Intent Embedding Propagation. At iteration  $t$ , for the target interaction  $(u, i)$ , we have the score vector, say  $\\{\\mathbf{S}_k(u, i) | \\forall k \\in \\{1, \\dots, K\\}\\}$ . To obtain its distribution over all intents, we then normalize these coefficients via the softmax function:\n\n$$\n\\tilde {S} _ {k} ^ {t} (u, i) = \\frac {\\exp S _ {k} ^ {t} (u , i)}{\\sum_ {k ^ {\\prime} = 1} ^ {K} \\exp S _ {k ^ {\\prime}} ^ {t} (u , i)}, \\tag {8}\n$$\n\nwhich is capable of illustrating which intents should get more attention to explain each user behavior  $(u, i)$ . As a result, we can get the normalized adjacency matrix  $\\tilde{\\mathbf{S}}_k^t$  for each intent  $k$ . We then perform embedding propagation [9, 18, 38] over individual graphs, such that the information, which are influential to the user intent  $k$ , are encoded into the representations. More formally, the weighted sum aggregator is defined as:\n\n$$\n\\mathbf {u} _ {k} ^ {t} = \\sum_ {i \\in \\mathcal {N} _ {u}} \\mathcal {L} _ {k} ^ {t} (u, i) \\cdot \\mathbf {i} _ {k} ^ {0}, \\tag {9}\n$$\n\nwhere  $\\mathbf{u}_k^t$  is  $u$ 's temporary representation to memorize signals refined from her neighbors  $\\mathcal{N}_u = \\{i|(u,i)\\in \\mathcal{G}\\}$ , after  $t$  iterations;  $\\mathbf{i}_k^0$  is the input representation for historical item  $i$ ; and and  $\\mathcal{L}_k^t (u,i)$  is the Laplacian matrix of  $\\tilde{\\mathbf{S}}_k^t$ , formulated as:\n\n$$\n\\mathcal {L} _ {k} ^ {t} (u, i) = \\frac {\\tilde {S} _ {k} ^ {t} (u , i)}{\\sqrt {D _ {k} ^ {t} (u) \\cdot D _ {k} ^ {t} (i)}}, \\tag {10}\n$$\n\nwhere  $D_k^t(u) = \\sum_{i' \\in \\mathcal{N}_u} \\tilde{S}_k^t(u, i')$  and  $D_k^t(i) = \\sum_{u' \\in \\mathcal{N}_i} \\tilde{S}_k^t(u', i)$  are the degrees of user  $u$  and item  $i$ , respectively;  $N_u$  and  $N_i$  are the one-hop neighbors of  $u$  and  $i$ , respectively. Obviously, when iteration  $t = 1$ ,  $D_k^1(u)$  and  $D_k^1(i)$  separately degrade as  $|\\mathcal{N}_u|$  and  $|\\mathcal{N}_i|$ , which is the fixed decay term widely adopted in prior studies [18, 43]. Such normalization can handle the varying neighbor numbers of nodes, making the training process more stable.\n\nIt is worth emphasizing that, we aggregate the initial chunked representations  $\\{\\mathbf{i}_k^0\\}$  as the distilled information for user  $u$ . This contains the signal from the first-order connectivities only, while excluding that from user  $u$  herself and her higher-hop neighbors. Moreover, inspired by recent SGC [43] and LightGCN [10], we argue that nonlinear transformation adopted by prior works [36, 40] is burdensome for CF and its black-box nature hinders the disentanglement process, thereby omitting the transformation and using ID embeddings only.\n\nIntent-Aware Graph Update. We iteratively adjust the edge strengths based on neighbors of a user (or an item) node. Examining the subgraph structure rooted at user node  $u$  with Equation (9),  $\\mathbf{u}_k^t$  can be seen as the centroid within the local pool  $\\mathcal{N}_u = \\{(u,i)\\}$ , which contains items  $u$  has interacted with before. Intuitively, historical items driven by the same intent tend to have similar chunked representations, further encouraging their relationships to\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/fd090a3879cd5c3c3058d9b5b955a621a70941fc6c2dc25fce6273cfd7fb46a4.jpg)  \nFigure 3: Illustration of iterative update rule.\n\nbe stronger. We hence iteratively update  $S_{k}^{t}(u,i)$  - more precisely, adjusting the strength between the centroid  $u$  and its neighbor  $i$ , as follows:\n\n$$\nS _ {k} ^ {t + 1} (u, i) = S _ {k} ^ {t} (u, i) + \\mathbf {u} _ {k} ^ {t} ^ {\\top} \\tanh  \\left(\\mathbf {i} _ {i} ^ {0}\\right), \\tag {11}\n$$\n\nwhere  $\\mathbf{u}_k^t\\top \\tanh (\\mathbf{i}_k^0)$  considers the affinity between  $\\mathbf{u}_k^t$  and  $\\mathbf{i}_k^0$ ; and  $\\tanh$  [37] is a nonlinear activation function to increase the representation ability of model.\n\nAfter  $T$  iterations, we ultimately obtain the output of one graph disentangling layer, which consists of disentangled representation i.e.,  $\\mathbf{e}_{ku}^{(1)} = \\mathbf{u}_k^T$ , as well as its intent-aware graph i.e.,  $\\mathbf{A}_k^{(1)} = \\tilde{\\mathbf{S}}_k^T$ ,  $\\forall k \\in \\{1, \\dots, K\\}$ . When performing such propagation forward, our model aggregates information pertinent to each intent and generates an attention flow, which can be viewed as explanations behind the disentanglement.\n\n3.1.4 Layer Combination. Having used the first-hop neighbors, we further stack more graph disentangling layers to gather the influential signals from higher-order neighbors. In particular, such connectivities carry rich semantics. For instance, the second-order connectivity like  $u_{1} \\rightarrow i_{2} \\rightarrow u_{3}$  suggests the intent similarity between  $u_{1}$  and  $u_{3}$  when consuming  $i_{2}$ ; meanwhile, the longer path  $u_{1} \\rightarrow i_{2} \\rightarrow u_{2} \\rightarrow i_{4}$  deepens their intents via the collaborative signal. To capture user intents from such higher-order connectivity, we recursively formulate the representation after  $l$  layers as:\n\n$$\n\\mathbf {e} _ {k u} ^ {(l)} = g \\left(\\mathbf {e} _ {k u} ^ {(l - 1)}, \\left\\{\\mathbf {e} _ {k i} ^ {(l - 1)} \\mid i \\in \\mathcal {N} _ {u} \\right\\}\\right), \\tag {12}\n$$\n\nwhere  $\\mathbf{e}_{ku}^{(l-1)}$  and  $\\mathbf{e}_{ki}^{(l-1)}$  are the representations of user  $u$  and item  $i$  conditioned on the  $k$ -th factor, memorizing the information being propagated from their  $(l-1)$ -hop neighbors. Moreover, each disentangled representation is also associated with its explanatory graphs to explicitly present the intents, i.e., the weighted adjacency matrix  $\\mathbf{A}_k^{(l)}$ . Such explanatory graphs are able to show reasonable evidences of what information construct the disentangled representations.\n\nAfter  $L$  layers, we sum the intent-aware representations at different layers up as the final representations, as follows:\n\n$$\n\\mathbf {e} _ {k u} = \\mathbf {e} _ {k u} ^ {(0)} + \\dots + \\mathbf {e} _ {k u} ^ {(L)}, \\quad \\mathbf {e} _ {k i} = \\mathbf {e} _ {k i} ^ {(0)} + \\dots + \\mathbf {e} _ {k i} ^ {(L)}. \\tag {13}\n$$\n\nBy doing so, we not only disentangle the CF representations, but also have the explanations for each part of representations. It is worthwhile to emphasize that the trainable parameters are only the embeddings at the 0-th layer, i.e.,  $\\mathbf{u}$  and  $\\mathbf{i}$  for all users and items (cf. Equation (5)).\n\n# 3.2 Independence Modeling Module\n\nAs suggested in [25, 29], dynamic routing mechanism encourages the chunked representations conditioned on different intents to be different from each others. However, the difference constraint\n\nenforced by dynamic routing is insufficient: there might be redundancy among factor-aware representations. For example, if one intent-aware representation  $\\mathbf{u}_k$  can be inferred by the others  $\\{\\mathbf{u}_{k'}|k'\\neq k\\}$ , the factor  $k$  is highly likely to be redundant and could be confounding.\n\nWe hence introduce another module, which can hire statistical measures like mutual information [1] and distance correlation [33, 34] as a regularizer, with the target of encouraging the factor-aware representations to be independent. We here apply distance correlation, leaving the exploration of mutual information in future work. In particular, distance correlation is able to characterize independence of any two paired vectors, from their both linear and nonlinear relationships; its coefficient is zero if and only if these vectors are independent [34]. We formulate this as:\n\n$$\nl o s s _ {\\text {i n d}} = \\sum_ {k = 1} ^ {K} \\sum_ {k ^ {\\prime} = k + 1} ^ {K} d C o r \\left(\\mathbf {E} _ {k}, \\mathbf {E} _ {k ^ {\\prime}}\\right), \\tag {14}\n$$\n\nwhere  $\\mathbf{E}_k = [\\mathbf{e}_{u_1k},\\dots ,\\mathbf{e}_{u_Nk},\\mathbf{e}_{i_1k},\\dots ,\\mathbf{e}_{i_Mk}]\\in \\mathbb{R}^{(M + N)\\times \\frac{d}{K}}$  is the embedding look-up table with  $N = |\\mathcal{U}|$  and  $M = |\\mathcal{I}|$ , which is built upon the intent-aware representations of all users and items; and,  $dCor(\\cdot)$  is the function of distance correlation defined as:\n\n$$\nd C o r \\left(\\mathbf {E} _ {k}, \\mathbf {E} _ {k ^ {\\prime}}\\right) = \\frac {d C o v \\left(\\mathbf {E} _ {k} , \\mathbf {E} _ {k ^ {\\prime}}\\right)}{\\sqrt {d V a r \\left(\\mathbf {E} _ {k}\\right) \\cdot d V a r \\left(\\mathbf {E} _ {k ^ {\\prime}}\\right)}} \\tag {15}\n$$\n\nwhere  $dCov(\\cdot)$  represents the distance covariance between two matrices;  $dVar(\\cdot)$  is the distance variance of each matrix. For a more detailed calculation, refer to prior works [34].\n\n# 3.3 Model Optimization\n\nHaving obtained the final representations for user  $u$  and item  $i$ , we use inner product (cf. Equation (2)) as the predictive function to estimate the likelihood of their interaction,  $\\hat{y}_{ui}$ . Thereafter, we use the pairwise BPR loss [28] to optimize the model parameters  $\\Theta = \\{\\mathbf{u},\\mathbf{i}|u\\in \\mathcal{U},i\\in I\\}$ . Specifically, it encourages the prediction of a user's historical items to be higher than that of unobserved items:\n\n$$\n\\operatorname {l o s s} _ {\\mathrm {B P R}} = \\sum_ {(u, i, j) \\in O} - \\ln \\sigma \\left(\\hat {y} _ {u i} - \\hat {y} _ {u j}\\right) + \\lambda \\| \\Theta \\| _ {2} ^ {2}, \\tag {16}\n$$\n\nwhere  $O = \\{(u,i,j)|(u,i)\\in O^{+},(u,j)\\in O^{-}\\}$  denotes the training dataset involving the observed interactions  $O^{+}$  and unobserved counterparts  $O^{-}$ ;  $\\sigma (\\cdot)$  is the sigmoid function;  $\\lambda$  is the coefficient controlling  $L_{2}$  regularization. During the training, we alternatively optimize the independence loss (cf. Equation (14)) and BPR loss (cf. Equation (16)).\n\n# 3.4 Model Analysis\n\nIn this subsection, we conduct model analysis, including the complexity analysis and DGCF's relations with existing models.\n\n3.4.1 Model Size. While we slice the embeddings into  $K$  chunks, the total embedding size remains the same as that set in MF and NGCF (i.e.,  $d = 64$ ). That is, our DGCF involves no additional model parameters to learn, whole trainable parameters are  $\\{\\mathbf{u}, \\mathbf{i} | u \\in \\mathcal{U}, i \\in \\mathcal{I}\\}$ . The model size of DGCF is identical to MF and lighter than NGCF that introduces additional transformation matrices.\n\nTable 1: Statistics of the datasets.  \n\n<table><tr><td>Dataset</td><td>#Users</td><td>#Items</td><td>#Interactions</td><td>Density</td></tr><tr><td>Gowalla</td><td>29,858</td><td>40,981</td><td>1,027,370</td><td>0.00084</td></tr><tr><td>Yelp2018*</td><td>31,668</td><td>38,048</td><td>1,561,406</td><td>0.00130</td></tr><tr><td>Amazon-Book</td><td>52,643</td><td>91,599</td><td>2,984,108</td><td>0.00062</td></tr></table>\n\n3.4.2 Relation with LightGCN. LightGCN [10] can be viewed as a special case of DGCF with only-one intent representation and no independence modeling. As the same datasets and experimental settings are used in these two models, we can directly compare DGCF with the empirical results reported in the LightGCN paper. In terms of the recommendation accuracy, DGCF and LightGCN are in the same level. However, benefiting from the disentangled representations, our DGCF has better interpretability since it can disentangle the latent intents in user representations (evidence from Table 4), and further exhibit the semantics of user intents (evidence from Section 4.4.2), while LightGCN fails to offer explainable representations.\n\n3.4.3 Relation with Capsule Network. From the perspective of capsule networks [14, 29], we can view the chunked representations as the capsules and the iterative update rule as the dynamic routing. However, distinct from typical capsule networks that adopt the routing across different layers, DGCF further incorporate the routing with the embedding propagation of GNNs, such that not only can the information be passed across layers, but also be propagated within the neighbors within the same layer. As a result, our DGCF is able to distill useful information, that are relevant to intents, from multi-hop neighbors.\n\n3.4.4 Relation with Multi-head Attetion Network. Although the intent-aware graphs can be seen as channels used in multihead attention mechanism [37, 38], they have different utilities. Specifically, multi-head attention is mainly used to stabilize the learning of attention network [38] and encourage the consistence among different channels; whereas, DGCF aims to collect different signals and leverage the independence module (cf. Section 3.2) to enforce independence of intent-aware graphs. Moreover, there is no information interchange between attention networks, while DGCF allows the intent-aware graphs to influence each other.\n\n3.4.5 Relation with DisenGCN. DisenGCN [25] is proposed to disentangle latent factors for graph representations, where the neighbor routing is also coupled with GNN models. Our DGCF distinguishes from DisenGCN from several aspects: 1) DisenGCN fails to model the independence between factors, easily leading to redundant representations, while DGCF applies the distance correlation to achieve independence; 2) the embedding propagation in DisenGCN mixes the information from the ego node self and neighbors together; whereas, DGCF can purely distill information from neighbors; and 3) DGCF's neighbor routing (cf. Section 3.1.3) is more effective than that of DisenGCN (cf. Section 4.2).\n",
  "experiments": "# 4 EXPERIMENTS\n\nTo answer the following research questions, we conduct extensive experiments on three public datasets:\n\n- RQ1: Compared with present models, how does DGCF perform?  \n- RQ2: How different components (e.g., layer number, intent number, independence modeling) affect the results of DGCF?\n\n- RQ3: Can DGCF provide in-depth analyses of disentangled representations w.r.t. disentanglement of latent user intents and interpretability of representations?\n\n# 4.1 Experimental Settings\n\n4.1.1 Dataset Description. We use three publicly available datasets: Gowalla, Yelp2018*, and Amazon-Book, released by NGCF [40]. Note that we revise Yelp2018 dataset, as well as the updated results of baselines, instead of the original reported in the NGCF paper  $[40]^1$ . We denote the revised as Yelp2018*. The statistics of datasets are summarized in Table 1. We closely follow NGCF and use the same data split as NGCF. In the training phase, each observed user-item interaction is treated as a positive instance, while we use negative sampling to randomly sample an unobserved item and pair it with the user as a negative instance.\n\n4.1.2 Baselines. We compare DGCF with the state-of-the-art methods, covering the CF-based (MF), GNN-based (GC-MC and NGCF), disentangled GNN-based (DisenGCN), and disentangled VAE-based (MacridVAE):\n\n- MF [28]: Such model treats user-item interactions as isolated data instances, and only uses ID embeddings as representations of users and items.  \n- GC-MC [36]: The method organizes historical user behaviors as a holistic interaction graph, and employs one GCN [18] encoder to generate representations. Note that only one-hop neighbors are involved.  \n- NGCF [40]: This adopts three GNN layers on the user-item interaction graph, aiming to refine user and item representations via at most three-hop neighbors' information.  \n- DisenGCN [25]: This is a state-of-the-art disentangled GNN model, which exploits the neighbor routing and embedding propagation to disentangle latent factors behind graph edges.  \n- MacridVAE [26]: Such model is tailored to disentangle user intents behind user behaviors. In particular, it adopts  $\\beta$ -VAE to estimate the generative process of a user's personal history, assuming that there are several latent factors affecting user behaviors, to achieved disentangled user representations.\n\n4.1.3 Evaluation Metrics. To evaluate top- $N$  recommendation, we use the same protocols as NGCF [40]: recall@ $N$  and ndcg@ $N^2$ , where  $N$  is set as 20 by default. In the inference phase, we view historical items of a user in the test set as the positive, and evaluate how well these items are ranked higher than all unobserved ones. The average results w.r.t. the metrics over all users are reported.\n\n4.1.4 Parameter Settings. We implement the DGCF model in Tensorflow. For a fair comparison, we tune the parameter settings of each model. In particular, we directly copy the best performance of MF, GC-MC, and NGCF reported in the original paper [40]. As for DisenGCN and DGCF, we fix the embedding size  $d$  as 64 (which is identical to MF, GC-MC, and NGCF), use Adam [17] as the optimizer, initialize model parameters with\n\nTable 2: Overall Performance Comparison.  \n\n<table><tr><td></td><td colspan=\"2\">Gowalla</td><td colspan=\"2\">Yelp2018*</td><td colspan=\"2\">Amazon-Book</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>MF</td><td>0.1291</td><td>0.1109</td><td>0.0433</td><td>0.0354</td><td>0.0250</td><td>0.0196</td></tr><tr><td>GC-MC</td><td>0.1395</td><td>0.1204</td><td>0.0462</td><td>0.0379</td><td>0.0288</td><td>0.0224</td></tr><tr><td>NGCF</td><td>0.1569</td><td>0.1327</td><td>0.0579</td><td>0.0477</td><td>0.0337</td><td>0.0266</td></tr><tr><td>DisenGCN</td><td>0.1356</td><td>0.1174</td><td>0.0558</td><td>0.0454</td><td>0.0329</td><td>0.0254</td></tr><tr><td>MacridVAE</td><td>0.1618</td><td>0.1202</td><td>0.0612</td><td>0.0495</td><td>0.0383</td><td>0.0295</td></tr><tr><td>DGCF-1</td><td>0.1794*</td><td>0.1521*</td><td>0.0640*</td><td>0.0522*</td><td>0.0399*</td><td>0.0308*</td></tr><tr><td>%improv.</td><td>10.88%</td><td>14.62%</td><td>4.58%</td><td>5.45%</td><td>4.17%</td><td>4.41%</td></tr><tr><td>p-value</td><td>6.63e-8</td><td>3.10e-7</td><td>1.75e-8</td><td>4.45e-9</td><td>8.26e-5</td><td>7.15e-5</td></tr></table>\n\nXarvier [8], and fix the iteration number  $T$  as 2. Moreover, a grid search is conducted to confirm the optimal settings - that is, the learning rate is searched in  $\\{0.001, 0.005, 0.0005, 0.0001\\}$ , and the coefficients  $\\lambda$  of  $L_{2}$  regularization term is tuned in  $\\{10^{-3}, 10^{-4}, 10^{-5}\\}$ . For MacridVAE [26], we search the number of latent factors being disentangled in  $\\{2, 4, 8, 16\\}$ , and tune the factor size in  $\\{50, 55, 60, 65, 70\\}$ .\n\nWithout specification, unique hyperparameters of DGCF are set as:  $L = 1$  and  $K = 4$ . We study the number of graph disentangling layer  $L$  in  $\\{0,1,2,3\\}$  and the number of latent intents  $K$  in  $\\{2,4,8,16\\}$ , and report their influences in Sections 4.3.1 and 4.3.2, respectively.\n\n# 4.2 Performance Comparison (RQ1)\n\nWe report the empirical results of all methods in Table 2. The improvements and statistical significance test are performed between DGCF-1 with the strongest baselines (highlighted with underline). Analyzing such performance comparison, we have the following observations:\n\n- Our proposed DGCF achieves significant improvements over all baselines across three datasets. In particular, its relative improvements over the strongest baselines w.r.t. recall@20 are  $10.88\\%$ ,  $4.58\\%$ , and  $4.17\\%$  in Gowalla, Yelp2018*, and AmazonBook, respectively. This demonstrates the high effectiveness of DGCF. We attribute such improvements to the following aspects - 1) by exploiting diverse user-item relationships, DGCF is able to better characterize user preferences, than prior GNN-based models that treat the relationships as uniform edges; 2) the disentangling module models the representations at a more granular level of user intents, endowing the recommender better expressiveness; and 3) embedding propagation mechanism can more effectively distill helpful information from one-hop neighbors, than one-layer GC-MC and three-layer NGCF.  \n- Jointly analyzing the results across three datasets, we find that the improvements on Amazon-Book is much less than that on the others. This might suggest that, purchasing books is a simpler scenario than visiting location-based business. Hence, the user intents on purchasing books are less diverse.  \n- MF performs poor on three datasets. This indicates that, modeling user-item interactions as isolated data instance could ignore underlying relationships among users and items, and easily lead to unsatisfactory representations.  \n- Compared with MF, GC-MC and NGCF consistently achieve better performance on three datasets, verifying the importance of user-item relationships. They take the one-hop neighbors (i.e.,\n\nTable 3: Impact of Layer Number (L).  \n\n<table><tr><td></td><td colspan=\"2\">Gowalla</td><td colspan=\"2\">Yelp2018*</td><td colspan=\"2\">Amazon-Book</td></tr><tr><td></td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td><td>recall</td><td>ndcg</td></tr><tr><td>DGCF-1</td><td>0.1794</td><td>0.1521</td><td>0.0640</td><td>0.0522</td><td>0.0399</td><td>0.0308</td></tr><tr><td>DGCF-2</td><td>0.1834</td><td>0.1560</td><td>0.0653</td><td>0.0532</td><td>0.0422</td><td>0.0324*</td></tr><tr><td>DGCF-3</td><td>0.1842*</td><td>0.1561*</td><td>0.0654*</td><td>0.0534*</td><td>0.0422*</td><td>0.0322</td></tr></table>\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/af8729f6b8c8ceab72866dd6b579217f62959940d59a5d81008e2d11e071ee09.jpg)  \n(a) Intent Impact in Gowalla\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/52e69f27cd4c5c58726cbdcf994bde4ec14bdb524cfc7fc0c94738a0dd23bb49.jpg)  \n(b) Intent Impact in Yelp208*  \nFigure 4: Impact of Intent Number  $(K)$ . Best Viewed in Color.\n\nreflecting behavioral similarity of users) and third-hop neighbors (i.e., carrying collaborative signals) into representations.\n\n- DisenGCN substantially outperforms GC-MC in most cases. It is reasonable since user intents are explicitly modeled as factors being disentangled in DisenGCN, which offer better guide to distill information from one-hop neighbors. However, its results are worse than that of DGCF. Possible reasons are that 1) its routing mechanism only uses node affinity to adjust the graph structure, without any priors to guide; and 2) many operations (e.g., nonlinear transformation) are heavy to CF [43]. This suggests its suboptimal disentanglement.  \n- MacridVAE serves as the strongest baseline in most cases. This justifies the effectiveness of estimating personal history via a generative process, and highlights the importance of the disentangled representations.\n\n# 4.3 Study of DGCF (RQ2)\n\nAblation studies on DGCF are also conducted to investigate the rationality and effectiveness of some designs - to be more specific, how the number of graph disentangling layers, the number of latent user intents, and independence modeling influence the model.\n\n4.3.1 Impact of Model Depth. The graph disentangling layer is at the core of DGCF, which not only disentangles user intents, but also collects information pertinent to individual intents into intent-aware representations. Furthermore, stacking more layers is able to distill the information from multi-hop neighbors. Here we investigate how the number of such layers,  $L$ , affects the performance. Towards that, we search  $L$  in the range of  $\\{1, 2, 3\\}$  and summarize the empirical results in Table 3. Here we use DGCF-1 to denote the recommender with one disentangling layer, and similar notations for others. We have several findings:\n\n- Clearly, increasing the model depth is capable of endowing the recommender with better representation ability. In particular, DGCF-2 outperforms DGCF-1 by a large margin. This makes sense since DGCF-1 just gathers the signals from one-hop neighbors, while DGCF-2 considers multi-hop neighbors.  \n- While continuing stacking more layer beyond DGCF-2 gradually improves the performance of DGCF-3, the improvements are not that obvious. This suggests that the second-order connectivity\n\nTable 4: Distance correlation of different methods (the lower the better).  \n\n<table><tr><td></td><td>Gowalla</td><td>Yelp*</td><td>Amazon-Book</td></tr><tr><td>MF</td><td>0.2332</td><td>0.2701</td><td>0.2364</td></tr><tr><td>NGCF</td><td>0.2151</td><td>0.2560</td><td>0.1170</td></tr><tr><td>DGCF-1</td><td>0.1389</td><td>0.1713</td><td>0.1039</td></tr><tr><td>DGCF-2</td><td>0.0920*</td><td>0.1217*</td><td>0.0751*</td></tr></table>\n\nTable 5: Impact of Independence w.r.t. recall in Yelp2018*.  \n\n<table><tr><td></td><td>DGCF-1</td><td>DGCF-2</td><td>DGCF-3</td></tr><tr><td>w/o ind</td><td>0.0637</td><td>0.0649</td><td>0.0650</td></tr><tr><td>w/ ind</td><td>0.0640</td><td>0.0653</td><td>0.0654</td></tr></table>\n\nmight be sufficient to distill intent-relevant information. Such observation is consistent to NGCF [40].\n\n- We compare the results across Tables 2 and 3 and find that DGCF with varying depths is consistently superior to other baselines. This again verifies the effectiveness of embedding propagation with neighbor routing.\n\n4.3.2 Impact of Intent Number. To study the influence of intent number, we vary  $K$  in range of  $\\{1, 2, 4, 8, 16\\}$  and demonstrate the performance comparison on Gowalla and Yelp* datasets in Figure 4. There are several observations:\n\n- Increasing the intent number from 1 to 4 significantly enhances the performance. In particular, DGCF performs the worst when  $K = 1$ , indicating that only uniform relationship is not sufficient to profile behavioral patterns of users. This again justifies the rationality of disentangling user intents.  \n- Interestingly, the recommendation performance drops when then intent number reaches 8 and 16. This suggests that, although benefiting from the proper intent chunks, the disentanglement suffers from too fine-grained intents. One possible reason is that the chunked representations only with the embedding size of 4 (e.g.,  $\\frac{d}{K} = 4$  when  $K = 16$ ) have limited expressiveness, hardly describing a complete pattern behind user behaviors.  \n- Separately analyzing the performance in Gowalla and Yelp208*, we find that the influences of intent number are varying. We attribute this to the differences between scenarios. To be more specific, Gowalla is a location-based social networking service which provides more information, such as social marketing, as the intents than Yelp2018*. This indicates that user behaviors are driven by different intents for difference scenarios.\n\n4.3.3 Impact of Independence Modeling. As introduced in Equation (14) distance correlation is a statistic measure to quantify the level of independence, we hence select MF, NGCF, and DGCF to separately represent three fashions to model user-item relationships - isolated data instance, holistic interaction graph, and intent-aware interaction graphs, respectively. We report the result comparison w.r.t. distance correlation in Table 4. Furthermore, conduct another ablation study to verify the influence of independence modeling. To be more specific, we disable this module in DGCF-1, DGCF-2, and DGCF-3 to separately build the variants DGCF-1<sub>ind</sub>, DGCF-2<sub>ind</sub>, and DGCF-3<sub>ind</sub>. We show the results in Table 5. There are some observations:\n\n- A better performance is substantially coupled with a higher level of independence across the board. In particular, focusing on one model group, the intents of  $\\mathrm{DGCF_{ind}}$  are more highly entangled\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/49a54b61fce7e4f026b33c8aabe51bf33570ed5f59e448a35fd9ada4481238bc.jpg)  \nReview for  $(u_{10362},i_{88})$  If by Fremont Street Experience, you mean the campy neon canopy show, eh, it's ok. The graphics and music are a throwback to what would have been impressive back in the late 90s.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/d6532a610b76e32bfc3c8f1f1945ac02b72b211b65274f5c9f064d44e41cef01.jpg)  \nReview for  $(u_{10362},i_{3520})$    \nAwesome customer service. Seriously, everyone in this place is warm and welcoming and willing to help out.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/ee9c5d83113d702517e2eae61f1f7050b1965ab5e1a3087eebd0fe99b56c1afb.jpg)  \nReview for  $(u_{10360},i_{29004})$  Great mom and pop place. The service couldn't have been better! This will definitely be a frequent location for us.  \nReview for  $\\left(u_{10362}, i_{4895}\\right)$   \nThe pricing was fair for what I got and I left full, so that's good.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/b36d375afd0065f3931b1ae2819503fa297b7c49310ef4fb4e41ee55254b198f.jpg)  \nReview for  $(u_{10362},i_{3672})$  This park is big enough to accommodate hikers, families, kids, dog people, and casual park goers. The trails up the mountain are people and dog friendly, and there are spots to take a break and take in the view.  \nReview for  $(u_{10360},i_{5722})$  The atmosphere of this place makes it great as soon as you walk in. The prices are a little on the high side but worth it.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/f2ac46865fa7f7e3a1fe7ec1fbf9ca2cdd42afc88dbbab73ded2372c06b20990.jpg)  \nReview for  $(u_{10360},i_{1732})$  This place was totally entertaining. From the hospital gown you have to wear once you walk in the door to the witty commentary on the menu to the sassy servers.  \n(a) Disentanglement w.r.t. recall.  \nFigure 6: Disentanglement of Intents. Best Viewed in Color.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/03aeab0e-8e4b-428c-a4cb-a171f144cb58/1c7b938551748e7b66673d74d3f4816ee849bc918c4e0309d0dc61e6ec12d80b.jpg)  \nReview for  $(u_{10360},i_{10235})$  We love this place! Every time we've ever been here they have been fair, honest, and upfront. They try to find the best solution for you at the lowest cost.  \nFigure 5: Illustration of the proposed disentangled graph collaborative filtering framework. Best viewed in color.  \n(b) Disentanglement w.r.t. ndcg.\n\nthan that of DGCF; meanwhile, DGCF is superior to its variants w.r.t. recall in Gowalla and Yelp2018 datasets. This reflects the correlation between the performance and disentanglement, which is consistent to the observation in [26].\n\n- When analyzing the comparison across model groups, we find that the performance drop caused by removing the independence modeling from DGCF-2 is more obvious in DGCF-1. This suggests that the independence regularizer is more beneficial to the deeper models, leading to better intent-aware representations.\n\n# 4.4 In-depth Analysis (RQ3)\n\nIn what follows, we conduct experiments to get deep insights into the disentangled representations w.r.t. the disentanglement of intents and interpretability of representations.\n\n4.4.1 Disentanglement of Intents. Disentangled representation learning aims to generate factorial representations, where a change in one factor is relatively invariant to changes in other factors [2]. We hence investigate whether the intent-aware representations of DGCF match this requirement well, and get deep insights into the disentanglement. Towards this end, we introduce a temperature factor,  $\\tau$ , into the graph disentangling module. Specifically, for each user/item factorial representation, we first select a particular intent  $k$ , whose  $A_{k}(u,i)$  is the smallest in  $K$  intents, and then use  $\\tau$  to reassign it with a smaller weight  $\\frac{A_k(u,i)}{\\tau}$ , while remaining the other intents unchanged. Wherein,  $\\tau$  is searched in the range of  $\\{10^0, 10^1, 10^2, \\cdots, 10^{10}\\}$ . Through this way, we study its influence on recommendation performance. The results in Gowalla dataset is shown in Figure 6. We have several interesting observations:\n\n- The discovered intents serve as vitamin to the disentangled representations. That is, even the intents in small quantities (i.e., when  $\\tau$  ranges from  $10^{0}$  to  $10^{3}$ ) are beneficial to the representation learning. This also verifies the independence of intents: the changes on the specific intent almost have no\n\ninfluence on the others, still leading to comparable performances to the optimal one.\n\n- The vitamin also means that the deficiency of any intent hinders the results severely. When  $\\tau$  is larger than  $10^{4}$ , the performance drops quickly, which indicates that lack of intents (whose influences is close to zero) negatively affects the model. Moreover, this verifies that one intent cannot be inferred by the others.\n\n4.4.2 Interpretability of Representations. We now explore the semantics of the explored intents. Towards this end, in Yelp2018* datasets, we use side information (i.e., user reviews) which possibly contain some cues for why users visit the local business (i.e., self-generated explanations on their behaviors). In particular, we randomly selected two users  $u_{10362}$  and  $u_{10360}$ , and disentangled the explored intents behind each historical user-item interactions, based on the learned intent-aware graphs. Thereafter, for each interaction, we coupled its review with the intent with the highest confidence score. For example, conditioned on the intent  $k_{1}$ , interaction  $(u_{10360}, i_{11732})$  has the highest score 0.313 among  $u_{10362}$ 's history. Figure 5 shows the post-hoc explanations of intents, and we have the following findings:\n\n- Jointly analyzing reviews for the same intent, we find that, while showing different fine-grained preferences, they are consistent with some high-level concepts. For example, intent  $k_{1}$  contributes the most to interactions  $(u_{10362}, i_{88})$  and  $(u_{10360}, i_{11732})$ , which suggests its high confidence of being the intents behind these behaviors. We hence analyze the reviews of interactions, and find that they are about users' special interests: the late 90s for  $u_{10362}$ , and hospital gown for  $u_{10360}$ .  \n- DGCF is able to discover user intents inherent in historical behaviors. Examining the corresponding reviews, we summarize the semantics (high-level concept) of these four intents  $k_{1}, k_{2}, k_{3}$ , and  $k_{4}$  as interesting matching, service, price&promotion, and passing the time, respectively. This verifies the rationality of our assumptions: user behaviors are driven by multiple intents with varying contributions.  \n- This inspires us to go beyond user-item interactions and consider extra information to model user intents in an explicit fashion. We plan to incorporate psychology knowledge in future work.\n",
  "hyperparameter": ""
}