[
  {
    "source": "LightGCN_2020",
    "target": "WeightedGCL_2025",
    "type": "in-domain",
    "similarities": "1. Both methods use LightGCN as the backbone/base model for learning user and item embeddings through graph convolution on the user-item interaction graph.\n2. Both adopt the same GCN propagation mechanism: neighborhood aggregation without feature transformation or nonlinear activation, using symmetric normalization (D^{-0.5} A D^{-0.5}).\n3. Both use the layer combination strategy that averages embeddings across all layers (from layer 0 to layer L) to obtain final representations, as shown in Equation (2) of WeightedGCL and the forward() method in LightGCN code.\n4. Both use BPR (Bayesian Personalized Ranking) loss as the primary recommendation optimization objective with L2 regularization on model parameters.\n5. Both methods predict user-item scores using inner product (dot product) between user and item final embeddings.\n6. Both maintain the same adjacency matrix construction for the bipartite user-item graph, with users and items as nodes and interactions as edges.\n7. Both methods have trainable parameters primarily consisting of the 0-th layer embeddings (initial embeddings for users and items).",
    "differences": "1. **Contrastive Learning Framework**: WeightedGCL introduces a contrastive learning component with InfoNCE loss (Equation 10), while LightGCN only uses BPR loss. For implementation, add a contrastive loss term that compares two perturbed views of the same node.\n2. **Robust Perturbation Strategy**: WeightedGCL applies random noise perturbation ONLY to the final GCN layer embeddings (Equation 3), creating two perturbed views by adding random matrices sampled from U(0,1). Implementation requires: after computing all layer embeddings, add two different random noise matrices to E^{(L)} before layer combination.\n3. **SENet Architecture (Squeeze-Excitation Network)**: WeightedGCL introduces a novel weight learning mechanism not present in LightGCN:\n   - Squeeze Network (Equation 5-6): Apply average pooling across the feature dimension d to compress each node's d-dimensional embedding into a single scalar, producing summary statistics S of shape (1, |N|).\n   - Excitation Network (Equation 7-8): A multi-layer feedforward network that expands S back to original dimensions (d, |N|) using K layers with progressively increasing dimensions at scale s = d^{1/K}, with sigmoid activation.\n4. **Recalibration Mechanism**: WeightedGCL applies element-wise multiplication between the learned weight matrix T and perturbed views F to obtain weighted views R (Equation 9). This dynamically weights features based on their importance.\n5. **Final Loss Function**: WeightedGCL combines BPR loss and contrastive loss with a balancing hyperparameter λ_c (Equation 12), requiring tuning of this additional hyperparameter.\n6. **Additional Trainable Parameters**: WeightedGCL has extra learnable parameters in the excitation network (W_1 to W_K and biases b_1 to b_K), whereas LightGCN only trains initial embeddings.\n7. **View Generation**: LightGCN produces a single final representation, while WeightedGCL generates two weighted contrastive views (R̄ and R̃) for self-supervised learning.\n8. **Temperature Hyperparameter**: WeightedGCL requires tuning the temperature τ in the InfoNCE loss, which is absent in LightGCN.\n9. **Implementation Flow for WeightedGCL**: (a) Compute layer embeddings E^{(0)} to E^{(L)} using LightGCN propagation, (b) Generate two noise matrices and add to E^{(L)}, (c) Compute two perturbed final representations F̄ and F̃ via layer averaging, (d) Apply squeeze (average pooling over d) to get S̄ and S̃, (e) Apply excitation network to get weight matrices T̄ and T̃, (f) Recalibrate: R̄ = T̄ ⊙ F̄ and R̃ = T̃ ⊙ F̃, (g) Compute InfoNCE loss between R̄ and R̃, (h) Combine with BPR loss for final optimization.",
    "rank": "rank1"
  },
  {
    "source": "SGL_2021",
    "target": "WeightedGCL_2025",
    "type": "in-domain",
    "similarities": "1. **Core Framework**: Both methods are built upon LightGCN as the backbone GCN encoder, using the same message aggregation mechanism across L layers and averaging embeddings from all layers (Equation 2 in WeightedGCL corresponds to the LightGCN aggregation in SGL).\n\n2. **Contrastive Learning Paradigm**: Both adopt the self-supervised contrastive learning framework with InfoNCE loss function. They create two augmented views and maximize agreement between the same node's representations across views while minimizing agreement between different nodes (Equation 10 in WeightedGCL mirrors Equation 10 in SGL).\n\n3. **Multi-task Training Strategy**: Both combine the main recommendation task (BPR loss) with the self-supervised contrastive loss using a weighted sum. The final loss follows the same pattern: L = L_rec + λ * L_cl (Equation 12 in WeightedGCL corresponds to Equation 11 in SGL).\n\n4. **Temperature Hyperparameter**: Both use a temperature parameter τ in the softmax of the InfoNCE loss to control the sharpness of the contrastive distribution.\n\n5. **User-Item Bipartite Graph Structure**: Both operate on the same user-item interaction graph structure and compute separate contrastive losses for users and items, then sum them together.\n\n6. **Embedding Initialization and Regularization**: Both use L2 regularization on model parameters and similar embedding initialization strategies for user and item embeddings.",
    "differences": "1. **Perturbation Strategy Location**: SGL applies perturbations (Node Dropout, Edge Dropout, or Random Walk) to the graph structure before/during GCN propagation, affecting all layers. WeightedGCL only applies random noise perturbation to the final GCN layer embeddings (E^(L)), keeping earlier layer representations stable. For implementation, instead of modifying the adjacency matrix like SGL, add uniform random noise matrices Δ̄ and Δ̃ ~ U(0,1) directly to the L-th layer embeddings.\n\n2. **Perturbation Type**: SGL uses structural perturbations (dropping nodes/edges from the graph). WeightedGCL uses feature-level perturbations by adding random noise to embedding representations. Implementation requires generating random noise matrices of shape (d × |N|) rather than masking graph edges/nodes.\n\n3. **Dynamic Feature Weighting via SENet**: WeightedGCL introduces a novel Squeeze-and-Excitation Network that SGL lacks entirely:\n   - **Squeeze Network**: Implement average pooling across the feature dimension d to compress each node's d-dimensional perturbed representation into a scalar, producing S ∈ R^(1×|N|).\n   - **Excitation Network**: Implement a multi-layer feedforward network that expands S back to T ∈ R^(d×|N|) using K layers with gradually increasing dimensions (scale s = d^(1/K)), with sigmoid activation.\n   - **Recalibration**: Element-wise multiply the weight matrix T with the perturbed view F to get the final weighted representation R.\n\n4. **Contrastive View Construction**: SGL creates views by graph augmentation (subgraph sampling), while WeightedGCL creates views by: (a) adding noise to final layer embeddings, then (b) applying learned feature weights. The contrastive loss in WeightedGCL operates on the recalibrated representations R̄ and R̃, not directly on the perturbed embeddings.\n\n5. **No Graph Reconstruction Per Epoch**: SGL requires regenerating augmented subgraphs at each training epoch (calling graph_construction()). WeightedGCL only needs to sample new noise matrices per forward pass, which is computationally simpler - no need to recompute normalized adjacency matrices.\n\n6. **Learnable Parameters in Augmentation**: SGL's augmentation is parameter-free (only dropout operations). WeightedGCL introduces trainable parameters in the excitation network (W_1, b_1, ..., W_K, b_K) that need to be optimized jointly with the recommendation model.\n\n7. **Implementation Flow for WeightedGCL**:\n   - Step 1: Run standard LightGCN forward pass to get E^(0) through E^(L)\n   - Step 2: Generate two random noise matrices and add to E^(L) to get Ē^(L) and Ẽ^(L)\n   - Step 3: Compute aggregated representations F̄ and F̃ by averaging all layer embeddings\n   - Step 4: Apply Squeeze operation (average pooling) to get S̄ and S̃\n   - Step 5: Apply Excitation network (multi-layer MLP with sigmoid) to get T̄ and T̃\n   - Step 6: Recalibrate: R̄ = T̄ ⊙ F̄, R̃ = T̃ ⊙ F̃\n   - Step 7: Compute InfoNCE loss using R̄ and R̃\n   - Step 8: Combine with BPR loss for final optimization", 
   "rank": "rank2"
  },
  {
    "source": "DiffRec_2022",
    "target": "FlowCF_2025",
    "type": "in-domain",
    "similarities": "1. Both papers describe generative frameworks that operate on full interaction matrices, indicating InputType.LISTWISE paradigm is appropriate.\n2. Both describe MLP architectures with timestep conditioning - papers mention sinusoidal timestep embeddings concatenated with input vectors.\n3. Papers describe both methods as predicting target interactions directly (x_0 in DiffRec, X_1 in FlowCF) using MSE loss, rather than predicting noise.\n4. Both discretize the time interval [0,1] into N steps as described in their training procedures.\n5. Training procedures in both papers involve random timestep sampling; inference sections describe starting from observed user interactions.\n6. Both papers note that inference can start from partially noised interactions rather than pure noise, since observed interactions provide meaningful signal.",
    "differences": "1. **Prior Distribution**: DiffRec paper describes Gaussian noise corruption; FlowCF paper proposes Behavior-Guided Prior where X_0 sampled from Bernoulli(f), with f computed from item interaction frequencies.\n2. **State Space**: DiffRec formulas show continuous-valued x_t; FlowCF paper emphasizes discrete binary space, maintaining X_t ∈ {0,1} throughout the process.\n3. **Interpolation Formula**: DiffRec uses noise schedule x_t = sqrt(α_t)x_0 + sqrt(1-α_t)ε; FlowCF specifies probabilistic masking to interpolate between initial and target interaction states.\n4. **Vector Field**: FlowCF paper explicitly defines vector field in Equation 17: v_t = (X_1 - E[X_t]) / (1-t), guiding state updates.\n5. **Inference Update**: DiffRec describes continuous posterior sampling; FlowCF uses discrete update rules for iterative refinement.\n6. **Interaction Preservation**: FlowCF explicitly preserves observed interactions during inference.\n7. **Noise Schedule**: DiffRec discusses schedule design; FlowCF uses uniform discretization without complex scheduling.\n8. **Sampling Efficiency**: FlowCF paper reports good results with 2 sampling steps, starting from later timesteps rather than t=0.",
    "rank": "rank1"
  },
  {
    "source": "RecVAE_2019",
    "target": "FlowCF_2025",
    "type": "in-domain",
    "similarities": "1. As generative recommenders, both likely inherit from GeneralRecommender and may use AutoEncoderMixin pattern for accessing user interaction vectors.\n2. Both papers describe MLP-based architectures: RecVAE explicitly uses encoder/decoder MLPs, FlowCF describes a flow model implemented as MLP.\n3. Both aim to predict scores for all items given user history, requiring full_sort_predict functionality for ranking-based evaluation.\n4. Both target implicit feedback scenarios with sparse user-item matrices, processing users in batches during training.\n5. Both optimize reconstruction-style objectives: RecVAE minimizes reconstruction loss, FlowCF minimizes MSE between predicted and true interactions.",
    "differences": "1. **Prior Distribution**: RecVAE paper describes Gaussian N(0,I) prior; FlowCF paper introduces Behavior-Guided Prior using Bernoulli distribution, with parameters derived from item popularity (interaction count per item / total users).\n2. **Architecture Pattern**: RecVAE describes encoder-decoder with latent bottleneck; FlowCF describes single flow model that conditions on both interaction state and timestep.\n3. **Time Conditioning**: FlowCF paper includes temporal conditioning of the generative process (details of embedding or scaling omitted).\n4. **Interpolation Mechanism**: FlowCF paper describes discretized interpolation using binary mask where each element is sampled probabilistically, giving an interpolated interaction state.\n5. **Loss Function**: RecVAE uses ELBO (reconstruction + KL divergence); FlowCF paper shows simpler MSE loss between predicted and true interactions.\n6. **Inference Pattern**: RecVAE describes single forward pass; FlowCF shows iterative process with discrete update rule and explicitly preserves observed interactions.\n7. **MLP Configuration**: FlowCF experimental details mention configurations such as activation functions and dropout rates (flow models often use simpler settings).",
    "rank": "rank2"
  },
  {
    "source": "LightGCN_2020",
    "target": "NLGCL_2025",
    "type": "in-domain",
    "similarities": "1. Both models build a bipartite user-item graph and use normalized adjacency matrix propagation; the `get_norm_adj_mat()` routine that computes  ̃A = D^{-1/2} A D^{-1/2}  as a sparse tensor can be reused verbatim for the graph-construction module of NLGCL.\n2. Both adopt layer-wise embedding propagation followed by a learnable/weighted aggregation across layers; the `forward()` loop that stacks [E^(0), ̃A E^(0), …, ̃A^K E^(0)] and the `torch.mean(..., dim=1)` reduction pattern can serve as the drop-in base for NLGCL’s multi-layer aggregation after its contrastive part is inserted.\n3. Training pipeline uses pairwise BPR loss with mini-batch Adam; the `calculate_loss()` helper (BPR computation + L2 reg on 0-th layer embeddings) and the `EmbLoss` regulariser can be copied directly—only the positive/negative sampling indices need to be replaced if NLGCL introduces its own hard negatives.\n4. All trainable parameters reside in the 0-th layer embedding tables (`user_embedding`, `item_embedding`); the same parameter initialisation (`xavier_normal_initialization`) and `restore_user_e` / `restore_item_e` caching for full-rank prediction can be preserved.",
    "differences": "1. NLGCL introduces a neighbourhood-aware contrastive learning objective that requires (i) stochastic subgraph extraction per mini-batch, (ii) two separate GNN encoders (online & target) with momentum update, and (iii) in-batch negative queue—none of these exist in LightGCN; new classes `NeighbourSampler`, `MomentumEncoder`, and `ContrastLoss` must be implemented.\n2. LightGCN uses uniform layer weights α_k = 1/(K+1); NLGCL learns layer-wise coefficients (possibly attention-based) that are optimised jointly with the contrastive loss, so the simple `torch.mean` reduction has to be replaced by a learnable `LayerCombine` module returning α_k parameters.\n3. LightGCN’s propagation is purely linear without dropout; NLGCL applies stochastic node/edge dropout during subgraph sampling for data augmentation, requiring new `drop_edge()` and `drop_node()` utilities inside the propagation loop.\n4. Evaluation protocol differs: LightGCN reports static full-rank metrics with cached embeddings, whereas NLGCL needs to compute dynamic representations for each augmented view at test time; thus `full_sort_predict()` must be modified to forward both views and ensemble/average logits.",
    "rank": "rank1"
  },
  {
    "source": "SGL_2021",
    "target": "NLGCL_2025",
    "type": "in-domain",
    "similarities": "1. Both papers build on LightGCN backbone: the provided SGL code already contains the full LightGCN forward pass (sparse graph convolution, layer-mean pooling, user/item embedding split) which can be reused verbatim for the base encoder in NLGCL.\n2. Contrastive InfoNCE loss with temperature τ: the SGL implementation of `calc_ssl_loss()`—cosine normalization, exp-positive numerator, full negative denominator—can be copied and only the positive/negative pair construction logic needs to be replaced.\n3. Multi-task training loop: the SGL `calculate_loss()` that combines BPR, L2 and SSL terms with tunable λ₁, λ₂ already matches the NLGCL joint-training objective; keep the same structure and swap the SSL call.\n4. Sparse adjacency dropout utilities: `_create_augmented_graph()` and `_create_rw_augmented_graph()` generate COO matrices, re-normalise with D^{-0.5}AD^{-0.5} and return PyTorch sparse tensors—reuse these helpers for any graph perturbations NLGCL needs (e.g., view-1/view-2 subgraphs).",
    "differences": "1. NLGCL needs a neighbour-level graph contrastive component (node–neighbour pairs) which SGL lacks; introduce a new `calc_nlgcl_loss()` that samples neighbours per node, builds neighbour-view positives and uses them in InfoNCE—this routine is absent in SGL.\n2. SGL augments once per epoch while NLGCL may require on-the-fly augmentation every mini-batch to generate neighbour views; refactor `graph_construction()` to be called inside each training step instead of only when `train(mode=True)`.\n3. NLGCL introduces learnable neighbour importance weighting (attention or MLP); add a lightweight `NeighbourWeightNet` module and integrate its output into the contrastive denominator—no analogue exists in SGL.\n4. SGL keeps full-batch negatives (`|U|×|U|` matrix) which is tractable for moderate datasets; NLGCL’s neighbour-wise loss can explode memory—implement in-batch negative sampling version of `calc_ssl_loss()` (already sketched in SGL comments) and expose a flag to toggle between full and sampled negatives.",
    "rank": "rank2"
  },
  {
    "source": "DiffRec_2022",
    "target": "LDiffRec_2023",
    "type": "in-domain",
    "similarities": "1. Core diffusion utilities are structurally identical: q_sample, p_mean_variance, p_sample, sample_timesteps, and reweight_loss follow the same schedule definitions and timestep handling.\n2. The denoising DNN keeps the same time embedding strategy (sinusoidal + MLP) and is trained with MSE on x0/eps depending on mean_type.\n3. Training still samples t per batch, injects noise, and backpropagates through the predicted target, matching the DiffRec training loop.\n4. Inference uses the same reverse diffusion loop to produce a full-item score vector for full_sort_predict.",
    "differences": "1. LDiffRec introduces an AutoEncoder stage: Encode -> latent z -> diffusion -> Decode, while DiffRec operates directly on the full item vector.\n2. When n_cate>1, LDiffRec requires item_emb.npy to run k-means and build category_idx/category_map; without it n_cate should be set to 1.\n3. The loss is a weighted sum of diffusion loss and VAE loss (lamda * diff_loss + vae_loss), with separate annealing counters update_count and update_count_vae.\n4. full_sort_predict encodes to latent, calls DiffRec.p_sample, decodes, and (when n_cate>1) rebuilds the full item vector via category_map.\n5. Latent size is controlled by in_dims[-1]; DNN dims must be consistent with latent_size to avoid shape mismatch.",
    "rank": "rank1"
  },
  {
    "source": "MultiVAE_2018",
    "target": "LDiffRec_2023",
    "type": "in-domain",
    "similarities": "1. The VAE Encode path outputs mu/logvar and supports reparameterization; KL computation follows the same formula and reduction pattern.\n2. Encoder/decoder are MLP-based (MLPLayers) with configurable activation and dropout placement.\n3. get_rating_matrix(user) produces the dense interaction vector used by the VAE, matching the data pipeline.\n4. AutoEncoder uses the same xavier initialization routine for linear weights.",
    "differences": "1. LDiffRec adds a diffusion backbone (DNN + timestep embedding + noise schedule) on top of the latent code.\n2. n_cate>1 splits the item vector and uses ModuleList encoders/decoders with category_map; MultiVAE uses a single encoder/decoder.\n3. Training optimizes a joint objective with separate annealing counters (update_count vs update_count_vae) and lamda weighting.\n4. Inference requires reverse diffusion (p_sample) before decoding, not a single decoder pass.\n5. If you implement a custom LDiffRec, keep in_dims/out_dims and anneal_steps/vae_anneal_steps aligned with the reference; swapping to unrelated latent_dimension/total_anneal_steps often degrades Recall@10.",
    "rank": "rank2"
  },
  {
    "source": "RecVAE_2019",
    "target": "LDiffRec_2023",
    "type": "in-domain",
    "similarities": "1. Both model a user interaction vector with a VAE-style encoder/decoder and use KL annealing.\n2. full_sort_predict produces dense scores over all items from the reconstructed vector.\n3. get_rating_matrix and history-based input preparation are compatible with the RecBole AutoEncoderMixin flow.",
    "differences": "1. LDiffRec adds a diffusion chain (q_sample/p_sample/DNN) that RecVAE does not have.\n2. LDiffRec may cluster items and maintain category_map when n_cate>1; RecVAE operates on the full vector.\n3. Loss includes diffusion MSE plus VAE loss with lamda, beyond RecVAE's ELBO-only objective.\n4. Reverse diffusion (p_sample) is required at inference time before decoding, adding iterative denoising steps.\n5. Recall is sensitive to noise_scale/anneal_cap; too small noise or too large KL weight can underfit the ranking signal.",
    "rank": "rank3"
  },
  {
    "source": "CDAE_2015",
    "target": "LDiffRec_2023",
    "type": "in-domain",
    "similarities": "1. Both take a dense user-item interaction vector as input and output full-item scores for ranking.\n2. They rely on vectorized computation and full_sort_predict for Top-K evaluation.",
    "differences": "1. LDiffRec is a two-stage pipeline (VAE + diffusion), while CDAE is a single denoising pass.\n2. LDiffRec requires item_emb.npy for clustering when n_cate>1; CDAE has no external dependency.\n3. LDiffRec inference requires iterative reverse diffusion before decoding, not a single forward pass.\n4. LDiffRec uses KL annealing and diffusion reweighting (update_count/anneal_steps), which are absent in CDAE.",
    "rank": "rank4"
  },
  {
    "source": "MultiVAE_2018",
    "target": "NCEPLRec_2021",
    "type": "in-domain",
    "similarities": "1. Both models operate on the implicit-feedback matrix R∈ℝ^{U×I} and are GeneralRecommender subclasses; NCEPLRec reads dataset.inter_matrix(form='csr') just like MultiVAE uses CSR inputs.\n2. Both cache user/item-side matrices on device for fast prediction; NCEPLRec stores user_embeddings (Q) and item_embeddings (W), similar to how MultiVAE caches decoder-side weights.\n3. Both use a batched matrix multiply for full-sort scoring; NCEPLRec computes user_embeddings @ item_embeddings, then flattens the result.\n4. Both rely on scipy.sparse for efficient row access and preprocessing of R.",
    "differences": "1. MultiVAE trains an encoder/decoder with KL annealing, while NCEPLRec is a closed-form pipeline (build D → randomized_svd → ridge regression) implemented entirely in NumPy/SciPy.\n2. NCEPLRec has no gradient-based loss; calculate_loss returns a dummy zero tensor and all computation happens in __init__.\n3. NCEPLRec builds D row-by-row using popularity reweighting: values = max(log(num_users / pop^beta), 0); this preprocessing is absent in MultiVAE.\n4. full_sort_predict returns a flattened 1D score vector in NCEPLRec, whereas MultiVAE typically returns a 2D [batch, n_items] matrix; reshape may be needed by the evaluator.\n5. Hyper-parameters change from neural keys (mlp_hidden_size, latent_dimension, dropout_prob) to closed-form keys (beta, rank, reg_weight, seed).",
    "rank": "rank1"
  },
  {
    "source": "CDAE_2015",
    "target": "NCEPLRec_2021",
    "type": "in-domain",
    "similarities": "1. Both use the implicit-feedback CSR matrix R and produce scores via user–item dot-products; NCEPLRec builds Q and W from R and then uses matrix multiply at prediction time.\n2. Both compute item popularity statistics from R (item_popularities = R.sum(axis=0)), which are reused in NCEPLRec's reweighting step.\n3. Both expose predict and full_sort_predict APIs for RecBole evaluation, even though NCEPLRec returns flattened full-sort scores.",
    "differences": "1. CDAE trains a denoising auto-encoder with back-prop; NCEPLRec solves a closed-form pipeline (build D → randomized_svd → ridge regression) with no gradient steps.\n2. NCEPLRec has no trainable embeddings; user_embeddings (Q) and item_embeddings (W) are computed once in __init__ and calculate_loss returns a dummy zero tensor.\n3. NCEPLRec reweights interactions with values = max(log(num_users / pop^beta), 0) when building D; CDAE has no such preprocessing.\n4. NCEPLRec is linear and deterministic with hyper-parameters beta/rank/reg_weight, while CDAE uses non-linear activations and corruption ratio.",
    "rank": "rank2"
  },
  {
    "source": "BPR_2009",
    "target": "NCEPLRec_2021",
    "type": "in-domain",
    "similarities": "1. Both methods learn latent user & item embeddings (u_i, v_j) and produce scores via dot-product u_i^T v_j; the BPR code’s nn.Embedding layers and torch.mul(user_e, item_e).sum(dim=1) pattern can be reused verbatim for the final prediction stage of NCEPLRec.\n2. Both are implicit-feedback, one-class CF models that never treat unobserved entries as ground-truth negatives; the pairwise BPR loss and the NCE objective share the same sigmoid σ(x) probability assumption, so the BPRLoss module can serve as a drop-in optimizer if one chooses to fine-tune NCE embeddings with additional BPR iterations.\n3. Training loops rely on mini-batch negative sampling—BPR samples one j′ on-the-fly while NCE pre-computes a popularity-based q(j′); the existing BPR dataloader that yields (u,i,j) triples can be extended by replacing the uniform negative sampler with a popularity-weighted sampler to reproduce NCE sampling without rewriting the data pipeline.\n4. Both papers regularize embeddings with L2 (λ_Θ in BPR, λ in NCE-PLRec); the weight-decay line in BPR’s SGD step (λ_Θ·Θ) is identical to the Ridge penalty used when solving W in NCE-PLRec, so the same regularization hyper-parameter handling code can be reused.",
    "differences": "1. BPR optimizes embeddings by stochastic gradient ascent on σ(ŷ_ui − ŷ_uj); NCEPLRec constructs a popularity-debiased matrix D and then computes embeddings via randomized_svd plus closed-form ridge regression (Q/W).\n2. BPR keeps embeddings in nn.Parameter and updates them online; NCEPLRec stores frozen Q and W tensors and calculate_loss returns a dummy zero tensor (no backprop).\n3. BPR uses uniform/user-frequency negatives; NCEPLRec uses popularity-based reweighting with beta and the log(num_users / pop^beta) transform before building D.\n4. NCEPLRec returns flattened full-sort scores in full_sort_predict, whereas BPR typically returns a 2D score matrix.",
    "rank": "rank3"
  },
  {
    "source": "LightGCN_2020",
    "target": "SGL_2021",
    "type": "in-domain",
    "similarities": "1. The backbone remains LightGCN-style linear propagation over a normalized bipartite graph, with layer-wise embeddings averaged at the end.\n2. Pairwise training still uses BPRLoss and EmbLoss, with USER_ID/ITEM_ID/NEG_ITEM_ID fields.\n3. full_sort_predict uses user_emb @ item_emb.T to produce dense scores for Recall@10.",
    "differences": "1. SGL must call graph_construction in train(mode=True) to build sub_graph1/sub_graph2 each epoch; stale views weaken SSL and hurt recall.\n2. calc_ssl_loss should use the full user/item embedding pools as negatives (batch vs all), mirroring the original formulation; batch-only negatives under-train the SSL signal.\n3. RW mode requires a list of per-layer subgraphs; forward must accept list graphs and apply sparse.mm per layer.\n4. full_sort_predict should return a 2D [batch_size, n_items] score matrix (no flatten) to match RecBole eval.\n5. Caches restore_user_e/restore_item_e should be cleared in calculate_loss to avoid stale embeddings after graph augmentation.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "SGL_2021",
    "type": "in-domain",
    "similarities": "1. Both use sparse graph propagation and mean aggregation of layer outputs.\n2. Pairwise BPR loss and L2 embedding regularization follow the same pattern.\n3. full_sort_predict is a single matmul over cached embeddings.",
    "differences": "1. SGL adds graph_construction to sample two augmented views (ND/ED/RW) every epoch.\n2. InfoNCE should be computed with normalize + temperature (ssl_tau) and all-user/all-item negatives for stable gradients.\n3. full_sort_predict should output a 2D score matrix (not flattened) for Top-K metrics.\n4. RW mode requires layer-wise subgraphs rather than a single adjacency.\n5. If using ED/ND, ensure the augmented graph remains symmetric and normalized with D^-0.5AD^-0.5.",
    "rank": "rank2"
  },
  {
    "source": "NCL_2021",
    "target": "SGL_2021",
    "type": "in-domain",
    "similarities": "1. Both compute contrastive loss with normalized embeddings and temperature scaling.\n2. Layer-wise aggregation is mean pooling to control over-smoothing.\n3. Main task loss is BPR with an added ssl_weight term.",
    "differences": "1. SGL uses ND/ED/RW to construct dual graph views; NCL adds prototype/cluster objectives.\n2. SGL applies SSL to both user and item views; NCL includes extra prototype loss terms.\n3. full_sort_predict should keep 2D output for RecBole evaluation.\n4. Graph views must be refreshed each epoch via train override.\n5. Keep ssl_weight modest; too large can suppress BPR signals and reduce recall.",
    "rank": "rank3"
  },
  {
    "source": "DGCF_2020",
    "target": "SGL_2021",
    "type": "in-domain",
    "similarities": "1. Both construct normalized sparse adjacency and propagate with torch.sparse.mm.\n2. Both use BPRLoss plus EmbLoss on ego embeddings.\n3. Prediction is dense user_emb @ item_emb.T scoring.",
    "differences": "1. SGL adds two augmented views and InfoNCE; DGCF has no contrastive branch.\n2. RW mode requires per-layer subgraphs and a forward path that accepts list graphs.\n3. full_sort_predict should not flatten outputs for Top-K evaluation.\n4. Regenerate subgraphs each epoch and clear cached embeddings before loss computation.\n5. Keep drop_ratio moderate (e.g., 0.1-0.2) to avoid destroying graph structure.",
    "rank": "rank4"
  },
  {
    "source": "MultiVAE_2018",
    "target": "SGL_2021",
    "type": "in-domain",
    "similarities": "1. Both need dense full-sort scores for Recall@10 evaluation.\n2. Vectorized matmul is used for efficient all-item scoring.",
    "differences": "1. SGL is graph + contrastive learning; MultiVAE is reconstruction-based VAE.\n2. SGL relies on sparse adjacency, not dense user vectors.\n3. full_sort_predict should output a 2D score matrix to match RecBole's evaluator.\n4. Graph augmentation should be refreshed each epoch to keep SSL effective.\n5. InfoNCE temperature and ssl_weight should be tuned jointly to avoid collapsing to MF-only behavior.",
    "rank": "rank5"
  },
  {
    "source": "SGL_2021",
    "target": "SimpleX_2021",
    "type": "in-domain",
    "similarities": "1. Both models use learnable user/item embeddings (nn.Embedding) with identical initialization (xavier_uniform_initialization) and regularization via EmbLoss; the embedding lookup and L2 regularization code in SGL.calculate_loss() can be directly reused for SimpleX.\n2. Both follow a pairwise training paradigm (InputType.PAIRWISE) and implement the same RecBole abstract class GeneralRecommender, so the dataset interfacing, batching, and train/validation loop scaffolding in SGL is 100 % reusable.\n3. Both compute (normalized) similarity scores between user and item vectors and return scalar prediction logits; the cosine_similarity helper in SimpleX can replace the dot-product scores in SGL.predict() and full_sort_predict() with minimal change.\n4. Both models cache full item embeddings for fast full-sort evaluation (restore_user_e / restore_item_e tensors); the caching pattern and device placement code in SGL can be copied verbatim.\n5. Both expose the same public API (calculate_loss, predict, full_sort_predict) and inherit the same config-driven hyper-parameter loading style, so the outer training script and config files need no modification.",
    "differences": "1. SGL is graph-based: it builds a sparse normalized adjacency matrix, performs multi-layer graph convolution (torch.sparse.mm in forward()), and augments the graph each epoch (graph_construction()); SimpleX has no graph module, so the entire SparseL handling, _create_augmented_graph(), and subgraph re-sampling logic must be removed.\n2. SimpleX introduces a behavior-sequence aggregation stage (_build_history_items, aggregate_behavior) that SGL lacks; a new data-preprocessing routine to build padded history tensors and three aggregation sub-modules (mean, self_attn, user_attn) must be implemented from scratch.\n3. Loss functions differ: SGL combines BPRLoss + SSL contrastive loss (InfoNCE over two views), whereas SimpleX uses the margin-based Cosine Contrastive Loss (CCL) with weighted negative terms; the calc_ssl_loss() and BPRLoss invocation in SGL must be replaced by a new calculate_ccl_loss() that clamps negative similarities w.r.t. margin m and applies the negative_weight / |N| scaling.\n4. SGL’s final user representation is obtained by graph propagation; SimpleX fuses a static user embedding with an aggregated behavior vector via a learnable gate (gamma * e_u + (1-gamma)*V p_u); the forward() method needs to be rewritten to (a) call aggregate_behavior, (b) apply the V linear transform, and (c) perform the weighted fusion—none of these steps exist in SGL.\n5. SimpleX requires two extra tensors per user (history_item_id, history_item_len) injected into the model, so the dataset collate function and data loader must be extended to supply these fields, whereas SGL only needs the standard interaction matrix.",
    "rank": "rank1"
  },
  {
    "source": "DGCF_2020",
    "target": "SimpleX_2021",
    "type": "in-domain",
    "similarities": "1. Both models adopt embedding-based matrix-factorization backbone: user/item look-up tables (`nn.Embedding`) of identical shape and Xavier init; DGCF’s `_get_ego_embeddings()` and SimpleX’s `user/item_embedding` can share the same memory layout—reuse the tensor creation, device placement, and `full_sort_predict()` caching pattern directly.\n2. Both are pairwise-training recommenders inheriting `GeneralRecommender` with identical `InputType.PAIRWISE` and the same BPR-style mini-batch fields (`USER_ID`, `ITEM_ID`, `NEG_ITEM_ID`); the data-loader, negative sampler and batch loop in DGCF can be dropped into SimpleX unchanged.\n3. L2-normalized cosine similarity is the final scoring function—DGCF already applies `F.normalize` inside `create_distance_correlation` and SimpleX uses the same normalize-then-dot pattern in `cosine_similarity`; the normalization utilities and numerical-stable cosine ops can be copied verbatim.\n4. Regularization & loss aggregation pattern: both compute task loss (`BPRLoss` vs `CCL`) plus `EmbLoss` on ego embeddings with a single coefficient (`reg_weight`); the loss reduction, backward call and optimizer setup code in DGCF’s `calculate_loss` can serve as template.",
    "differences": "1. SimpleX replaces DGCF’s multi-intent GNN encoder with a behavior-sequence aggregation layer (`aggregate_behavior`) that has no graph propagation; the entire `build_matrix`, sparse adjacency tensors (`edge2head_mat`, etc.), iterative routing loops (`n_iterations`, `n_factors`) and layer combination logic must be removed and substituted by sequence padding + attention (mean/self/user-attention) modules.\n2. Loss function changes from BPR to Cosine Contrastive Loss: new hyper-parameters `margin`, `negative_weight`, and the margin-based hinge inside `calculate_ccl_loss` need to be implemented—DGCF has no concept of margin or weighted negative averaging.\n3. User representation is no longer a single embedding lookup: SimpleX fuses a user ID vector with a transformed behavior pooled vector via learnable gate `gamma` and linear transform `V`; the fusion code (`forward`) and the extra `nn.Linear` parameters are absent in DGCF and must be added.\n4. Negative sampling strategy differs: DGCF draws one negative per positive (standard BPR), while SimpleX supports many negatives per positive (`n_negs` tensor of shape `[batch_size, n_negs]`) and averages their hinge losses with weight `w`; the negative item tensor handling inside `calculate_ccl_loss` and corresponding data-loader collation need extension.",
    "rank": "rank2"
  },
  {
    "source": "LightGCN_2020",
    "target": "SimpleX_2021",
    "type": "in-domain",
    "similarities": "1. Both models inherit from `GeneralRecommender` and use `InputType.PAIRWISE`, enabling direct reuse of the base training loop, data loader integration, and evaluation routines from LightGCN’s `train()` and `full_sort_predict()` pipelines.\n2. Embedding tables (`nn.Embedding(num_users, latent_dim)`, `nn.Embedding(num_items, latent_dim)`) are initialized identically with `xavier_uniform_initialization`; the ego-embedding extraction pattern (`user_embedding.weight`, `item_embedding.weight`) can be copied verbatim.\n3. L2-regularisation on 0-th layer embeddings is implemented through the same `EmbLoss()` helper and added to the total loss with a configurable `reg_weight`, so the regularisation term in `calculate_loss()` can be transplanted without change.\n4. The pairwise negative-sampling minibatch structure (`[user, pos_item, neg_item]`) and the `interaction` dictionary keys (`USER_ID`, `ITEM_ID`, `NEG_ITEM_ID`) are identical; hence LightGCN’s batch-generation and GPU-transfer code can be reused.\n5. For fast full-rank inference both models cache the complete item embedding matrix (`self.restore_item_e`) and perform a single matrix multiplication; the caching logic and `full_sort_predict()` routine can be lifted from LightGCN with only the similarity function swapped.",
    "differences": "1. SimpleX replaces the multi-layer graph convolution of LightGCN with a behaviour-sequence aggregation module (`aggregate_behavior`) that requires building a padded history matrix (`history_item_id, history_item_len`) and implementing three attention/pooling strategies—this entire component is absent in LightGCN and must be written from scratch.\n2. LightGCN uses inner-product prediction and BPR loss; SimpleX adopts cosine similarity plus the Cosine-Contrastive Loss (CCL) with margin `m` and negative weight `w`. A new `calculate_ccl_loss()` routine, L2-normalisation of embeddings before similarity computation, and the margin-based hinge on negatives have to be implemented.\n3. SimpleX fuses the raw user embedding with the behaviour-pooled vector through a learnable gate `gamma` and a linear transform `V`; the fusion step (`Equation 5`) is not present in LightGCN and needs an extra `nn.Linear` layer and corresponding forward pass.\n4. LightGCN’s adjacency-matrix normalisation (`get_norm_adj_mat`) and sparse graph propagation (`torch.sparse.mm`) are completely removed in SimpleX; therefore the whole graph-construction preprocessing and the `norm_adj_matrix` buffer can be deleted, but the freed memory must be replaced by the history-sequence buffer and attention parameters.",
    "rank": "rank3"
  },
  {
    "source": "EASE_2019",
    "target": "SimpleX_2021",
    "type": "in-domain",
    "similarities": "1. Both use item-item co-occurrence statistics: EASE computes G=XᵀX in `_train_ease()`, SimpleX can re-use the same CSR matrix construction (`dataset.inter_matrix(form=\"csr\")`) to build user history sequences in `_build_history_items()`.\n\n2. Shared sparse-matrix infrastructure: the `interaction_matrix` attribute and CSR handling in EASE can be directly reused by SimpleX for efficient user-history look-ups instead of building history from raw interaction lists.\n\n3. Both models isolate the pure scoring logic from RecBole boiler-plate: EASE puts the closed-form solution in `_train_ease()`; SimpleX can analogously isolate CCL computation in `calculate_ccl_loss()` and cosine similarity in `cosine_similarity()` for unit testing and reproducibility.\n\n4. Common evaluation path: `predict()` and `full_sort_predict()` in EASE already demonstrate how to map user/item indices to rows of the interaction matrix; SimpleX can reuse the same indexing pattern to fetch user history tensors and pre-computed item embeddings.",
    "differences": "1. EASE is a closed-form linear auto-encoder with NO learnable embeddings; SimpleX requires trainable `user_embedding` and `item_embedding` tables—completely new `nn.Embedding` layers must be added.\n\n2. EASE needs only point-wise data (`InputType.POINTWISE`); SimpleX trains with pairwise negative sampling (`InputType.PAIRWISE`)—the data-loader must provide `NEG_ITEM_ID` fields and the training loop must sample negatives, none of which exist in EASE.\n\n3. EASE has no aggregation component; SimpleX must implement three attention/pooling mechanisms (`aggregate_behavior()` with `mean`/`self_attn`/`user_attn`) plus a fusion gate (Eq. 5)—these modules are absent in EASE.\n\n4. EASE uses squared-loss on reconstructed X; SimpleX optimises the Cosine-Contrastive-Loss with margin and negative weight—`calculate_ccl_loss()` and its margin-clamped negative term must be written from scratch.",
    "rank": "rank4"
  },
  {
    "source": "RecVAE_2019",
    "target": "SimpleX_2021",
    "type": "in-domain",
    "similarities": "1. Both models learn user/item embeddings in a shared latent space; RecVAE’s encoder output (mu) can be reused as the user representation module in SimpleX, and the item embedding matrix (decoder.weight) is directly transferable.\n2. Both exploit the user–item interaction matrix: RecVAE’s get_rating_matrix() and the history-building routine in SimpleX._build_history_items() iterate over the same (user, item) tuples—history construction code can be copied and only the aggregation step added.\n3. Both adopt negative sampling for implicit feedback; RecVAE’s pairwise InputType and mini-batch sampler can be kept—SimpleX only needs to add a NEG_ITEM_ID field and feed negatives into its CCL routine.\n4. Both use L2-normalised vectors before the final score: RecVAE’s F.normalize in predict() mirrors SimpleX’s F.normalize inside cosine_similarity(); the same in-batch normalisation utilities can be shared.\n5. Both expose full_sort_predict() for efficient all-item ranking; the caching pattern (restore_user_e / restore_item_e) is identical and can be refactored into a common base mixin.",
    "differences": "1. SimpleX replaces the VAE likelihood + KL objective with a Cosine Contrastive Loss (CCL); the entire ELBO calculation, KL-weight scheduling (gamma), composite prior, and reparameterisation code in RecVAE must be deleted and replaced by a new calculate_ccl_loss() module that implements margin-based negative sampling.\n2. SimpleX adds a behaviour-sequence aggregation layer (mean/self-att/user-att) that is absent in RecVAE; a new aggregate_behavior() path with attention weights (W1/W2, query vector) and mask handling has to be implemented from scratch.\n3. SimpleX fuses two user views (ID embedding + aggregated history) via a learnable gate g and transformation matrix V; RecVAE has no fusion layer—new forward() logic and parameters (V, gamma) must be added.\n4. SimpleX requires L2-normalised cosine similarity as the raw score, whereas RecVAE uses softmax(logits) for multinomial likelihood; the final score head (predict / full_sort_predict) must switch from softmax probability to cosine similarity without temperature scaling.\n5. SimpleX uses pure embedding regularisation (EmbLoss) while RecVAE relies on KL regularisation and denoising dropout; the loss factory must drop KL-weight schedules and plug in the EmbLoss module plus the CCL term.",
    "rank": "rank5"
  },
  {
    "source": "FISM_2013",
    "target": "ItemKNN_2001",
    "type": "in-domain",
    "similarities": "1. Both are item-centric top-N recommenders for implicit feedback (0/1 matrix); reuse FISM's csr interaction-matrix loader (`dataset.inter_matrix(form='csr')`) and history builder (`get_history_info`) to feed ItemKNN's `ComputeSimilarity`.\n2. Both need fast sparse similarity search; port FISM's block-wise matrix multiplication (`user_forward` with `torch.bmm`) to implement ItemKNN's `ComputeSimilarity.compute_similarity` by replacing cosine logic with the α-asymmetric formula and keeping the same `block_size` loop and `argpartition` top-k selection.\n3. Both expose a `full_sort_predict` that scores every item for a user; reuse FISM's CPU→GPU transfer pattern (`torch.from_numpy(...).to(self.device)`) and user-row slicing (`pred_mat[user,:]`) in ItemKNN's `full_sort_predict` to avoid rewriting the evaluation pipeline.",
    "differences": "1. ItemKNN needs an explicit α-asymmetric cosine module (`S_α(i,j)`); FISM has no such similarity kernel—implement a new `AsymmetricCosine` class that computes `|U(i)∩U(j)|/(|U(i)|^α |U(j)|^(1-α))` in sparse CSR form and returns top-k neighbors and weight matrix.\n2. ItemKNN is non-learned (no embeddings, no SGD); delete FISM's `item_src/dst_embedding`, `bceloss`, `reg_loss`, and the entire `calculate_loss`/`forward` training graph; replace with a lightweight `ComputeSimilarity` constructor that pre-computes and stores the static `w` matrix.\n3. ItemKNN supports both item-based and user-based variants (`knn_method` flag) plus locality power `q`, calibration, and hybrid aggregation—none exist in FISM; add config flags and post-processing steps (`f(w)=w^q`, `C(ˆr_ui)`) that operate on the static score matrix after similarity computation.\n4. ItemKNN requires shrinkage and α hyper-parameters in the similarity denominator; FISM uses α only for normalization power—extend the config parser to accept `shrink`, `alpha`, `q`, `theta`, `beta` and wire them into the new `AsymmetricCosine` and calibration routines.",
    "rank": "rank3"
  },
  {
    "source": "CDAE_2015",
    "target": "ItemKNN_2001",
    "type": "in-domain",
    "similarities": "1. Both models consume the same sparse user-item interaction matrix (CSR format) and output a dense score matrix for top-N ranking; the `interaction_matrix` attribute and `dataset.inter_matrix(form='csr')` call in ItemKNN can be directly copied from CDAE’s `get_rating_matrix()` utility.\n2. Top-N recommendation logic is identical: compute a score vector per user and pick the largest values among un-consumed items; CDAE’s `full_sort_predict` already materializes this vector via `self.o_act(predict)` and can be reused as-is once ItemKNN’s `pred_mat` is built.\n3. Negative/unseen item masking is already implemented in CDAE (via the history mask inside `get_rating_matrix`); the same mask can be applied to zero-out already-interacted entries in ItemKNN’s `pred_mat` before final ranking.\n4. Both papers support item-centric computation and can exploit the same sparse linear algebra backend (SciPy CSR/CSC) for batch similarity and prediction; the `ComputeSimilarity` class in ItemKNN can inherit CDAE’s device-aware tensor conversion pattern for GPU compatibility.",
    "differences": "1. CDAE is a parametric model that learns latent representations via gradient descent, whereas ItemKNN is non-parametric and requires pre-computing an explicit item-item similarity matrix; the entire `ComputeSimilarity` block and the `w` sparse matrix construction are new and have no counterpart in CDAE.\n2. ItemKNN needs asymmetric cosine with tunable α and shrinkage, plus optional power-law locality adjustment (w^q); these similarity transformations are absent in CDAE, so a new `asymmetric_cosine` kernel and `shrink` logic must be coded.\n3. ItemKNN supports two prediction modes (item-based and user-based) and ranking aggregation (Borda/linear/stochastic); CDAE only has one forward pass, so new aggregation utilities and a `knn_method` flag must be added.\n4. Calibration step (score rescaling via item-specific mean/max statistics) is unique to ItemKNN and requires an extra pass over the training data to compute per-item m_i and M_i; this calibration layer has no equivalent in CDAE and must be implemented from scratch.",
    "rank": "rank4"
  },
  {
    "source": "MultiVAE_2018",
    "target": "ItemKNN_2001",
    "type": "in-domain",
    "similarities": "1. Both models operate on the same implicit-feedback click matrix of shape (U×I) stored as CSR sparse matrix; the source’s self.interaction_matrix can be reused verbatim, including the dataset.inter_matrix(form='csr') loader.\n2. Top-N recommendation is produced by a full dense score vector over all items; the target’s full_sort_predict() can reuse the source’s pattern of returning a 1-D torch tensor created by .toarray().flatten() followed by torch.from_numpy(...).to(device).\n3. The training pipeline in RecBole (calculate_loss → predict → full_sort_predict) is identical; the dummy Parameter(torch.zeros(1)) loss trick used in ItemKNN is already present in MultiVAE’s code base so the same training loop and evaluator can be used without change.\n4. GPU/CPU dispatch and batch-inference plumbing (interaction[self.USER_ID] indexing, .cpu().numpy(), tensor re-upload) are already implemented in MultiVAE and can be copied to ItemKNN as-is.\n5. Hyper-parameter configuration mechanism (config['k'], config['shrink'], etc.) and the xavier_normal_initialization utility are shared infrastructure; the config file reader and logger remain unchanged.",
    "differences": "1. ItemKNN needs a new ComputeSimilarity class that calculates asymmetric cosine Sα with α parameter and shrink term; this entire module is absent in MultiVAE which instead learns a neural encoder/decoder.\n2. The similarity matrix W must be pre-computed once and stored as a sparse CSC matrix; MultiVAE has no such offline phase—implement a build_similarity_matrix() routine and add it to the constructor.\n3. Prediction in ItemKNN is a sparse matrix-vector product (pred_mat = X·W), whereas MultiVAE performs a dense forward pass through MLPs; replace the forward() method with a lightweight lookup that indexes the cached pred_mat.\n4. Hyper-parameters k (neighborhood size), α (asymmetric cosine exponent), q (locality power), and β (prediction asymmetry) do not exist in MultiVAE; extend the config dictionary and __init__ signature to accept them.\n5. ItemKNN requires no gradient-based training, so remove optimizer instantiation, annealing schedule, KL-loss terms, and dropout from the original MultiVAE implementation; keep only the parameter tensor for API compatibility.",
    "rank": "rank5"
  },
  {
    "source": "NGCF_2019",
    "target": "BPR_2009",
    "type": "in-domain",
    "similarities": "1. Both models adopt the identical BPR pairwise ranking loss (Equation 11 in NGCF vs. Equation 4 in BPR) with sigmoid and L2-regularization; the `BPRLoss()` class in NGCF can be reused verbatim for BPR training.\n2. The final prediction layer is the same inner-product interaction: `torch.mul(u_e, i_e).sum(dim=1)` appears in both `calculate_loss()` and `predict()`, so NGCF’s scoring code can be copied directly.\n3. Embedding look-up tables (`nn.Embedding` for users/items) and xavier initialization pattern are identical; the `get_user_embedding()` / `get_item_embedding()` helper pattern in BPR is already present in NGCF’s `get_ego_embeddings()`, enabling drop-in reuse of the embedding access logic.\n4. Both use the same pairwise sampling dataloader (`InputType.PAIRWISE`) and mini-batch Adam training loop; the training harness and batch tuple `(user, pos_item, neg_item)` construction in NGCF’s `calculate_loss()` can be reused without change.",
    "differences": "1. BPR has NO graph propagation: remove the entire `GNNlayers` module, `norm_adj_matrix`, `BiGNNLayer`, `SparseDropout`, `eye_matrix`, and the multi-layer LeakyReLU propagation loop; the forward pass reduces to a single embedding lookup.\n2. BPR does NOT concatenate multi-hop representations: eliminate the `embeddings_list` and final `torch.cat()` along dim=1; instead return the raw embeddings straight from `user_embedding.weight` and `item_embedding.weight`.\n3. BPR needs no dropout regularization beyond L2: delete `message_dropout`, `node_dropout`, `self.emb_dropout`, and the `F.normalize()` calls; only keep weight-decay in the optimizer.\n4. BPR’s `full_sort_predict()` can be simplified to a single matrix multiplication `user_e @ all_item_e.T` without caching layer-wise propagated tensors, so the `restore_user_e` / `restore_item_e` caching logic in NGCF should be removed to save memory.",
    "rank": "rank1"
  },
  {
    "source": "MultiVAE_2018",
    "target": "CDAE_2015",
    "type": "in-domain",
    "similarities": "1. Both models use a user-wise auto-encoder pipeline: encoder maps a sparse user–item vector x_u to a latent z_u, decoder maps z_u back to the full item space—reuse MultiVAE’s get_rating_matrix() batching and the xavier_normal_initialization() pattern.\n2. They share an identical top-N prediction routine: rank all unscored items by the decoder logits and return the largest values—MultiVAE.full_sort_predict() can be copied almost verbatim; only the forward() call has to be exchanged.\n3. Training loops are user-sampled SGD with negative implicit feedback handled inside the loss—keep MultiVAE’s batch iterator and optimizer block, simply replace the loss computation line.\n4. Both implementations keep the user ID as an integer tensor and materialise the binary rating vector on-the-fly—reuse the history_items lookup table and the collate_fn logic.",
    "differences": "1. CDAE adds a user-specific embedding h_user that is summed to the item projection before the hidden activation—this nn.Embedding layer is absent in MultiVAE; create it and inject the user index into forward().\n2. CDAE uses input dropout as the ‘corruption’ process instead of VAE’s reparametrized Gaussian noise—remove the reparameterize() method and insert nn.Dropout(p=corruption_ratio) on the input rating vector.\n3. Loss function changes from KL-regularised multinomial likelihood to plain reconstruction loss (MSE/BCEWithLogitsLoss) plus L1/L2 weight decay—delete the KL term computation and add the explicit reg_weight_* penalties on h_user.weight and h_item.weight.\n4. Output activation is configurable (sigmoid/ReLU) and applied after the decoder logits—introduce self.o_act and apply it inside predict() and full_sort_predict(); MultiVAE has no final activation.\n5. Latent dimension in CDAE equals embedding_size, whereas MultiVAE splits the MLP output into μ and logσ—adapt the encoder MLP so its last layer outputs embedding_size values directly.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "CDAE_2015",
    "type": "in-domain",
    "similarities": "1. Both models learn user/item embeddings via matrix factorization-style look-up tables (NGCF: `user_embedding`/`item_embedding`; CDAE: `h_user`/`h_item`) that can be directly reused—copy the `nn.Embedding` initialization and Xavier-normal initialization pattern from NGCF.\n2. Both employ a reconstruction objective with L2 regularization on embedding weights; NGCF’s `EmbLoss` and `reg_weight` can be reused—simply adapt the regularization coefficients to CDAE’s `reg_weight_1`/`reg_weight_2`.\n3. Both use element-wise activation functions (LeakyReLU in NGCF, Sigmoid/ReLU/Tanh in CDAE); the activation factory pattern in NGCF’s `forward` loop can be ported to CDAE’s `h_act`/`o_act`.\n4. Both support dropout for denoising—NGCF’s `SparseDropout` on the adjacency and `nn.Dropout` on embeddings can be repurposed for CDAE’s input corruption (`self.dropout` with `corruption_ratio`).",
    "differences": "1. CDAE needs a full user-item rating vector as input (`x_items` of shape `[batch, n_items]`); NGCF never materializes dense rating vectors—implement a `get_rating_matrix` helper that builds a binary sparse tensor from user histories.\n2. CDAE is a single hidden-layer auto-encoder (`h_item`→`embedding_size`→`out_layer`) without graph propagation; remove NGCF’s `GNNlayers` loop and `BiGNNLayer` entirely and replace with one `nn.Linear` encoder and one `nn.Linear` decoder.\n3. CDAE trains point-wise (MSE/BCE over observed + sampled negatives) while NGCF trains pair-wise (BPR); replace NGCF’s `BPRLoss` with `nn.MSELoss` or `nn.BCEWithLogitsLoss` and switch dataloader from `PAIRWISE` to `POINTWISE`.\n4. CDAE requires user-specific bias vector `V_u` injected into the hidden pre-activation; add `h_user` embeddings to the encoder output (`torch.add(h_u, h_i)`)—this additive user vector is absent in NGCF.",
    "rank": "rank2"
  },
  {
    "source": "NGCF_2019",
    "target": "DMF_2015",
    "type": "in-domain",
    "similarities": "1. Both embed users/items into latent space via learnable matrices (reuse `nn.Embedding` initialization and `xavier_normal_initialization` from NGCF; the embedding look-up table pattern in `get_ego_embeddings` can be adapted to build DMF’s rating-vector inputs).\n2. Both adopt multi-layer non-linear transformations with dropout for representation refinement (NGCF’s `BiGNNLayer`+`LeakyReLU`+`emb_dropout` pipeline maps directly to DMF’s `MLPLayers` with `ReLU` and `dropout`; reuse the `MLPLayers` class already defined in DMF stub).\n3. Both optimize with mini-batch Adam and L2 regularization (copy NGCF’s `BPRLoss`/`EmbLoss` pattern; DMF uses `BCEWithLogitsLoss` but the `reg_loss` utility and `reg_weight` scheduling remain identical).\n4. Matrix-form sparse propagation logic in NGCF (`get_norm_adj_mat`, `SparseDropout`) can be recycled to construct DMF’s sparse rating vectors `history_user_matrix`/`history_item_matrix` efficiently on GPU.",
    "differences": "1. DMF takes entire rating vectors (`Y_i*`, `Y_*j`) as raw input, requiring a dense-sparse conversion layer (`user_linear`/`item_linear`) that NGCF does not have; implement `get_user_embedding`/`get_item_embedding` to scatter rating values into fixed-size tensors.\n2. DMF uses cosine-similarity (later replaced by dot-product) for final prediction, whereas NGCF concatenates multi-hop embeddings and uses simple dot-product; remove NGCF’s layer-wise stacking and concatenation (`embeddings_list`) and directly output the last MLP layer.\n3. DMF optimizes a point-wise normalized cross-entropy loss over explicit ratings, while NGCF uses pair-wise BPR loss; replace `BPRLoss` with `BCEWithLogitsLoss` and feed normalized ratings (`label/max_rating`) instead of triplets `(u,i,j)`.\n4. DMF does not propagate information on a graph, so no adjacency matrix, Laplacian normalization, or message-passing loops are needed; strip out `norm_adj_matrix`, `GNNlayers`, and `SparseDropout` from NGCF pipeline.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "ENMF_2011",
    "type": "in-domain",
    "similarities": "1. Both models inherit from `GeneralRecommender`, use `xavier_normal_initialization`, keep embeddings in `nn.Embedding` tables and expose `embedding_size`, `reg_weight` config keys—reuse the entire constructor boilerplate and `reg_loss()` computation from NGCF.\n2. They adopt the same latent-factor dot-product interaction: `torch.mul(u_e, i_e)` followed by a linear projection (`self.H_i` in ENMF vs. concatenated layer-wise embeddings⋅h in NGCF); the element-wise product kernel is already coded in NGCF’s `BiGNNLayer` and can be extracted into a shared utility.\n3. Training loops rely on RecBole’s point-wise/pair-wise data loader; `calculate_loss()` returns a single scalar—keep the mini-batch Adam orchestration, early-stop hooks and GPU transfer logic from NGCF unchanged.\n4. Matrix pre-processing routines (`get_norm_adj_mat`, `get_eye_mat`) show how to build sparse tensors on GPU; the same sparse tensor utilities can be kept for ENMF when it needs to cache Gram matrices `item_sum` and `user_sum`.\n5. Both expose `predict()` and `full_sort_predict()` with identical signature—copy the vectorised all-item scoring pattern (`torch.matmul(u_e, all_i_e.t())`) from NGCF’s `full_sort_predict` into ENMF to keep fast evaluation.",
    "differences": "1. ENMF is a non-sampling (whole-data) model: its loss is a closed-form weighted squared loss over |U|×|V| entries, so implement the efficient reformulation that decouples `negative_weight * (h hᵀ)⊙(PᵀP)⊙(QᵀQ)`—a new `compute_gram()` helper and Gram caching mechanism must be added; NGCF never materialises such global statistics.\n2. ENMF requires access to the entire item set per user batch to compute `∑_v c_v⁻ q_v q_vᵀ`; add `dataset.history_item_matrix()` (already present in ENMF code) and build a dense padding mask—NGCF only uses the local neighbourhood given by the Laplacian.\n3. Dropout is applied to the latent vector before interaction in ENMF (`self.dropout(user_embedding)`), whereas NGCF applies message dropout after GNN aggregation; insert `nn.Dropout` on the user/item embedding outputs and disable it during `predict`.\n4. Loss function changes from BPR (`BPRLoss`) to a regression term plus regularisation; remove `BPRLoss` instantiation and implement `calculate_loss()` with the two Gram-based trace terms and the observed-entry MSE part as derived in the paper.\n5. No graph propagation: strip out `GNNlayers`, `SparseDropout`, `norm_adj_matrix`, `eye_matrix` and layer-wise embedding list; ENMF uses a single user/item embedding table and a single prediction vector `H_i`.",
    "rank": "rank2"
  },
  {
    "source": "NGCF_2019",
    "target": "FISM_2013",
    "type": "in-domain",
    "similarities": "1. Both models learn latent factor matrices P and Q for items; NGCF's item embedding table `item_embedding.weight` can be reused as FISM's `item_src_embedding` and `item_dst_embedding` after splitting into two matrices.\n2. Both use inner-product interaction (NGCF in Eq.10, FISM in Eq.1) – NGCF's `torch.mul(u,e).sum(dim=1)` in `predict()` is directly reusable for FISM's score computation.\n3. Both adopt L2 regularization on embeddings; NGCF's `EmbLoss` utility can be reused for FISM's `reg_loss()` without change.\n4. Both support mini-batch training with negative sampling; NGCF's `BPRLoss` can be swapped for FISM's `BCEWithLogitsLoss` by only replacing the loss constructor in `calculate_loss()`.\n5. Sparse adjacency matrix construction (`get_norm_adj_mat`) and history-item lookup (`history_item_matrix`) in NGCF can be reused to build FISM's user history tensors.",
    "differences": "1. FISM needs two separate item embedding matrices (P and Q) while NGCF keeps one; create `item_src_embedding` and `item_dst_embedding` and remove user embeddings entirely.\n2. FISM requires user-specific normalization factor `(n_u^+)^{-alpha}` computed on-the-fly; implement `torch.pow(history_lens, -alpha)` in `inter_forward()` – NGCF has no such per-user scaling.\n3. FISM excludes the target item from its own neighborhood (Eq.2); add a masking step to zero out the diagonal entry in the user history tensor before summation.\n4. FISM supports both pointwise (BCE) and pairwise (BPR) losses, whereas NGCF only implements BPR; add a BPR variant of `calculate_loss()` that samples negatives and uses margin loss.\n5. FISM's `full_sort_predict()` slices items into chunks (`split_to`) to fit GPU memory; replicate this chunked evaluation loop which is absent in NGCF's single-matrix implementation.",
    "rank": "rank1"
  },
  {
    "source": "LightGCN_2020",
    "target": "LINE_2015",
    "type": "in-domain",
    "similarities": "1. Both models learn low-dimensional embeddings (user/item) via bipartite interaction graph; reuse LightGCN's `user_embedding` & `item_embedding` nn.Embedding tables and Xavier init for LINE's vertex & context embeddings.\n2. Training objective is negative-sampling-based pairwise ranking: LightGCN's `BPRLoss` with σ(pos-neg) can be directly substituted for LINE's `NegSamplingLoss`; the `calculate_loss` mini-batch construction (user, pos, neg) is identical—no rewrite needed.\n3. Edge list & COO sparse matrix utilities (`interaction_matrix`, `dataset.inter_feat`) used by LightGCN for norm-adj can be reused to hold the weighted edge list LINE needs; the alias-table sampler for weighted edge sampling can be plugged into the same data-loader pipeline.\n4. Prediction layer is inner-product ⟨e_u,e_i⟩; `predict()` and `full_sort_predict()` routines in LightGCN can be copied verbatim—only tensor names change from user/item to vertex/context.\n5. L2 regularization on 0-th embeddings (`reg_loss` + `reg_weight`) is already implemented; LINE's first-order objective uses the same weight decay on vertex vectors, so the existing regularization hook is reusable.",
    "differences": "1. LINE needs TWO embedding matrices per entity (vertex + context); extend LightGCN's `user_embedding` → (`user_embedding`, `user_context_embedding`) and likewise for items—add two extra nn.Embedding tables.\n2. LightGCN propagates through multi-hop normalized adjacency (LGC layers); LINE has NO propagation—remove the entire `forward()` loop over `n_layers` and the `norm_adj_matrix`; embeddings are used raw after lookup.\n3. Loss must switch between first-order (symmetric, u⋅v) and second-order (conditional, u⋅v′) scores inside one batch; implement a field-aware `context_forward(h,t,field)` and a stochastic selector as shown in LINE code—LightGCN's single `forward` is insufficient.\n4. Negative sampling distribution: LightGCN uses uniform item negatives; LINE draws from degree^(3/4) noise distribution and also samples negatives for the context role. Build a `P_n(v) ∝ d_v^(3/4)` alias sampler and integrate it into the existing `sampler()` routine.\n5. Weighted edge handling: LightGCN binarizes interactions; LINE keeps original weights for KL objective. Add edge-weight tensor to the dataset and multiply log-sigmoid terms by w_ij before reduction in `calculate_loss`.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "LINE_2015",
    "type": "in-domain",
    "similarities": "1. Both models learn user/item embeddings via look-up tables (nn.Embedding) and adopt the same pairwise-training interface (InputType.PAIRWISE) with identical train/neg-item triples; NGCF’s user_embedding & item_embedding modules can be reused verbatim for LINE’s first-order part.\n2. Negative sampling & BPR-style logistic loss: NGCF’s BPRLoss(sigmoid(y_ui − y_uj)) is structurally identical to LINE’s NegSamplingLoss(log σ(y_pos) + log σ(−y_neg)); the loss computation and mini-batch sampling loops can be copied with only variable renaming.\n3. Alias-table-based edge sampler is already implemented in RecBole’s SamplerMixin; NGCF’s sparse_dropout and SparseDropout utilities can be dropped into LINE to obtain weighted edge sampling without extra coding.\n4. Prediction heads are pure inner-product (torch.mul(u,e).sum); full-sort_predict matrix-multiply trick is identical—reuse the torch.matmul line unchanged.\n5. Xavier init, L2 reg helper (EmbLoss), and get_used_ids() negative-pool builder are already coded and can be imported directly.",
    "differences": "1. LINE needs TWO embeddings per node (u_i and u'_i context vectors) when order=2; add user_context_embedding & item_context_embedding tables—NGCF only keeps one set.\n2. LINE alternates between u-i, u-u and i-i relations inside a single batch; implement a new field-aware sampler (currently only ui/uj triples exist) and extend calculate_loss() to draw neg users for items with probability 0.5.\n3. LINE does NOT propagate embeddings through a graph—remove the entire GNNlayers stack, norm_adj_matrix pre-computation, Laplacian building (get_norm_adj_mat), eye_matrix, and the LeakyReLU + normalize sequence inside forward().\n4. Edge-weight sampling: NGCF treats every interaction as binary; for LINE re-enable the alias-table sampler that respects original edge weights and injects sampled weight into the logistic loss (currently missing—must port from GraphEmbedding or re-implement Alias method).\n5. Degrees for noise distribution: accumulate out-degrees d_i, build P_n(v) ∝ d_v^(3/4) table for negative drawing; NGCF has no degree-based noise distribution code.",
    "rank": "rank2"
  },
  {
    "source": "GCMC_2018",
    "target": "LINE_2015",
    "type": "in-domain",
    "similarities": "1. Both models learn low-dimensional user/item embeddings via lookup tables (`nn.Embedding`) and reuse the identical initialization utility `xavier_normal_initialization`; the embedding tensors can be kept and only the propagation/loss parts need to be swapped.\n2. Both treat the user-item interaction matrix as a bipartite graph and finally output a user-item score vector for ranking; the evaluation protocol (`predict`, `full_sort_predict`) and the cached-embedding pattern (`restore_user_e`, `restore_item_e`) in GCMC can be copied verbatim to LINE.\n3. Negative sampling and the sigmoid-log trick are central to both: GCMC’s `NegSamplingLoss` (σ(logits)) is the same functional form LINE uses; the sampler utility (`sampler`, `random_num`) that keeps a global alias list and avoids already-interacted items can be transplanted directly.\n4. Mini-batch construction is identical—pairs of observed edges plus randomly drawn negatives—so the data-loader and batching loop in the GCMC script can be reused with no change except the loss call.",
    "differences": "1. GCMC encodes neighborhoods through multi-relational graph convolutions (`_graph_convolution` with stacked `W_r` and sparse adjacency matrices) while LINE has no propagation at all—its embeddings are free parameters. The entire `rating_matrices`, `T_matrices`, `_normalize_adjacency`, and `torch.sparse.mm` blocks must be removed; training complexity collapses to O(|E|) look-ups.\n2. GCMC’s decoder is a rating-aware bilinear classifier (`_bilinear_decoder` + softmax over 5 levels) trained with cross-entropy; LINE uses two separate dot-product scores (first- and second-order) with sigmoid negative-sampling loss. The `basis_matrices`, `decoder_weights`, and `CrossEntropy` computation need to be replaced by two embedding tables (`user_context_embedding`, `item_context_embedding`) and `NegSamplingLoss` calls.\n3. GCMC supports node dropout on the graph and feature dropout in the dense layer; LINE needs no graph dropout, so `node_dropout_prob`, the keep-mask logic, and the re-scaling code can be deleted.\n4. GCMC’s loss is point-wise over observed ratings only (`Ω` mask); LINE requires pairwise loss with user–neg-item and item–neg-user terms plus a configurable second-order weight. The `calculate_loss` method must be rewritten to return the four-term negative-sampling objective and to sample negative users via the existing `sampler`.",
    "rank": "rank3"
  },
  {
    "source": "MultiVAE_2018",
    "target": "LINE_2015",
    "type": "in-domain",
    "similarities": "1. Both models learn low-dimensional latent embeddings (user/item or node/context) via dot-product interaction: MultiVAE's decoder output `z` is fed into a softmax that implicitly uses dot-products with item embeddings; LINE directly computes `u^T v` for proximity scores—re-use the embedding lookup tables (`nn.Embedding`) and batched dot-product routines in `forward()`.\n2. Negative-sampling-based logistic loss: MultiVAE’s multinomial likelihood with softmax reduces to a sampled sigmoid loss in practice; LINE explicitly uses `log σ(pos) + Σ log σ(neg)`—re-purpose the `NegSamplingLoss` module and the mini-batch (user, pos, neg) sampling loop from LINE’s `calculate_loss()`; only the KL term must be removed.\n3. Training loop structure is identical: sample a batch → compute positive & negative scores → apply sigmoid cross-entropy → SGD update—lift the entire training step (including learning-rate scheduling and early-stopping hooks) from `MultiVAE` and simply drop the KL-annealing variable `anneal`.\n4. Efficient full-matrix prediction for ranking: both models expose `full_sort_predict()` that returns a |U|×|I| score matrix via a single `user_embedding @ item_embedding.T`—reuse the vectorized implementation and GPU memory layout from MultiVAE; LINE’s two embedding tables (`user_embedding`, `item_embedding`) map 1-to-1 to MultiVAE’s encoder output and decoder weight matrix.",
    "differences": "1. LINE has no stochastic latent variable: MultiVAE samples `z~N(μ,σ)` via reparameterization; LINE is a deterministic embedding model—remove the `reparameterize()` method, the KL divergence term, and the latent Gaussian prior; the encoder MLP collapses to an identity map and the 2K-dimensional encoder output becomes two separate K-dimensional embedding tables.\n2. LINE keeps two distinct vectors per node (`u_i` and `u'_i` for vertex & context) and optionally trains two proximity orders—implement a second set of `user_context_embedding` and `item_context_embedding`, plus a switch (`order`) in `calculate_loss()` that toggles between first- and second-order objectives; MultiVAE’s single latent `z` has no role split.\n3. Edge-weight sampling vs. uniform user sampling: MultiVAE draws users uniformly and uses their full row; LINE must sample individual edges proportionally to `w_ij` (alias table) and apply negative sampling with degree-based noise distribution `P_n(v) ∝ d_v^(3/4)`—write a new `EdgeSampler` class that builds an alias table from the interaction weights and replaces the current `random_num()` logic.\n4. No dropout or β-annealing in LINE: MultiVAE relies on input dropout and KL-annealing for regularization; LINE relies purely on negative sampling—delete dropout layers and the annealing schedule, but optionally add embedding normalization or weight decay to prevent over-fitting.",
    "rank": "rank4"
  },
  {
    "source": "EASE_2019",
    "target": "SLIMElastic_2011",
    "type": "in-domain",
    "similarities": "1. Both models learn an item-item coefficient matrix with zero diagonal constraint (diag(B)=0) to prevent self-similarity, enabling direct reuse of EASE's diagonal masking logic in SLIMElastic's coefficient matrix construction\n2. Both use the same prediction rule S = X·W where X is the user-item interaction matrix and W is the learned item-item coefficient matrix, allowing reuse of EASE's predict() and full_sort_predict() methods with minimal modification\n3. Both models can be trained on the item-item Gram matrix G = X^T X and support sparse matrix operations throughout inference, enabling reuse of EASE's sparse CSR matrix handling and interaction_matrix storage pattern\n4. Both share the same RecBole boilerplate structure (GeneralRecommender base class, dummy_param for compatibility, forward() and calculate_loss() stubs), making the overall class architecture reusable",
    "differences": "1. SLIMElastic requires column-wise independent ElasticNet regression with L1+L2 regularization and non-negativity constraints, needing new sklearn ElasticNet-based training loop replacing EASE's single closed-form matrix inversion\n2. SLIMElastic needs per-column training with temporary masking of the target item column (hide_item logic) and iterative sklearn model fitting, requiring implementation of a column-wise training loop with convergence warning suppression\n3. SLIMElastic produces a sparse coefficient matrix via L1 regularization, requiring sp.vstack() assembly of sparse coef_ arrays and conversion to CSR format, unlike EASE's dense numpy array\n4. SLIMElastic introduces hyperparameters alpha (overall regularization strength) and l1_ratio (L1/L2 mix) along with positive_only flag, requiring new config parameter handling and validation set tuning not present in EASE's single lambda",
    "rank": "rank1"
  },
  {
    "source": "LightGCN_2020",
    "target": "SLIMElastic_2011",
    "type": "in-domain",
    "similarities": "1. Both models use the user-item interaction matrix as the sole input, stored as CSR/LIL sparse matrices in code, enabling direct reuse of LightGCN's `dataset.inter_matrix(form='csr')` construction and sparse-dense multiplication utilities (`torch.sparse.mm`) for SLIM's `X @ W` inference.\n2. Recommendation generation is a sparse-matrix-vector product: LightGCN's `full_sort_predict` computes `user_embedding @ item_embedding.T` while SLIM computes `user_row @ W`; the same GPU-accelerated batching logic (`full_sort_predict` with pre-materialized `item_similarity` matrix) can be transplanted.\n3. Both expose identical public API (`predict`, `full_sort_predict`, `calculate_loss`) and inherit from `GeneralRecommender`, so the training loop, early-stopping handler and evaluator in RecBole can be reused without change; only the internal coefficient matrix `W` needs to replace the learned embeddings.\n4. L2-regularization is applied to the trainable parameters (`reg_weight` in LightGCN, `alpha` in SLIM) and both support tunable coefficients; the existing `EmbLoss` module can be dropped because SLIM's ElasticNet already includes the penalty.",
    "differences": "1. LightGCN learns continuous embeddings (two `nn.Embedding` tables) while SLIM learns a single sparse item-item coefficient matrix `W` via column-wise ElasticNet regression; the embedding tables must be removed and replaced by a `scipy.sparse` matrix populated column-by-column with `sklearn.linear_model.ElasticNet`.\n2. Training paradigm shifts from end-to-end mini-batch BPR (pairwise) in LightGCN to item-wise supervised regression (pointwise) in SLIM; `calculate_loss` returns a dummy zero tensor and actual training happens offline inside `__init__`—a new `fit` method must be added to loop over items, call `model.fit`, and collect sparse coefficients.\n3. Negative sampling is implicit in LightGCN (via BPR tuples) but absent in SLIM; the data-loader can keep `InputType.POINTWISE` but the negative-item field (`NEG_ITEM_ID`) must be ignored and the BPR loss module replaced by a no-op.\n4. Graph propagation layers (`n_layers` iterations of `norm_adj_matrix @ embeddings`) are eliminated; instead, a new hyper-parameter block (`alpha`, `l1_ratio`, `positive_only`, `hide_item`) must be exposed in the config and passed to each ElasticNet solver, and the resulting sparse matrices vertically stacked with `sp.vstack`.",
    "rank": "rank2"
  },
  {
    "source": "MultiVAE_2018",
    "target": "SLIMElastic_2011",
    "type": "in-domain",
    "similarities": "1. Both models operate on the same implicit-feedback user-item interaction matrix (binarized click matrix) and expose identical public APIs: full_sort_predict() that returns a 1-D tensor of scores for all items, plus a dummy calculate_loss() that returns zero, enabling plug-and-play replacement inside RecBole’s evaluation loop without changing the trainer.\n2. They share the same sparse–dense conversion pattern in predict(): user row is extracted as a sparse CSR vector, multiplied with the learned parameters (item_similarity for SLIM, decoder weight@encoder output for VAE), and the result is materialised as a dense torch tensor on the correct device—code in SLIM’s predict() can reuse the .to(self.device) pattern and torch.from_numpy() casting used in MultiVAE.\n3. Both models pre-compute the entire parameter matrix once (W for SLIM, θ/ϕ for VAE) and store it as a class attribute; the forward pass at serving time is therefore a single matrix-vector multiplication—MultiVAE’s self.item_similarity analogue is the decoder weight matrix, so the same “no forward() needed” lazy evaluation pattern can be copied.",
    "differences": "1. SLIM must solve I independent Elastic-Net optimisation problems (one per item) to learn the sparse non-negative item-item matrix W, whereas MultiVAE learns encoder/decoder weights via mini-batch SGD with KL annealing; the whole sklearn ElasticNet loop (fit per column, handle ConvergenceWarning, collect sparse_coef_) is completely new code with no counterpart in the VAE repository.\n2. SLIM enforces sparsity and non-negativity explicitly through α, l1_ratio and positive=True, leading to a scipy.sparse CSR matrix W, while MultiVAE keeps dense parameters and relies on β-annealed KL regularisation; therefore a new utility to vertically stack scipy sparse vectors and transpose them into self.item_similarity must be written—this sparse assembly step is absent in the VAE code.\n3. Training workflow is fundamentally different: SLIM is a “traditional” model that expects the full interaction matrix at __init__ and learns W offline before any RecBole trainer loop starts, whereas MultiVAE is a neural model whose parameters are updated every batch inside the trainer; consequently SLIMElastic needs to override RecBole’s fit() mechanism to forbid gradient steps and instead materialise W immediately.",
    "rank": "rank4"
  },
  {
    "source": "NGCF_2019",
    "target": "SLIMElastic_2011",
    "type": "in-domain",
    "similarities": "1. Both models operate on the same user-item interaction matrix R and use CSR/COO sparse formats—NGCF’s `interaction_matrix` (lil→coo) can be reused directly for SLIM’s `X` (csr→lil) preprocessing, saving I/O and parsing code.\n2. Both expose a `full_sort_predict()` routine that returns a dense score vector for all items per user—NGCF’s `torch.matmul(u_emb, i_emb.T)` pattern can be adapted to SLIM’s sparse-dense multiplication `r = user_row @ W` by swapping embeddings with the learned sparse `item_similarity` matrix, re-using the batching and GPU-transfer logic.\n3. Both share the same negative-sampling dataset iterator and `GeneralRecommender` base class—`recbole` dataloaders, train/eval loops, early-stopping and metric computation remain identical, so the runner scripts and config yaml files are copy-paste reusable.\n4. Regularization philosophy overlaps: NGCF’s `EmbLoss` (L2 on embeddings) and SLIM’s ElasticNet (L1+L2 on `W`) both prevent overfitting—hyper-parameter grids for `reg_weight`, `alpha`, `l1_ratio` can be migrated with minimal renaming.",
    "differences": "1. SLIM has NO learnable embeddings or GNN layers; instead it solves a column-wise convex optimization with `sklearn.linear_model.ElasticNet`—the entire `BiGNNLayer` stack, `W1/W2` matrices, `LeakyReLU`, dropout and embedding table must be removed and replaced by a CPU-bound coordinate-descent solver that outputs a sparse `W`.\n2. SLIM is trained ONCE offline (point-wise regression) while NGCF is trained end-to-end with BPR loss and Adam—`calculate_loss()` in NGCF returns a real gradient-based loss, whereas SLIM’s `calculate_loss()` is a dummy zero tensor; the training loop must be switched from mini-batch GPU back-prop to a single parallel `joblib` loop over items.\n3. SLIM enforces explicit constraints: non-negativity (`positive_only=True`) and zero diagonal (`hide_item` flag toggles column zeroing during fit)—these are absent in NGCF and require post-processing the `ElasticNet.coef_` to zero out `W[j,j]` and clamp negatives.\n4. Inference math differs: NGCF uses dense embedding dot-product; SLIM uses sparse-matrix-vector multiplication—`full_sort_predict` must be rewritten from `torch.matmul` to `scipy.sparse.csr_matrix @ dense_vector`, and GPU tensors converted to/from CPU SciPy arrays on-the-fly.",
    "rank": "rank5"
  },
  {
    "source": "MacridVAE_2019",
    "target": "DGCF_2020",
    "type": "in-domain",
    "similarities": "1. **Disentangled representation learning**: Both models explicitly decompose user/item embeddings into K intent-specific chunks (MacridVAE: z_u = [z_u^(1);...;z_u^(K)], DGCF: u = (u_1,...,u_K)) and enforce independence via KL divergence (MacridVAE β-VAE term) or distance correlation (DGCF cor_loss). The source code’s factor-wise encoder loop `for k in range(self.kfac):` and chunking `torch.chunk(ego_embeddings, self.n_factors, 1)` can be reused almost verbatim.\n2. **Intent-aware interaction modeling**: Both allocate each observed interaction (u,i) to a soft distribution over K latent intents via learnable scores (MacridVAE: c_{i,k} via cosine(h_i,m_k)/τ, DGCF: S_k(u,i) via tanh(u_k^t)·i_k^0). The source’s Gumbel-softmax assignment `F.gumbel_softmax(cates_logits, tau=1, hard=False)` and τ-scaled cosine similarity `torch.matmul(items, cores.transpose(0, 1))/self.tau` can be adapted for DGCF’s intent routing.\n3. **Iterative refinement of intent graphs**: Both update intent-specific adjacency matrices over training steps (MacridVAE: re-estimate c_{i,k}, DGCF: update S_k^{t+1}(u,i) = S_k^t(u,i) + u_k^t·tanh(i_k^0)). The source’s training loop that alternates embedding update and concept assignment can be reused; only the aggregation rule (neighbour routing vs. prototype cosine) needs replacement.\n4. **BPR-based pairwise training**: Both employ BPR loss on positive vs. negative samples with L2 regularisation; MacridVAE’s `ce_loss = -(F.log_softmax(z,1)*rating_matrix).sum(1).mean()` can be swapped for DGCF’s `BPRLoss()` without structural change.\n5. **Sparse-tensor message passing**: Both build sparse adjacency tensors (MacridVAE implicit in rating_matrix, DGCF explicit `edge2head_mat`, `head2edge_mat`). The source’s `F.dropout(rating_matrix, self.drop_out, training=self.training)` and normalised sparse mm patterns can be reused for DGCF’s Laplacian normalisation `build_matrix()`.",
    "differences": "1. **Graph vs. VAE architecture**: DGCF replaces MacridVAE’s variational encoder–decoder with multi-layer graph convolution. New components: (a) `n_layers` stacked disentangling layers, (b) neighbour routing inside each layer, (c) intent-aware Laplacian `build_matrix()` and `factor_edge_weight` lists—none exist in the source.\n2. **Neighbour routing mechanism**: DGCF introduces an iterative intra-layer update (`n_iterations` loops) that refines edge weights via `tanh(u_k^t)·i_k^0` and re-normalises with softmax. The source only has a single forward pass; the whole `_create_distance_correlation` and `A_values = A_values + A_iter_values` block must be implemented from scratch.\n3. **Distance-correlation independence regulariser**: MacridVAE uses a single KL term `β·D_KL(q||p)`; DGCF adds a second `cor_loss` computed via centred pairwise distance matrices `_create_centered_distance` and `dcor = dcov_12 / sqrt(dcov_11·dcov_22)`. This 30-line routine and the sampling helper `sample_cor_samples()` are entirely new.\n4. **Higher-order connectivity**: DGCF aggregates multi-hop neighbours by stacking `n_layers` graph propagations and residual layer-sum `e_ku = Σ_l e_ku^(l)`. MacridVAE is limited to first-order user-item co-occurrence; no layer combination or residual summation exists in the source.\n5. **Sparse-graph data loader**: DGCF pre-builds COO edge lists (`all_h_list`, `all_t_list`) and sparse tensor indices (`edge2head`, `head2edge`, `tail2edge`) to enable efficient `torch.sparse.mm`. MacridVAE uses dense rating matrices; the entire sparse scaffolding and `_build_sparse_tensor()` utility must be newly coded.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "DGCF_2020",
    "type": "in-domain",
    "similarities": "1. **Graph-based CF backbone**: both stack L multi-hop propagation layers on the user-item bipartite graph; the PyTorch sparse-adjacency engine (`SparseDropout`, `get_norm_adj_mat`, `BiGNNLayer`) can be reused almost verbatim—only the adjacency values become intent-specific in DGCF.\n2. **Embedding & prediction pipeline**: identical embedding lookup → layer-wise propagation → concatenation/sum across layers → inner-product score; `user_embedding`, `item_embedding`, `forward()` skeleton and `full_sort_predict()` can be copied with minor renaming.\n3. **BPR training loop**: same pairwise BPRLoss + L2 reg + mini-batch Adam; `calculate_loss()`, `mf_loss`, `reg_loss` and the training data iterator require no change.\n4. **Dropout & regularization**: message/node dropout pattern is the same; reuse `SparseDropout` and the dropout probability config keys.\n5. **Sparse tensor utilities**: `build_sparse_tensor`, `edge2head_mat`, `head2edge_mat`, `tail2edge_mat` in DGCF are direct extensions of NGCF’s Laplacian matrix construction—factorize the code once and parameterize over intents.",
    "differences": "1. **Intent-aware embedding chunking**: embeddings must be sliced into K factors (`torch.chunk`) and each factor propagated on its own graph—add `n_factors`, `n_iterations`, chunking logic inside `forward()`; this is completely absent in NGCF.\n2. **Dynamic neighbor routing**: DGCF introduces learnable intent scores `A_values` per edge that are updated via softmax & tanh affinity (Eq. 8 & 11); implement iterative `build_matrix()` that returns K factor-specific normalized edge weights and an in-layer `for t in range(n_iterations)` loop.\n3. **Distance-correlation independence regularizer**: new loss term `loss_ind` computed over sampled factor embeddings; write `create_cor_loss()` and `_create_distance_correlation()` using batched pairwise distance covariance—NGCF has no such module.\n4. **Layer aggregation**: NGCF concatenates all layer outputs (`||`), whereas DGCF sums them (`+`) after averaging across layers; change the final `torch.cat(embeddings_list, dim=1)` to `torch.mean(torch.stack(all_embeddings), dim=1)`.\n5. **No nonlinear transformation in propagation**: DGCF drops the `W1, W2` matrices and LeakyReLU inside each layer to keep disentanglement transparent; remove `BiGNNLayer` and the activation lines from NGCF code.",
    "rank": "rank2"
  },
  {
    "source": "GCMC_2018",
    "target": "DGCF_2020",
    "type": "in-domain",
    "similarities": "1. Both models adopt a bipartite-graph view of the user-item matrix and perform embedding propagation via sparse-matrix multiplications—GCMC’s _build_rating_matrices() and DGCF’s _build_sparse_tensor() both cache adjacency indices/values for O(|E|) GPU-friendly ops; re-use GCMC’s sparse-tensor construction and degree-normalization utilities almost verbatim.\n2. Both stack multiple graph layers and concatenate/sum layer-wise representations (GCMC Eq. 7–9, DGCF Eq. 13); GCMC’s all_embeddings list and torch.cat pattern in forward() can be copied for DGCF’s layer combination.\n3. Node/edge dropout for regularization: GCMC’s node-dropout mask inside _graph_convolution() can be transplanted into DGCF’s embedding propagation loop with minimal change (just apply mask before edge_val * edge_weight).\n4. Training mini-batching & negative sampling: GCMC’s mini-batch slicing of user/item rows and DGCF’s pairwise BPRLoss both sample fixed-size subsets; reuse RecBole’s BPRLoss and RegLoss modules already imported in GCMC code.",
    "differences": "1. GCMC keeps one embedding per node and uses rating-type-specific weight matrices; DGCF slices each embedding into K intent chunks (n_factors) and maintains K separate adjacency scores—requires new chunked embedding tensors (torch.chunk) and a learnable score matrix A_values of shape (|E|, K) that is absent in GCMC.\n2. GCMC’s message passing is a single non-iterative sparse-mm; DGCF introduces T iterations of neighbor routing inside every layer where edge scores are updated via Eq. 11 (dot-product + tanh). Must implement an inner iteration loop and in-place A_values update that GCMC never uses.\n3. GCMC predicts a distribution over explicit ratings with a bilinear decoder + softmax; DGCF outputs a single interaction score via inner product and employs BPR loss—remove GCMC’s entire _bilinear_decoder(), rating_values_tensor and cross-entropy parts, replace with BPRLoss.\n4. GCMC has no independence regularizer; DGCF adds a distance-correlation loss (create_cor_loss) computed on sampled factor embeddings—implement _create_distance_correlation() and the cor_users/cor_items sampling routine, plus the extra cor_weight term in the total loss.",
    "rank": "rank3"
  },
  {
    "source": "MultiVAE_2018",
    "target": "EASE_2019",
    "type": "in-domain",
    "similarities": "1. Both models treat the user-item interaction matrix X as a binary sparse matrix and perform item-to-item collaborative filtering—MultiVAE’s decoder outputs item scores for a user vector, EASE directly learns an item-item weight matrix B; hence the RecBole dataset.inter_matrix(form='csr') loader and the sparse-matrix handling utilities (sp.csr_matrix, .toarray(), .astype(np.float32)) can be reused verbatim.\n2. Both papers rank all items for a user in one shot: MultiVAE’s full_sort_predict returns the entire score vector fθ(μϕ(xu)), EASE’s full_sort_predict returns X_u B; the RecBole predict / full_sort_predict interface, batch-user indexing (interaction[self.USER_ID]) and the final flatten() + small-rand-noise tie-break pattern are identical and can be copied.\n3. Regularisation hyper-parameter handling is structurally the same: MultiVAE’s anneal_cap and dropout_prob are read from config, EASE reads reg_weight; the constructor pattern super().__init__(config, dataset) and the member assignment self.reg_weight = config['reg_weight'] can be reused.",
    "differences": "1. EASE needs a completely new closed-form training routine (_train_ease) that computes G = XᵀX, adds λ to the diagonal, inverts once, and sets B = I − P / diag(P); MultiVAE has no closed-form step—its encoder/decoder nets and the KL-annealing training loop must be removed.\n2. EASE stores a dense |I|×|I| item-similarity matrix (self.item_similarity) instead of neural weights; the encoder, decoder, reparameterize, mlp_layers modules and all torch.nn.Parameter objects must be deleted and replaced by a single numpy array B.\n3. Loss computation is radically different: MultiVAE returns kl_loss + ce_loss and requires back-prop; EASE has nothing to optimise at training time, so calculate_loss must return a dummy 0-D tensor and the model must be marked ModelType.TRADITIONAL; the entire training-loop infrastructure (optimizer, scheduler, gradient steps) becomes unused.\n4. Prediction arithmetic changes from neural forward pass to sparse-dense matrix multiply: MultiVAE’s scores = decoder(z) is replaced by scores = user_interactions @ self.item_similarity; GPU tensors are only created at the very end, so the early torch allocations and .to(self.device) calls must be postponed after the numpy compute.",
    "rank": "rank1"
  },
  {
    "source": "CDAE_2015",
    "target": "EASE_2019",
    "type": "in-domain",
    "similarities": "1. Both models treat implicit feedback as a sparse binary user-item matrix and predict scores via a linear transformation of the input row; CDAE’s `get_rating_matrix()` and EASE’s `self.interaction_matrix` are identical CSR matrices that can be reused verbatim.\n2. Recommendation step is the same sparse-dense matrix multiplication: CDAE’s `out = self.out_layer(h)` with h∈ℝᴷ and EASE’s `scores = user_interactions @ self.item_similarity` both reduce to a row-vector × item-matrix product; the PyTorch → NumPy conversion is the only extra line needed.\n3. Regularization philosophy is identical—weight-decay on all learned parameters; CDAE already exposes `reg_weight_1/2` in its config, so the single `reg_weight` hyper-parameter of EASE can be mapped directly to `reg_weight_2` and the L1 term simply disabled by setting `reg_weight_1=0`.",
    "differences": "1. EASE replaces the entire neural encoder (CDAE’s `h_item`, `h_user`, `h_act`, `out_layer`) with one closed-form item-item weight matrix B; the `_train_ease()` routine (Gram + inverse + Eq. 8) is brand-new code that has no analogue in CDAE.\n2. CDAE relies on mini-batch SGD with negative sampling (`loss_func` computed on `O_u ∪ S_u`), whereas EASE has no optimizer loop at all—training is a single NumPy call; the `calculate_loss()` method in EASE must return a dummy tensor to satisfy RecBole’s trainer, a pattern absent in CDAE.\n3. CDAE applies user-specific embedding `h_user(x_users)` and dropout corruption; EASE is strictly item-centric and user-agnostic—no user embeddings, no corruption, and no `nn.Dropout` layer.\n4. EASE enforces diag(B)=0 explicitly after the matrix inverse; CDAE has no such structural constraint, so the post-processing step `B[diag_indices] = 0.0` must be added.",
    "rank": "rank2"
  },
  {
    "source": "SLIMElastic_2011",
    "target": "EASE_2019",
    "type": "in-domain",
    "similarities": "1. Both models learn an item-item weight matrix (W in SLIMElastic, B in EASE) with identical zero-diagonal constraint to prevent self-similarity, enabling direct reuse of the diagonal masking logic from SLIMElastic's code\n2. Both use sparse linear aggregation: prediction = user_history @ item_similarity_matrix, allowing complete reuse of SLIMElastic's predict() and full_sort_predict() methods with only variable name changes\n3. Both process the same input format (sparse user-item interaction matrix in CSR format) and output identical prediction structure, making the dataset loading and preprocessing code in SLIMElastic directly transferable\n4. Both are traditional models with closed-form solutions that bypass iterative training, enabling reuse of SLIMElastic's dummy loss calculation pattern and forward() method structure",
    "differences": "1. SLIMElastic solves L1+L2 regularized ElasticNet problems column-by-column using sklearn.ElasticNet, while EASE requires implementing a closed-form matrix inversion solution via numpy.linalg.inv - this is the core new implementation needed\n2. SLIMElastic's training is embarrassingly parallel across items (independent column solves), whereas EASE requires a single O(n_items^3) matrix inversion step - necessitating new _train_ease() method that computes G=X^T X, adds λ to diagonal, inverts, then computes B = I - P / diag(P)\n3. SLIMElastic produces inherently sparse weight matrices via L1 regularization, while EASE produces dense matrices - the target implementation must handle dense numpy arrays instead of scipy.sparse matrices, requiring changes to item_similarity storage format\n4. SLIMElastic has two regularization parameters (alpha, l1_ratio) for elastic net, while EASE only has one (λ) for L2 - the config handling needs simplification in the target implementation",
    "rank": "rank4"
  },
  {
    "source": "MultiVAE_2018",
    "target": "GCMC_2018",
    "type": "in-domain",
    "similarities": "1. Both models frame collaborative filtering as an auto-encoding problem: MultiVAE learns user→latent→user reconstruction, GCMC learns rating→embedding→rating reconstruction; the `forward()`→`calculate_loss()`→`predict()` pipeline in MultiVAE can be reused almost verbatim—only swap the VAE stochastic layer with GCMC’s graph-convolution stack.\n2. Shared PyTorch skeleton: `GeneralRecommender` base class, Xavier init, `full_sort_predict()` caching, `Interaction` dictionary feeding user/item indices—copy these files unchanged.\n3. Bilinear decoder in GCMC (Eq. 4) is mathematically identical to the “multinomial logits” construction already coded in MultiVAE’s decoder head; refactor `self.decoder` Sequential into a `nn.ModuleList` of `Q_r` matrices and reuse the same `F.softmax` + expected-value trick for rating prediction.\n4. KL-annealing loop in MultiVAE (`anneal_cap`, `total_anneal_steps`) can be ported as-is to GCMC to gradually grow the graph-convolution dropout regularizer instead of the KL term.\n5. Negative-sampling-free training: both papers optimize only on observed entries via cross-entropy; the `rating_matrix` sparse-row slicing utility in MultiVAE (`get_rating_matrix`) can be recycled to build per-rating adjacency slices `M_r` for GCMC.",
    "differences": "1. Graph vs. set modeling: GCMC needs a new `self._build_rating_matrices()` routine that creates one sparse bipartite adjacency matrix per rating level—this is completely absent in MultiVAE; implement with `scipy.sparse` COO → `torch.sparse.FloatTensor` conversion.\n2. Message-passing engine: GCMC requires an efficient sparse-matrix multiplication stack (`_graph_convolution`) that performs `D^{-1}M_r X W_r^T` for each rating; MultiVAE has only dense MLPs—write a custom `SparseGCMLayer` and register it in the computation graph.\n3. Ordinal weight sharing (Eq. 11) and basis-matrix decoder (Eq. 12) are GCMC-specific; add `nn.ParameterList` of `T_s` and `P_s` plus gather/scatter logic to construct `W_r` and `Q_r` on-the-fly—none of these parameters exist in MultiVAE.\n4. Node dropout: GCMC drops an entire user/item node (all outgoing edges) with probability `node_dropout_prob`, then rescales; MultiVAE uses standard feature dropout—replace `F.dropout` with a sparse mask generator that zeros whole rows of the adjacency.\n5. Rating-level loss: GCMC predicts a distribution over discrete rating classes (1–5) and uses `F.cross_entropy` against the true label, whereas MultiVAE predicts a multinomial over items; change the final layer output dimension from `n_items` to `n_ratings` and adapt the loss accordingly.",
    "rank": "rank1"
  },
  {
    "source": "CDAE_2015",
    "target": "GCMC_2018",
    "type": "in-domain",
    "similarities": "1. Both treat recommendation as matrix reconstruction: CDAE's auto-encoder directly reconstructs the user-item rating vector, while GCMC's graph auto-encoder reconstructs rating probabilities via a bilinear decoder—reuse CDAE's `forward()` pattern but replace the dense output layer with GCMC's `_bilinear_decoder()` that returns a `[batch, n_ratings]` probability tensor.\n2. Shared negative-sampling & mini-batch training loop: CDAE's `calculate_loss()` samples observed + negative items and computes MSE/BCE; GCMC keeps the same mini-batch structure but switches to cross-entropy over rating classes—keep the `interaction` slicing logic and simply swap `F.cross_entropy(probs, rating_indices)` for the loss call.\n3. Regularization & dropout strategy: CDAE already applies L1/L2 on `h_user`, `h_item` and uses `nn.Dropout` for input corruption; GCMC extends this with node-wise dropout (dropping all outgoing messages of a node)—reuse the existing weight-decay lines and add a `_graph_convolution()` helper that masks & rescales embeddings exactly like CDAE's `self.dropout` but on the whole adjacency list.\n4. Embedding table initialization & caching: CDAE caches `get_rating_matrix()` to avoid rebuilding sparse inputs; GCMC caches full user/item embeddings in `restore_user_e`/`restore_item_e` for fast evaluation—lift the `other_parameter_name` list and the `if self.restore_user_e is None` pattern verbatim into GCMC.",
    "differences": "1. GCMC needs a new bipartite-graph message-passing stack: build rating-specific sparse adjacency matrices `M_r` (`_build_rating_matrices`) and implement `_graph_convolution()` with ordinal weight sharing `W_r = sum_{s=1}^r T_s`—this entire pipeline is absent in CDAE and must be coded from scratch.\n2. Rating-aware decoder: CDAE outputs a single score per item; GCMC must implement `_bilinear_decoder()` that produces a softmax over `n_ratings` classes using learnable basis matrices `Q_r = sum_s a_rs P_s`—add `nn.ParameterList` for `basis_matrices` and `decoder_weights`, then compute `u^T Q_r v` logits in batch form.\n3. Two-stage encoder architecture: GCMC stacks a graph-convolution layer followed by a dense layer, whereas CDAE uses a single feed-forward hidden layer—replace CDAE’s single `self.out_layer` with separate `_graph_convolution()` and `_dense_layer()` modules, and wire them sequentially in `forward()`.\n4. Node-dropout regularization vs. input corruption: CDAE corrupts the input rating vector with `nn.Dropout(p=corruption_ratio)`; GCMC drops entire nodes in the graph with probability `node_dropout_prob` and rescales messages—implement this by sampling a node mask inside `_graph_convolution()` and multiplying/scaling embeddings before sparse matrix multiplication, a pattern not present in CDAE.",
    "rank": "rank2"
  },
  {
    "source": "NGCF_2019",
    "target": "LightGCN_2020",
    "type": "in-domain",
    "similarities": "1. **Graph-based collaborative filtering framework**: Both models build a bipartite user-item graph from the interaction matrix R, normalize it via the symmetric Laplacian D^{-0.5} A D^{-0.5}, and perform K-hop embedding propagation; the helper `get_norm_adj_mat()` and sparse-matrix multiplication `torch.sparse.mm()` in NGCF can be reused verbatim for LightGCN.\n2. **Embedding look-up & BPR training**: Both start from trainable ID embeddings (nn.Embedding for users/items), concatenate 0-th layer embeddings into a single matrix `E^(0)`, and optimize the same BPRLoss with negative sampling; the data loader, mini-batch Adam optimizer, and `calculate_loss()` skeleton (sample triplets, score with dot-product, add L2 reg) are identical—only the regularization target changes from {E,W1,W2} to {E^(0)}.\n3. **Layer-wise propagation loop & final dot-product prediction**: Both unroll K graph layers, cache intermediate embeddings in a Python list, aggregate them into final user/item representations, and score pairs via `torch.mul(u,e_i).sum(dim=1)`; the `forward()` list management and `predict()/full_sort_predict()` routines can be copied with minor edits (replace NGCF’s layer-specific transformations with LightGCN’s pure propagation).\n4. **Sparse-dropout & evaluation acceleration**: NGCF’s `SparseDropout` for message dropout and the caching pattern (`restore_user_e`, `restore_item_e`) for full-sort evaluation are directly applicable; LightGCN can optionally keep node dropout by reusing the same `self.sparse_dropout(A_hat)` line.",
    "differences": "1. **Remove feature transformation & non-linearity**: NGCF uses trainable W1^(l), W2^(l) and LeakyReLU at every layer; LightGCN needs NONE of these—delete the `BiGNNLayer` modules, `self.GNNlayers` ModuleList, and the LeakyReLU + normalize calls inside the loop.\n2. **Replace self-connection + concatenation with layer-mean**: NGCF adds self-loops via `((L+I) E W1)` and concatenates all layer outputs `torch.cat(embeddings_list, dim=1)`; LightGCN omits self-loops and averages layers `torch.stack(embeddings_list, dim=1).mean(dim=1)`—implement a single line change in `forward()` after the loop.\n3. **Dropout & regularization scope**: NGCF has separate `message_dropout` (on embeddings) and `node_dropout` (on A); LightGCN authors disable both and only L2-regularize the 0-th layer embeddings—remove `self.emb_dropout`, `self.sparse_dropout` (optional), and change `reg_loss` input from all propagated embeddings to `u_ego_embeddings, pos_ego_embeddings, neg_ego_embeddings` retrieved directly from the embedding tables.\n4. **Model parameter set & size**: NGCF introduces 2K×d×d′ extra weights; LightGCN keeps only the embedding matrices (same size as MF)—delete all `hidden_size_list` logic and simply set `latent_dim = embedding_size` without any intermediate projection layers.",
    "rank": "rank1"
  },
  {
    "source": "MultiVAE_2018",
    "target": "LightGCN_2020",
    "type": "in-domain",
    "similarities": "1. Both models treat the user-item interaction matrix as the sole input and learn low-dimensional embeddings for users/items—reuse the `get_rating_matrix`/`interaction_matrix` loading and batch-sampling logic from MultiVAE’s `calculate_loss` and `predict` methods.\n2. Both are trained with pairwise ranking objectives (MultiVAE uses the multinomial cross-entropy which is equivalent to a softmax BPR; LightGCN uses explicit BPR)—the `BPRLoss` class already exists in RecBole and can be plugged in place of MultiVAE’s CE term with minimal change.\n3. Both adopt embedding dropout/L2 regularization to avoid over-fitting—MultiVAE’s `reg_loss` computation (`EmbLoss` invoked on encoder/decoder weights) can be reused for LightGCN’s `reg_weight` term applied to the 0-th layer embeddings.\n4. Both expose an `all-rank` interface (`full_sort_predict`) that returns scores for every item—re-use the caching pattern (`restore_user_e`, `restore_item_e`) and the final dot-product scoring in MultiVAE’s `full_sort_predict` after replacing the VAE forward pass with LightGCN’s graph propagation.",
    "differences": "1. LightGCN needs a graph propagation engine (sparse LGC layers) that is absent in MultiVAE—implement `get_norm_adj_mat` to build the symmetrically-normalized sparse adjacency matrix and a `forward` that iteratively applies `torch.sparse.mm(L, E)` for K layers.\n2. LightGCN has no encoder/decoder MLPs; trainable parameters are only the 0-th layer embedding tables—remove MultiVAE’s `encoder`, `decoder`, `reparameterize`, and the VAE-specific ELBO loss; create `nn.Embedding` tables for users/items instead.\n3. LightGCN combines embeddings from all layers via uniform averaging (Eq. 4)—add a `torch.stack`+`torch.mean` step after the propagation loop; MultiVAE does not perform layer combination.\n4. LightGCN uses negative sampling (`NEG_ITEM_ID`) inside the BPR loss—extend the data loader to expose negatives and replace MultiVAE’s user-wise CE loss with the pairwise BPR loss; no KL annealing or β scheduling is required.",
    "rank": "rank2"
  },
  {
    "source": "MultiVAE_2018",
    "target": "MacridVAE_2019",
    "type": "in-domain",
    "similarities": "1. Both adopt the VAE framework with identical encoder-decoder architecture: a tanh-MLP encoder outputs mean/log-var vectors, reparameterization trick samples latent code, and an MLP decoder maps back to item logits; the `mlp_layers()` helper, `reparameterize()` method, and forward-pass pattern in `MultiVAE.forward()` can be copied almost verbatim.\n2. Training objective is the same ELBO formulation: cross-entropy reconstruction on the full user row plus a KL term on the latent Gaussian; the `calculate_loss()` skeleton (CE + KL × anneal) and the annealing schedule (`anneal_cap`, `total_anneal_steps`, `self.update`) are reusable directly from `MultiVAE`.\n3. Both models ingest the same binarized user–item matrix, apply row-wise L2-normalization, use dropout on the input row, and predict via `full_sort_predict()` that returns scores for all items; the history-matrix builder `build_histroy_items()` and the `get_rating_matrix()` utility can be reused without change.\n4. Predict-time behavior is identical: take the mean latent vector, run the decoder once, apply softmax over items, and read off the score for the requested item; the `predict()` and `full_sort_predict()` implementations differ only in tensor names but share the same control flow and can be migrated as-is.",
    "differences": "1. MacridVAE introduces K-factor disentanglement: the latent space is split into K concept blocks {z^(k)} and items are softly assigned to concepts via a learnable prototype matrix (`k_embedding`) using cosine similarity / Gumbel-softmax; this requires new modules `k_embedding`, `item_embedding`, a concept-assignment branch (`cates_logits`) and a loop over K blocks—none of which exist in MultiVAE.\n2. Encoder and decoder are concept-conditioned: instead of a single encoder call, MacridVAE runs the encoder K times on the row masked by concept attention, and the decoder logits are aggregated across concepts; a new `for k in range(self.kfac):` loop, per-concept masked input `x_k`, and incremental `probs` accumulation must be implemented.\n3. KL loss is computed per block: MacridVAE sums K separate KL terms (one for each z^(k)) whereas MultiVAE computes a single KL; the loss function needs a loop `for i in range(self.kfac): kl_ = …` and the annealing factor is applied to the summed KL.\n4. Additional regularization & normalization: MacridVAE normalizes every embedding table and every sampled latent vector to unit length (cosine space), adds an L2 regularizer on embeddings (`reg_loss()` with `EmbLoss`), and uses a temperature τ=0.1 in cosine logits; these normalization steps and the extra regularization term are absent in MultiVAE and must be added.",
    "rank": "rank1"
  },
  {
    "source": "CDAE_2015",
    "target": "MultiVAE_2018",
    "type": "in-domain",
    "similarities": "1. Both models are user-level auto-encoders: they ingest a complete user interaction vector x_u ∈ {0,1}^I and reconstruct it; hence the source helper methods get_rating_matrix(), the item→hidden Linear layer and the decoder Linear layer can be reused almost verbatim—only tensor shapes change.\n2. Shared PyTorch skeleton: both inherit GeneralRecommender & AutoEncoderMixin, keep an encoder–decoder nn.Sequential, apply xavier_normal_initialization, expose calculate_loss()/predict()/full_sort_predict() with identical signatures—copying the file structure and training loop from CDAE saves boiler-plate code.\n3. Negative-sampling-free training: neither model iterates over (u,i) pairs; they both process a full user row per mini-batch and let the loss (MSE/BCE for CDAE, multinomial CE for MultiVAE) supervise every item logit—therefore the batch-generation logic (user sampling, GPU tensor construction) in CDAE’s dataloader can be reused unchanged.\n4. Dropout as the only stochastic regulariser: CDAE uses nn.Dropout on the input (corruption); MultiVAE uses F.dropout on the normalised input—replacing CDAE’s self.dropout = nn.Dropout(p) with F.dropout(..., training=self.training) inside forward() is sufficient.\n5. Latent projection pattern: CDAE’s h_item Linear(I→K) and h_user Embedding(U→K) can be concatenated to seed MultiVAE’s encoder first layer (I→hidden); the decoder layer weights are similarly transposed, so the same nn.Linear initialisation routine applies.",
    "differences": "1. Probabilistic encoder vs. deterministic: MultiVAE must output 2·K values (μ_u, logσ²_u) and perform reparameterisation sampling; CDAE only outputs K activations. A new reparameterize(mu, logvar) method and splitting the encoder’s last layer into two parallel Linear modules are required—this is completely absent in CDAE.\n2. KL-regularised ELBO vs. plain reconstruction loss: MultiVAE adds a KL(q_φ(z|x_u)‖N(0,I)) term weighted by an anneal factor β; CDAE only has L2 weight decay. Implementing the closed-form KL for diagonal Gaussian and scheduling β via self.update/total_anneal_steps is a new training component.\n3. Multinomial likelihood vs. MSE/BCE: MultiVAE optimises −∑_i x_ui log π_i with π = softmax(logits), whereas CDAE uses element-wise MSE or BCE. The loss therefore changes from nn.MSELoss to −(F.log_softmax(z,1)*rating_matrix).sum(1).mean()—a one-line but conceptually different objective.\n4. User embedding vs. amortised inference: CDAE stores a per-user vector V_u via nn.Embedding; MultiVAE abolishes user-specific parameters and amortises latent variables through the encoder network. Removing self.h_user and folding its capacity into the encoder MLP is mandatory.\n5. Annealing schedule & extra hyper-parameters: MultiVAE introduces anneal_cap, total_anneal_steps, latent_dimension, dropout_prob that have no counterpart in CDAE; these must be added to the configuration parser and training loop.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "MultiVAE_2018",
    "type": "in-domain",
    "similarities": "1. Both models are collaborative-filtering recommenders that operate on the user-item interaction matrix R; NGCF’s `interaction_matrix` (coo) can be reused as-is for MultiVAE’s binarized `rating_matrix`—no new data-loading code is needed.\n2. They share the same high-level training loop: mini-batch user sampling, forward pass to obtain scores, loss computation, and Adam optimizer; the `RecBole` base-class boilerplate (`GeneralRecommender`, `InputType.PAIRWISE`, `calculate_loss`, `predict`, `full_sort_predict`) is identical and can be copied verbatim.\n3. Both use a form of negative sampling implicit in the loss: NGCF’s BPR uses (u,i,j) triples while MultiVAE uses the full-row cross-entropy that treats unclicked items as negatives; the existing negative-item indexing logic in `calculate_loss` can be kept.\n4. Dropout regularization is implemented the same way: NGCF’s `self.emb_dropout` applied after propagation maps 1-to-1 to MultiVAE’s `F.dropout(h, self.drop_out, training=self.training)`—reuse the same dropout pattern.\n5. Xavier initialization and `full_sort_predict` caching (`restore_user_e`, `restore_item_e`) are already coded in NGCF and can be transferred unchanged.",
    "differences": "1. Core architecture: NGCF is a graph convolution network with `BiGNNLayer` message passing; MultiVAE needs a variational auto-encoder—delete all `GNNlayers`, `norm_adj_matrix`, `get_norm_adj_mat()`, `BiGNNLayer` and replace with encoder/decoder MLP modules (`self.encoder`, `self.decoder`).\n2. Loss function: NGCF uses BPRLoss (`BPRLoss()` class); MultiVAE requires an ELBO loss combining multinomial cross-entropy and KL divergence—implement `kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())` plus `ce_loss = -(F.log_softmax(z, 1) * rating_matrix).sum(1).mean()` with an annealing coefficient.\n3. Latent variable sampling: NGCF has deterministic embeddings; MultiVAE must sample `z` via reparameterization—add `reparameterize(mu, logvar)` method and return `(z, mu, logvar)` from `forward()`.\n4. Input representation: NGCF stores sparse adjacency; MultiVAE needs dense user history vectors—add `get_rating_matrix(user)` that gathers the binarized row slice from the interaction matrix and normalizes it (`F.normalize`).\n5. KL annealing scheduler: NGCF does not anneal; MultiVAE needs a step counter (`self.update`) and linear annealing up to `anneal_cap`—insert annealing logic inside `calculate_loss`.\n6. Prediction head: NGCF outputs final embeddings and uses inner-product; MultiVAE outputs logits over all items via softmax—replace `torch.mul(u,e).sum(dim=1)` with `scores[[…, item]]` indexing of the softmax logits.",
    "rank": "rank2"
  },
  {
    "source": "SLIMElastic_2011",
    "target": "MultiVAE_2018",
    "type": "in-domain",
    "similarities": "1. Both models operate on the same implicit-feedback user-item matrix A ∈ ℝ^{U×I} stored as CSR sparse matrix; SLIMElastic code already shows how to load and normalize this matrix (dataset.inter_matrix), so MultiVAE can reuse the same data-loading pipeline and CSR→dense mini-batch conversion utilities.\n2. Top-N ranking objective is identical: after computing a score vector ŷ_u ∈ ℝ^I, both discard already-consumed items and return the rest sorted; SLIMElastic’s full_sort_predict() pattern (sparse_mat @ W → dense vector → torch tensor) can be copied for MultiVAE’s decoder output before the final sort.\n3. Training loop skeleton is the same: iterate over users, feed a user’s binary row vector, compute loss, back-propagate; SLIMElastic already inherits from GeneralRecommender and implements calculate_loss() returning a dummy 0-tensor, so MultiVAE can keep the same RecBole trainer integration and only replace the loss computation block.\n4. GPU/CPU agnostic tensor handling: both use torch.from_numpy(...).to(self.device) to move data; the device plumbing in SLIMElastic can be reused as-is for MultiVAE encoder/decoder tensors.",
    "differences": "1. SLIMElastic has no learnable torch parameters (only sklearn ElasticNet), whereas MultiVAE needs a full torch.nn.Module encoder & decoder with ~2×(I→H→K) layers; therefore create new nn.Linear modules, xavier init, and forward() graph—completely absent in SLIMElastic.\n2. SLIMElastic’s loss is zero (pre-trained W), while MultiVAE must implement Evidence Lower BOund: multinomial-likelihood cross-entropy + KL(𝒩(μ,σ²)‖𝒩(0,I)) with β-annealing; a new calculate_loss() with KL annealing step counter and separate CE & KL terms must be written—no equivalent code exists in SLIMElastic.\n3. Reparameterization trick & stochastic latent variable z are unique to MultiVAE; add reparameterize() method that samples ε∼𝒩(0,I) and returns μ+ε⊙σ during training (eval uses μ only)—SLIMElastic has no sampling code.\n4. SLIMElastic predicts via sparse matrix multiply (a_u^T W), whereas MultiVAE predicts via neural decoder softmax(logits); hence the final scoring tensor in full_sort_predict must be obtained by passing the latent code through self.decoder then F.log_softmax, not by a sparse dot-product—requires new computation graph.",
    "rank": "rank3"
  },
  {
    "source": "NGCF_2019",
    "target": "NAIS_2018",
    "type": "in-domain",
    "similarities": "1. Both models adopt an embedding look-up table (Embedding layers) for users/items; NGCF’s user_embedding & item_embedding tensors can be reused as item_src_embedding & item_dst_embedding in NAIS after tensor splitting.\n2. Both rely on element-wise product to express user–item interaction: NGCF uses it inside message construction (Eq.3) and NAIS uses it inside attention MLP when algorithm='prod' (Eq.7); the same torch.mul() pattern can be copied.\n3. Both employ a point-wise MLP sub-network with ReLU activation to transform embeddings; NGCF’s BiGNNLayer (W1,W2 linear + LeakyReLU) can be refactored into NAIS’s MLPLayers (concat/prod + ReLU) by removing the graph-adjacency multiplication and keeping the weight matrices initialization style (xavier_normal).\n4. L2 regularization on embeddings/weights is implemented identically (EmbLoss in NGCF vs. reg_loss() in NAIS); the same parameter-group loop and norm-2 accumulation code can be reused.\n5. Training loop skeleton (mini-batch sampling, forward pass, loss back-prop, Adam/Adagrad optim) is identical; only loss function changes from BPRLoss to BCEWithLogitsLoss—swap the loss module and keep the remaining trainer code.",
    "differences": "1. NGCF is graph-based: it needs the Laplacian norm_adj_matrix and eye_matrix to propagate embeddings over the user–item bipartite graph; NAIS is pooling-based and does NOT use any adjacency structure—therefore norm_adj_matrix construction, BiGNNLayer, SparseDropout and all graph propagation logic must be removed and replaced by history_item_matrix padding + mask_mat handling.\n2. Attention mechanism is absent in NGCF; NAIS requires an extra attention MLP (concat or prod) plus a masked softmax with smoothing exponent β. Implement: (a) attention_mlp() method, (b) mask_softmax()/softmax() helpers with β-scaled denominator, (c) exp_logits caching to achieve O(a k |R_u+|) complexity.\n3. NGCF concatenates multi-hop embeddings (‖ layers) to form final user/item vectors; NAIS computes a dynamic user representation on-the-fly via weighted sum over historical item embeddings—no layer stacking or embedding concatenation is used. Remove embeddings_list and the final torch.cat() in forward().\n4. Loss changes from pairwise BPR (comparing one positive vs one negative per user) to point-wise log-loss with uniform negative sampling (4 negatives per positive). Replace BPRLoss with BCEWithLogitsLoss and adapt the data-loader to yield (u,i,label) triples instead of (u,i,j).\n5. NAIS supports FISM pre-training: implement _load_pretrain() that copies FISM item embeddings into item_src/dst embeddings and initializes attention weights; add pretrain_path option to config. This path and checkpoint loading logic do not exist in NGCF.",
    "rank": "rank1"
  },
  {
    "source": "FISM_2013",
    "target": "NAIS_2018",
    "type": "in-domain",
    "similarities": "1. Both models adopt the same item-based CF paradigm: user representation is computed as a weighted sum of item embeddings from the user's interaction history, and the score for a candidate item i is the dot-product between i's embedding and this aggregated user vector; the provided FISM code already implements the history-item matrix, mask handling, and dot-product aggregation—reuse these verbatim.\n\n2. Identical data pre-processing pipeline: the code blocks get_history_info(), history_item_matrix, history_lens and mask_mat are copy-pasted between FISM and NAIS; padding, masking and mini-batch generation logic can be reused without change.\n\n3. Shared embedding layer design: two separate embedding tables (item_src_embedding, item_dst_embedding) with identical shapes (n_items × k) and the same initialization strategy (normal_(0,0.01)); the NAIS implementation literally copies the FISM embeddings at pre-training via _load_pretrain().\n\n4. Common training routine: pointwise binary-cross-entropy with logits, negative sampling ratio of 4, Adagrad optimizer, and the same regularisation formula (L2 on P, Q plus optional biases); the calculate_loss() skeleton and full_sort_predict() loop (including split_to tiling for GPU memory) are inherited unchanged.\n\n5. Reusable coefficient (n_u^+)^{-α}: the power-scaling of the aggregate is kept in NAIS and computed with the same torch.pow(item_num, -self.alpha) idiom; the α hyper-parameter search grid [0.4,0.5] recommended by FISM is still valid.",
    "differences": "1. Attention subnet is completely new: NAIS inserts an MLP (concat or prod) that outputs an attention logit for every (i,j) pair; this requires implementing MLPLayers module, attention_mlp() method, plus extra parameters W, b, h and reg_weights[2]—none of these exist in FISM.\n\n2. Softmax smoothing layer: NAIS replaces the uniform weighting of FISM with a differentiable attention mask_softmax() that applies a learnable β exponent to the denominator; this softmax + β logic (Equation 9) must be coded from scratch, including numerically stable masked softmax over variable-length histories.\n\n3. Pre-training stage: FISM trains from random init, whereas NAIS mandates a two-stage procedure—first train FISM, save its state_dict, then load it into NAIS via _load_pretrain(); the reproduction pipeline must therefore add a pre-training script and checkpoint handling not present in the original FISM repo.\n\n4. Additional hyper-parameters: NAIS introduces attention_factor (weight_size), algorithm ∈ {concat,prod}, β ∈ [0,1], and a third regularisation weight for the MLP; the config yaml and argument parser need to be extended accordingly.\n\n5. Item bias only: FISM keeps both user_bias and item_bias, but NAIS drops user_bias (it cancels in pairwise losses) and retains only self.bias (size n_items); the forward signatures and loss computation must be adjusted to remove user_bias terms.",
    "rank": "rank2"
  },
  {
    "source": "BPR_2009",
    "target": "NAIS_2018",
    "type": "in-domain",
    "similarities": "1. Both learn latent item embeddings (item_embedding in BPR vs. item_src_embedding & item_dst_embedding in NAIS) with identical initialization (xavier_normal_initialization) and L2 regularization on embeddings; NAIS can reuse the embedding table creation pattern and weight-decay logic from BPR.\n2. Both adopt a pairwise ranking objective that internally uses the sigmoid of a real-valued score difference: BPR’s ln σ(𝑥̂_ui − 𝑥̂_uj) is mathematically equivalent to the log-likelihood term used in NAIS when the negative item is treated as the “j” item; the BPRLoss class in BPR can be adapted to supply the σ(·) and log(·) primitives for NAIS’s pointwise binary loss.\n3. Both sample negative items on-the-fly via the DataLoader (BPR’s NEG_ITEM_ID field) and share the same mini-batch training loop structure; the PairwiseDataLoader configuration and negative-sampling utility in the BPR repo can be reused directly for NAIS’s negative generation (4 negatives per positive as in the paper).\n4. Both expose an identical top-k prediction interface (full_sort_predict) that materialises the full user×item score matrix through a single matrix multiplication (user_e @ all_item_e.T); NAIS can copy this CUDA-friendly batched GEMM pattern and only needs to replace the user vector with the history-weighted aggregate.",
    "differences": "1. NAIS requires a history-aware user representation: a learnable attention MLP that outputs an α-smoothed softmax over the user’s past items (mask_softmax with β exponent). This entire attention_mlp submodule, the weight_layer vector, and the corresponding _load_pretrain() logic for FISM warm-start are completely absent in BPR and must be implemented from scratch.\n2. NAIS uses two separate item embedding tables (item_src & item_dst) to avoid information leakage when the target item appears in the user’s history, whereas BPR keeps a single item_embedding; the embedding lookup and parameter update logic has to be duplicated and the regularisation term extended to both tables.\n3. NAIS is trained point-wise with BCEWithLogitsLoss plus an explicit regularisation loss (reg_loss) that sums L2 norms of both embedding tables and the MLP weights, while BPR uses the implicit weight-decay inside BPRLoss; a new reg_weights hyper-parameter set and the reg_loss() routine must be added.\n4. NAIS needs an efficient history masking and normalisation scheme (mask_mat, pow(item_num, -α), pow(exp_sum, β)) to handle variable-length user histories, including a CPU-side pre-computation of the history_item_matrix; BPR’s simple user-id lookup has no such data structure, so the dataset collate_fn and batch preprocessing pipeline must be rewritten.",
    "rank": "rank3"
  },
  {
    "source": "GCMC_2018",
    "target": "NGCF_2019",
    "type": "in-domain",
    "similarities": "1. Both models cast collaborative filtering as bipartite-graph link prediction and share the same sparse-matrix infrastructure—`scipy.sparse` construction of the `(N_u+N_v, N_u+N_v)` Laplacian, degree-based `D^{-0.5} A D^{-0.5}` normalisation, and `torch.sparse.FloatTensor` storage—so GCMC’s `get_norm_adj_mat()` and `_convert_sp_mat_to_sp_tensor()` can be reused verbatim in NGCF.\n2. Both stack trainable weight matrices along rating/edge types and perform message passing with dropout; GCMC’s `_graph_convolution()` with `SparseDropout` and node-wise rescaling already implements the core sparse-dense multiply that NGCF’s `BiGNNLayer` needs, so only the message formula inside the layer must be swapped.\n3. Both cache full user/item embeddings for fast evaluation (`restore_user_e`, `restore_item_e`) and expose the same `full_sort_predict()` signature; the caching logic and batch-splitting helper (`torch.split(..., [n_users, n_items])`) can be copied unchanged.\n4. Both adopt a single `embedding_size` configurable lookup table initialised with Xavier uniform; GCMC’s `user_embedding`/`item_embedding` declaration and initialisation call can be reused, eliminating redundant code.",
    "differences": "1. GCMC uses rating-aware ordinal weight matrices `T_s` and a bilinear softmax decoder for multi-class rating regression, whereas NGCF is implicit-feedback only and needs a new `BiGNNLayer` that outputs `W1·e_j + W2·(e_j⊙e_u)` messages; the entire decoder (`_bilinear_decoder`, `decoder_weights`, `basis_matrices`, rating-specific loss) must be removed and replaced by BPR ranking loss.\n2. NGCF concatenates embeddings from **all** propagation layers (`e^(0)‖…‖e^(L)`) while GCMC keeps only the final layer; GCMC’s forward loop that overwrites `all_embeddings` each iteration must be changed to accumulate each layer’s output in `embeddings_list` and final `torch.cat(..., dim=1)`.\n3. NGCF trains with pairwise negative sampling (`POS_ITEM_ID`, `NEG_ITEM_ID`) and `BPRLoss`, but GCMC uses point-wise `cross_entropy` over observed ratings; the data-loader interface and `calculate_loss` must be refactored to sample negatives and return `mf_loss + reg_loss` instead of classification loss.\n4. NGCF applies **message dropout** (dropping individual propagated messages with probability `p1`) in addition to node dropout; GCMC only implements node dropout inside `_graph_convolution`, so a new `SparseDropout` on the adjacency matrix (already present in NGCF code) and an extra `emb_dropout` after each layer must be added.",
    "rank": "rank1"
  },
  {
    "source": "BPR_2009",
    "target": "NGCF_2019",
    "type": "in-domain",
    "similarities": "1. Both models adopt the BPR pairwise ranking loss (BPRLoss) as the training objective, implemented identically in RecBole as `BPRLoss()`; the `calculate_loss()` boilerplate that samples `(user, pos_item, neg_item)` and calls `self.loss(pos_score, neg_score)` can be copied verbatim from BPR to NGCF.\n2. They share the same implicit-feedback data pipeline: the `InputType.PAIRWISE` dataloader in RecBole already yields triples `(user, pos, neg)`; no new sampler needs to be written.\n3. The final prediction layer is a simple inner-product (`torch.mul(u, i).sum(dim=1)`) between user and item vectors; `predict()` and `full_sort_predict()` routines are structurally identical and can be reused after embeddings are obtained.\n4. Both use `nn.Embedding` tables initialized with `xavier_normal_initialization` and regularized with L2; the embedding look-up and reg-loss utility `EmbLoss()` are already present in BPR’s code base.\n5. The training loop, negative-item injection, and early-stopping callback in RecBole’s trainer are agnostic to the forward pass, so the entire training harness is reusable.",
    "differences": "1. NGCF requires a **graph-propagation module** that BPR lacks: implement `get_norm_adj_mat()` to build the sparse Laplacian `L = D^{-0.5} A D^{-0.5}` and a custom `BiGNNLayer` that executes Equation (6) (message construction with `W1, W2` and element-wise product).\n2. Multi-layer embeddings must be **concatenated** (`torch.cat(embeddings_list, dim=1)`); BPR only keeps a single embedding per user/item—add a list collector inside the forward pass.\n3. **Dropout variants** (node vs. message) are absent in BPR: implement `SparseDropout` on the adjacency matrix and `nn.Dropout` on the propagated messages with separate rates `node_dropout` and `message_dropout`.\n4. **Embedding size changes across layers**: BPR uses constant `embedding_size`, while NGCF uses a list `hidden_size_list` to define per-layer transformation dims; the ModuleList of `BiGNNLayer` must be constructed dynamically from this list.\n5. **Forward pass is stateful**: NGCF’s `forward()` returns the final propagated embeddings for all users/items, cached in `restore_user_e`/`restore_item_e` to accelerate `full_sort_predict`; BPR recomputes on-the-fly—add caching logic and cache-clear in `calculate_loss()`.",
    "rank": "rank3"
  },
  {
    "source": "NGCF_2019",
    "target": "NNCF_2020",
    "type": "in-domain",
    "similarities": "1. Both models start from learnable ID embeddings (user_embedding & item_embedding nn.Embedding) that can be reused; NGCF’s embedding_size ↔ NNCF’s ui_embedding_size, so the same initialization & look-up code applies.\n2. They both inject neighborhood signal: NGCF via multi-hop graph convolution and NNCF via explicit neighbor lists; the sparse interaction_matrix construction (coo → csr) and the neighbor indexing logic in NGCF’s get_norm_adj_mat can be copied to build NNCF’s u_neigh/i_neigh tensors.\n3. Layer-wise dropout & activation pipelines are identical: NGCF’s emb_dropout (message_dropout) and LeakyReLU can be reused for NNCF’s MLP layers; the sequential dropout → linear → activation pattern in NGCF’s GNNlayers mirrors NNCF’s MLPLayers.\n4. Both optimize with mini-batch Adam and L2; NGCF’s reg_loss (EmbLoss) and weight-decay term λ||Θ||² can be transplanted directly into NNCF’s BCEWithLogitsLoss training loop.\n5. Matrix-form propagation in NGCF (Equation 7) uses the same Laplacian normalization coefficient 1/√(|Nu||Ni|) that NNCF needs for neighbor re-weighting; the pre-computed SparseL tensor can be reused as a static prior when sampling neighbors.",
    "differences": "1. NNCF needs a new neighbor-sampling module (get_neigh_knn/get_neigh_louvain/get_neigh_random) that returns fixed-length neighbor indices; NGCF has no such sampler—implement from scratch and cache the result as torch.LongTensor of shape [N, neigh_num].\n2. NNCF introduces 1-D CNN + max-pool on neighbor embeddings (user_conv & item_conv) to yield fixed-size vectors; NGCF uses pure matrix multiplications—add nn.Conv1d & nn.MaxPool1d layers and handle variable-length padding with Max_ner().\n3. Interaction function differs: NGCF concatenates multi-layer embeddings and uses inner-product score; NNCF element-wise multiplies user & item vectors (mf_vec) and feeds the concatenated triple [mf_vec, v_u^N, v_i^N] into an MLP—replace NGCF’s final dot-product with an MLPLayers sequence ending in nn.Linear(1) plus BCEWithLogitsLoss.\n4. Loss & label format: NGCF is pairwise (BPR) with negative sampling on-the-fly; NNCF is point-wise binary classification—remove BPRLoss, adopt BCEWithLogitsLoss, and pre-generate 4:1 negative labels in the dataset.\n5. Regularization: NGCF uses node & message dropout on the adjacency; NNCF only uses standard dropout inside MLP—drop SparseDropout & eye_matrix usage, keep simple nn.Dropout.",
    "rank": "rank1"
  },
  {
    "source": "BPR_2009",
    "target": "NNCF_2020",
    "type": "in-domain",
    "similarities": "1. Both rely on latent embedding matrices (user_embedding, item_embedding) that map one-hot IDs to dense vectors; the source already provides the nn.Embedding boilerplate and Xavier init that NNCF can reuse verbatim for its ui_embedding_size vectors.\n2. The element-wise product interaction (user_e * item_e) in BPR’s MF branch is exactly NNCF’s “MF vector” v_u,i; the same torch.mul() pattern can be copied and only needs to be concatenated with neighborhood features instead of being fed to a BPR-loss margin.\n3. BPR’s pairwise training loop (sample (u,i,j), forward both items, compute score difference) can be adapted to NNCF’s pointwise setting by replacing the BPRLoss class with BCEWithLogitsLoss and feeding a single (u,i) pair with label 1/0; the sampler, batching and parameter update code in the trainer remains identical.\n4. Both models expose the same public API (calculate_loss, predict, full_sort_predict) inherited from GeneralRecommender; the source file structure, config parsing and dataset.inter_matrix() usage can be reused without change.",
    "differences": "1. NNCF needs two additional embedding tables (user_neigh_embedding, item_neigh_embedding) of shape (n_items, neigh_embedding_size) and (n_users, neigh_embedding_size) to encode neighbors; these tables do not exist in BPR and must be created and initialized with the same normal_(0,0.01) used for the MF embeddings.\n2. Neighborhood pre-processing module (get_neigh_knn/get_neigh_louvain) is completely new: it builds an interaction graph, runs community detection or KNN similarity, truncates/pads to neigh_num=50, and caches two IntTensors (u_neigh, i_neigh) of size [n_users/items, neigh_num]; none of this logic appears in BPR.\n3. 1-D CNN + MaxPool stack (nn.Conv1d → nn.MaxPool1d → ReLU) on variable-length neighbor embeddings must be implemented; BPR has no convolutional layers. The conv output is flattened and concatenated with the MF vector before the MLP.\n4. MLP component (MLPLayers class with dropout, ReLU, optional BN) and final nn.Linear(1) with BCEWithLogitsLoss replace BPR’s simple dot-product + BPRLoss; the forward path therefore returns a raw logit instead of a pairwise margin, and the trainer must feed negative labels (0) for sampled negatives rather than using the implicit j item in a triplet.",
    "rank": "rank2"
  },
  {
    "source": "EASE_2019",
    "target": "RecVAE_2019",
    "type": "in-domain",
    "similarities": "1. Both models operate on the same sparse implicit-feedback matrix X∈ℝ^{|U|×|I|} and output a |I|×|I| item-item weight matrix (EASE’s B vs. RecVAE’s decoder W) that is used to score every item for a user via a single matrix multiplication; hence the dataset-loading, CSR-matrix creation and user-history slicing logic in EASE.__init__ can be reused verbatim for RecVAE’s get_rating_matrix().\n2. Both papers treat the prediction step as a dense linear operation after the model is trained: EASE does S_u = X_u B, RecVAE does S_u = softmax(z_u W) with z_u = encoder(x_u); the RecBole full_sort_predict() boiler-plate (flatten output, add tie-breaking noise, return 1-D tensor) is identical and can be copied.\n3. They share the same evaluation protocol—rank all unseen items for a user and compute top-N metrics—so the whole RecBole evaluation harness (predict(), full_sort_predict(), interaction unpacking) in the EASE implementation can be kept unchanged; only the internal score computation changes from a sparse-dense matmul to an encoder→softmax pass.\n4. Both models are trained without negative sampling: EASE uses the closed-form least-squares objective on the entire X, while RecVAE uses the multinomial likelihood over the entire item set; therefore the RecBole data-loader that yields full user vectors (no negative-item column) is reusable.",
    "differences": "1. EASE has a closed-form solution that needs only one matrix inversion, whereas RecVAE is a deep variational model that requires stochastic mini-batch training with the reparameterization trick; the whole _train_ease() method must be replaced by a PyTorch training loop with ELBO computation, KL annealing and Monte-Carlo sampling.\n2. RecVAE introduces a dense encoder network (DenseEncoder with layer-norm, Swish, dropout and dense residual connections) plus a linear decoder—none of these neural components exist in EASE; a new file/module containing DenseEncoder, Swish and the RecVAE class inheriting from AutoEncoderMixin must be created from scratch.\n3. RecVAE needs two-phase alternating training (M_enc encoder steps vs. M_dec decoder steps) and a composite prior that caches the previous epoch’s encoder weights; this requires new bookkeeping (encoder_old_state, update_encoder_old_state()) and a training-mode flag that switches between denoising (encoder) and non-denoising (decoder) updates—completely absent in the single-phase EASE code.\n4. RecVAE replaces the global L2 penalty λ with a user-adaptive KL weight β′(x_u)=γ|X_u^o| and a mixture prior; the loss computation in calculate_loss() must be rewritten to (a) sample latent codes, (b) compute Monte-Carlo KL against the mixture prior, and (c) scale KL by the user’s feedback count—far beyond EASE’s simple Frobenius-norm term.\n5. EASE stores a single sparse item-similarity matrix B, while RecVAE must cache user embeddings (mu) at inference time to avoid recomputation; the restore_user_e / restore_item_e caching mechanism and the corresponding other_parameter_name list are new infrastructure not needed for EASE.",
    "rank": "rank1"
  },
  {
    "source": "MultiVAE_2018",
    "target": "RecVAE_2019",
    "type": "in-domain",
    "similarities": "1. **Core VAE Architecture**: Both papers use the same variational autoencoder framework with multinomial likelihood for collaborative filtering - the encoder-decoder structure, reparameterization trick, and KL divergence regularization can be directly reused from MultiVAE's implementation\n\n2. **Multinomial Likelihood Design**: Both models use identical multinomial distribution as the reconstruction loss (cross-entropy over softmax outputs) - MultiVAE's `calculate_loss()` method implementing `-(F.log_softmax(z, 1) * rating_matrix).sum(1).mean()` can be reused without modification\n\n3. **Training Infrastructure**: The base training loop, user batching strategy, and rating matrix preprocessing (`get_rating_matrix()`, `build_histroy_items()`) from MultiVAE can be directly adapted - both use the same pairwise input type and user-centric mini-batch training\n\n4. **Prediction Pipeline**: The prediction methods (`predict()` and `full_sort_predict()`) follow identical patterns - encode user history to latent space, decode to item scores, and apply softmax - enabling code reuse with minimal adaptation",
    "differences": "1. **Composite Prior Implementation**: RecVAE requires implementing a novel mixture prior `p(z) = α·N(0,I) + (1-α)·q_φ_old(z|x)` - this needs new KL divergence computation logic that samples from current posterior and evaluates under previous epoch's posterior, not present in MultiVAE\n\n2. **Dense Encoder Architecture**: RecVAE replaces MultiVAE's simple MLP with DenseNet-style connections, layer normalization, and Swish activations - requires implementing `DenseEncoder` class with concatenated skip connections and proper layer normalization modules\n\n3. **Alternating Training Strategy**: RecVAE trains encoder and decoder separately with different objectives (denoising+VAE loss for encoder, pure reconstruction for decoder) - needs new training loop logic to alternate between `training_encoder=True/False` modes with different loss computations\n\n4. **User-Specific KL Scaling**: RecVAE introduces `β'(x) = γ·|X_u^o|` scaling factor proportional to user feedback count - requires modifying KL loss computation to use dynamic per-user weights instead of MultiVAE's fixed annealing schedule",
    "rank": "rank3"
  },
  {
    "source": "CDAE_2015",
    "target": "RecVAE_2019",
    "type": "in-domain",
    "similarities": "1. Both models inherit from GeneralRecommender and AutoEncoderMixin, sharing the same base class structure, initialization patterns (xavier_normal_initialization), and the build_history_items() method for handling user-item interaction matrices - the CDAE's get_rating_matrix() can be directly reused for RecVAE's data preprocessing.\n\n2. Both employ denoising autoencoder principles with Bernoulli dropout corruption on input ratings - CDAE's nn.Dropout(p=corruption_ratio) implementation pattern can be adapted for RecVAE's noise injection mechanism, sharing the same corruption philosophy though applied at different training phases.\n\n3. Both use single linear decoder layers mapping latent representations to item space - CDAE's self.out_layer = nn.Linear(embedding_size, n_items) architecture can be directly reused as RecVAE's decoder, with the same weight interpretation as item embeddings.\n\n4. Both optimize multinomial likelihood through cross-entropy loss on user-item interactions - CDAE's BCEWithLogitsLoss approach can guide RecVAE's log_softmax + cross-entropy implementation, sharing the same underlying multinomial modeling assumption.",
    "differences": "1. RecVAE requires implementing a variational inference framework with reparameterization trick - completely absent in CDAE - requiring new components: DenseEncoder for amortized inference, reparameterize() method for sampling latent variables, and KL divergence computation between approximate posterior and composite prior.\n\n2. RecVAE introduces a novel composite prior (Eq.17) mixing standard Gaussian with previous epoch's posterior - requiring implementing compute_kl_divergence() with mixture-of-Gaussians KL estimation via Monte Carlo sampling, plus maintaining encoder_old_state for parameter snapshots across epochs.\n\n3. RecVAE implements alternating training (Algorithm 1) with asymmetric encoder/decoder updates - requiring new training logic: set_training_mode() to switch between encoder/decoder phases, different loss computations (with/without KL term), and n_enc_epochs vs n_dec_epochs hyperparameters for update frequency control.\n\n4. RecVAE employs user-specific KL weight scaling (β'(x) = γ|X_u^o|) proportional to feedback amount - requiring dynamic KL weight computation based on rating_matrix.sum(1) per user, plus γ hyperparameter for scaling control - completely different from CDAE's fixed L2 regularization weights.",
    "rank": "rank4"
  },
  {
    "source": "SLIMElastic_2011",
    "target": "RecVAE_2019",
    "type": "in-domain",
    "similarities": "1. Both papers model user-item interactions using matrix factorization principles - SLIM learns a sparse item-item similarity matrix W while RecVAE learns item embeddings through decoder weights, both enabling top-N recommendation via matrix multiplication (SLIM: a_i^T * W, RecVAE: decoder(z)). The source's sparse matrix handling patterns can be reused for RecVAE's item embedding storage.\n2. Both employ regularized optimization with similar hyperparameter structures - SLIM uses ElasticNet (l1_ratio, alpha) while RecVAE uses gamma for KL scaling and alpha for composite prior. The source's parameter initialization and configuration loading patterns in __init__ can be directly adapted.\n3. Both implement column-wise parallelization - SLIM solves independent optimization problems for each item column while RecVAE trains decoder parameters (item embeddings) independently. The source's decoupled learning approach can guide RecVAE's alternating training implementation.\n4. Both use sparse input handling with CSR/LIL matrix formats and implement efficient batch prediction through matrix operations. The source's interaction_matrix storage and prediction methods can be reused for RecVAE's rating matrix processing.",
    "differences": "1. SLIM uses closed-form ElasticNet optimization with sklearn's ElasticNet, while RecVAE requires implementing variational inference with reparameterization trick, KL divergence computation, and Monte Carlo sampling - entirely new components not present in source.\n2. RecVAE introduces a complex encoder-decoder architecture with DenseEncoder containing layer normalization, swish activation, and dense connections - SLIM has no neural network components, requiring complete implementation of forward/backward passes and gradient computation.\n3. RecVAE implements alternating training (Algorithm 1) with separate encoder/decoder phases, composite prior requiring storing old encoder parameters, and user-specific KL weight scaling - SLIM's single-phase training loop needs complete restructuring with training mode flags and parameter state management.\n4. RecVAE requires implementing variational-specific loss components: KL divergence between Gaussian and mixture of Gaussians (compute_kl_divergence), multinomial likelihood with cross-entropy, and reparameterization sampling - SLIM's simple MSE loss with regularization is insufficient.",
    "rank": "rank5"
  },
  {
    "source": "GCMC_2018",
    "target": "SpectralCF_2018",
    "type": "in-domain",
    "similarities": "1. Both models treat the user-item interaction matrix as a bipartite graph and use learnable embeddings for users/items—GCMC’s `user_embedding`/`item_embedding` tables can be reused verbatim, only the initialization distribution changes (Gaussian vs Xavier).\n2. Both stack multiple graph-aware layers and concatenate layer-wise representations (GCMC’s `embeddings_list` and SpectralCF’s `embeddings_list`)—the list management and final `torch.cat(..., dim=1)` pattern in GCMC’s `forward()` can be copied directly.\n3. Both employ a dense, differentiable “convolution” operator that is pre-computed once and applied via sparse-dense matrix multiply—GCMC’s `_graph_convolution()` already materializes a fixed sparse operator per rating type; the same caching strategy (build once, store in `self.spectral_operator`) is used in SpectralCF.\n4. Both use mini-batch training with negative sampling and expose `calculate_loss()` and `predict()` APIs—GCMC’s training loop, batch generation, and `full_sort_predict()` caching logic can be reused with only the inner loss call swapped to BPR.\n5. Both models apply dropout (node or standard) and L2 regularization on embeddings—GCMC’s `F.dropout`, `reg_weight * reg_loss` pattern, and parameter-list traversal for L2 can be transplanted without change.",
    "differences": "1. GCMC performs edge-type-specific spatial graph convolution (one adjacency per rating) while SpectralCF needs a **spectral** convolution based on the eigen-decomposition of the normalized Laplacian—**new code required** for `get_laplacian_matrix()`, `eigendecompose_laplacian()`, and `compute_spectral_operator()`.\n2. GCMC uses a **bilinear decoder** with ordinal weight sharing and cross-entropy over explicit ratings; SpectralCF uses **inner-product decoder** with BPR loss for implicit feedback—**replace** `_bilinear_decoder()` and `F.cross_entropy` with `torch.mul(u,v).sum(dim=1)` and `BPRLoss()`.\n3. GCMC keeps separate rating-specific adjacency matrices (`rating_matrices` list); SpectralCF needs a **single normalized Laplacian** and its top eigen-pairs—**drop** the rating-splitting logic in `_build_rating_matrices()` and instead build one symmetrically-normalized Laplacian.\n4. GCMC’s graph convolution is **sparse-message-passing** (`torch.sparse.mm(adj_r, transformed)`); SpectralCF’s is **dense spectral filtering** (`torch.mm(spectral_operator, x_transformed)`)—**new operator** must be implemented and cached as a dense tensor.\n5. GCMC initializes embeddings via Xavier and uses ReLU; SpectralCF requires **Gaussian N(0.01, 0.02)** initialization and **sigmoid** activation inside the spectral layer—adjust `nn.init` calls and change `F.relu` to `torch.sigmoid` in the convolution step.",
    "rank": "rank1"
  },
  {
    "source": "NGCF_2019",
    "target": "SpectralCF_2018",
    "type": "in-domain",
    "similarities": "1. Both models operate on the same user-item bipartite graph structure and reuse the identical Laplacian construction (`get_norm_adj_mat()` in NGCF vs `get_laplacian_matrix()` in SpectralCF) — the sparse COO adjacency, degree-normalization D^{-0.5} A D^{-0.5}, and block matrix [[0,R],[R^⊤,0]] can be copied verbatim.\n2. Layer-wise embedding propagation followed by concatenation: NGCF’s `embeddings_list` and final `torch.cat(embeddings_list, dim=1)` can be kept; only the inner `gnn()` call has to be replaced by the spectral convolution routine.\n3. Prediction & loss pipeline is identical — inner-product interaction, BPR loss with `BPRLoss()`, L2 regularization via `EmbLoss()`, and the same `full_sort_predict()` caching pattern; all training loops, negative sampling and batch-generation code can be reused without change.\n4. Parameter initialization and dropout utilities (Xavier, `SparseDropout`, `nn.Dropout`) are already available in the NGCF file and can be applied to the new spectral filter matrices.",
    "differences": "1. Core propagation operator: NGCF uses spatial-domain message passing (`W1 e_j + W2(e_j ⊙ e_u)` with neighbour aggregation), whereas SpectralCF needs spectral convolution `σ( (UU^⊤ + UΛU^⊤) X Θ′ )`; thus the whole `BiGNNLayer` must be replaced by a new `spectral_convolution()` module that pre-computes the dense spectral operator `UU^⊤ + UΛU^⊤` after explicit eigendecomposition (`torch.linalg.eigh`).\n2. Eigendecomposition overhead: SpectralCF requires full eigendecomposition of the Laplacian (O(N³) dense) and storage of dense eigen-matrices; NGCF avoids this completely. A new `eigendecompose_laplacian()` method and the resulting dense `spectral_operator` tensor must be implemented and moved to GPU.\n3. Filter parameters per layer: NGCF keeps two weight matrices per layer (`W1,W2`); SpectralCF keeps a single filter matrix `Θ′_k` (shape C×F for k=0, F×F for k>0) stored in a `nn.ParameterList`. The parameter creation and forward pass must be rewritten accordingly.\n4. Activation & normalization: NGCF uses LeakyReLU + L2 norm after each layer; SpectralCF uses sigmoid activation and no explicit normalization — remove the `LeakyReLU` and `F.normalize` lines in the forward loop.",
    "rank": "rank2"
  },
  {
    "source": "BPR_2009",
    "target": "SpectralCF_2018",
    "type": "in-domain",
    "similarities": "1. Both models adopt the identical BPR pairwise loss (BPRLoss class) for implicit feedback: −ln σ(ŷ_{ui}−ŷ_{uj}) with negative sampling, so the complete BPR training loop and negative-item minibatch construction in BPR.calculate_loss can be reused verbatim—only the embedding tensors fed into the loss change.\n2. Both inherit GeneralRecommender, use the same InputType.PAIRWISE dataloader interface and produce user/item embeddings that are dot-producted for ranking; hence the predict() and full_sort_predict() patterns (caching, batched matmul) are directly copy-pasteable.\n3. Embedding look-up layers (nn.Embedding for users/items) and Xavier normal initialization utility are identical; the initialization block and get_user/item_embedding helpers can be kept without modification.",
    "differences": "1. SpectralCF replaces the simple latent matrix factorization of BPR with K-layer spectral graph convolution; a completely new spectral_convolution() operator must be implemented that applies the precomputed (UUᵀ + UΛUᵀ) matrix to embedding matrices and includes eigendecomposition preprocessing (get_laplacian_matrix, eigendecompose_laplacian, compute_spectral_operator) which BPR lacks.\n2. SpectralCF introduces multi-channel / multi-filter parameters Θ′_k (filter_params ParameterList) and layer-wise propagation plus concatenation (Eq. 12) producing final embeddings of size C+KF; BPR has only one static embedding per user/item—thus the forward() pass and parameter sets are entirely new.\n3. SpectralCF needs a bipartite graph construction step (interaction_matrix → sparse adjacency → normalized Laplacian) and offline eigen-decomposition on the full |U|+|I| graph; BPR requires no graph or spectral preprocessing, so dataset preparation and model construction pipelines must be extended.\n4. Regularization in SpectralCF is applied only to the initial (ego) embeddings via EmbLoss, while BPR’s L2 term acts on all parameters; the reg_loss computation therefore differs and the reg_weight hyper-parameter is newly exposed.",
    "rank": "rank3"
  }
]