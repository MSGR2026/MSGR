{
  "domain": "Recsys",
  "task": "GeneralRecommendation",
  "items": [
    {
      "id": "recbole_framework_spec",
      "title": "RecBole 框架工程规范",
      "content": "模型类继承 GeneralRecommender，__init__ 首行调用 super().__init__(config, dataset)，使用父类属性 self.n_users、self.n_items、self.device。配置访问用 config['key']，且读取的参数必须在 __init__ 中绑定为 self 属性。必须实现 forward()、calculate_loss(interaction)、predict(interaction)、full_sort_predict(interaction)。interaction 通过 self.USER_ID、self.ITEM_ID、self.NEG_ITEM_ID 索引。传统闭形式解方法需设 type = ModelType.TRADITIONAL 避免框架调用 backward()。参数初始化用 self.apply(xavier_normal_initialization)。",
      "tags": [
        "framework",
        "recbole",
        "interface",
        "critical"
      ],
      "source": "framework_analysis",
      "priority": 10
    },
    {
      "id": "input_type_paradigm_matching",
      "title": "InputType 与模型范式的对应关系",
      "content": "InputType 由模型的数据处理范式决定，不可随意选择。自编码器范式的模型（VAE、DAE、Flow、Diffusion等生成式模型）必须使用 InputType.LISTWISE，因为它们输入输出都是完整的用户-物品交互向量。这类模型的数据流为 get_rating_matrix(user) → [batch, n_items] → model → [batch, n_items]，通常同时继承 AutoEncoderMixin。如果论文描述模型为生成式、自编码器、扩散模型、流模型、使用重构损失或处理完整交互向量，必须使用 InputType.LISTWISE。排序学习范式（BPR、协同过滤）使用 InputType.PAIRWISE，需要 NEG_ITEM_ID 并使用 BPRLoss。错误的 InputType 会导致框架行为完全错误。",
      "tags": [
        "input_type",
        "paradigm",
        "critical",
        "autoencoder"
      ],
      "source": "paradigm_analysis",
      "priority": 10
    },
    {
      "id": "config_parameter_consistency",
      "title": "配置参数命名的一致性要求",
      "content": "YAML 配置文件采用扁平结构，每行一个参数，值为标量或必要的列表。配置文件中的键名必须与代码中 config['key'] 完全一致，大小写、下划线均不能有差异。典型错误包括配置写 ssl_temp 但代码读 ssl_tau、配置写 dropout_prob 但代码读 drop_ratio。不一致会导致 KeyError 或默认值回退，性能骤降。特定 Trainer 对模型属性有隐式要求，例如 RecVAETrainer 期望 model.encoder 和 model.decoder 属性存在。",
      "tags": [
        "config",
        "yaml",
        "naming",
        "critical"
      ],
      "source": "error_analysis",
      "priority": 10
    },
    {
      "id": "timestep_embedding_patterns",
      "title": "时间步嵌入的标准模式与变体",
      "content": "时间条件模型（扩散、流模型）需要时间步嵌入：\n\n标准 timestep_embedding(t, dim):\n  half = dim // 2\n  freqs = exp(-log(max_period) * arange(half) / half).to(device)\n  args = t[:, None] * freqs[None]\n  emb = cat([cos(args), sin(args)], dim=-1)\n\n变体 timestep_embedding_pi(t, dim) - 注意 2π 缩放:\n  freqs = exp(-log(max_period) * arange(half) / half) * 2 * π\n  # 其余相同\n\n关键：论文中如果函数名为 timestep_embedding_pi 或明确提到 2π，必须添加该缩放因子。这影响频率范围和时间感知能力。时间嵌入后通常通过 Linear 层处理，然后与输入拼接后送入 MLP。",
      "tags": [
        "time_embedding",
        "diffusion",
        "flow",
        "sinusoidal"
      ],
      "source": "architecture_pattern",
      "priority": 9
    },
    {
      "id": "mlp_construction_patterns",
      "title": "MLP 构建的两种模式与选择",
      "content": "RecBole 中构建 MLP 有两种方式：\n\n方式1：使用 RecBole 的 MLPLayers 工具类（推荐）\n  from recbole.model.layers import MLPLayers\n  mlp = MLPLayers(\n    layers=[input_dim, hidden1, hidden2, output_dim],\n    dropout=0.1,\n    activation='tanh',  # 'relu', 'sigmoid', 'tanh'\n    last_activation=False  # 输出层是否激活\n  )\n  优点：简洁、与框架统一、少出错\n  何时用：论文未明确指定特殊激活/归一化时优先使用\n\n方式2：手动构建 nn.Sequential\n  layers = []\n  for hidden_dim in hidden_dims:\n    layers.append(nn.Linear(prev_dim, hidden_dim))\n    layers.append(nn.LayerNorm(hidden_dim))  # 或 BatchNorm\n    layers.append(nn.GELU())  # 或其他激活\n    layers.append(nn.Dropout(p))\n  mlp = nn.Sequential(*layers)\n  何时用：论文明确要求 LayerNorm/BatchNorm/GELU 等特殊配置\n\n选择原则：\n- 如果论文说\\\"MLP\\\"、\\\"多层感知机\\\"、\\\"全连接层\\\"但未详细说明，用 MLPLayers\n- 激活函数：传统模型（2020前）多用 tanh/relu，现代模型可能用 GELU\n- dropout：通常 0.1-0.2 是安全值，除非论文明确指定\n- 避免过度工程化：不要无依据地添加 LayerNorm、残差连接等",
      "tags": [
        "mlp",
        "architecture",
        "layers",
        "activation"
      ],
      "source": "implementation_pattern",
      "priority": 9
    },
    {
      "id": "generative_model_components",
      "title": "生成式推荐模型的核心组件",
      "content": "生成式推荐模型（VAE、Diffusion、Flow）通常包含以下组件：时间步嵌入用于时间条件模型，标准实现使用正弦位置编码，某些变体会额外乘以 2π 改变频率范围；先验分布可能是高斯先验或行为引导先验，后者基于物品交互频率构建 Bernoulli 分布；插值机制在训练时混合观测数据和先验，连续插值使用加权和，离散插值使用 Bernoulli 采样的二值掩码；推理过程可能是单次前向（VAE）或迭代采样（Diffusion、Flow），迭代方法需要多步更新并可能需要保留已知交互避免推荐用户已交互物品；MLP 架构选择上，简单场景可用 RecBole 的 MLPLayers 工具类，复杂场景需要手动构建包含 LayerNorm、特殊激活函数等。传统推荐模型（2020年前）多用 tanh/relu 和较小的 dropout（0.1），现代模型可能用 GELU 和较大的 dropout（0.5）。",
      "tags": [
        "generative",
        "vae",
        "diffusion",
        "flow",
        "architecture"
      ],
      "source": "model_pattern",
      "priority": 9
    },
    {
      "id": "gnn_contrastive_learning",
      "title": "图神经网络中的对比学习模式",
      "content": "GNN 的不同层嵌入捕获了不同范围的邻域信息，第 k 层嵌入聚合了 k 跳邻居的信息。这种层次结构为对比学习提供了多种可能的视图构建方式。传统方法通过数据增强（节点删除、边扰动）创建多个视图，但增强可能引入语义噪声。某些方法利用 GNN 层间的自然差异作为对比视图，无需额外增强。对比学习的正样本定义有多种策略：自我对比将同一节点在不同视图中的表示作为正样本对，每个节点只有 1 个正样本；邻域对比将节点与其图邻居作为正样本对，每个节点有 |N_u| 个正样本且数量可变；层间对比利用节点在相邻 GNN 层的表示关系。InfoNCE 损失的实现取决于正样本数量：单正样本可以直接向量化计算，多正样本需要对每个节点分别计算其多个正样本的相似度并除以正样本数量归一化。负样本池的选择影响计算效率和学习效果，batch 内负样本计算快但可能质量低，全局负样本质量高但计算开销大，某些方法使用类型限制的负样本（如用户只以物品作为负样本）。温度参数 τ 通常在 0.1-0.5 之间，控制分布的平滑度。实现时使用 logsumexp 技巧保证数值稳定性。",
      "tags": [
        "gnn",
        "contrastive_learning",
        "ssl",
        "infonce"
      ],
      "source": "gnn_cl_pattern",
      "priority": 9
    },
    {
      "id": "graph_computation_efficiency",
      "title": "图计算的效率考虑",
      "content": "图推荐系统中涉及大量的图结构操作，效率优化至关重要。图传播通常采用对称归一化的稀疏邻接矩阵与嵌入矩阵相乘，使用 torch.sparse.mm 进行稀疏矩阵运算比密集矩阵快数十倍。GNN 层数一般控制在 2-3 层，过深容易过平滑。邻居信息的访问方式影响性能：如果只需要做 GCN 传播，稀疏邻接矩阵隐式查询即可；如果需要显式获取每个节点的邻居列表（如某些对比学习方法需要遍历邻居），可以从 interaction_matrix 的 COO 格式提取邻居关系。显式构建邻居列表时，Python 循环遍历节点可能成为瓶颈，可以预先将邻居索引转为 Tensor 并移到 GPU，减少设备间传输。批量计算优于逐个计算，例如负样本相似度可以用矩阵乘法 anchor @ all_neg.T 一次性计算，而可变长度的正样本可能需要循环处理。对比学习如果使用全局负样本池，计算复杂度为 O(|batch| × |全体节点|)，如果训练很慢应检查是否可以优化负样本计算或减少对比层数。",
      "tags": [
        "efficiency",
        "graph",
        "sparse_matrix",
        "optimization"
      ],
      "source": "performance_pattern",
      "priority": 8
    },
    {
      "id": "general_rec_paradigms",
      "title": "通用推荐的主流建模范式",
      "content": "通用推荐常见三类建模路线：协同过滤类（MF、GCN）直接学习 user/item 嵌入并用内积打分，使用 InputType.PAIRWISE；生成式/自编码类（VAE、DAE、扩散、流）以重构交互向量为核心，使用 InputType.LISTWISE；自监督增强类（对比学习）在主任务上叠加多视图一致性损失。复现时需要明确模型是否需要全量物品打分、是否依赖稀疏矩阵传播、是否包含额外的自监督项。",
      "tags": [
        "paradigm",
        "mf",
        "autoencoder",
        "ssl"
      ],
      "source": "model_taxonomy",
      "priority": 7
    },
    {
      "id": "efficient_contrastive_implementation",
      "title": "对比学习的高效工程实现",
      "content": "学术论文中常将对比损失描述为‘对所有负样本求和’或‘遍历所有邻居’。在工程实现时，**不要使用显式循环（for-loop）**。需要采用以下范式：1. **批次内负采样 (In-Batch Negatives)**：利用矩阵乘法 `(Batch, Dim) @ (Dim, Batch)` 一次性计算样本与批次内所有其他样本的相似度；2. **隐式邻居 (Implicit Neighbor)**：在 BPR 训练的 `(user, pos_item)` 数据流中，`pos_item` 本身就是 `user` 的采样邻居，直接将其作为正样本对，无需额外查询邻接表。",
      "tags": ["efficiency", "contrastive_learning", "optimization", "critical"],
      "priority": 10
    },
    {
      "id": "flow_model_architecture_patterns",
      "title": "流匹配模型的架构偏好",
      "content": "流匹配（Flow Matching）与扩散模型不同，其目标是学习确定性向量场。工程经验表明：1. **激活函数**：相比于无界的 GELU/ReLU，**Tanh** 往往能提供更稳定的梯度，尤其是在输出空间受限（如推荐系统的评分）时；2. **正则化**：由于 Flow 需要精确拟合轨迹，过高的 Dropout（如 0.5）会导致欠拟合，通常应设为 **0.0 到 0.1**；3. **网络深度**：相比图像任务，推荐系统的 Flow 模型通常较浅（2-3层 MLP）。",
      "tags": ["generative", "flow_matching", "architecture", "hyperparameter"],
      "priority": 9
    },
    {
      "id": "discrete_process_boundary",
      "title": "离散过程的边界处理",
      "content": "在复现涉及离散步数（Steps）的算法（如 Diffusion 或 Discrete Flow）时，必须通过**手动推演**来验证循环边界。论文算法伪代码通常从 1 到 N，而 Python 代码是 0 到 N-1。特别注意：**推理过程通常需要包含最后一步（t=1 或 t=N）的计算**，简单的 `range(N)` 可能会漏掉最后一次精修（Refinement），导致性能显著下降。",
      "tags": ["implementation", "logic", "discrete_process", "debugging"],
      "priority": 9
    }
  ]
}