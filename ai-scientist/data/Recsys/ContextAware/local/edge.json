[
  {
    "source": "DeepFM_2017",
    "target": "AFM_2017",
    "type": "in-domain",
    "similarities": "1. Both models inherit from `ContextRecommender` and share identical embedding initialization (`xavier_normal_`) and field concatenation (`concat_embed_input_fields`)—reuse DeepFM’s embedding lookup and sparse→dense conversion verbatim.\n2. First-order linear part is identical: `first_order_linear(interaction)` returns the same sparse linear score in both codes; copy the module without change.\n3. FM-style pairwise interaction vectors are already built inside DeepFM’s `BaseFactorizationMachine`; AFM only needs to extract the element-wise product `vi⊙vj` (same tensor op) and feed it to attention instead of summing—reuse the `torch.mul(p, q)` pattern in `build_cross`.",
    "differences": "1. DeepFM has NO attention network; AFM requires a new `AttLayer` module (tiny MLP + softmax) that takes `[B, num_pairs, k]` interaction tensors and outputs `[B, num_pairs]` weights—implement from scratch or adapt the inlined `AttLayer` class.\n2. DeepFM pools second-order terms with simple sum-reduction; AFM pools with learned attention weights followed by a learnable projection vector `p`—add `self.p` parameter and weighted-sum logic in `afm_layer`.\n3. DeepFM concatenates all embeddings and sends them through an MLP for high-order signals; AFM discards any MLP after attention pooling—remove `MLPLayers` and `deep_predict_layer` entirely.\n4. AFM applies pair-wise dropout and L2 regularization **only** on attention weight matrix `W`; DeepFM uses dropout inside MLP and no special reg—add `reg_weight * ||W||2` term in `calculate_loss` and a `Dropout` layer on `att_pooling`.",
    "rank": "rank1"
  },
  {
    "source": "FwFM_2018",
    "target": "AFM_2017",
    "type": "in-domain",
    "similarities": "1. Both inherit from ContextRecommender and share identical scaffolding: embedding lookup, first_order_linear(), forward(), calculate_loss(), predict(), BCEWithLogitsLoss, xavier_normal_ init, and the same batch-oriented data flow—reuse FwFM’s dataset/field parsing and training loop entirely.\n2. Pair-wise interaction enumeration is structurally the same: nested loops over field indices (i<j) producing a list/Tensor of size num_pairs = n_field(n_field-1)/2; the AFM build_cross() can directly adopt FwFM’s double-loop pattern and row/col index tensors with only a rename.\n3. Embedding tensor shape [B, num_fields, K] and the subsequent element-wise product (vi⊙vj) appear in both; FwFM already materializes infeature[:,i]*infeature[:,j] so the AFM pairwise_inter tensor can be obtained by a lightweight refactor of fwfm_inter creation without extra kernels.",
    "differences": "1. FwFM uses a static scalar r_{F(i),F(j)} matrix (n_field×n_field) for each field pair; AFM replaces this with a learnable attention network (W,b,h) that outputs a context-aware weight a_{ij} per sample—requires implementing AttLayer (Linear + ReLU + softmax) and integrating it into the interaction pipeline.\n2. AFM pools interactions with a_{ij}-weighted sum followed by a projection vector p (K→1); FwFM simply sums r-weighted dot-products. New code path: att_signal = softmax(h^T ReLU(W(vi⊙vj)+b)) → weighted sum → dot with p → dropout → scalar score.\n3. AFM introduces L2 regularization on attention weights (reg_weight*||W||^2) inside calculate_loss(); FwFM has no such term—add explicit l2_loss computation and include it in the final loss.\n4. AFM applies dropout to the pooled interaction vector after attention pooling; FwFM applies dropout earlier to the summed field products—move dropout_layer invocation to sit after the weighted-sum step in AFM.",
    "rank": "rank2"
  },
  {
    "source": "NFM_2017",
    "target": "AFM_2017",
    "type": "in-domain",
    "similarities": "1. Embedding layer reuse: both models inherit `ContextRecommender` and use `concat_embed_input_fields()` to obtain [B, F, k] dense embeddings; NFM’s embedding table and initialization (`xavier_normal_`) can be copied verbatim.\n2. First-order linear part: both papers keep FM-style linear regression `w_0 + ∑w_i x_i`; NFM’s `self.first_order_linear(interaction)` module is 100 % reusable in AFM.\n3. Element-wise (Hadamard) interaction: NFM’s `Bi-Interaction` layer already computes `∑_{i<j} v_i x_i ⊙ v_j x_j` in vectorized form; the same `torch.mul(p, q)` pattern appears in AFM’s `build_cross()` and can be adapted.\n4. Training pipeline: squared-loss with `BCEWithLogitsLoss`, `sigmoid` output, `xavier_normal_` init, mini-batch Adagrad, dropout and early stopping hyper-parameters are identical—copy the training & evaluation loops.\n5. Regularization utilities: dropout layer (`nn.Dropout`), L2 norm helper (`torch.norm`) and batch-size / learning-rate schedules from NFM can be reused without change.",
    "differences": "1. Attention subnet is **new**: implement `AttLayer` with learnable `W` (k×t) and `h` (t) that maps each pairwise interaction vector `[B, num_pairs, k]` → attention score `[B, num_pairs, 1]` via ReLU + softmax; NFM has no such parameters.\n2. Pooling strategy swap: NFM sums all interaction vectors then feeds the single `k`-vector to MLP; AFM keeps individual `num_pairs` vectors, weights them with attention, then sums to one `k`-vector—replace NFM’s `Bi-Interaction` + `MLPLayers` with attention-weighted sum (`att_pooling`).\n3. Projection head: AFM adds final learnable vector `p ∈ ℝ^k` that dot-products the pooled interaction vector; create `nn.Parameter(torch.randn(k))` and insert `torch.mul(att_pooling, self.p)` before the scalar output—absent in NFM.\n4. Regularization scope: AFM needs explicit `L2` on attention matrix `W` (`reg_weight * ‖W‖²`) added to the loss; NFM only uses dropout—extend `calculate_loss()` accordingly.\n5. Computational graph change: NFM’s complexity is `O(k N_x + MLP)`; AFM must materialize all `num_pairs = F(F-1)/2` interactions and store attention scores, so memory scales quadratically with field count—no MLP layers are required, delete NFM’s `mlp_layers` and `predict_layer` modules.",
    "rank": "rank3"
  },
  {
    "source": "PNN_2016",
    "target": "AFM_2017",
    "type": "in-domain",
    "similarities": "1. Both models begin with the same embedding lookup and field-wise concatenation (`concat_embed_input_fields` in PNN vs. `afm_all_embeddings` in AFM), so the entire embedding-dictionary and batch-to-tensor routine can be copied verbatim.\n2. Pairwise interaction enumeration is identical: the nested loops that build `row` and `col` index lists in `InnerProductLayer.forward` and `OuterProductLayer.forward` can be reused 1-to-1 in `AFM.build_cross` to obtain the `p`, `q` tensors for element-wise product.\n3. Element-wise product of two embedding vectors (`p*q` in PNN, `torch.mul(p,q)` in AFM) is already implemented; the resulting tensor shape `[B, num_pairs, emb_dim]` is the direct input to the attention network, so no reshaping glue code is needed.\n4. Both papers use the same supervised binary-classification setup (sigmoid output + `BCEWithLogitsLoss`) and the same parameter initialization (`xavier_normal_` on embeddings and linear layers), hence the `_init_weights` method and the loss-computation skeleton can be reused.",
    "differences": "1. PNN feeds the interaction vector to an MLP; AFM needs an attention module (`AttLayer`) that is **not present** in PNN—this requires implementing a small network `h^T ReLU(W·x+b)` followed by softmax over pairs.\n2. PNN keeps raw inner/outer products as fixed features; AFM **learns a weight `a_ij` per pair**, so the static `InnerProductLayer` and `OuterProductLayer` classes must be replaced by a trainable `AttLayer` that outputs `[B, num_pairs]` attention scores.\n3. AFM pools the weighted interaction vectors with `sum(a_ij * (v_i⊙v_j))` and then projects with a **single vector `p`**; PNN instead concatenates everything and sends it through deep layers—hence the large `MLPLayers` module in PNN is **dropped** and replaced by one `nn.Parameter(p)` and a `torch.sum(..., dim=1)` reduction.\n4. AFM introduces **dropout directly on the pooled interaction vector** and an explicit `L2` penalty only on the attention matrix `W`, whereas PNN uses dropout inside the deep tower and an overall `reg_weight` on all MLP weights; the regularization logic in `calculate_loss` must be rewritten accordingly.",
    "rank": "rank4"
  },
  {
    "source": "xDeepFM_2018",
    "target": "AFM_2017",
    "type": "in-domain",
    "similarities": "1. **Embedding & Linear components**: Both models inherit from `ContextRecommender`, reuse `concat_embed_input_fields()` to obtain `[B,F,E]` embedding tensor, and share the same `first_order_linear()` module for 1-order terms—copy these two lines verbatim from xDeepFM to bootstrap AFM.\n2. **Pair-wise feature interaction construction**: xDeepFM’s `build_cross()` (row/col indexing) produces the same `[B,num_pairs,E]` pairwise tensors `p,q` that AFM needs; refactor this helper into AFM to avoid re-implementing the double-loop indexing logic.\n3. **Hadamard product & reduction**: Both compute element-wise product `p*q` and later reduce with `sum(dim=1)`; the tensor shapes and einsum-free reduction pattern in xDeepFM can be reused for AFM’s `pair_wise_inter`.\n4. **Regularized BCE loss pipeline**: `BCEWithLogitsLoss()`, `sigmoid()` in `predict()`, and L2 reg on specific weights (`reg_loss()` helper) are already coded in xDeepFM—lift the loss-calculation skeleton and only change the parameter list passed to `reg_loss()`.\n5. **Xavier init & dropout placement**: Parameter init function `_init_weights()` and dropout before final pooling in xDeepFM can be transplanted directly to AFM.",
    "differences": "1. **Attention module vs CIN**: AFM needs a new `AttLayer` (FC → ReLU → weight vector → softmax) that outputs `[B,num_pairs,1]` attention scores; xDeepFM has no attention and instead uses 1×1-conv layers—implement `AttLayer` from scratch.\n2. **Pooling strategy**: AFM performs *weighted* sum pooling with learned attention, whereas xDeepFM does *plain* sum pooling over CIN feature maps; replace the `torch.sum(result, -1)` line with `torch.sum(att_signal * pair_wise_inter, dim=1)` and add a projection vector `p`.\n3. **Interaction order & depth**: xDeepFM can model arbitrary-order interactions via stacked CIN layers; AFM is strictly 2-order—remove the CIN loop and `conv1d_list`, keeping only one interaction layer.\n4. **Dropout & reg target**: AFM applies dropout to the *interaction representations* and regularizes only the attention matrix `W`; xDeepFM regularizes every conv kernel—adjust `calculate_reg_loss()` to `torch.norm(self.attlayer.w.weight, p=2)` and insert `self.dropout_layer` right after the weighted interaction tensor.\n5. **Output tensor contraction**: AFM needs an extra `torch.mul(pooling, self.p)` followed by `sum(dim=1)` to project the `k`-dim interaction vector to a scalar; xDeepFM uses a `Linear(final_len,1)`—add this two-line projection in AFM’s `afm_layer()`.",
    "rank": "rank5"
  },
  {
    "source": "DCNV2_2020",
    "target": "DCN_2017",
    "type": "in-domain",
    "similarities": "1. Both models share the same parallel architecture: embedding → [cross network] + [deep network] → concat → logits; the overall forward() skeleton in DCNV2 can be reused by simply deleting the mixed/stacked branches and keeping the parallel branch.\n2. The embedding layer, MLP module (MLPLayers), RegLoss, BCELoss and sigmoid prediction head are identical; all related code (concat_embed_input_fields, mlp_layers, predict_layer, calculate_loss) can be copied verbatim.\n3. Cross-layer residual formula is structurally the same: x_{l+1}=x_0⊙(W_l x_l+b_l)+x_l (DCNV2) vs x_{l+1}=x_0⊙(x_l^T w_l)+b_l+x_l (DCN); the outer loop over cross_layer_num and the in-place update pattern in DCNV2.cross_network() can be kept—only the inner matmul needs to be replaced by a rank-1 tensordot.",
    "differences": "1. DCN uses rank-1 interaction (x_l^T w_l produces a scalar per sample) while DCNV2 uses full-matrix interaction (W_l x_l produces a vector); the core cross_network() method must be re-implemented with torch.tensordot(x_l, w_l, dims=([1],[0])) followed by the outer-product trick shown in the 2017 code.\n2. DCN keeps w_l and b_l as vectors (d params each), so ParameterList storage and reg_loss computation must switch from DCNV2’s nn.Parameter(shape=(d,d)) to nn.Parameter(shape=(d,)) and adjust the l2-regularisation term accordingly.\n3. DCN has no mixture-of-experts, low-rank matrices, gating nets, or stacked structure; all mixed flags, expert_num, low_rank, cross_layer_u/v/c tensors, and the entire cross_network_mix() method must be removed to reproduce the 2017 model.\n4. DCN’s original code uses BCEWithLogitsLoss (numerically stable) whereas DCNV2 uses BCELoss after sigmoid; for strict reproduction the predict() method should return sigmoid(forward()) while calculate_loss feeds raw logits to BCEWithLogitsLoss.",
    "rank": "rank1"
  },
  {
    "source": "xDeepFM_2018",
    "target": "DCN_2017",
    "type": "in-domain",
    "similarities": "1. **Embedding & stacking layer**: both papers flatten field embeddings into a single vector `x_0` of shape `[batch, num_field*embed_dim]`; the xDeepFM implementation already concatenates embeddings via `concat_embed_input_fields()`—this tensor can be fed directly to DCN’s cross and deep branches without change.\n2. **Explicit high-order interaction module**: CIN in xDeepFM and Cross Network in DCN are both additive residual structures that multiply the current hidden state with the original input; the outer-product-then-1×1-conv pattern in CIN (`torch.einsum + nn.Conv1d`) can be simplified to DCN’s rank-one operation `x₀·(xₗᵀ w)` by replacing the conv kernel with a single vector `w` and removing the feature-map expansion, reusing the same `x_0` tensor.\n3. **Parallel DNN branch**: both models append a standard MLP after the interaction module; xDeepFM’s `MLPLayers` utility (with ReLU, dropout, BN) can be instantiated unchanged for DCN’s deep network—only the input size list needs to be adjusted to `[num_field*embed_dim] + mlp_hidden_size`.\n4. **Output & loss head**: final prediction is `σ(linear(concat(interaction_out, dnn_out)))` with `BCEWithLogitsLoss` and L2 reg on interaction weights; the xDeepFM snippet `predict_layer = nn.Linear(final_len + mlp_hidden_size[-1], 1)` and `calculate_loss()` can be copied verbatim after setting `final_len = num_field*embed_dim` for DCN.",
    "differences": "1. **Cross layer math vs CIN**: DCN needs a new `cross_network()` function that implements the exact residual formula `x_{l+1} = x₀⊙(xₗᵀ w_l) + b_l + x_l` with `torch.tensordot` and broadcasting; CIN’s 3-D convolution path must be removed.\n2. **Parameter tensors**: DCN requires `nn.ParameterList` of vectors `cross_layer_w` and `cross_layer_b` (one per layer), whereas xDeepFM stores `nn.Conv1d` kernels; these tensors have to be created and initialized explicitly in `__init__`.\n3. **No feature-map pooling**: CIN performs sum-pooling over embedding dimension to yield a fixed-length vector per layer; DCN keeps the full `d`-dim output of the last cross layer, so the pooling loop and `torch.sum(result, -1)` call must be deleted.\n4. **L2 regularisation scope**: xDeepFM regularises conv kernels, first-order linear and MLP weights; DCN only regularises the cross-layer weight vectors `cross_layer_w`, so `calculate_reg_loss()` should be trimmed to avoid registering unused conv parameters.",
    "rank": "rank2"
  },
  {
    "source": "DeepFM_2017",
    "target": "DCN_2017",
    "type": "in-domain",
    "similarities": "1. Both models adopt a parallel two-tower design that concatenates a low-order interaction path (FM component / cross network) and a high-order interaction path (Deep component / deep network); the provided DeepFM `DeepFM` class can be copied as `DCN` and only the interaction modules replaced.\n2. Embedding strategy is identical: one-hot categorical fields are mapped to dense vectors of size `embedding_size` and then concatenated; the helper `concat_embed_input_fields()` and the embedding tables initialized by `xavier_normal_` in DeepFM can be reused verbatim in DCN.\n3. The deep MLP component is structurally the same (stack of fully-connected layers with dropout and ReLU); the `MLPLayers` utility and the `self.mlp_layers` construction in DeepFM can be kept, only the input/output sizes adjusted.\n4. Final prediction head is a single `nn.Linear` mapping the fused representation to 1-D logit followed by `BCEWithLogitsLoss`; the `predict_layer`, `sigmoid`, and `loss` attributes together with the `predict()` method can be inherited with no change.\n5. Parameter initialization routine (`_init_weights`) and the overall training loop (`calculate_loss`) are identical; only an extra L2 term on cross-layer weights needs to be added for DCN.",
    "differences": "1. DeepFM uses an FM module (`BaseFactorizationMachine`) to capture explicit 2-order interactions, whereas DCN replaces this with a novel `cross_network` that automatically generates arbitrary-degree interactions via residual layer-wise crosses (`x_{l+1}=x_0 x_l^T w_l + b_l + x_l`); a new `cross_network()` method and `nn.ParameterList` for `cross_layer_w` and `cross_layer_b` must be implemented from scratch.\n2. DeepFM sums the FM score and DNN logit (`y=y_fm+y_deep`), while DCN concatenates the final cross vector and deep vector before the last linear layer; the forward pass has to be rewritten to `torch.cat([cross_output, deep_output], dim=-1)`.\n3. DCN introduces layer-wise L2 regularization on cross-network weights (`self.reg_loss(self.cross_layer_w)`); an explicit `RegLoss` term added to `calculate_loss` is required, which is absent in DeepFM.\n4. DeepFM keeps embeddings in shape `[B, num_field, embed_dim]` for FM pairwise dot-products, whereas DCN reshapes them into `[B, num_field*embed_dim]` (`view(batch_size, -1)`) to feed the cross layers; the reshape op must be inserted right after `concat_embed_input_fields` in DCN.\n5. Complexity control differs: DeepFM’s interaction complexity is fixed (order-2), while DCN exposes a new hyper-parameter `cross_layer_num` that controls the maximum polynomial degree; this config key and corresponding layer loop must be added to the DCN constructor.",
    "rank": "rank3"
  },
  {
    "source": "WideDeep_2016",
    "target": "DCN_2017",
    "type": "in-domain",
    "similarities": "1. Embedding & stacking layer: both concatenate dense embeddings of categorical features with normalized dense features into a single vector x₀; reuse the concat_embed_input_fields() helper and the Embedding table initialization (xavier_normal_) verbatim.\n2. Deep tower: identical ReLU MLP structure—same size_list construction, dropout, batch-norm option and MLPLayers utility; deep_output tensor can be copied from WideDeep’s mlp_layers() call.\n3. Joint training & loss: both use nn.BCEWithLogitsLoss() on the final logit and optimize end-to-end with SGD variants; the calculate_loss() skeleton (label fetch, reg_weight * reg_loss addition) and predict() wrapper (sigmoid) can be kept unchanged.\n4. Parameter init routine: _init_weights() that applies xavier_normal_ to embeddings/Linears and zero-bias init is directly transferable.\n5. Input pipeline: they share the same ContextRecommender base class and field-aware batch iterator—no data-loader rewrite needed.",
    "differences": "1. Cross network vs wide component: DCN replaces the sparse wide part (cross-product φ(x) + linear layer) with an explicit multi-layer Cross Network; implement cross_network() that loops over L_c layers performing x_{l+1}=x₀⊙(x_l^T w_l)+b_l+x_l via torch.tensordot and hadamard product—this module is completely new.\n2. ParameterList management: DCN introduces nn.ParameterList for cross-layer weights/biases (size d=L·E each) that must be registered separately and L2-regularized; WideDeep has no such lists.\n3. Input dimension to predictor: DCN concatenates cross_output (d) and deep_output (m) before the final Linear, so in_feature_num=d+m; WideDeep only adds two scalars (fm_output + deep_output). Adjust predict_layer input size accordingly.\n4. Regularization: DCN adds an explicit L2 term on cross weights via RegLoss(); WideDeep relies on FTRL/AdaGrad implicit regularizers—insert reg_loss() call in calculate_loss().\n5. No first-order linear term: remove WideDeep’s first_order_linear() (FM-like) helper; DCN’s cross layers already subsume first- and higher-order interactions.",
    "rank": "rank4"
  },
  {
    "source": "FM_2010",
    "target": "DCN_2017",
    "type": "in-domain",
    "similarities": "1. Both models embed sparse categorical features into dense vectors via learnable embedding matrices (FM’s latent vectors v_i ↔ DCN’s W_embed,i) and concatenate them with normalized dense features; the FM implementation’s concat_embed_input_fields() can be reused verbatim for DCN’s embedding & stacking layer.\n\n2. Both capture feature interactions through parameter-sharing factorization: FM uses <v_i,v_j> for 2-order, DCN uses outer-product x_0x_l^T with rank-1 projection w_l; the FM BaseFactorizationMachine module’s dot-product reduction can be adapted as a building block for DCN’s cross_network() loop.\n\n3. Both employ identical CTR-prediction pipeline—BCEWithLogitsLoss, sigmoid output, L2 regularization, and SGD-compatible parameter updates; FM’s calculate_loss() and predict() methods can serve as drop-in templates, only adding the reg_loss term for cross-layer weights in DCN.\n\n4. Code structure overlap: both inherit ContextRecommender, override _init_weights with xavier_normal_ on embeddings, and expose forward() → loss → predict(); the FM skeleton (initialization, device handling, batch-size inference) can be cloned and extended for DCN.",
    "differences": "1. FM supports only 2-order interactions via closed-form O(kn) reformulation, whereas DCN learns explicit higher-order crosses up to degree L_c+1 through iterative residual layers; the entire cross_network() method—ParameterList of w_l, b_l and the tensordot-plus-hadamard recurrence—must be newly implemented.\n\n2. DCN adds a parallel deep tower (MLPLayers with ReLU, dropout, batch-norm) whose output is concatenated with cross-net output before a final linear logits layer; none of these DNN components exist in FM, so MLPLayers, predict_layer and the cat([cross_output, deep_output]) step require fresh code.\n\n3. FM’s factorization is latent (k-dimensional dot products), while DCN’s interaction weights are scalar coefficients generated on-the-fly by x_0x_l^Tw_l; consequently DCN introduces O(d·L_c) additional parameters (cross_layer_w & cross_layer_b) that have no counterpart in FM and need explicit registration as nn.ParameterList.\n\n4. Complexity & memory pattern: FM’s linear-time sparse trick is fixed, but DCN’s cross layer implicitly materializes d^2 pairwise products then projects back to d via rank-1 matrix; the efficient vectorized implementation (torch.tensordot + broadcasting) is DCN-specific and must be coded from scratch.",
    "rank": "rank5"
  },
  {
    "source": "xDeepFM_2018",
    "target": "DeepFM_2017",
    "type": "in-domain",
    "similarities": "1. Both models share the same embedding layer design: reuse `self.concat_embed_input_fields()` to obtain `[batch_size, num_field, embed_dim]` tensor; embeddings are initialized with `xavier_normal_` and trained jointly with FM/DNN parts.\n2. DNN component is identical: same `MLPLayers(size_list, dropout_prob)` class, same `xdeepfm_input.view(batch_size, -1)` flattening, same final `nn.Linear(hidden[-1],1)` scoring layer; entire `self.mlp_layers` and `self.deep_predict_layer` blocks can be copied.\n3. Training pipeline is interchangeable: `BCEWithLogitsLoss()`, `sigmoid()` for inference, `_init_weights()` helper, and `calculate_loss()` / `predict()` entry points are structurally the same—only remove CIN-related regularisation lines.\n4. First-order linear term is already coded in `self.first_order_linear(interaction)`; in xDeepFM it is reused verbatim, so the same module can be dropped into DeepFM without change.",
    "differences": "1. DeepFM has NO CIN module—delete `compressed_interaction_network()`, `self.conv1d_list`, `self.cin_linear`, `self.field_nums`, `self.direct`, `self.final_len`, `self.reg_loss()` and all CIN-specific hyper-parameters (`cin_layer_size`, `reg_weight`).\n2. Replace CIN output with FM second-order interaction: implement `BaseFactorizationMachine(reduce_sum=True)` that performs `∑<v_i,v_j>x_i·x_j` on the embedded matrix; this class is missing in xDeepFM and must be added.\n3. Remove CIN regularisation term in `calculate_loss()`; DeepFM uses only the standard `BCEWithLogitsLoss` without extra L2 on convolution weights.\n4. Simplify forward path: xDeepFM sums `first_order + cin_output + dnn_output`, whereas DeepFM sums `first_order + fm_output + dnn_output`; adjust one line in `forward()` to swap `cin_output` with `self.fm(deepfm_all_embeddings)`.",
    "rank": "rank1"
  },
  {
    "source": "NFM_2017",
    "target": "DeepFM_2017",
    "type": "in-domain",
    "similarities": "1. **Shared embedding infrastructure**: Both models inherit from `ContextRecommender` and use `self.concat_embed_input_fields()` to build field-aware embeddings of shape `[batch_size, num_field, embed_dim]`—this entire block can be copied verbatim.\n2. **Identical first-order linear part**: The `self.first_order_linear(interaction)` module that learns a sparse weight for every feature is exactly the same; reuse the NFM implementation without change.\n3. **Same MLP backbone**: `MLPLayers` class, Xavier init, dropout, optional BN, final `nn.Linear(…,1)` output layer and `BCEWithLogitsLoss` are interchangeable—only the tensor fed into the MLP differs (Bi-Interaction vector vs flattened embeddings).\n4. **Training & regularization recipe**: Mini-batch Adam-style optimizer, early stopping, dropout on embeddings/MLP, and optional BN are already coded in NFM; keep the same training loop and hyper-parameter grid.",
    "differences": "1. **FM component must be re-instantiated**: NFM uses `BaseFactorizationMachine(reduce_sum=False)` to output a k-dim vector for the Bi-Interaction pool; DeepFM needs `reduce_sum=True` to output a scalar `y_FM` that is added to the DNN score—replace the FM line and remove the BN wrapper.\n2. **Need a parallel DNN branch**: DeepFM keeps FM and DNN separate; insert a new `self.deep_predict_layer` after the MLP and add the two scalars (`y=y_fm+y_deep`). This branch does NOT exist in NFM.\n3. **Input tensor reshaping**: DeepFM flattens embeddings (`view(batch_size,-1)`) before feeding the MLP, whereas NFM feeds the k-dim Bi-Interaction vector; add the reshape line in `forward()`.\n4. **Remove Bi-Interaction & BN layers**: Strip out `self.fm()` call that returns a vector, `self.bn()`, and the manual Bi-Interaction pooling logic—DeepFM needs none of these.",
    "rank": "rank2"
  },
  {
    "source": "PNN_2016",
    "target": "DeepFM_2017",
    "type": "in-domain",
    "similarities": "1. **Shared embedding layer & concat pattern**: both models map one-hot fields to dense embeddings of size `M` and concatenate them into a tensor of shape `[batch, num_fields, M]`; PNN’s `concat_embed_input_fields()` can be reused verbatim for DeepFM.\n2. **MLP backbone with identical interface**: the hidden-layer stack `MLPLayers(size_list, dropout_prob, bn=False)` and final `nn.Linear(hidden[-1],1)` followed by `sigmoid` are literally the same module; PNN’s `mlp_layers` + `predict_layer` can be copied and only the input dimension changes.\n3. **Pair-wise interaction computed with inner product**: IPNN’s `InnerProductLayer` that returns `[batch, num_pairs]` by `<f_i,f_j>` is exactly the FM second-order term; the same class can be dropped into DeepFM and its output added to `y_fm`.\n4. **Supervision & regularization recipe**: both use `BCEWithLogitsLoss`, an optional L2 reg on MLP weights (`reg_loss()`), and the same Xavier/constant weight init; the whole `calculate_loss()` and `_init_weights()` routines are transferable.",
    "differences": "1. **FM component must be added**: DeepFM needs an explicit linear term `<w,x>` plus the inner-product interactions; create a `first_order_linear()` module (sparse-dense matmul with shape `[batch,1]`) that PNN does not have.\n2. **No product-layer tensorization**: PNN concatenates linear+inner+outer products into one flat vector before MLP; DeepFM keeps FM and DNN streams separate and only adds their logits—remove `OuterProductLayer` and the `output = torch.cat([linear_part, inner_product, outer_product])` logic.\n3. **Interaction order fusion point**: PNN fuses signals in the first hidden layer (`l1 = relu(l_z + l_p + b1)`); DeepFM fuses at logit level (`y = y_fm + y_deep`)—replace the early-fusion MLP input with the simple flattened embedding `[batch, num_fields*M]`.\n4. **Outer product kernel absent**: DeepFM has no `f_i f_j^T` term; delete `OuterProductLayer` class and the `use_outer` flag to halve parameters and avoid the `kernel` tensor of shape `[M, num_pairs, M]`.",
    "rank": "rank3"
  },
  {
    "source": "WideDeep_2016",
    "target": "DeepFM_2017",
    "type": "in-domain",
    "similarities": "1. **Embedding & MLP Pipeline**: Both models concatenate dense embeddings of sparse fields and feed them into a multi-layer perceptron; the source code block `deepfm_all_embeddings.view(batch_size, -1) → mlp_layers → deep_predict_layer` can be copied verbatim—only variable names change.\n2. **First-order Linear Term**: The wide component’s linear part in WideDeep (`self.first_order_linear`) is identical to FM’s linear term; reuse the same `first_order_linear` module and its initialization.\n3. **Joint Sigmoid Output**: The final prediction `sigmoid(linear + dnn)` is structurally identical; the last two lines of `forward()` in WideDeep (`y = fm_output + deep_output`) remain valid after renaming `fm_output → y_fm` and `deep_output → y_deep`.\n4. **Training Loop & Loss**: `BCEWithLogitsLoss` and the `calculate_loss` / `predict` pattern are already correct—no modification needed.\n5. **Weight Init & Regularization**: The `_init_weights` function (Xavier for embeddings/Linears, zero bias) is directly transferable; dropout placement inside `MLPLayers` is compatible.",
    "differences": "1. **Replace Cross-Product with FM Second-Order**: WideDeep’s wide part uses manual cross-product features; DeepFM needs a trainable pairwise interaction term `Σ<V_i,V_j>x_i x_j`. **New code**: insert `self.fm = BaseFactorizationMachine(reduce_sum=True)` and add `y_fm += self.fm(deepfm_all_embeddings)`—this module does not exist in WideDeep.\n2. **Shared Embedding Matrices**: WideDeep allows separate vocabularies for wide (hashing) and deep (embedding) paths; DeepFM enforces **one** embedding matrix per field shared by FM and DNN. Ensure `self.token_field_offsets` and `self.float_field_dims` are used consistently so the same `V_i` vectors reach both components.\n3. **No Manual Feature Engineering**: Remove any hard-coded `AND(gender=female,language=en)` style transformations; DeepFM learns interactions end-to-end.\n4. **Regularization Strategy**: WideDeep uses FTRL+L1 for wide and AdaGrad for deep; DeepFM typically employs a single optimizer (Adam). Drop the FTRL-related config and apply standard L2 weight decay to embeddings to prevent over-fitting the FM parameters.",
    "rank": "rank4"
  },
  {
    "source": "DeepFM_2017",
    "target": "DSSM_2013",
    "type": "in-domain",
    "similarities": "1. Both models embed categorical fields into dense vectors and concatenate them before feeding into DNNs; DeepFM’s `concat_embed_input_fields()` (line 66) and resulting tensor shape `[B, num_field, embed_dim]` can be reused as-is for DSSM’s user & item embedding stacks.\n2. Shared MLP building block: DeepFM uses `MLPLayers(size_list, dropout_prob)` (line 42) with ReLU; DSSM uses the same helper with `activation='tanh'`—only the activation flag needs to be changed, no new layer code required.\n3. Identical training pipeline: BCEWithLogitsLoss, sigmoid prediction, xavier_normal_ init, and `_init_weights()` (lines 55-61) are copy-paste compatible; mini-batch SGD loop and `calculate_loss()`/`predict()` patterns (lines 73-81) remain unchanged.\n4. Final score-to-probability conversion: DeepFM sums FM & DNN logits then applies sigmoid; DSSM outputs cosine similarity and still uses sigmoid—sigmoid wrapper (line 80) can stay, only the tensor that enters it changes.",
    "differences": "1. DSSM needs TWO separate MLP towers (`user_mlp_layers` & `item_mlp_layers`) instead of DeepFM’s single tower; instantiate two `MLPLayers` with identical `size_list` but independent weights—this dual-tower structure is absent in DeepFM and must be newly coded.\n2. Replace DeepFM’s FM first-order + second-order module with cosine-similarity layer: after the two towers emit 128-dim vectors, compute `torch.cosine_similarity(..., dim=1)`—this scoring layer is brand-new and not present in DeepFM.\n3. Input field splitting: DeepFM concatenates ALL fields; DSSM must split into user fields vs. item fields via `double_tower_embed_input_fields()` (line 94) and build two distinct embedding tensors—requires new dataset metadata (`user_feature_num`, `item_feature_num`) and routing logic.\n4. Word-hashing preprocessing layer: DSSM’s original paper uses a fixed 30k letter-trigram projection; if reproducing the exact 2013 model, add a sparse-to-dense linear layer with frozen weights (no gradient) before the first MLP—this static hashing step is completely outside DeepFM’s vocabulary-based embedding lookup.",
    "rank": "rank1"
  },
  {
    "source": "xDeepFM_2018",
    "target": "DSSM_2013",
    "type": "in-domain",
    "similarities": "1. Both models use deep neural networks with embedding layers to map sparse, high-dimensional categorical inputs into dense, low-dimensional vectors; xDeepFM’s `embedding_size * num_feature_field` can be reused as the input size for DSSM’s user/item MLPs.\n2. Both employ multi-layer perceptrons (MLP) with tanh activation (`activation='tanh'` in xDeepFM’s `MLPLayers`) and Xavier initialization (`xavier_normal_` in `_init_weights`)—the `MLPLayers` utility and weight-init routine can be copied verbatim.\n3. Both are trained with the same binary cross-entropy loss (`BCEWithLogitsLoss`) and sigmoid output (`self.sigmoid` in `predict`)—the entire loss-computation block (`calculate_loss`, `predict`) is directly reusable.",
    "differences": "1. DSSM replaces explicit feature-interaction modules (CIN, linear, DNN trio in xDeepFM) with two separate MLP towers (user vs item) that emit 128-d semantic vectors; the CIN component and its convolutional layers must be removed and two `MLPLayers` instances created instead.\n2. DSSM uses cosine similarity between the two final 128-d vectors as the relevance score, whereas xDeepFM sums three logits (linear + CIN + DNN); a new `torch.cosine_similarity` forward pass must be implemented and the final linear layers that fuse logits are deleted.\n3. DSSM optionally applies a fixed word-hashing layer (letter-trigram projection) to reduce 500 K one-hot terms to ≈ 30 K dimensions; this static sparse projection matrix is absent in xDeepFM and must be added as a non-trainable `nn.EmbeddingBag` or pre-computed sparse tensor.\n4. Training data sampling differs: DSSM maximizes softmax likelihood over one positive and four randomly sampled negatives per query, requiring a new negative-sampling data-loader and a temperature-scaled softmax loss in place of xDeepFM’s plain BCE.",
    "rank": "rank2"
  },
  {
    "source": "PNN_2016",
    "target": "DSSM_2013",
    "type": "in-domain",
    "similarities": "1. Both models embed sparse categorical inputs into dense vectors via embedding matrices; PNN's embedding lookup (lines 88-90 in pnn.py) can be reused for DSSM's user/item feature embeddings with field-aware concatenation already implemented in double_tower_embed_input_fields().\n2. Shared MLP tower design: PNN's MLPLayers module (line 66) is directly reusable for DSSM's user_mlp_layers and item_mlp_layers; only the activation needs to switch from default relu to tanh via MLPLayers(..., activation='tanh').\n3. Identical loss interface: both use nn.BCEWithLogitsLoss() (line 69 in pnn.py vs. line 56 in dssm.py); the calculate_loss() pattern (lines 108-112) can be copied verbatim—just replace the forward() call.\n4. Parameter initialization helper _init_weights() (lines 95-102) is identical; reuse without change for Xavier normal on embeddings and linear layers.\n5. Batch-wise forward signature and interaction dict unpacking pattern (lines 104-107) is already compatible—DSSM's forward() simply returns cosine similarity instead of CTR logits, so the surrounding training loop stays the same.",
    "differences": "1. DSSM requires a dual-tower architecture: two separate MLP towers (user vs. item) whose outputs are cosine-similarity-scored; PNN uses a single tower after the product layer—must refactor forward() to keep towers separate until final cosine_similarity().\n2. No product layer in DSSM: remove InnerProductLayer and OuterProductLayer classes and all associated logic (lines 115-178) as DSSM relies on MLPs alone for implicit interaction.\n3. Activation change: DSSM mandates tanh in both hidden and output layers; ensure MLPLayers(..., activation='tanh') and remove final sigmoid inside forward() (cosine_similarity already produces [-1,1], logits are fed to BCEWithLogitsLoss).\n4. Output head differs: PNN ends with a predict_layer Linear(…,1) plus sigmoid; DSSM ends with torch.cosine_similarity—replace the last Linear and sigmoid with cosine computation on the two 128-d vectors.\n5. Feature grouping: DSSM expects user_feature_num vs. item_feature_num counts to split concatenated embeddings before the towers; add these counts in config and use double_tower_embed_input_fields() helper already provided in RecBole's DSSM code but absent in PNN.",
    "rank": "rank3"
  },
  {
    "source": "WideDeep_2016",
    "target": "DSSM_2013",
    "type": "in-domain",
    "similarities": "1. Shared embedding-to-MLP pipeline: both concatenate dense embeddings and feed them through stacked fully-connected layers (WideDeep’s `mlp_layers` vs DSSM’s `user_mlp_layers`/`item_mlp_layers`)—the same `MLPLayers` utility class can be reused verbatim; only the activation changes from ReLU to tanh.\n2. Identical supervised binary-classification objective: both models end with a single logit that is optimized with `nn.BCEWithLogitsLoss()` and evaluated with `Sigmoid()`; the `calculate_loss()` and `predict()` boilerplate can be copied with no change.\n3. Common initialization & regularization pattern: Xavier normal for embeddings and linear layers, zero bias; the `_init_weights` helper in WideDeep can be imported as-is into DSSM.\n4. Batch-wise dense input handling: both flatten embedded features with `.view(batch_size, -1)` before the MLP—this tensor reshaping code is already debugged in WideDeep and can be reused.",
    "differences": "1. Twin-tower vs single-tower: DSSM needs two separate MLP branches (`user_mlp_layers` and `item_mlp_layers`) whose outputs are cosine-similarity-scored; WideDeep has only one deep tower—add a second `MLPLayers` instance and a `torch.cosine_similarity` layer.\n2. No wide memorize path in DSSM: remove WideDeep’s `first_order_linear` (FTRL-ready) component and its cross-product feature transform logic; DSSM relies purely on the deep semantic vectors.\n3. Activation & normalization change: DSSM specifies tanh activation and batch-norm (`bn=True`) inside the MLP; WideDeep uses ReLU without BN—pass `activation='tanh'` and `bn=True` to `MLPLayers`.\n4. Output dimensionality: WideDeep’s final `deep_predict_layer` outputs 1 logit; DSSM’s two towers each output 128-dim vectors (per paper) before cosine similarity—set the last element of `mlp_hidden_size` to 128 instead of 1.",
    "rank": "rank4"
  },
  {
    "source": "FM_2010",
    "target": "FFM_2016",
    "type": "in-domain",
    "similarities": "1. **Shared base class & training pipeline**: Both inherit from `ContextRecommender`, reuse `first_order_linear()`, `BCEWithLogitsLoss()`, `sigmoid()` predictor and the same `calculate_loss()` / `predict()` interface—porting FM’s training loop to FFM requires zero changes.\n2. **Embedding initialization & device handling**: Xavier-normal initialization pattern (`_init_weights`), `concat_embed_input_fields()` device placement and sparse-tensor handling can be copied verbatim; only the embedding table shape changes from `[n_feat, k]` to `[n_feat, n_field, k]`.\n3. **Sparse feature ingestion**: The token/float/token-seq/float-seq field extraction helpers (`get_ffm_input()` vs FM’s `concat_embed_input_fields`) iterate over identical field name lists; the pre-processing code that builds `interaction` dicts is 100 % reusable.\n4. **SGD-ready gradient isolation**: FM’s `O(k·m(x))` closed-form gradient w.r.t. `v_if` is computed feature-wise; FFM keeps the same per-sample gradient structure (only adds field index) so the Hogwild! parallel loop in FM can be reused after adding the field look-up table.",
    "differences": "1. **Field-aware embedding tables**: FM keeps one matrix `V ∈ ℝ^{n×k}`; FFM needs `n_field` matrices `V^{(f)} ∈ ℝ^{n×k}`—requires replacing the single `nn.Embedding` in `BaseFactorizationMachine` by a `ModuleList` of `n_field` embeddings (already done in `FieldAwareFactorizationMachine`).\n2. **Interaction pairing logic**: FM uses the vector-reduction trick; FFM must materialise every `(j1,j2)` pair and pick `v_{j1,f2}`, `v_{j2,f1}`—implement a nested loop (`for i in range(num_features-1): for j in range(i+1, num_features): …`) that indexes embeddings via `feature2field` mapping; this double loop is **new** code not present in FM.\n3. **Feature-to-field mapping pipeline**: FM has no concept of fields; FFM needs `_get_feature2field()` that builds `feature2field` dict from config and handles categorical/numerical/seq feature types—this mapping creation is **additional** implementation work.\n4. **AdaGrad state tensor**: FM uses plain SGD; FFM requires per-parameter squared-gradient accumulator `G ∈ ℝ^{n×f×k}` (initialised to 1) and the adaptive update rule—add a `G` buffer and override the optimiser step (lines 9-11 in Algorithm 1) which is **not** in FM code.",
    "rank": "rank1"
  },
  {
    "source": "FwFM_2018",
    "target": "FFM_2016",
    "type": "in-domain",
    "similarities": "1. Both models inherit from `ContextRecommender` and share identical data-pipeline code: `_get_feature2field()`, `feature2id`/`feature2field` dict construction, and the `fields` config parser that maps feature names → field ids; this entire block can be copied verbatim.\n2. First-order (linear) part is identical: `self.first_order_linear(interaction)` is called in `forward()` and the same `BCEWithLogitsLoss` + sigmoid predictor pattern is used; no extra work needed for the linear component.\n3. Feature-type handling pattern (token, float, token_seq, float_seq) and the corresponding embedding tables with Xavier init are structurally the same—re-use the embedding initialization loop and the offset-computation logic (`np.cumsum` for one-hot cumulated indices).\n4. Training loop, loss computation and `predict()` signature are identical; the trainer/validation code from FwFM can be reused without change.",
    "differences": "1. Interaction tensor shape changes from `[B, num_fields, embed_dim]` (FwFM) to `[B, num_features, num_fields, embed_dim]` (FFM) because each feature now owns n_fields distinct latent vectors; the embedding module must be upgraded from `nn.Embedding(sum_dims, K)` to a `ModuleList` of n_fields separate embedding tables per feature type.\n2. FwFM’s `fwfm_layer()` computes one inner-product `<v_i, v_j>` then multiplies by scalar field-pair weight `r[F(i),F(j)]`; FFM replaces this with `<v_{i,F(j)}, v_{j,F(i)}>`—no scalar weight, but a gather/scatter on the 3-D tensor. A new `FieldAwareFactorizationMachine` class (already sketched in target code) must be implemented; the nested double loop over feature pairs needs CUDA-efficient vectorization (batched gather) which is absent in the provided FwFM.\n3. Parameter growth from `mK + n(n-1)/2` (FwFM) to `m(n-1)K` (FFM) requires memory-aware initialization: instead of a single `self.weight` matrix of size `[n, n, 1]`, create `n_fields` embedding tables per feature; add explicit GPU memory checks and optionally use `sparse_grad=True` for embeddings.\n4. Dropout placement differs: FwFM applies dropout once on the aggregated interaction vector; FFM paper uses no dropout inside the field-aware product—remove `self.dropout_layer` from the interaction path and only keep it on final MLP if later stacked.\n5. Optimizer setup: FFM original code uses AdaGrad with per-parameter learning-rate accumulator `G` (shown in target paper algo); either switch the optimizer in the training script or replicate the AdaGrad logic inside the custom `FieldAwareFactorizationMachine` to reproduce the paper’s exact update rule.",
    "rank": "rank2"
  },
  {
    "source": "DeepFM_2017",
    "target": "FFM_2016",
    "type": "in-domain",
    "similarities": "1. Both inherit from ContextRecommender and share the identical skeleton: constructor → _init_weights → forward → calculate_loss → predict; the boiler-plate code (BCEWithLogitsLoss, sigmoid, xavier_normal_ init) can be copied verbatim.\n2. First-order linear part is identical: DeepFM reuses `self.first_order_linear(interaction)`; FFM can keep the same linear layer without change.\n3. Embedding look-up & sparse-input batching logic in `concat_embed_input_fields()` (DeepFM) and `get_ffm_input()` (FFM) are structurally the same—token/float/sequence tensors are extracted and concatenated; the token-offset trick (`offsets = cumsum(dims)`) is already implemented and reusable.\n4. Field-to-feature mapping dictionary `feature2field` is built in the same loop-over-field-names style; DeepFM’s `num_feature_field` directly corresponds to FFM’s `num_fields`, so the mapping code can be adopted with only renaming.",
    "differences": "1. DeepFM keeps ONE embedding matrix per feature; FFM needs f matrices per feature (one for every target field). Replace `nn.Embedding(sum(dims), k)` lists of size 1 by lists of size `num_fields`—this is the core new structure to implement.\n2. Interaction computation: DeepFM calls `BaseFactorizationMachine` that does a single `V_i·V_j` inner product; FFM must implement the triple-index product `V_{i,F(j)}·V_{j,F(i)}` with nested loops over non-zero pairs and field-aware indexing—no ready-made module exists in DeepFM.\n3. DeepFM outputs a scalar `y_FM` plus `y_DNN`; FFM has no DNN part—remove the entire `mlp_layers` and `deep_predict_layer` blocks.\n4. DeepFM uses standard SG with Adam; FFM paper prescribes AdaGrad with per-coordinate learning-rate `η/√G`. A new optimizer wrapper (or manual parameter update loop) that maintains the `G` tensor of size `[n,f,k]` must be written; Hogwild! parallelisation is extra code not present in DeepFM.",
    "rank": "rank3"
  },
  {
    "source": "NFM_2017",
    "target": "FFM_2016",
    "type": "in-domain",
    "similarities": "1. Both inherit from `ContextRecommender`, so the base dataset parsing, feature-type handling (`token_field_names`, `float_field_names`, etc.) and `first_order_linear` interaction can be reused verbatim – no need to re-implement sparse-input preprocessing or label encoding.\n2. Embedding tables are initialized with identical Xavier/constant strategy; the source’s `_init_weights` method can be copied directly into FFM without change.\n3. Both expose the same public API (`forward`, `calculate_loss`, `predict`) and use `BCEWithLogitsLoss` + final sigmoid for CTR; training loop & evaluation code from NFM scripts remain valid for FFM.\n4. Mini-batch tensor assembly (`concat_embed_input_fields` in NFM vs `get_ffm_input` in FFM) follows the same pattern—packing token, float, token_seq, float_seq features—so the NFM collate-function can be adapted by simply routing tensors to the FFM submodule instead of the FM+MLP stack.",
    "differences": "1. NFM keeps one embedding vector per feature; FFM needs a tensor `W ∈ ℝ^{n×f×k}` (feature×field×dim). Replace the single `nn.Embedding` table with a `ModuleList` of `f` tables or a 3-D parameter tensor and rewrite the lookup logic—this structure is absent in NFM.\n2. Interaction layer: NFM uses a parameter-free Bi-Interaction pool plus MLP; FFM requires an explicit double loop over non-zero feature pairs performing field-aware inner products. Implement a new `FieldAwareFactorizationMachine` module that returns a matrix of pairwise interactions instead of NFM’s `k`-dim pooled vector.\n3. Gradient & optimizer detail: FFM paper relies on per-coordinate AdaGrad with a dedicated `G` tensor of squared gradients; NFM code relies on PyTorch’s `Adagrad` optimizer but does not expose the diagonal accumulators. For faithful reproduction, create a custom optimizer step that updates `G` and applies the learning-rate schedule exactly as in the FFM paper.\n4. Complexity: NFM’s interaction is computed in `O(k N_x)` via the closed-form BI layer; FFM’s interaction is `O(\bar{n}^2 k)` and must materialize all field-paired vectors. Expect a larger memory footprint; no GPU kernel fusion provided in the reference, so consider custom CUDA code or CPU Hogwild! for speed parity.",
    "rank": "rank4"
  },
  {
    "source": "PNN_2016",
    "target": "FFM_2016",
    "type": "in-domain",
    "similarities": "1. **Embedding layer reuse**: Both models start with field-wise embeddings; PNN’s `concat_embed_input_fields()` can be reused to produce the `[batch_size, num_field, embed_dim]` tensor that FFM needs as input to its `FieldAwareFactorizationMachine` module.\n2. **Pairwise interaction indexing**: PNN’s `InnerProductLayer` already builds the `row/col` index lists that enumerate all unordered feature pairs; the same indexing logic can be copied into FFM’s forward loop to visit each `(i,j)` pair without change.\n3. **Batch-wise dot-product kernel**: The element-wise multiply-reduce pattern `inner_product = (p*q).sum(dim=-1)` inside PNN’s `InnerProductLayer` is exactly the `<v_{i,F(j)}, v_{j,F(i)}>` operation that FFM requires—just replace `p` and `q` with the two field-aware vectors selected by the row/col indices.\n4. **Training pipeline**: `BCEWithLogitsLoss`, `sigmoid` prediction, `xavier_normal_` embedding init, and the `_init_weights` helper are identical; the whole `calculate_loss`/`predict` skeleton of PNN can be kept for FFM.\n5. **Sparse gradient update ready**: Both models only touch non-zero features per mini-batch; PNN’s slicing `interaction[tn]` can be reused to build the sparse gradient masks that FFM’s AdaGrad update will exploit.",
    "differences": "1. **Field-aware embedding table**: FFM needs `n_features × n_fields` separate embedding matrices (`torch.nn.ModuleList([nn.Embedding(sum_dims, k) for _ in range(num_fields)])`)—a new component not present in PNN’s single embedding matrix per field; must be implemented from scratch.\n2. **Interaction weighting disappears**: PNN learns `W_p^n` and `W_z^n` to mix linear & quadratic signals; FFM has no learned first-order/second-order weight matrices—remove the entire `product_out_dim` concatenation and the following `MLPLayers` block.\n3. **No MLP tower**: PNN feeds the product signals into deep layers; FFM stops at the sum of field-aware inner products and adds only a global linear term—delete `self.mlp_layers` and `self.predict_layer` and directly return `w0 + Σ<v_iF(j), v_jF(i)>x_i x_j`.\n4. **AdaGrad optimizer with per-parameter learning-rate tensors**: PNN uses standard Adam/SDG; FFM requires maintaining a `G[j,f]` tensor of squared gradients for every scalar parameter—implement a custom parameter-group level optimizer or wrap `torch.optim.Adagrad` with the field-aware parameter layout.\n5. **Hogwild! CPU parallelization support**: PNN’s code is GPU-oriented; FFM’s paper trains on CPU with lock-free parallel updates—add optional shared-memory threading logic and ensure embeddings are stored in `torch.nn.Parameter` objects that can be updated in-place without autograd graph conflicts.",
    "rank": "rank5"
  },
  {
    "source": "FFM_2016",
    "target": "FM_2010",
    "type": "in-domain",
    "similarities": "1. Both models share the same FM-style pairwise-interaction backbone: the target’s ∑⟨v_i,v_j⟩x_i x_j is a subset of the source’s ∑(w_{j1,f2}·w_{j2,f1})x_{j1}x_{j2} when every feature is forced into a single dummy field; hence the inner-product interaction loops in the source (`FieldAwareFactorizationMachine.forward` lines that do `input_x_emb[f_j][:,i]*input_x_emb[f_i][:,j]`) can be reused by collapsing the field dimension and keeping only one embedding matrix per feature.\n2. Embedding tables, sparse-input handling, and mini-batch assembly are identical: the source already builds `token_embeddings`, `float_embeddings`, offset tensors, and concatenates them—this whole utility (`_emb_*_ffm_input`, `_get_input_x_emb`) can be copied verbatim; FM only needs one embedding matrix per feature instead of `num_fields` copies.\n3. Training pipeline (BCE-with-logits loss, sigmoid prediction, Xavier init, `_init_weights`) and the high-level `ContextRecommender` subclassing pattern are already present; the target can keep `FM(ContextRecommender)` and reuse the `calculate_loss`/`predict` routines without change.\n4. First-order linear part is already computed by `self.first_order_linear(interaction)` in the source; FM retains exactly the same component, so this module can be imported as-is.",
    "differences": "1. Field-aware embedding tensor must be collapsed: FFM stores `n×f×k` parameters (`ModuleList` of `num_fields` embedding tables), whereas FM needs only `n×k`. Remove the outer `ModuleList` loop and keep a single `nn.Embedding(sum(feature_dims), embed_dim)` per feature type.\n2. Interaction computation simplifies from dual-index lookup (`v_{i,F(j)}`, `v_{j,F(i)}`) to single-index (`v_i`, `v_j`). The nested `for i<j` loops that multiply two field-specific vectors become one inner-product call; the efficient FM reformulation `0.5*∑_f[(∑_i v_{i,f}x_i)^2 − ∑_i v_{i,f}^2 x_i^2]` should be newly implemented (or plug in `BaseFactorizationMachine` from RecBole) to drop complexity from O(n²) to O(kn).\n3. Field metadata (`feature2field` dict, `num_fields`, `fields` config) is irrelevant; remove `_get_feature2field`, `self.num_fields`, and any code conditioned on `feature2field[i]`—FM treats all features as belonging to one global field.\n4. No Hogwild! or AdaGrad-specific gradient accumulators (`G` tensor) are exposed in the provided FM snippet; if reproducing the 2010 paper’s vanilla SGD, strip the Adagrad update equations (lines that accumulate `(G_{j1,f2})_d` and divide by its square root) and replace with a simple `param -= lr * grad`.",
    "rank": "rank1"
  },
  {
    "source": "DeepFM_2017",
    "target": "FM_2010",
    "type": "in-domain",
    "similarities": "1. Both papers share the identical FM-component implementation: the BaseFactorizationMachine layer with reduce_sum=True computes ∑<v_i,v_j>x_i x_j exactly as in FM_2010; DeepFM’s forward() already calls self.fm() on the dense field embeddings, so this line can be reused verbatim for FM_2010 reproduction.\n2. Embedding and first-order linear terms are already isolated in DeepFM: self.first_order_linear(interaction) returns w·x and the embedding table used for v_i is the same self.token_embedding; FM_2010 only needs these two tensors—no extra embedding code is required.\n3. Loss & sigmoid head are identical: DeepFM already uses BCEWithLogitsLoss and a final sigmoid for CTR prediction; FM_2010 can reuse the same self.loss and self.sigmoid modules without change.\n4. Data pipeline is shared: concat_embed_input_fields() in DeepFM produces the [B,F,k] tensor that FM_2010 needs; the sparse-to-dense conversion and field alignment logic can be copied directly.",
    "differences": "1. DeepFM adds a deep tower (MLP) whose output y_deep is summed with y_fm; FM_2010 must remove the entire MLP stack—delete self.mlp_layers, self.deep_predict_layer and the y_deep term in forward().\n2. DeepFM omits the global bias w_0; FM_2010 requires an explicit scalar parameter w0 that must be added to the forward graph and updated by SGD.\n3. DeepFM uses Xavier init for all embeddings; FM_2010 paper recommends initializing v_i ∼ N(0,σ²) with σ² tuned per data set, so a custom _init_weights override or per-parameter learning-rate schedule is needed.\n4. FM_2010 supports arbitrary loss functions (squared, hinge, logit); the reproduction should expose a config flag to switch losses, whereas DeepFX is hard-wired to BCEWithLogitsLoss.",
    "rank": "rank2"
  },
  {
    "source": "NFM_2017",
    "target": "FM_2010",
    "type": "in-domain",
    "similarities": "1. Embedding layer & first-order linear term: both papers map sparse categorical features to dense k-dimensional embeddings and add a linear regression part w₀ + Σ wᵢ xᵢ; NFM code re-uses the same `concat_embed_input_fields()` and `first_order_linear()` helpers, so these two modules can be copied verbatim into the FM implementation.\n2. Second-order interaction kernel: both compute Σᵢ<ⱼ ⟨vᵢ, vⱼ⟩ xᵢ xⱼ; NFM’s `BaseFactorizationMachine(reduce_sum=False)` already returns the pairwise dot-products before MLP, so instantiating it with `reduce_sum=True` directly yields FM’s interaction score—no new code needed.\n3. Optimization & loss pipeline: mini-batch gradient descent with BCE-with-logits-loss, Xavier init, and `predict()`/`calculate_loss()` structure are identical; the training loop and data-loader from NFM can be reused without change.\n4. Sparse input handling: both exploit the same `O(k Nₓ)` trick—NFM’s Bi-Interaction reformulation is exactly FM’s equality—so the efficient vectorized implementation in `BaseFactorizationMachine` is already the correct FM kernel.\n5. Regularization philosophy: both papers abandon explicit L₂ on embeddings and rely on dropout; NFM’s dropout prob can simply be set to 0.0 to reproduce the original FM regularization regime.",
    "differences": "1. NO neural tower in FM: NFM feeds the Bi-Interaction vector into stacked fully-connected layers; FM stops at the interaction sum. Hence delete the `mlp_layers`, `predict_layer`, `bn`, and `sigmoid` modules from NFM to obtain the FM forward pass.\n2. Output activation: NFM outputs raw logit and applies `BCEWithLogitsLoss`; FM historically uses squared loss for regression or logit loss for classification—keep the same PyTorch loss but remove the final `Linear(hidden→1)` and `hᵀ z_L` term.\n3. Parameter set shrinkage: FM’s Θ = {w₀, wᵢ, vᵢ} only; NFM’s Θ additionally contains {Wₗ, bₗ, h}. Remove all weight tensors related to hidden layers and the prediction vector h.\n4. Dropout & BN removal: FM paper predates BN; to reproduce the 2010 setup, disable batch-norm (`bn=False`) and set dropout probability to 0 in `BaseFactorizationMachine` and embedding layers.\n5. Prediction interface: NFM’s `predict()` applies `sigmoid` for CTR; FM paper leaves the decision function to the task—remove the `self.sigmoid` wrapper so the model returns the real-valued score directly, letting the caller apply sigmoid or sign as needed.",
    "rank": "rank3"
  },
  {
    "source": "PNN_2016",
    "target": "FM_2010",
    "type": "in-domain",
    "similarities": "1. Embedding layer is identical: reuse `concat_embed_input_fields()` and the embedding-table initialisation (`xavier_normal_`) from PNN; both map one-hot/field vectors to dense vectors of size `embedding_size`.\n2. Second-order pairwise interaction is computed the same way: the `InnerProductLayer` in PNN already produces `<f_i,f_j>` for every unordered pair (row/col indexing) – this tensor can be fed directly into FM’s interaction term without change.\n3. Supervised binary classification pipeline is reusable: `BCEWithLogitsLoss`, `Sigmoid` for prediction, parameter init helper `_init_weights`, and the `calculate_loss`/`predict` interface can be copied verbatim.\n4. Mini-batch iterator and field-aware sparse input handling coded in `ContextRecommender` base class are shared; no extra I/O or sampling code needs to be written.",
    "differences": "1. FM needs a *linear* part `w·x` (first-order) – PNN has no explicit linear weights; add `nn.Embedding(num_features,1)` and look-up each active feature value to obtain `∑w_i x_i`.\n2. FM merges interaction scores with the *reduction* `0.5*(∑_f[(∑_i v_{i,f}x_i)^2 − ∑_i v_{i,f}^2 x_i^2])`; PNN keeps the pair vector. Replace `InnerProductLayer` forward with the closed-form FM equation to get a single scalar per sample instead of `num_pairs` outputs.\n3. FM has no MLP; remove the `MLPLayers`, `predict_layer`, `l_1`, `l_2` logic and send the sum of linear + interaction straight to the loss. Output dimension becomes 1 without any `ReLU` hidden layers.\n4. Regularisation in PNN is implemented inside `reg_loss()` that loops over MLP weights; FM usually regularises `w`, `v` directly – add an `L2` term over the embedding and linear tables (`self.fm.embedding.weight`, `self.first_order.weight`) and drop the MLP-weight loop.\n5. Drop the `use_inner`, `use_outer`, `num_pair`, `kernel` flags and tensors – FM uses only one fixed second-order term, so the product-layer abstraction is unnecessary.",
    "rank": "rank4"
  },
  {
    "source": "WideDeep_2016",
    "target": "FM_2010",
    "type": "in-domain",
    "similarities": "1. Both models share the same base class ContextRecommender and inherit identical data-handling utilities (concat_embed_input_fields, first_order_linear), so the feature-to-embedding pipeline and dataset parsing logic can be reused verbatim.\n2. The embedding tables and their Xavier initialization are identical; the FM’s latent vectors Vᵢ∈ℝᵏ can be directly mapped to the embedding layer already implemented in WideDeep, avoiding redundant code.\n3. The final prediction head (nn.Linear + sigmoid + BCEWithLogitsLoss) and the train/loss/predict routine are structurally the same—only the interaction computation changes—so the outer training loop and metric reporting code can be copied without modification.",
    "differences": "1. WideDeep has no explicit pairwise-interaction module; FM requires a new BaseFactorizationMachine layer that implements the O(kn) reformulation ½∑ᶠ[(∑ᵢvᵢ,𝒻xᵢ)² – ∑ᵢv²ᵢ,𝒻x²ᵢ]—this entire block must be written from scratch.\n2. WideDeep keeps both first-order linear weights (wide) and MLP towers (deep); FM collapses everything into one first-order term plus the factorized second-order term, so the MLP layers, deep_predict_layer and dropout machinery must be removed.\n3. FM uses a single set of latent vectors V for both first- and second-order information, whereas WideDeep maintains separate embeddings for the deep tower and for the wide cross-product transformations—embedding reuse logic needs to be refactored to share the same table.\n4. Regularization: WideDeep relies on AdaGrad/FTRL per component; FM needs explicit L₂ on w₀, w and V added to the optimizer or loss function—no L₂ wrapper is present in the source code and must be introduced.",
    "rank": "rank5"
  },
  {
    "source": "DeepFM_2017",
    "target": "NFM_2017",
    "type": "in-domain",
    "similarities": "1. Both inherit from ContextRecommender, use identical embedding initialization (xavier_normal_), field concatenation (concat_embed_input_fields), first-order linear term (first_order_linear), sigmoid output and BCEWithLogitsLoss—entire data pipeline and loss can be reused verbatim.\n2. Shared MLP building block: MLPLayers utility is invoked with same config keys (mlp_hidden_size, dropout_prob); hidden-layer sizes, dropout, and activation strings can be copied from DeepFM’s __init__.\n3. Embedding tensor shape management is identical: [batch_size, num_field, embed_dim] produced by concat_embed_input_fields; any batch-wide reshaping or view(-1) patterns in DeepFM.forward can be reused for NFM’s Bi-Interaction input.",
    "differences": "1. Replace DeepFM’s two-stream addition (y_fm + y_deep) with NFM’s single-stack: FM layer must be switched to reduce_sum=False to return k-dim vector, followed by a custom Bi-Interaction pooling (element-wise square & sum trick) that is not present in DeepFM—requires new pooling module/layer.\n2. Insert Bi-Interaction → BatchNorm1d → MLP chain: add nn.BatchNorm1d(num_features=embedding_size) after pooling and before first linear; DeepFM has no internal BN, so this block must be newly coded.\n3. Drop first-order FM interaction term from deep path: DeepFM keeps FM’s pairwise scalar output; NFM discards it and feeds only the k-dim Bi-Interaction vector into MLP—adjust forward() to remove fm() contribution from deep side and add it later via first_order_linear().\n4. Prediction layer bias: DeepFM’s deep_predict_layer has bias=True; NFM uses bias=False in predict_layer to exactly reproduce the paper’s h^T z_L form—change Linear constructor argument.",
    "rank": "rank1"
  },
  {
    "source": "WideDeep_2016",
    "target": "NFM_2017",
    "type": "in-domain",
    "similarities": "1. **Shared embedding & MLP backbone**: Both models inherit from `ContextRecommender`, use `xavier_normal_` initialization on embeddings/Linears, concatenate dense+sparse embeddings via `concat_embed_input_fields()`, and feed them into an `MLPLayers` module that is configurable through `mlp_hidden_size` and `dropout_prob`; the entire deep branch of WideDeep (embedding → flatten → MLP → predict_layer) can be reused as-is in NFM by simply changing the input from “flattened embeddings” to “Bi-Interaction vector”.",
    "differences": "1. **New Bi-Interaction layer**: NFM replaces the “deep component input” with a 2nd-order pooling vector computed in O(kNx) time; implement a new layer `f_BI(Vx)=½[(Σxivi)²−Σ(xivi)²]` that outputs a k-dim vector before the MLP—this layer has no learnable parameters but needs a custom autograd function to return the correct gradient `Σj≠i xixjvj` for each embedding vi.\n2. **FM term absorbed into network**: NFM removes the explicit FM-prediction branch; instead of returning `fm_output + deep_output` (WideDeep style), only the first-order linear part `self.first_order_linear(interaction)` is added to the MLP output. Delete the separate FM-prediction tensor and ensure the Bi-Interaction vector alone carries all 2nd-order signal.\n3. **Batch-Norm & Dropout placement**: NFM applies BatchNorm1d **immediately after** the Bi-Interaction vector and optionally inside every hidden layer (`bn=True` in `MLPLayers`), whereas WideDeep has no norm layer. Insert `self.bn = nn.BatchNorm1d(self.embedding_size)` and call it on the Bi-Interaction result before feeding `mlp_layers`; dropout is now handled inside `MLPLayers` instead of manually after concatenation.\n4. **Activation & output scaling**: NFM uses sigmoid activations in hidden layers (`activation=\"sigmoid\"`) while WideDeep uses ReLU; change the activation string when instantiating `MLPLayers` and ensure the final `predict_layer` has `bias=False` to exactly reproduce the paper’s `h^T z_L` form.",
    "rank": "rank2"
  },
  {
    "source": "PNN_2016",
    "target": "NFM_2017",
    "type": "in-domain",
    "similarities": "1. Embedding-layer design: both concatenate field-wise embeddings into a 3-D tensor `[B,F,E]`; PNN’s `concat_embed_input_fields()` can be reused verbatim in NFM.\n2. MLP backbone: the `MLPLayers` utility and its dropout/batch-norm options are identical; PNN’s hidden-size list construction and `predict_layer` Linear(·,1) can be copied.\n3. First-order linear part: PNN already computes a flattened linear signal `linear_part` and appends it to the interaction signals; NFM’s `first_order_linear()` can be obtained by simply dropping the product branches and reusing the same embedding→sum pooling logic.\n4. Output head & loss: both use `nn.Linear(final_dim,1)` followed by `BCEWithLogitsLoss()`; the sigmoid-for-predict pattern is identical, so the forward → predict → calculate_loss pipeline can be kept.",
    "differences": "1. Interaction operator: PNN keeps pairwise inner/outer products as extra input columns; NFM replaces them with a single Bi-Interaction vector `0.5*(sum vi)^2 - sum(vi^2)` that must be implemented from scratch—no such pooling exists in PNN.\n2. BatchNorm placement: NFM applies BN **before** the MLP (`bn(self.fm(...))`) while PNN does not normalize the product layer; a `nn.BatchNorm1d(embedding_size)` layer has to be inserted and its running stats handled.\n3. No product branches in NFM: the `use_inner/use_outer` flags and the whole `InnerProductLayer`/`OuterProductLayer` classes are deleted; the model constructor must drop the `product_out_dim` logic and set MLP input dim simply to `embedding_size`.\n4. Regularization: PNN adds an explicit L2 term on MLP weights (`reg_loss()`), whereas NFM relies purely on dropout; the `reg_weight` path and its accumulation in `calculate_loss()` must be removed.",
    "rank": "rank3"
  },
  {
    "source": "FFM_2016",
    "target": "NFM_2017",
    "type": "in-domain",
    "similarities": "1. Both models start with a first-order linear part (w₀ + Σ wᵢxᵢ) computed by the same `first_order_linear()` helper; the FFM implementation can be reused verbatim for NFM’s linear term.\n2. Both adopt an embedding layer that maps each feature to a k-dim vector; FFM’s `token_embeddings`/`float_embeddings` tables (initialized with Xavier uniform) can be reused as the NFM feature-embedding lookup with only the field-aware indexing removed.\n3. Both exploit sparsity: FFM loops only over non-zero pairs and NFM only over non-zero features; the batch-wise sparse-tensor handling in `get_ffm_input()` (offset indexing, padding masks) can be copied for building NFM’s `concat_embed_input_fields()`.\n4. Both use AdaGrad-style optimization and identical BCEWithLogitsLoss; the training loop, early-stopping monitor, and `calculate_loss()`/`predict()` signatures in FFM trainer can be transferred directly to NFM.",
    "differences": "1. Interaction component must be replaced: FFM uses explicit field-aware pairwise inner products (O(n̄²k)) implemented in `FieldAwareFactorizationMachine.forward()`; NFM needs a parameter-free Bi-Interaction pooling layer that computes ½[(Σxᵢvᵢ)² − Σ(xᵢvᵢ)²] in O(kNₓ). A new `BiInteractionPool` module has to be written.\n2. DNN stack is absent in FFM: NFM requires a multi-layer MLP (`mlp_layers`) with configurable widths, dropout, batch-norm and sigmoid activations. The `MLPLayers` utility is not present in the FFM repo and must be added.\n3. Field information usage differs: FFM keeps f separate embedding matrices per feature (field-aware), whereas NFM uses one embedding matrix per feature and discards field divisions after the embedding lookup. Thus the `feature2field` dictionary and field-wise `ModuleList` in FFM should be removed; instead a single `nn.Embedding` table per feature type suffices.\n4. Regularization strategy: FFM relies on L2 via AdaGrad, while NFM expects dropout + batch-norm inside the MLP and on the Bi-Interaction output. Extra dropout masks and `nn.BatchNorm1d` layers need to be inserted that are not in the FFM code.",
    "rank": "rank5"
  },
  {
    "source": "DeepFM_2017",
    "target": "PNN_2016",
    "type": "in-domain",
    "similarities": "1. Embedding layer reuse: DeepFM's `concat_embed_input_fields()` returns `[batch_size, num_field, embed_dim]` which is exactly the input format PNN needs; the same `ContextRecommender` base class and embedding lookup can be reused without change.\n2. MLP tower design: Both models concatenate interaction signals and feed them into identical `MLPLayers(size_list, dropout_prob)`; the hidden-size list construction, dropout, and final `nn.Linear(mlp_hidden_size[-1], 1)` followed by `sigmoid` can be copied verbatim.\n3. Training & loss pipeline: `calculate_loss()` and `predict()` routines are identical—BCEWithLogitsLoss on raw logits plus optional regularization—so the whole training loop and metric computation remain valid.\n4. Parameter initialization: Xavier normal for embeddings and linear weights, zero bias—shared `_init_weights()` method can be inherited directly.",
    "differences": "1. Interaction module must be built from scratch: DeepFM has no product layer; PNN needs two new modules—`InnerProductLayer` that computes `<f_i,f_j>` for all C(N,2) pairs and `OuterProductLayer` that learns a kernel `ℝ^{embed_size × num_pairs × embed_size}` to compress outer-product matrices into pairwise scalars.\n2. Feature tensor routing: DeepFM keeps embeddings separate for FM and MLP; PNN must flatten the embedding tensor into `linear_part` and optionally append `inner_product` and `outer_product` vectors before the first dense layer—requires dynamic concatenation logic controlled by `config['use_inner']` and `config['use_outer']`.\n3. Complexity-reduction math: IPNN rewrites `∑_{i<j} θ^n_i θ^n_j <f_i,f_j>` as `‖∑_i θ^n_i f_i‖^2`—implement this vector reduction with a single `einsum` or `bmm` to avoid O(N²) memory; OPNN superposes outer products into `f_Σ f_Σ^T`—implement via `f_sum = feat_emb.sum(dim=1)` then `torch.bmm(f_sum.unsqueeze(2), f_sum.unsqueeze(1))` before kernel contraction.\n4. L2 regularization on product-layer kernels: add a `reg_loss()` method that iterates only over `OuterProductLayer.kernel` and `MLPLayers` weights, scaling by `config['reg_weight']`, and include it in the total loss—this component does not exist in DeepFM.",
    "rank": "rank1"
  },
  {
    "source": "NFM_2017",
    "target": "PNN_2016",
    "type": "in-domain",
    "similarities": "1. **Shared embedding layer design**: both models concatenate field-specific embeddings into a 3-D tensor `[batch, num_fields, embed_dim]`; NFM’s `concat_embed_input_fields` can be reused verbatim for PNN’s embedding layer.\n2. **MLP backbone & prediction head**: the same `MLPLayers` helper and final `nn.Linear( last_hidden, 1 )` pattern is used; NFM’s `mlp_layers` + `predict_layer` can be copied, only the input size needs to be enlarged to accommodate the extra product signals.\n3. **Drop-out & Xavier init convention**: identical dropout placement between Bi-Interaction/concat stage and hidden layers, and the same `_init_weights` function applies to all `nn.Embedding`/`nn.Linear` modules.\n4. **Supervised binary classification pipeline**: both employ `BCEWithLogitsLoss`, sigmoid-free forward pass, and a separate `predict()` that applies sigmoid; training loop, early stopping, and batch-wise Adagrad logic from NFM can be retained.",
    "differences": "1. **Interaction generation layer is completely new**: NFM uses a parameter-free Bi-Interaction pooling (`∑ x_i v_i ⊙ x_j v_j`), whereas PNN requires an explicit **Product Layer** that outputs `num_pair` inner or outer products; implement `InnerProductLayer` and `OuterProductLayer` classes—none exist in NFM.\n2. **Input dimension to MLP changes dynamically**: NFM feeds a fixed `k`-dim vector into the MLP; PNN concatenates `num_fields*k` (linear) + `num_pair` (inner) + `num_pair` (outer), so the first layer size must be computed at runtime—adapt NFM’s `size_list[0]` to `product_out_dim`.\n3. **Outer-product kernel parameters**: OPNN introduces a learnable 3-D kernel `ℝ^{embed_size × num_pairs × embed_size}`; add `nn.Parameter` initialization and custom einsum-style multiplication inside `OuterProductLayer.forward`—no analogue in NFM.\n4. **Explicit L2 regularization on MLP weights**: NFM relies solely on dropout; PNN adds an auxiliary `reg_loss()` that iterates over `mlp_layers.named_parameters()` and accumulates `reg_weight * ‖W‖₂`; insert this term into the total loss.",
    "rank": "rank2"
  },
  {
    "source": "WideDeep_2016",
    "target": "PNN_2016",
    "type": "in-domain",
    "similarities": "1. Embedding-to-MLP pipeline: both concatenate field embeddings into a dense vector (`widedeep_all_embeddings.view(batch_size, -1)` vs `linear_part = pnn_all_embeddings.view(batch_size, -1)`) and feed it into an identical `MLPLayers` module—reuse the whole `MLPLayers` class and the `predict_layer` Linear(·,1) + sigmoid pattern.\n2. Joint training with binary cross-entropy: both use `nn.BCEWithLogitsLoss()` and expose `calculate_loss()`/`predict()` methods—copy the loss-computation boilerplate and `sigmoid()` invocation verbatim.\n3. Parameter seeding & regularization: Xavier init on Embedding/Linear plus bias-zeroing is already implemented; PNN’s `reg_loss()` can be added around the same `named_parameters()` loop used for the deep part of WideDeep.",
    "differences": "1. **Product layer is completely new**: neither cross-product transformation nor deep component in WideDeep computes pairwise inner/outer products—must implement `InnerProductLayer` and `OuterProductLayer` classes that generate `num_pair=N(N-1)/2` interaction signals and append them to the linear embedding vector before the first dense layer.\n2. **No wide memorization path**: WideDeep adds a separate `first_order_linear` (FTRL-ready) logit; PNN fuses everything into one MLP input—remove the `fm_output` branch and the corresponding FTRL/AdaGrad split-training logic.\n3. **Outer-product kernel parameters**: OPNN introduces a learnable 3-D kernel `θ∈ℝ^{E×P×E}` (E=embed_dim, P=num_pairs) that does not exist in WideDeep—allocate and initialize this extra `nn.Parameter` with Xavier uniform and include it in the optimizer.\n4. **Complexity-reduction math**: IPNN rewrites pairwise dot products as `‖Σ δ_i‖²` and OPNN superposes outer products into `f_Σf_Σ^T`; these tensor reformulations are absent in WideDeep—code the efficient batch implementations (`sum(dim=-1)`, `unsqueeze`+`mul`+`sum`) inside the new product layers.",
    "rank": "rank4"
  },
  {
    "source": "FM_2010",
    "target": "PNN_2016",
    "type": "in-domain",
    "similarities": "1. Both models start with an embedding layer that maps one-hot field vectors to dense vectors of size M; the FM implementation already concatenates `fm_all_embeddings` of shape [B, num_field, embed_dim] which can be fed directly into PNN’s product layers without change.\n\n2. Second-order interaction modeling: FM’s ⟨v_i,v_j⟩x_i x_j and IPNN’s inner-product g(f_i,f_j)=⟨f_i,f_j⟩ compute exactly the same pairwise signal; the `BaseFactorizationMachine` class that returns pairwise dot-products can be reused as the `InnerProductLayer` in PNN (only reshape & indexing code needs to be added).\n\n3. Training pipeline (BCEWithLogitsLoss, sigmoid output, SGD/Adam optimizer, L2 regularization) is identical; the `calculate_loss` and `predict` methods in `FM` can be copied to `PNN` with the only addition of an extra `reg_loss` term for MLP weights.\n\n4. Sparse feature handling: both papers assume multi-field categorical input and use the same `concat_embed_input_fields` helper that the FM code already implements; this helper can be called verbatim inside PNN’s forward pass.\n\n5. Parameter initialization strategy (xavier_normal_ for embeddings) is already present in FM and can be kept for all embedding matrices in PNN; the MLP weights in PNN can use the same initializer extended to Linear layers.",
    "differences": "1. PNN adds deep MLP towers after the product layer; FM has no hidden layers. You must newly implement `MLPLayers` (stacked Linear-ReLU-Dropout) and a final `nn.Linear(…,1)` prediction head—none of these exist in the FM repository.\n\n2. FM interaction is a single scalar ⟨v_i,v_j⟩ multiplied by x_i x_j, whereas PNN keeps the raw embedding vectors and computes either (a) IPNN: element-wise product vectors of length M, or (b) OPNN: outer-product matrices M×M. You need to write new `InnerProductLayer` and `OuterProductLayer` modules that output interaction tensors of shape [B, num_pairs, M] or [B, num_pairs, M, M], respectively—far richer than FM’s scalar.\n\n3. FM’s linear-time reformulation (O(km)) is not used in PNN; instead PNN materialises all N(N-1)/2 pairs. Complexity-reduction tricks (θ^n θ^{nT} for IPNN, f_Σ f_Σ^T for OPNN) must be coded from scratch; nothing in FM helps here.\n\n4. FM needs only one embedding table V ∈ ℝ^{n×k}; PNN needs an additional field-level embedding table W_0^i per field and, for OPNN, a learnable kernel tensor K ∈ ℝ^{M × num_pairs × M}. These extra parameters and their forward passes are completely absent in the FM codebase.\n\n5. FM supports arbitrary real-valued features and higher-order PARAFAC extensions; PNN is strictly designed for one-hot categorical inputs and sticks to second-order interactions. Re-purposing FM’s feature pre-processing for PNN is safe, but the reverse is not true.",
    "rank": "rank5"
  },
  {
    "source": "xDeepFM_2018",
    "target": "WideDeep_2016",
    "type": "in-domain",
    "similarities": "1. **Shared base class & embedding pipeline**: both inherit from `ContextRecommender` and reuse `self.concat_embed_input_fields(interaction)` to obtain `[B, num_field, embed_dim]` tensors—this whole helper can be copied verbatim.\n2. **Deep component skeleton**: the MLP stack (`MLPLayers` + final `nn.Linear(…) → 1`) is identical—xDeepFM’s `mlp_layers` + `deep_predict_layer` can be renamed/kept for WideDeep with only hidden-size config changes.\n3. **Joint-training objective**: both sum two logits (wide-style + deep-style) and feed the scalar into `BCEWithLogitsLoss`; the loss-calculation routine (`calculate_loss`, `predict`, sigmoid wrapping) is structurally the same and can be reused.\n4. **Parameter initialization & device plumbing**: `_init_weights`, `self.apply(…)`, `reg_loss` helpers and the device-agnostic tensor creation patterns are already implemented and require no change.\n5. **Feature-field enumeration**: `self.num_feature_field`, `self.embedding_size` and the config dictionary plumbing are already in place—only the wide-part feature transform logic has to be added.",
    "differences": "1. **Wide component is missing**: xDeepFM has no explicit cross-product transformer; WideDeep needs a new `WideLayer` that materializes cross-product indicators (sparse 0/1 vector) and a `nn.Linear` without bias to realize `w_wide^T [x, φ(x)]`—this module does not exist in the source.\n2. **First-order linear is FM-style, not wide-style**: xDeepFM’s `first_order_linear` is a simple per-field embedding sum; WideDeep’s wide part must accept raw one-hot concatenation (or hashed feature crosses) and be trained with FTRL-style sparsity—requires rewriting the forward path and data input hook.\n3. **No CIN branch**: remove the entire `compressed_interaction_network`, `conv1d_list`, `cin_linear`, `field_nums` bookkeeping and the related `calculate_reg_loss` entries; freeing the graph cuts GPU time from O(mH²DT) to O(mHD+H²T).\n4. **Optimizer & regularization mismatch**: xDeepFM uses global L2 on all weights; WideDeep originally applies L1 only to the wide part (FTRL) and AdaGrad to embeddings—config pipeline must expose two optimizers and sparsity handling, none of which is present in the source code.\n5. **Feature transform generation**: WideDeep needs an offline/online cross-product generator (`AND(gender=female, language=en)` etc.); xDeepFM’s code has no such transform module—this preprocessing step has to be implemented from scratch.",
    "rank": "rank1"
  },
  {
    "source": "DeepFM_2017",
    "target": "WideDeep_2016",
    "type": "in-domain",
    "similarities": "1. Both models fuse a low-order (memorization) and a high-order (generalization) signal into a single logit that is fed to the same BCEWithLogitsLoss; the source code already concatenates field embeddings and adds two sub-scores (y_fm + y_deep), so the identical forward pattern (wide_logit + deep_logit) can be reused with no change.\n2. The embedding layer, MLP hidden blocks, final 1-d prediction layer and sigmoid predictor are structurally identical—DeepFM’s concat_embed_input_fields(), MLPLayers, and deep_predict_layer can be copied verbatim to WideDeep, only renaming variables.\n3. Parameter initialization (xavier_normal_ for embeddings/Linears, zero bias) and the whole _init_weights helper are already coded in DeepFM; the same snippet satisfies WideDeep’s initialization needs.\n4. Both inherit from ContextRecommender, rely on the same config keys (mlp_hidden_size, dropout_prob) and expose calculate_loss()/predict() with identical signatures, so the training loop and evaluator require zero modification when switching models.",
    "differences": "1. DeepFM’s ‘wide’ part is an FM layer (second-order pairwise interactions computed via inner products of latent vectors), whereas WideDeep’s wide part is a pure linear model on ONE-HOT raw features plus user-defined cross-product transforms (φ_k(x)); the FM class and its reduce-sum pairwise kernel must be replaced by a sparse linear layer that accepts the original one-hot vector (or hashed crosses) and is trained with FTRL+L1 rather than SGD.\n2. DeepFM shares the SAME embedding vectors between FM and DNN; WideDeep keeps the embeddings ONLY inside the deep tower and feeds the wide component with independent sparse indices—code must branch: embeddings go to MLP, while raw categorical indices go through an nn.EmbeddingBag(mode='sum') or a custom FTRL-compatible linear layer without embedding reuse.\n3. DeepFM performs no manual feature engineering; WideDeep requires explicit cross-product features (e.g., AND(gender=female, language=en)) generated offline and stored as additional one-hot fields—implement a preprocessing routine or a transform layer that materialises φ(x) on-the-fly and registers them in dataset.fields so the first_order_linear() can consume them.\n4. Optimizer split: DeepFM uses one torch optimizer for all parameters; WideDeep mandates two optimizers (FTRL+L1 for wide, AdaGrad/Adam for deep) with different learning rates and L1 regularisation applied solely to the wide weights—build a multi-optimizer training step that freezes the appropriate subgraph in each backward pass.",
    "rank": "rank2"
  },
  {
    "source": "DCN_2017",
    "target": "WideDeep_2016",
    "type": "in-domain",
    "similarities": "1. **Embedding & Dense Concatenation Layer**: Both models share identical input preprocessing—sparse categorical features are embedded (Eq. 1-2 in DCN, Sec 3.2 in WideDeep) and concatenated with normalized dense features into a single vector `x₀`. The source code’s `concat_embed_input_fields()` and the resulting `[batch, num_field*embed_dim]` tensor can be reused verbatim.\n2. **Deep Tower Implementation**: The deep component is a standard ReLU MLP; DCN’s `MLPLayers(size_list, dropout, bn=True)` and final `deep_predict_layer` can be copied—only the variable names (`dcn_all_embeddings` → `widedeep_all_embeddings`) need to change.\n3. **Joint Training & Log-Loss**: Both papers use mini-batch SGD with `BCEWithLogitsLoss` and combine two streams of logits before the sigmoid; the training loop (`calculate_loss`, `predict`) is structurally identical and can be reused.\n4. **Parameter Initialization**: Xavier normal for embeddings/Linears and zero bias is already implemented in `_init_weights`; no change required.",
    "differences": "1. **Wide Component vs Cross Component**: WideDeep requires an explicit **wide linear model** (`y = wᵀx + b`) fed with **manual cross-product transformations** (e.g., `AND(gender=female, language=en)`). DCN has no such module; you must implement a sparse linear layer that accepts both raw one-hot indices and pre-built cross-product indices, and train it with FTRL + L₁ (Google’s paper) instead of AdaGrad. In code: add `self.wide_linear = nn.Linear(wide_dim, 1, bias=True)` and a feature-column parser that materializes cross-product sparse tensors.\n2. **Input Streams**: DCN sends the **same** concatenated embedding vector to both towers; WideDeep sends **embedded features only to the deep tower** and **raw+transformed sparse indices to the wide tower**. You need to split the data path—keep `concat_embed_input_fields` for deep, but create a second helper `build_wide_features(interaction)` that returns sparse tensors of cross-product bins.\n3. **Combination Point**: DCN concatenates the **outputs** of cross & deep networks and applies a single final linear layer (Eq. 5). WideDeep adds the **logits** of the two components (`wide_logit + deep_logit`) before the sigmoid. Replace DCN’s `torch.cat([cross_output, deep_output], dim=-1)` → `output = fm_output + deep_output` (already shown in target skeleton).\n4. **Regularization Strategy**: DCN applies L₂ only to cross-layer weights (`RegLoss` on `cross_layer_w`). WideDeep uses L₁ on wide weights (FTRL) and L₂ on deep weights; extend the loss function accordingly.",
    "rank": "rank3"
  },
  {
    "source": "NFM_2017",
    "target": "WideDeep_2016",
    "type": "in-domain",
    "similarities": "1. **Shared embedding layer and dense MLP backbone**: both models inherit from `ContextRecommender`, use `concat_embed_input_fields()` to obtain `[B,F,k]` embedding tensors, pass them through fully-connected towers (`MLPLayers`) with Xavier-initialized weights, and add a final `nn.Linear(·,1)` to output a logit; NFM’s `mlp_layers` + `predict_layer` can be reused verbatim for WideDeep’s deep tower—only the input tensor reshaping (`view(batch_size,-1)`) needs to be inserted.\n2. **Joint use of first-order linear term**: NFM’s `first_order_linear(interaction)` and WideDeep’s wide part both compute Σ w_i x_i in the same way; the identical module can be imported and summed to the deep tower logit without change.\n3. **Identical training pipeline**: sigmoid + `BCEWithLogitsLoss`, mini-batch Adagrad-style optimisers, dropout, BN inside `MLPLayers`, same `_init_weights()` helper—entire `calculate_loss()` and `predict()` routines can be copied with only the forward path differing.\n4. **Feature field handling**: both papers treat categorical and real-valued features through the same `ContextRecommender` base, so the dataset parsing, vocabulary building and embedding lookup code are fully reusable.",
    "differences": "1. **Wide component requires explicit cross-product features**: WideDeep needs binary indicator features like `AND(gender=female, language=en)` that are **not** created by NFM; implement a `CrossProductTransform` layer that generates sparse 1-hot vectors from raw features and feed them into a separate `nn.Linear` (FTRL-ready) to form the wide logit—this entire path is absent in NFM.\n2. **Deep-tensor reshaping only**: NFM keeps embeddings separate (`[B,F,k]`) and applies Bi-Interaction pooling internally, whereas WideDeep simply flattens to `[B,F*k]` before the MLP; insert a single `.view(batch_size, -1)` line—no new class needed.\n3. **No Bi-Interaction pooling in WideDeep**: remove NFM’s `BaseFactorizationMachine` and `BatchNorm1d` on the pooled vector; WideDeep’s interaction capacity is delegated entirely to the MLP, so those modules must be deleted to avoid incorrect signal.\n4. **Different regularisation regimes**: NFM relies on dropout/BN; WideDeep’s wide part is designed for **L₁/FTRL** sparsity. Expose an option to swap the optimiser group for the wide weights (FTRL with L₁) while keeping AdaGrad for embeddings/MLP—this dual-optimiser logic is not present in the NFM codebase.",
    "rank": "rank4"
  },
  {
    "source": "xDeepFM_2018",
    "target": "AutoInt_2019",
    "type": "in-domain",
    "similarities": "1. Both models inherit from `ContextRecommender`, share the same base embedding lookup `self.concat_embed_input_fields(interaction)` returning `[B, num_field, embed_dim]`, and use `nn.BCEWithLogitsLoss()` plus L2 regularization—so the data pipeline, loss computation and optimizer setup can be reused verbatim.\n2. They adopt an identical dual-tower pattern: a shallow linear part (`self.first_order_linear`) and a deep interaction tower whose output is summed before sigmoid; hence the skeleton `output = linear_logits + tower_logits` and the `predict()` routine can be copied directly.\n3. Embedding initialization (`xavier_normal_` on `nn.Embedding.weight`) and numerical-field handling (scale-by-one-vector) are the same; the embedding tables created in the parent class need no change.\n4. Both papers concatenate final representations into a single vector and reduce it with one `nn.Linear(·, 1)` layer; so the output projection layer initialization and bias-zeroing code can be reused.",
    "differences": "1. xDeepFM uses CIN (outer-product + Conv1d) while AutoInt needs multi-head self-attention blocks (`nn.MultiheadAttention` or custom Q/K/V projections); the entire `compressed_interaction_network()` method must be replaced by an `autoint_layer()` that implements Equations 5–8 including residual ReLU.\n2. AutoInt requires three new weight matrices per head per layer (`W_query`, `W_key`, `W_value`) plus a residual projection `W_res`; these are absent in xDeepFM, so explicit `nn.Linear` layers or `nn.MultiheadAttention` modules have to be created and properly shaped (`d×d′` with `d′=attention_size`).\n3. xDeepFM keeps field dimension constant through CIN, whereas AutoInt transposes to `[field, B, d′]` for attention and transposes back; the forward pass needs dimension reordering (`transpose(0,1)`) before and after the self-attention loop.\n4. AutoInt stacks `n_layers` of interacting layers with residual connections, while xDeepFM stacks CIN layers with sum-pooling; the layer loop and hidden-state carry-over logic differ, and dropout on attention weights must be injected (`dropout_probs[0]`).",
    "rank": "rank2"
  },
  {
    "source": "DCN_2017",
    "target": "AutoInt_2019",
    "type": "in-domain",
    "similarities": "1. **Shared embedding & stacking layer**: both papers concatenate dense embeddings of categorical fields with normalized numerical features into a single vector x₀ (DCN Eq-2, AutoInt Eq-3/4). DCN’s `concat_embed_input_fields()` and `dcn_all_embeddings.view(batch_size, -1)` can be reused verbatim for AutoInt’s `autoint_all_embeddings` tensor of shape [B, M, D].\n2. **Parallel DNN backbone**: each model keeps a fully-connected tower (`MLPLayers` in the repo) that receives the same flattened embedding vector; DCN’s `self.mlp_layers` and `deep_predict_layer` can be copied and only the input dimension needs to be adjusted.\n3. **Log-loss with sigmoid output**: both use `nn.BCEWithLogitsLoss()` and a final linear layer that outputs a single logit; DCN’s `predict_layer`, `loss` and `predict()` routines can be adopted after changing the concatenated input size.\n4. **Residual-style information preservation**: DCN’s cross layers add back xl (Eq-3), while AutoInt’s interacting layer adds the raw embedding back via `v_res` (Eq-8). The DCN pattern `x_l = f(...) + x_l` offers a ready PyTorch template for implementing AutoInt’s residual branch `cross_term += v_res`.\n5. **Joint end-to-end training**: both models back-propagate through the interaction module and the DNN simultaneously; the training loop and optimizer setup in DCN’s `calculate_loss()` can be reused with only the regularization term removed (AutoInt does not use explicit L2 on interaction weights).",
    "differences": "1. **Interaction mechanism must be replaced**: DCN uses an explicit polynomial cross network (`cross_network()` with `x₀·xₗᵀ·wₗ` and `ParameterList` for each layer), whereas AutoInt needs a multi-head self-attention stack (`nn.MultiheadAttention` repeated `n_layers` times). Re-implement the interacting layer from scratch—no DCN cross-layer code is reusable here.\n2. **Attention-specific parameters**: AutoInt introduces query/key/value projection matrices per head (`W_Q^h`, `W_K^h`, `W_V^h`) and an optional residual projection `W_Res`; these are not present in DCN and must be created as `nn.Linear` layers inside `self_attns` and `v_res_embedding`.\n3. **Output fusion differs**: DCN concatenates the final cross vector and deep vector before one logits layer (Eq-5), while AutoInt sums two separate scores—an attention-based score (`attn_fc` on flattened interacting output) and a DNN score (`deep_predict_layer`). A new additive forward path must be coded (`output = first_order_linear(...) + autoint_layer(...) + deep_predict_layer(...)`).\n4. **Multi-head and layer loops**: DCN’s cross loop is over `cross_layer_num` with manually defined `ParameterList`; AutoInt needs a `ModuleList` of `MultiheadAttention` cells plus transpose gymnastics (`transpose(0,1)`) to satisfy PyTorch’s seq-first convention. Insert the proper tensor reshaping and residual dropout that do not exist in DCN.\n5. **Complexity footprint**: DCN’s cross part is O(d·L_c); AutoInt’s interacting part is O(M²d′H·L) with M fields. Memory and time benchmarks should be re-evaluated; no guidance from DCN code here.",
    "rank": "rank3"
  },
  {
    "source": "NFM_2017",
    "target": "AutoInt_2019",
    "type": "in-domain",
    "similarities": "1. Embedding Layer Reuse: Both models convert sparse one-hot/multi-hot and scalar features into dense vectors of size `embedding_size`; the `concat_embed_input_fields()` method and `first_order_linear()` (ContextRecommender base class) can be copied verbatim from NFM to AutoInt.\n2. Training Pipeline Reuse: Both use `BCEWithLogitsLoss()`, `Sigmoid()` for inference, and identical `_init_weights()` helper (Xavier for Embedding/Linear + zero bias); the `calculate_loss()`/`predict()` loops remain unchanged.\n3. DNN Backbone Reuse: NFM’s `MLPLayers` utility (with dropout & activation) is already imported and can supply the `deep_predict_layer` MLP in AutoInt; only the size list changes from `[k] + mlp_hidden_size` to `[num_field*k] + mlp_hidden_size`.\n4. Regularization Patterns: Dropout, batch-norm placement, and early-stopping monitoring (RMSE/LogLoss) are identically configured; hyper-parameter dict structure (`dropout_prob` vs `dropout_probs`) is trivial to remap.",
    "differences": "1. New Multi-Head Self-Attention Stack: AutoInt requires `nn.ModuleList` of `nn.MultiheadAttention` layers that accept 3-D tensors (seq_len, batch, att_size); this entire block is absent in NFM and must be implemented from scratch, including parameter initialization and mask handling for variable-length fields.\n2. Residual Projection Layer: `v_res_embedding = Linear(embedding_size, attention_size)` is needed to align residual shortcut dimensions; NFM has no residual connections, so this layer and the ReLU-fused residual add (`cross_term += v_res`) are new.\n3. Attention-Specific Parameter Set: Query/Key/Value matrices (`W_Q, W_K, W_V`) per head plus output projection `attn_fc` introduce ~3·H·d·d′ + H·d′·M extra weights; NFM’s parameter count is O(k·d + ∑d_l·d_{l+1}), so a new weight-init & L2-regularization scope must be added.\n4. Feature Interaction Paradigm Shift: NFM compresses all pairwise interactions into one k-vector via Bi-Interaction pooling, whereas AutoInt keeps M feature vectors throughout and lets attention dynamically weight them; this abolishes the `BaseFactorizationMachine` call and demands memory for attention scores of shape (batch, H, M, M).",
    "rank": "rank4"
  },
  {
    "source": "DCN_2017",
    "target": "DCNV2_2020",
    "type": "in-domain",
    "similarities": "1. **Shared embedding & stacking pipeline**: both papers start with the identical sparse/dense preprocessing—`concat_embed_input_fields` in the source can be reused verbatim; only the tensor reshape (`view(batch_size, -1)`) is kept.\n2. **Residual cross-layer philosophy**: the update rule still reads `x_{l+1} = x_0 ⊙ f(x_l) + x_l`; the source’s loop skeleton (`for i in range(cross_layer_num): ... x_l = xl_dot + bias + x_l`) is preserved, so the outer iteration and bias addition code can be copied.\n3. **Parallel/concatenation & prediction head**: the final `torch.cat([cross_output, deep_output], dim=-1)` and `nn.Linear(in_feature_num + mlp_hidden_size[-1], 1)` followed by `sigmoid` is exactly the same—reuse the source’s `predict_layer` initialization and forward path when `structure==\"parallel\"`.",
    "differences": "1. **Vector → matrix upgrade in cross layer**: DCN uses rank-1 `w_l ∈ R^d`, DCNV2 needs a full `W_l ∈ R^{d×d}`; the source’s `torch.tensordot(x_l, self.cross_layer_w[i], dims=([1],[0]))` must be replaced by `torch.matmul(self.cross_layer_w[i], x_l)` with `x_l` unsqueezed to 3-D.\n2. **Low-rank mixture-of-experts (MoE) block**: entirely new `cross_network_mix` with experts `U,V,C`, gating network `nn.Linear(d,1)` per expert, and `softmax` weighted sum—none of these modules exist in the source and must be written from scratch.\n3. **Stacked structure option**: source only implements parallel; target adds `structure==\"stacked\"` where cross output feeds the MLP—requires a second forward branch and a different `predict_layer` input size (`mlp_hidden_size[-1]` only).\n4. **Regularization expansion**: source regularizes only `cross_layer_w`; DCNV2-mix must sum `RegLoss` over `cross_layer_u, cross_layer_v, cross_layer_c`—extend the `calculate_loss` method accordingly.",
    "rank": "rank1"
  },
  {
    "source": "AutoInt_2019",
    "target": "DCNV2_2020",
    "type": "in-domain",
    "similarities": "1. **Shared embedding layer & field concatenation**: Both models start with identical sparse/dense feature preprocessing—one-hot/multi-hot categorical embeddings, scalar-to-vector numerical mapping, and `concat_embed_input_fields()`; AutoInt code can be reused verbatim for DCNV2’s `dcn_all_embeddings` input tensor.\n2. **Residual-style identity shortcut**: AutoInt’s `ReLU(e_m + W_Res*e_m)` and DCNV2’s `x_{l+1}=x_0⊙(…)+x_l` both add the original embedding vector to the transformed one; the `self.v_res_embedding` Linear in AutoInt can be repurposed as the learnable projection inside DCNV2’s cross layer when dimensions change.\n3. **Parallel deep tower**: Both keep a standard MLP path (`MLPLayers` in RecBole) that runs on the flattened embeddings; the same `mlp_hidden_size`, dropout, and BN configuration blocks can be copied from AutoInt to DCNV2 with no change.\n4. **Log-loss & sigmoid output head**: Both papers use `BCEWithLogitsLoss` (AutoInt) or `BCELoss` (DCNV2) with a final `Linear(·,1)+Sigmoid`; the prediction wrapper (`predict()`, `calculate_loss()`) and `RegLoss` regularizer pattern in the provided AutoInt file can serve as the scaffold for DCNV2’s training loop.",
    "differences": "1. **Interaction engine must be replaced**: AutoInt’s multi-head self-attention (`nn.MultiheadAttention` stack) is completely removed; implement DCNV2’s cross layer `x_{l+1}=x_0⊙(W_l x_l+b_l)+x_l` with a `nn.ParameterList` of `W_l` matrices and `bias` vectors—this is a brand-new module not present in AutoInt.\n2. **Low-rank & MoE experts (mixDCN)**: DCNV2 optionally decomposes `W_l` into `U_l V_l^T` plus nonlinear `C_l` and adds a mixture-of-experts gating network; these `cross_layer_u/v/c` tensors and `gating` Linears have no counterpart in AutoInt and must be coded from scratch.\n3. **Stacked vs parallel structure switch**: DCNV2 supports two topologies—parallel concatenation of cross and deep outputs or stacked (cross output feeds MLP); AutoInt only has parallel attention+MLP summation. Add a `structure` flag and branch logic inside `forward()` to handle both cases.\n4. **Rank-based regularization**: DCNV2 explicitly regularizes the low-rank factors (`reg_loss` on `U,V,C`) whereas AutoInt only applies weight decay to attention matrices; extend the loss function to include the new parameter sets.",
    "rank": "rank2"
  },
  {
    "source": "xDeepFM_2018",
    "target": "DCNV2_2020",
    "type": "in-domain",
    "similarities": "1. Both models adopt a **parallel deep & cross architecture**: the source code’s `concat_embed_input_fields`, `MLPLayers`, and final `predict` pattern can be reused verbatim—only the cross branch changes.\n2. **Embedding-level input pipeline** is identical: the source tensor shape `[B, num_field, embed_dim]` → flatten/view to `[B, in_feature_num]` is already implemented in xDeepFM’s `forward` and can be kept for DCNV2.\n3. **Regularized binary-cross-entropy training loop**: `calculate_loss` in xDeepFM (BCEWithLogitsLoss + L2 on conv/linear weights) can be copied; only the parameter list handed to `reg_loss` needs to be extended with `cross_layer_w` (or U/V/C in mixture mode).\n4. **Parameter initialization pattern** (`xavier_normal_` for embeddings & linear layers) is reusable; the same `apply(xavier_normal_initialization)` can seed DCNV2’s cross matrices.",
    "differences": "1. **Cross-unit implementation is completely new**: xDeepFM uses 1-D conv (`nn.Conv1d`) on the outer-product tensor `Z`, while DCNV2 needs a **matrix-based recurrence** `x_{l+1}=x_0 ⊙(W_l x_l +b_l)+x_l`; none of the CIN convolution/modules can be salvaged—write `cross_network` from scratch.\n2. **Low-rank & mixture-of-experts block** (`cross_network_mix`) is absent in xDeepFM: introduce `nn.ParameterList` for U,V,C matrices plus a gating network (`nn.Linear` + `softmax`) that must be trained jointly; expect ~30 % extra GPU memory for K experts.\n3. **Interaction granularity differs**: CIN produces **vector-wise** feature maps (`H_k × D`) and sums over embedding dimension, whereas DCNV2 keeps the **embedding-size dimension intact** (`d`)—no sum-pooling stage exists; remove xDeepFM’s `torch.sum(result,-1)` line.\n4. **Architecture choice flag**: DCNV2 supports **stacked** mode (cross output feeds MLP); xDeepFM is hard-wired parallel—add a structure branch in `forward` that routes `cross_output` straight into `mlp_layers` when `config['structure']=='stacked'`.",
    "rank": "rank3"
  },
  {
    "source": "DeepFM_2017",
    "target": "DCNV2_2020",
    "type": "in-domain",
    "similarities": "1. **Embedding Layer**: Both models share the same sparse/dense feature preprocessing and embedding lookup pattern; DeepFM’s `concat_embed_input_fields()` and tensor shape `[batch_size, num_field, embed_dim]` can be reused verbatim for DCNV2’s `dcn_all_embeddings`.\n2. **Parallel DNN Backbone**: DeepFM’s `MLPLayers` + `deep_predict_layer` stack is identical to DCNV2’s `mlp_layers`; the same `MLPLayers` utility, dropout, BN and Xavier init can be copied directly.\n3. **Joint Training & Loss**: Both use `BCEWithLogitsLoss` (DeepFM) or `BCELoss` (DCNV2) with L2 regularization; the `calculate_loss()` pattern—compute prediction, add reg term, return total loss—can be reused.\n4. **End-to-End API**: The `forward()`, `calculate_loss()`, `predict()` signatures and the `ContextRecommender` base class are identical; the whole training/evaluation loop and batch iterator code from DeepFM can be kept unchanged.",
    "differences": "1. **Cross Network Module (NEW)**: DCNV2 replaces FM with a multi-layer cross network (`cross_network` / `cross_network_mix`) that performs `x_{l+1}=x_0 ⊙(W_l x_l +b_l)+x_l`; this requires implementing `ParameterList` of `W` (or low-rank `U,V,C`) and the explicit for-loop over layers—completely absent in DeepFM.\n2. **MoE Low-Rank Experts (NEW)**: When `mixed=True`, DCNV2 introduces `expert_num` low-rank experts, gating network `gating[k]=Linear(in_feature_num,1)` and `softmax` gating weights; DeepFM has no mixture-of-experts or low-rank decomposition, so these matrices and gating logic must be written from scratch.\n3. **Dual Architecture Modes (NEW)**: DCNV2 supports `structure=parallel` (concatenate cross & deep outputs) and `stacked` (feed cross output into deep); DeepFM’s fixed parallel addition `y=y_fm+y_deep` must be extended with a conditional branch and reshaping logic inside `forward()`.\n4. **Efficiency-oriented Regularization**: DCNV2 adds a custom `RegLoss()` applied separately to `cross_layer_u/v/c` when `mixed=True`; DeepFM only regularizes embeddings via weight decay, so a new reg-loss component and corresponding `reg_weight` config must be introduced.",
    "rank": "rank5"
  },
  {
    "source": "FFM_2016",
    "target": "FwFM_2018",
    "type": "in-domain",
    "similarities": "1. Both models inherit from ContextRecommender base class and share identical initialization patterns (feature2field mapping, field-aware embedding tables, xavier_normal_ weight init), allowing direct reuse of _get_feature2field() and _init_weights() methods\n2. Feature preprocessing pipeline is identical - both use concat_embed_input_fields() to generate field-aware embeddings [batch_size, num_fields, embed_dim], enabling reuse of entire embedding extraction logic\n3. Both implement the same first-order linear component via first_order_linear(interaction) and use identical BCEWithLogitsLoss for CTR prediction, making forward() and loss calculation modules reusable\n4. Code structure matches perfectly - both have calculate_loss() and predict() methods with identical signatures, allowing complete reuse of training/prediction loops\n5. Field mapping configuration system is identical - both accept 'fields' dict parameter and handle automatic field assignment when fields=None, enabling direct reuse of configuration parsing",
    "differences": "1. NEW: Must implement field-pair weight matrix self.weight[num_fields, num_fields, 1] - FFM uses separate embedding tables per field while FwFM shares embeddings but adds learnable field interaction weights\n2. NEW: Must replace FFM's nested embedding lookups with FwFM's fwfm_layer() that computes r_{F(i),F(j)} * <v_i, v_j> interactions - this requires implementing the double-loop over feature pairs with field indexing\n3. NEW: Must add dropout_layer for regularizing field interaction weights (FFM has no dropout) - implement as nn.Dropout(p=config['dropout_prob'])\n4. NEW: Embedding storage differs - FFM stores n*f*k parameters in multiple ModuleList embeddings while FwFM stores only m*k parameters in single embedding table plus n*n field weights, requiring complete rewrite of embedding initialization\n5. NEW: Interaction computation changes from FFM's field-specific embedding lookups to FwFM's shared embedding with field weight multiplication - must implement the core interaction logic in fwfm_layer() method",
    "rank": "rank1"
  },
  {
    "source": "FM_2010",
    "target": "FwFM_2018",
    "type": "in-domain",
    "similarities": "1. **Shared embedding-first architecture**: both models start by projecting every categorical/continuous feature to the same latent dimension K (concat_embed_input_fields), so the source’s [batch_size, num_field, embed_dim] tensor can be fed into FwFM unchanged—no new embedding table logic is required.\n2. **Identical first-order linear part**: the source’s self.first_order_linear(interaction) (a simple nn.Linear on one-hot concatenation) is reused verbatim in FwFM; no code change is needed for the bias and linear term branch.\n3. **Pairwise interaction skeleton**: the double loop over feature pairs (i<j) and the final reduction sum(torch.sum(..., dim=1)) appear in both; the source’s BaseFactorizationMachine.reduce_sum=True already materialises ⟨v_i,v_j⟩x_i x_j, so only an extra field-weight multiply has to be inserted inside the loop.\n4. **Same loss & heads**: BCEWithLogitsLoss + sigmoid for CTR prediction, and identical calculate_loss()/predict() entry points; the whole training loop and evaluation metrics can be copied.\n5. **Parameter initialisation routine**: xavier_normal_ on embeddings is already implemented in the source and can be kept for both embedding and the new field-weight matrix.",
    "differences": "1. **Field-aware weight matrix r_{F(i),F(j)}**: introduce an [n_fields, n_fields, 1] tensor (self.weight) that did not exist in FM; requires new nn.Parameter registration, init, and L2-regularisation logic.\n2. **Feature-to-field mapping**: FwFM needs a feature2field dictionary built from config['fields'] or auto-numbering; this lookup step is absent in FM and must be coded in _get_feature2field().\n3. **Interaction computation kernel**: replace the O(k·m²) vectorised FM reduction with an explicit nested loop that indexes weight[Fi,Fj]; implement this in a new fwfm_layer() method—FM’s BaseFactorizationMachine class cannot be reused directly.\n4. **Optional linear-term variants**: FwFM paper explores FeLV (feature-wise linear vectors) and FiLV (field-wise linear vectors) that dot-product v_i with extra weights; these flavours require additional [m,K] or [n,K] parameters and a separate inner-product branch, none of which exist in the FM codebase.\n5. **Dropout on interaction output**: FwFM adds a dropout_layer on the summed interaction vector before final reduction; insert nn.Dropout(p=config['dropout_prob']) in fwfm_layer().",
    "rank": "rank2"
  },
  {
    "source": "DeepFM_2017",
    "target": "xDeepFM_2018",
    "type": "in-domain",
    "similarities": "1. **Shared embedding layer & first-order linear term**: both inherit from `ContextRecommender` and keep the same `first_order_linear` module and `concat_embed_input_fields()` helper, so the entire embedding/linear block can be copied verbatim when reproducing xDeepFM.\n2. **Parallel deep (MLP) branch**: the DNN sub-network is structurally identical—`MLPLayers(size_list, dropout)` followed by a `nn.Linear(hidden[-1], 1)`—so the source `DeepFM.mlp_layers` and `deep_predict_layer` can be reused with only a rename to `dnn_layers`.\n3. **Sum fusion & sigmoid output**: final logits are produced by summing component scores (`y_linear + y_fm + y_deep` vs `y_linear + y_cin + y_dnn`) and fed into the same `BCEWithLogitsLoss`; the source `forward()` pattern and loss-computation code can be adopted without change.\n4. **L2-regularized empirical risk minimization**: both models add an L2 term on weights and use identical optimizer & evaluation routines; the `reg_loss()` helper already present in DeepFM can be extended to cover CIN conv-weights.\n5. **Initialization & device handling**: `xavier_normal_` on embeddings/Linears and constant-0 bias is identical; the `_init_weights` method can be reused for all shared components.",
    "differences": "1. **CIN module is completely new**: xDeepFM replaces the FM second-order unit with a multi-layer Compressed Interaction Network implemented via 1-D convolutions (`conv1d_list`) and Hadamard outer-products (`torch.einsum`). This entire `compressed_interaction_network()` function must be written from scratch.\n2. **Explicit high-order interactions vs pairwise FM**: DeepFM’s `BaseFactorizationMachine` computes only 2nd-order inner-products; xDeepFM learns arbitrary k-order interactions explicitly—requires building the outer-product tensor `Z^{k+1}` and sliding convolutional filters across it, a pattern absent in DeepFM.\n3. **Dual direct/split connectivity inside CIN**: xDeepFM supports both “direct” (all feature maps forwarded) and “split” (half forwarded, half summed) modes controlled by `self.direct`, necessitating extra branching logic not present in the simpler FM pooling.\n4. **Layer-size parity constraint**: CIN enforces even `layer_size` except the last when `direct=False`; the constructor must validate and auto-correct sizes, adding complexity absent in DeepFM.\n5. **Extra hyper-parameters & regularization scope**: `cin_layer_size`, `direct`, and `reg_weight` are new config keys; the regularization routine must iterate over `conv1d_list` weights in addition to MLP and linear parts.",
    "rank": "rank1"
  },
  {
    "source": "PNN_2016",
    "target": "xDeepFM_2018",
    "type": "in-domain",
    "similarities": "1. Both models start with the same embedding layer: reuse PNN's `concat_embed_input_fields()` and embedding lookup table verbatim; the field-wise dense embedding matrix [batch, num_field, embed_dim] is the common input to all interaction modules.\n2. Both stack an MLP on top of interaction signals: PNN's `MLPLayers(size_list, dropout)` and final `nn.Linear(hidden[-1],1)` can be copied directly into xDeepFM's DNN branch—same initialization (`xavier_normal_`) and dropout pattern.\n3. L2-regularization utility: PNN's `reg_loss()` that iterates over `(name, param)` and sums `.norm(2)` for weights ending in \"weight\" is identical to what xDeepFM needs; lift the method and call it for CIN conv-layers, linear layer and MLP.\n4. Sigmoid-output + BCEWithLogitsLoss pipeline: the final `self.sigmoid(self.forward())` and `nn.BCEWithLogitsLoss()` training pattern in PNN is exactly the same in xDeepFM—no code change required.\n5. Pair-wise indexing helper: the nested loops that build `row/col` indices for all field pairs (`i<j`) in `InnerProductLayer` and `OuterProductLayer` can be reused to generate the CIN outer-products mask if one ever needs sparse pair selection.",
    "differences": "1. PNN only captures 2nd-order interactions (inner/outer product of pairs), whereas xDeepFM needs arbitrary k-order via CIN. A completely new `compressed_interaction_network()` module must be written—it performs multi-layer Hadamard outer-products `Z=einsum('bhd,bmd->bhmd')` followed by 1×1 convolutions, none of which exists in PNN.\n2. xDeepFM keeps an explicit linear (FM-like) part (`first_order_linear`) that PNN lacks; implement a separate embedding table of size 1 per field and sum-pool for this term.\n3. CIN layers require learnable 1-D conv (`nn.Conv1d`) with carefully managed `field_nums` list that grows with layer depth; PNN uses only static product kernels—no convolution code is present.\n4. Interaction output fusion differs: PNN concatenates linear + product signals and feeds one MLP; xDeepFM produces three logits (linear, CIN, DNN) and adds them before sigmoid. Refactor the forward pass to accumulate three terms instead of one.\n5. Complexity-reduction tricks in PNN (e.g., OPNN’s `f_sum f_sum^T` matrix collapse) are irrelevant to CIN; instead implement optional tensor-factorization (`direct=False`) that splits each CIN layer output into two halves to save parameters.",
    "rank": "rank2"
  },
  {
    "source": "WideDeep_2016",
    "target": "xDeepFM_2018",
    "type": "in-domain",
    "similarities": "1. Both inherit from `ContextRecommender` and share identical base components: `concat_embed_input_fields`, `first_order_linear`, `MLPLayers`, `BCEWithLogitsLoss`, and `sigmoid` prediction—therefore 80 % of WideDeep’s `__init__`, `_init_weights`, `calculate_loss`, and `predict` can be reused verbatim.\n2. Embedding pipeline is identical: same `embedding_size`, `num_feature_field`, Xavier/constant init, and field concatenation `[batch_size, num_field, embed_dim]`—no rewrite needed for embedding lookup or tensor shaping.\n3. DNN branch design is the same: flatten embeddings → stack of fully-connected `MLPLayers` with dropout → linear output → logit; the existing `mlp_layers` and `deep_predict_layer` code can be kept with only width/depth hyper-param change.\n4. Joint training objective: sum of individual branch logits (`fm_output + deep_output` in WideDeep vs `first_order_linear + cin_output + dnn_output` in xDeepFM) followed by single `BCEWithLogitsLoss`; the WideDeep forward pattern directly shows where to plug the new CIN logit.\n5. Regularization & optimization pattern: both use L2 on weights and allow per-component reg_weights; `calculate_reg_loss()` in WideDeep can be extended (not rewritten) to loop over CIN conv1d parameters.",
    "differences": "1. NEW: Compressed Interaction Network (CIN) – entire `compressed_interaction_network()` method plus `conv1d_list` ModuleList must be implemented; it performs outer-product → 1×1 conv → Hadamard-compressed feature maps, none of which exist in WideDeep.\n2. NEW: CIN-specific hyper-parameters (`cin_layer_size`, `direct`, `field_nums` list) and dynamic 1-D conv layer creation; WideDeep has no convolutional logic.\n3. NEW: Sum-pooling layer after each CIN depth (`torch.sum(result, -1)`) to produce fixed-length vector `p+`; WideDeep lacks any pooling operation.\n4. NEW: Additional linear layer `cin_linear` to project CIN pooled vector to 1-dim logit; WideDeep only has one `deep_predict_layer`.\n5. Different complexity: CIN adds O(m²H²DT) time vs WideDeep’s O(mHD+H²T); GPU memory and training loop must accommodate intermediate 4-D tensors (`bhmd`) and extra convolution weights.",
    "rank": "rank3"
  },
  {
    "source": "DCN_2017",
    "target": "xDeepFM_2018",
    "type": "in-domain",
    "similarities": "1. Both models inherit from ContextRecommender and share the same embedding & stacking layer: concat_embed_input_fields() returns [B, num_field, embed_dim] and is then flattened to [B, num_field×embed_dim] – this exact tensor shape is consumed by both DCN’s cross_network() and xDeepFM’s MLP, so the embedding pipeline can be copied verbatim.\n2. Both combine an explicit feature-interaction component (cross network vs CIN) with a standard MLP; the MLP construction (MLPLayers class, ReLU, dropout, batch-norm, identical size_list) is literally the same module – instantiating self.mlp_layers in DCN is plug-and-play in xDeepFM.\n3. Logistic-loss training loop is identical: BCEWithLogitsLoss, L2 reg on weights, joint end-to-end training; reg_loss computation pattern (iterating named_parameters() with ‘weight’ suffix) can be reused from DCN’s RegLoss utility.\n4. Final prediction head: concat explicit & implicit vectors, feed to a single nn.Linear(…) then sigmoid – the combination layer code in DCN forward() (torch.cat + predict_layer) is the same architectural pattern xDeepFM uses for cin_output + dnn_output + first_order_linear.",
    "differences": "1. DCN’s cross_network() is a scalar-wise residual loop (x_{l+1}=x_0⊙(x_l^T w_l)+b_l+x_l) requiring only two ParameterList tensors; xDeepFM needs a completely new Compressed Interaction Network: outer-product tensor Z via torch.einsum, 1-D conv layers (conv1d_list) to generate feature maps, sum-pooling per map, and optional split/direct logic – none of these ops exist in DCN.\n2. DCN keeps embedding vectors flattened throughout; CIN must preserve the 3-D tensor [B, field, D] to perform Hadamard-vector interactions, so xDeepFM cannot flatten before CIN and needs extra view/sum/cat gymnastics – reshape logic has to be written from scratch.\n3. DCN has no concept of layer-wise sum pooling or concatenation of pooled vectors (p^+); xDeepFM must accumulate each CIN layer’s pooled vector and finally cin_linear(self.final_len, 1) – requires bookkeeping of field_nums list and final_len arithmetic absent in DCN.\n4. DCN parameter count is O(d×L_c); CIN parameter count is O(∑_k H_k H_{k-1} m) and optionally factorised via low-rank matrices U,V – the corresponding nn.Conv1d weight initialisation and optional L-order decomposition code must be newly added.",
    "rank": "rank4"
  },
  {
    "source": "xDeepFM_2018",
    "target": "FiGNN_2019",
    "type": "in-domain",
    "similarities": "1. **Embedding layer & field concatenation**: Both start with `concat_embed_input_fields()` to obtain [batch, m, d] field embeddings; FiGNN can reuse the xDeepFM embedding table without change.\n2. **Vector-wise feature interaction**: CIN’s Hadamard-product interactions (Eq. 6) and FiGNN’s edge-wise message passing both operate on full embedding vectors, not bit-wise; the same `embedding_size` hyper-parameter and tensor shape conventions apply.\n3. **Residual-style low-order reuse**: xDeepFM keeps raw embeddings for CIN’s X⁰ term; FiGNN adds h¹ residual in Eq. 12—both preserve low-order signals; the same tensor `att_feature` can be kept in memory for residual add.\n4. **Log-loss & sigmoid output**: Both use `nn.BCEWithLogitsLoss()` and `Sigmoid()` for final prediction; `calculate_loss()` and `predict()` skeleton can be copied verbatim.\n5. **Modular layer design**: xDeepFM’s `compressed_interaction_network()` returns a flat vector later fed to `nn.Linear`; FiGNN’s `fignn_layer()` can follow the same pattern (return logit vector) so upper-level training loop stays unchanged.\n6. **Parameter init & reg**: Xavier init and optional L2 reg helpers (`_init_weights`, `reg_loss`) are identical; reuse as-is.",
    "differences": "1. **Graph-based interaction module**: FiGNN needs a new `GraphLayer` (edge-weighted GGNN) implementing Eqs. 1–6; xDeepFM has no graph adjacency or message passing—this entire component must be coded from scratch.\n2. **Attentional edge weights**: Requires `W_attn` + `LeakyReLU` + masked softmax to build the m×m attention matrix A; CIN uses fixed convolutional filters instead—no attention logic exists in xDeepFM.\n3. **Multi-head self-attention pre-process**: FiGNN inserts `nn.MultiheadAttention` plus `att_embedding` projection before GNN; xDeepFM feeds embeddings directly into CIN/DNN—this transformer block is new.\n4. **GRU cell & residual inside GNN**: FiGNN layers contain `nn.GRUCell` and Eq. 12 residual add; xDeepFM uses simple `Conv1d` + activation—GRU state update logic must be implemented.\n5. **Attentional scoring layer vs sum pooling**: FiGNN ends with two MLPs (`mlp1`, `mlp2`) producing per-node scores/weights (Eq. 13–15); xDeepFM concatenates CIN sum-pooled vectors and uses a single linear layer—new MLP modules needed.\n6. **Hyper-parameters**: FiGNN introduces `attention_size`, `num_heads`, `n_layers` (GNN steps), `attn_dropout_prob`; xDeepFM uses `cin_layer_size`, `mlp_hidden_size`—config and constructor must expose these new keys.\n7. **Loop over edges vs convolution**: CIN reshapes into `batch×(H_k·m)×D` and applies 1×1 conv; FiGNN loops over all m² node pairs (`product(range(m),repeat=2)`) and performs `bmm` with dense adjacency—different compute pattern, no reuse of `conv1d_list`.",
    "rank": "rank1"
  },
  {
    "source": "DCN_2017",
    "target": "FiGNN_2019",
    "type": "in-domain",
    "similarities": "1. **Shared embedding pipeline** – both start with field-aware embeddings (`concat_embed_input_fields`) followed by a dense projection; DCN’s `x_0` tensor (shape `[B, F*E]`) can be reshaped to `[B, F, E]` and fed directly into FiGNN’s `att_embedding` Linear layer, re-using the same embedding lookup table and field concatenation logic.\n2. **Residual-style low-order preservation** – DCN’s cross layers implicitly keep the raw `x_0` alive via `x_{l+1} = f(x_l)+x_l`; FiGNN’s `h += att_feature` residual path plays an identical role. The DCN code’s in-place tensor additions can therefore be copied to implement FiGNN’s residual connections without redesign.\n3. **Parallel DNN backbone** – DCN’s `MLPLayers` module (with dropout & BN) can be dropped untouched into FiGNN as the `MLP_1`/`MLP_2` towers that produce per-field logits `ŷ_i` and attention weights `a_i`, saving the need to re-implement dense stacks.\n4. **Log-loss training loop** – both use `nn.BCEWithLogitsLoss` plus an optional `RegLoss` on weights; the `calculate_loss` signature and label slicing (`interaction[self.LABEL]`) are identical, so the DCN trainer loop can be reused verbatim.",
    "differences": "1. **Graph structure construction** – FiGNN needs a fully-connected weighted adjacency matrix `graph` (`[B, F, F]`) computed on-the-fly with Leaky-ReLU attention (`W_attn`); DCN has no graph component, so the entire `GraphLayer`, `W_attn`, `src_nodes`/`dst_nodes` pairing, and masked softmax must be written from scratch.\n2. **Multi-head self-attention block** – FiGNN inserts `nn.MultiheadAttention` (`self_attn`) to create contextual field representations before graph propagation; DCN lacks any attention, therefore the module, its Q/K/V projections, and the post-attention residual `+ v_res` are new code paths.\n3. **GRU-based state update** – FiGNN propagates node states via `nn.GRUCell` inside each GNN layer; DCN’s cross network is a simple `x_0*x_l^T*w` recurrence without recurrent gates, so the `gru_cell` calls and hidden-state reshape logic have no counterpart in DCN and must be implemented anew.\n4. **Attentional scoring layer** – FiGNN produces a weighted sum of per-field scores with learnable `MLP_1` & `MLP_2`; DCN only concatenates cross & deep outputs and feeds one final linear layer. The two extra small MLPs and the `(weight * score).sum(dim=1)` reduction are additional components to code.",
    "rank": "rank2"
  },
  {
    "source": "NFM_2017",
    "target": "FiGNN_2019",
    "type": "in-domain",
    "similarities": "1. Both models start with field-aware embeddings: reuse `concat_embed_input_fields()` and the embedding table initialization (`xavier_normal_(module.weight.data)` for nn.Embedding) from NFM code.\n2. Both apply a linear projection to obtain attention-compatible vectors: the NFM `predict_layer` (nn.Linear) can be cloned as `att_embedding` and `v_res_embedding` in FiGNN—only output dimension changes from 1 to `attention_size`.\n3. Both use dropout + ReLU after the first transformation: reuse NFM’s `self.dropout_layer` and `F.relu` pattern; BN layer is optional but can be dropped since FiGNN omits it.\n4. Both end with a scalar logit plus BCEWithLogitsLoss: the last nn.Linear in NFM (`predict_layer`) is structurally identical to FiGNN’s `(weight * score).sum(dim=1).unsqueeze(-1)` pipeline—only the inner fusion changes, so the loss module can be reused verbatim.",
    "differences": "1. Interaction paradigm: NFM collapses all pairs into one k-dim vector via Bi-Interaction pooling (O(kNx)); FiGNN keeps m separate nodes and models m×m edges—requires building a fully-connected adjacency tensor (`self.graph`) and edge-wise transformation matrices (`W_in`, `W_out`), none of which exist in NFM.\n2. Attention mechanism: NFM has no attention; FiGNN needs a multi-head self-attention module (`nn.MultiheadAttention`) and a Leaky-ReLU edge-attention network (`W_attn`)—must be implemented from scratch.\n3. Recurrent message passing: NFM’s MLP is feed-forward; FiGNN performs T steps of neighbour aggregation via a custom `GraphLayer`, followed by GRU updates—requires writing a `GRUCell` loop and residual add-backs (`h += att_feature`).\n4. Prediction layer fusion: NFM outputs a single logit; FiGNN predicts per-node scores and attention weights with two extra MLPs (`mlp1`, `mlp2`) and aggregates them—new code path not present in NFM.",
    "rank": "rank3"
  },
  {
    "source": "AFM_2017",
    "target": "FiGNN_2019",
    "type": "in-domain",
    "similarities": "1. Both models use an attention mechanism to weigh pairwise feature interactions: AFM’s `AttLayer` (Eq. 3–4) and FiGNN’s edge attention (Eq. 2) learn scalar importance scores via a small MLP (`W`+`h` vs. `W_attn`+`LeakyReLU`)—the AFM `AttLayer` code can be reused almost verbatim for FiGNN’s edge-weight sub-network by simply changing the input from `v_i⊙v_j` to `[e_i||e_j]`.\n2. Embedding pipeline is identical: field-level embeddings of size `embedding_size` are first created (`concat_embed_input_fields` in both codes), followed by an optional projection to `attention_size` (`att_embedding` linear in FiGNN equals `self.att_embedding` in AFM); the same Xavier init and dropout pattern can be copied.\n3. Both papers adopt the same supervised setup—BCEWithLogitsLoss, Sigmoid output, and label-balanced mini-batch training—so the training loop, metric computation and batch-generation utilities from AFM’s implementation are directly transferable.\n4. Regularization philosophy overlaps: AFM applies L2 on attention weights (`reg_weight * ‖W‖²`) and dropout on interaction vectors; FiGNN also uses dropout on hidden states and weight decay—re-using AFM’s `self.reg_weight` pattern in FiGNN’s optimizer keeps the same reproducibility profile.",
    "differences": "1. FiGNN requires a full graph neural network stack that is absent in AFM: a multi-head self-attention layer (`nn.MultiheadAttention`), `GraphLayer` modules with node-specific `W_in/W_out` tensors, GRU cells for recurrent state update, and residual connections—none of these classes exist in AFM and must be newly implemented.\n2. Interaction granularity differs: AFM only models second-order interactions (`num_pair = m(m-1)/2`) via element-wise product, whereas FiGNN performs T-step message passing over a complete directed graph (m² edges) to capture arbitrary high-order relations—this demands efficient batch-wise sparse-dense matrix multiplication and a custom `gru_cell` loop, which AFM’s code base lacks.\n3. Attention purpose is orthogonal: AFM uses attention to pool interaction vectors into a single representation, while FiGNN employs two attention stages—edge attention to build the adjacency matrix and node attention (`mlp2`) to produce final logits—necessitating an extra `mlp2` network and a summation step that AFM does not have.\n4. Complexity & memory footprint: AFM’s memory scales with `O(m·k)` embeddings plus `O(m²·k)` pairwise products, but FiGNN keeps `O(T·m²·k')` activations across GNN steps; the AFM implementation’s simple `build_cross` indexing is insufficient—FiGNN needs precomputed `src_nodes`, `dst_nodes` tensors and efficient batched `bmm` operations to stay within GPU memory.",
    "rank": "rank5"
  },
  {
    "source": "DeepFM_2017",
    "target": "EulerNet_2021",
    "type": "in-domain",
    "similarities": "1. Both inherit from ContextRecommender and share the same data flow: concat_embed_input_fields() produces [B,F,E] embeddings, and the calculate_loss/predict skeletons are compatible.\n2. Both return a single logit from forward(), use BCEWithLogitsLoss for training, and apply sigmoid in predict(); labels come from interaction[self.LABEL].\n3. Both use xavier_normal_ initialization for embeddings/linears and compress to a single logit via a linear projection.",
    "differences": "1. EulerNet builds complex-valued interactions with mu and EulerInteractionLayer (r=mu*cos(x), p=mu*sin(x)), while DeepFM uses FM + MLP with no complex path.\n2. EulerInteractionLayer introduces inter_orders (softmax init), bias_lam/bias_theta, and a shared im linear for r/p; DeepFM has none of these components.\n3. EulerNet uses separate drop_ex/drop_im (and optional LayerNorm) inside interaction layers; DeepFM only applies dropout in the MLP.\n4. EulerNet regularizes only Euler_interaction_layers/mu/reg via RegularLoss, whereas DeepFM has no extra regularization term.",
    "rank": "rank1"
  },
  {
    "source": "AutoInt_2019",
    "target": "EulerNet_2021",
    "type": "in-domain",
    "similarities": "1. Both use ContextRecommender with concat_embed_input_fields(), so input handling and label access are aligned.\n2. Both output a single logit, train with BCEWithLogitsLoss, and apply sigmoid in predict().\n3. Both use xavier_normal_ initialization for embeddings/linear layers with zeroed bias.",
    "differences": "1. AutoInt models interactions with multi-head self-attention, while EulerNet replaces attention with complex-space EulerInteractionLayer.\n2. EulerNet controls interaction order via order_list and inter_orders, whereas AutoInt only changes attention hidden sizes/heads.\n3. EulerNet maintains r/p dual streams with cos/sin and log/exp transforms; AutoInt operates purely on real-valued tensors.\n4. EulerNet applies drop_ex/drop_im separately; AutoInt uses dropout on attention outputs/weights.",
    "rank": "rank2"
  },
  {
    "source": "FiGNN_2019",
    "target": "EulerNet_2021",
    "type": "in-domain",
    "similarities": "1. Both are built on ContextRecommender, take [B,F,E] embeddings, and keep the same training/prediction interfaces.\n2. Both output a single logit, use BCEWithLogitsLoss for CTR training, and apply sigmoid in predict().\n3. Both use xavier_normal_ initialization and dropout for regularization.",
    "differences": "1. FiGNN relies on GNN message passing with GRU updates; EulerNet has no graph and learns interactions purely via EulerInteractionLayer.\n2. EulerNet uses inter_orders, bias_lam/bias_theta, and a shared im linear for complex transforms; FiGNN has no polar/complex operations.\n3. EulerNet maintains r/p dual streams with separate dropout; FiGNN applies dropout on a single real-valued stream.\n4. EulerNet merges re/im into the final logit; FiGNN aggregates node scores with attention before scoring.",
    "rank": "rank3"
  },
  {
    "source": "xDeepFM_2018",
    "target": "EulerNet_2021",
    "type": "in-domain",
    "similarities": "1. Both share the embedding pipeline and ContextRecommender interface, so I/O shapes and training hooks are consistent.\n2. Both train with BCEWithLogitsLoss and apply sigmoid at prediction time.\n3. Both use xavier_normal_ initialization and reg_weight-style regularization controls.",
    "differences": "1. xDeepFM uses CIN for high-order interactions, while EulerNet replaces CIN with complex-valued EulerInteractionLayer.\n2. EulerNet shapes interaction depth via order_list and inter_orders (softmax init); xDeepFM uses cin_layer_size.\n3. EulerNet applies log/exp transforms and dual dropouts (drop_ex/drop_im); xDeepFM only drops in CIN/MLP.\n4. EulerNet regularizes only Euler_interaction_layers/mu/reg, whereas xDeepFM regularizes a broader set of modules.",
    "rank": "rank4"
  },
  {
    "source": "FM_2010",
    "target": "KD_DAGFM_2021",
    "type": "in-domain",
    "similarities": "1. Both share the ContextRecommender input pipeline: concat_embed_input_fields() yields [B,F,E] embeddings.\n2. Both target CTR probability with sigmoid outputs and use a BCE-style loss.\n3. Both require explicit embedding_size/field_num settings to define interaction dimensions.",
    "differences": "1. KD_DAGFM has teacher_training/distillation/finetuning phases controlled by phase/alpha/beta; FM has no distillation workflow.\n2. KD_DAGFM student uses DAG-based interactions with an upper-triangular adj_matrix, learnable connect_layer, and depth-wise state accumulation; FM is plain 2nd-order FM.\n3. Distillation uses MSE between teacher_network.logits and student_network.logits and freezes the teacher in eval; FM has no such logic.\n4. KD_DAGFM outputs sigmoid probabilities internally (BCELoss), while FM uses a pure logit path with BCEWithLogitsLoss.",
    "rank": "rank1"
  },
  {
    "source": "DeepFM_2017",
    "target": "KD_DAGFM_2021",
    "type": "in-domain",
    "similarities": "1. Both share the ContextRecommender embedding pipeline and field handling logic.\n2. Both are CTR binary classifiers; predict returns probabilities and calculate_loss reads interaction[self.LABEL].\n3. Both use xavier_normal_ initialization with zeroed linear bias.",
    "differences": "1. KD_DAGFM has teacher/student networks with phase control; DeepFM is a single-network model.\n2. KD_DAGFM student uses DAG recursion (einsum + depth), while DeepFM uses FM + MLP interactions.\n3. Distillation switches to loss = alpha*CTR + beta*MSE(logits_T, logits_S) with warm_up; DeepFM has no KD loss.\n4. KD_DAGFM uses BCELoss on sigmoid outputs, while DeepFM uses BCEWithLogitsLoss on logits.",
    "rank": "rank2"
  },
  {
    "source": "NFM_2017",
    "target": "KD_DAGFM_2021",
    "type": "in-domain",
    "similarities": "1. Both use concat_embed_input_fields() to get [B,F,E] and reduce interactions to a scalar output.\n2. Both train with CTR binary loss and read labels from interaction[self.LABEL].\n3. Both expose configurable embedding_size and regularization controls.",
    "differences": "1. KD_DAGFM student uses a DAG structure (p/q parameters + upper-triangular adj_matrix), while NFM uses Bi-Interaction pooling.\n2. KD_DAGFM includes a teacher_network (CrossNet/CIN) configured with t_ keys; NFM has no teacher.\n3. Distillation computes KD loss between teacher_network.logits and student_network.logits with teacher in eval; NFM has no such stage.\n4. KD_DAGFM outputs sigmoid probabilities (BCELoss), whereas NFM typically uses logits with BCEWithLogitsLoss.",
    "rank": "rank3"
  },
  {
    "source": "AutoInt_2019",
    "target": "KD_DAGFM_2021",
    "type": "in-domain",
    "similarities": "1. Both share the ContextRecommender embedding/field pipeline with the same input shapes.\n2. Both predict CTR probabilities and read labels from interaction[self.LABEL].\n3. Both use xavier_normal_ initialization and reg_weight-style regularization.",
    "differences": "1. KD_DAGFM introduces teacher/student structure with phase switching; AutoInt is a single self-attention network.\n2. KD_DAGFM student relies on DAG depth recursion (einsum + depth), while AutoInt uses multi-head self-attention.\n3. KD_DAGFM distillation uses warm_up and alpha/beta weighting; AutoInt has no KD loss.\n4. KD_DAGFM uses BCELoss on sigmoid outputs; AutoInt typically uses BCEWithLogitsLoss on logits.",
    "rank": "rank4"
  }
]
