{
  "id": "xDeepFM_2018",
  "paper_title": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems",
  "alias": "xDeepFM",
  "year": 2018,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "xDeepFM proposes a Compressed Interaction Network (CIN) that models explicit high-order feature interactions at the vector-wise level, where interactions are measured explicitly and the degree increases with layer depth. The model combines CIN with a plain DNN to jointly learn both explicit and implicit feature interactions in an end-to-end manner, addressing the limitation that DNNs learn implicit interactions while traditional models like FM only capture low-order explicit interactions. CIN uses a CNN-like structure with outer products along embedding dimensions and compression via learnable filters, achieving polynomial approximation with O(km³) parameters instead of O(m^k).",
  "introduction": "# 1 INTRODUCTION\n\nFeatures play a central role in the success of many predictive systems. Because using raw features can rarely lead to optimal results, data scientists usually spend a lot of work on the transformation of raw features in order to generate best predictive systems [14, 24] or to win data mining games [21, 22, 26]. One major type of feature transformation is the cross-product transformation over categorical features [5]. These features are called cross features or multi-way features, they measure the interactions of multiple raw features. For instance, a 3-way feature AND (user Organization=msra, item_category=deeplearning, time=monday) has value 1 if the user works at Microsoft Research Asia and is shown a technical article about deep learning on a Monday.\n\nThere are three major downsides for traditional cross feature engineering. First, obtaining high-quality features comes with a high cost. Because right features are usually task-specific, data scientists need spend a lot of time exploring the potential patterns from the product data before they become domain experts and extract meaningful cross features. Second, in large-scale predictive systems such as web-scale recommender systems, the huge number of raw features makes it infeasible to extract all cross features manually. Third, hand-crafted cross features do not generalize to unseen interactions in the training data. Therefore, learning to interact features without manual engineering is a meaningful task.\n\nFactorization Machines (FM) [32] embed each feature  $i$  to a latent factor vector  $\\mathbf{v}_i = [v_{i1}, v_{i2}, \\dots, v_{iD}]$ , and pairwise feature interactions are modeled as the inner product of latent vectors:  $f^{(2)}(i,j) = \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j$ . In this paper we use the term bit to denote a element (such as  $v_{i1}$ ) in latent vectors. The classical FM can be extended to arbitrary higher-order feature interactions [2], but one\n\nmajor downside is that, [2] proposes to model all feature interactions, including both useful and useless combinations. As revealed in [43], the interactions with useless features may introduce noises and degrade the performance. In recent years, deep neural networks (DNNs) have become successful in computer vision, speech recognition, and natural language processing with their great power of feature representation learning. It is promising to exploit DNNs to learn sophisticated and selective feature interactions. [46] proposes a Factorisation-machine supported Neural Network (FNN) to learn high-order feature interactions. It uses the pre-trained factorization machines for field embedding before applying DNN. [31] further proposes a Product-based Neural Network (PNN), which introduces a product layer between embedding layer and DNN layer, and does not rely on pre-trained FM. The major downside of FNN and PNN is that they focus more on high-order feature interactions while capture little low-order interactions. The Wide&Deep [5] and DeepFM [9] models overcome this problem by introducing hybrid architectures, which contain a shallow component and a deep component with the purpose of learning both memorization and generalization. Therefore they can jointly learn low-order and high-order feature interactions.\n\nAll the abovementioned models leverage DNNs for learning high-order feature interactions. However, DNNs model high-order feature interactions in an implicit fashion. The final function learned by DNNs can be arbitrary, and there is no theoretical conclusion on what the maximum degree of feature interactions is. In addition, DNNs model feature interactions at the bit-wise level, which is different from the traditional FM framework which models feature interactions at the vector-wise level. Thus, in the field of recommender systems, whether DNNs are indeed the most effective model in representing high-order feature interactions remains an open question. In this paper, we propose a neural network-based model to learn feature interactions in an explicit, vector-wise fashion. Our approach is based on the Deep & Cross Network (DCN) [40], which aims to efficiently capture feature interactions of bounded degrees. However, we will argue in Section 2.3 that DCN will lead to a special format of interactions. We thus design a novel compressed interaction network (CIN) to replace the cross network in the DCN. CIN learns feature interactions explicitly, and the degree of interactions grows with the depth of the network. Following the spirit of the Wide&Deep and DeepFM models, we combine the explicit high-order interaction module with implicit interaction module and traditional FM module, and name the joint model eXtreme Deep Factorization Machine (xDeepFM). The new model requires no manual feature engineering and release data scientists from tedious feature searching work. To summarize, we make the following contributions:\n\n- We propose a novel model, named eXtreme Deep Factorization Machine (xDeepFM), that jointly learns explicit and implicit high-order feature interactions effectively and requires no manual feature engineering.  \n- We design a compressed interaction network (CIN) in xDeepFM that learns high-order feature interactions explicitly. We show that the degree of feature interactions increases at each layer, and features interact at the vector-wise level rather than the bit-wise level.\n\n- We conduct extensive experiments on three real-world dataset, and the results demonstrate that our xDeepFM outperforms several state-of-the-art models significantly.\n\nThe rest of this paper is organized as follows. Section 2 provides some preliminary knowledge which is necessary for understanding deep learning-based recommender systems. Section 3 introduces our proposed CIN and xDeepFM model in detail. We will present experimental explorations on multiple datasets in Section 4. Related works are discussed in Section 5. Section 6 concludes this paper.",
  "method": "# 3 OUR PROPOSED MODEL\n\n# 3.1 Compressed Interaction Network\n\nWe design a new cross network, named Compressed Interaction Network (CIN), with the following considerations: (1) interactions are applied at vector-wise level, not at bit-wise level; (2) high-order feature interactions is measured explicitly; (3) the complexity of network will not grow exponentially with the degree of interactions.\n\nSince an embedding vector is regarded as a unit for vector-wise interactions, hereafter we formulate the output of field embedding as a matrix  $\\mathbf{X}^0\\in \\mathbb{R}^{m\\times D}$ , where the  $i$ -th row in  $\\mathbf{X}^0$  is the embedding vector of the  $i$ -th field:  $\\mathbf{X}_{i,*}^0 = \\mathbf{e}_i$ , and  $D$  is the dimension of the field embedding. The output of the  $k$ -th layer in CIN is also a matrix  $\\mathbf{X}^k\\in \\mathbb{R}^{H_k\\times D}$ , where  $H_{k}$  denotes the number of (embedding) feature vectors in the  $k$ -th layer and we let  $H_0 = m$ . For each layer,  $\\mathbf{X}^k$  are\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/92725dac5470577cdc0582e9825d4b592880694661bf8da4002d0665fb6875de.jpg)  \n(a) Outer products along each dimension for feature interactions. The tensor  $Z^{k + 1}$  is an intermediate result for further learning.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/2231171fd77b4c17343413c928859c77324cdcaebd4009c37d4409a49b651aa7.jpg)  \n(b) The  $k$ -th layer of CIN. It compresses the intermediate tensor  $\\mathbf{Z}^{k+1}$  to  $H_{k+1}$  embedding vectors (aslo known as feature maps).\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/033cbb4b612a2dda26c8c525c92de22d5f7127cf49bc2a1e3462c3598b1db587.jpg)  \n(c) An overview of the CIN architecture.  \nFigure 4: Components and architecture of the Compressed Interaction Network (CIN).\n\ncalculated via:\n\n$$\n\\mathbf {X} _ {h, *} ^ {k} = \\sum_ {i = 1} ^ {H _ {k - 1}} \\sum_ {j = 1} ^ {m} \\mathbf {W} _ {i j} ^ {k, h} \\left(\\mathbf {X} _ {i, *} ^ {k - 1} \\circ \\mathbf {X} _ {j, *} ^ {0}\\right) \\tag {6}\n$$\n\nwhere  $1 \\leq h \\leq H_{k}$ ,  $\\mathbf{W}^{k,h} \\in \\mathbb{R}^{H_{k-1} \\times m}$  is the parameter matrix for the  $h$ -th feature vector, and  $\\circ$  denotes the Hadamard product, for example,  $\\langle a_{1}, a_{2}, a_{3} \\rangle \\circ \\langle b_{1}, b_{2}, b_{3} \\rangle = \\langle a_{1}b_{1}, a_{2}b_{2}, a_{3}b_{3} \\rangle$ . Note that  $\\mathbf{X}^k$  is derived via the interactions between  $\\mathbf{X}^{k-1}$  and  $\\mathbf{X}^0$ , thus feature interactions are measured explicitly and the degree of interactions increases with the layer depth. The structure of CIN is very similar to the Recurrent Neural Network (RNN), where the outputs of the next hidden layer are dependent on the last hidden layer and an additional input. We hold the structure of embedding vectors at all layers, thus the interactions are applied at the vector-wise level.\n\nIt is interesting to point out that Equation 6 has strong connections with the well-known Convolutional Neural Networks (CNNs) in computer vision. As shown in Figure 4a, we introduce an intermediate tensor  $\\mathbf{Z}^{k + 1}$ , which is the outer products (along each embedding dimension) of hidden layer  $\\mathbf{X}^k$  and original feature matrix  $\\mathbf{X}^0$ . Then  $\\mathbf{Z}^{k + 1}$  can be regarded as a special type of image and  $\\mathbf{W}^{k,h}$  is a filter. We slide the filter across  $\\mathbf{Z}^{k + 1}$  along the embedding dimension (D) as shown in Figure 4b, and get an hidden vector  $\\mathbf{X}_{i,*}^{k + 1}$ , which is usually called a feature map in computer vision. Therefore,  $\\mathbf{X}^k$  is a collection of  $H_{k}$  different feature maps. The term \"compressed\" in the name of CIN indicates that the  $k$ -th hidden layer compress the potential space of  $H_{k - 1} \\times m$  vectors down to  $H_{k}$  vectors.\n\nFigure 4c provides an overview of the architecture of CIN. Let  $\\mathrm{T}$  denotes the depth of the network. Every hidden layer  $\\mathbf{X}^k, k \\in [1, T]$  has a connection with output units. We first apply sum pooling on each feature map of the hidden layer:\n\n$$\np _ {i} ^ {k} = \\sum_ {j = 1} ^ {D} \\mathbf {X} _ {i, j} ^ {k} \\tag {7}\n$$\n\nfor  $i\\in [1,H_k]$ . Thus, we have a pooling vector  $\\mathbf{p}^k = [p_1^k,p_2^k,\\dots,p_{H_k}^k ]$  with length  $H_{k}$  for the  $k$ -th hidden layer. All pooling vectors from\n\nhidden layers are concatenated before connected to output units:  $\\mathbf{p}^{+} = [\\mathbf{p}^{1},\\mathbf{p}^{2},\\dots,\\mathbf{p}^{T}]\\in \\mathbb{R}^{\\sum_{i = 1}^{T}H_{i}}$  . If we use CIN directly for binary classification, the output unit is a sigmoid node on  $\\mathbf{p}^{+}$  ..\n\n$$\ny = \\frac {1}{1 + \\exp \\left(\\mathbf {p} ^ {+ T} \\mathbf {w} ^ {o}\\right)} \\tag {8}\n$$\n\nwhere  $\\mathbf{w}^o$  are the regression parameters.\n\n# 3.2 CIN Analysis\n\nWe analyze the proposed CIN to study the model complexity and the potential effectiveness.\n\n3.2.1 Space Complexity. The  $h$ -th feature map at the  $k$ -th layer contains  $H_{k-1} \\times m$  parameters, which is exactly the size of  $\\mathbf{W}^{k,h}$ . Thus, there are  $H_k \\times H_{k-1} \\times m$  parameters at the  $k$ -th layer. Considering the last regression layer for the output unit, which has  $\\sum_{k=1}^{T} H_k$  parameters, the total number of parameters for CIN is  $\\sum_{k=1}^{T} H_k \\times (1 + H_{k-1} \\times m)$ . Note that CIN is independent of the embedding dimension  $D$ . In contrast, a plain  $T$ -layers DNN contains  $m \\times D \\times H_1 + H_T + \\sum_{k=2}^{T} H_k \\times H_{k-1}$  parameters, and the number of parameters will increase with the embedding dimension  $D$ .\n\nUsually  $m$  and  $H_{k}$  will not be very large, so the scale of  $\\mathbf{W}^{k,h}$  is acceptable. When necessary, we can exploit a  $L$ -order decomposition and replace  $\\mathbf{W}^{k,h}$  with two smaller matrices  $\\mathbf{U}^{k,h} \\in \\mathbb{R}^{H_{k-1} \\times L}$  and  $\\mathbf{V}^{k,h} \\in \\mathbb{R}^{m \\times L}$ :\n\n$$\n\\mathbf {W} ^ {k, h} = \\mathbf {U} ^ {k, h} \\left(\\mathbf {V} ^ {k, h}\\right) ^ {T} \\tag {9}\n$$\n\nwhere  $L \\ll H$  and  $L \\ll m$ . Hereafter we assume that each hidden layer has the same number (which is  $H$ ) of feature maps for simplicity. Through the  $L$ -order decomposition, the space complexity of CIN is reduced from  $O(mTH^2)$  to  $O(mTHL + TH^2L)$ . In contrast, the space complexity of the plain DNN is  $O(mDH + TH^2)$ , which is sensitive to the dimension (D) of field embedding.\n\n3.2.2 Time Complexity. The cost of computing tensor  $\\mathbf{Z}^{k + 1}$  (as shown in Figure 4a) is  $O(mHD)$  time. Because we have  $H$  feature maps in one hidden layer, computing a  $T$ -layers CIN takes  $O(mH^2 DT)$  time. A  $T$ -layers plain DNN, by contrast, takes  $O(mHD +$\n\n$H^2 T)$  time. Therefore, the major downside of CIN lies in the time complexity.\n\n3.2.3 Polynomial Approximation. Next we examine the high-order interaction properties of CIN. For simplicity, we assume that numbers of feature maps at hidden layers are all equal to the number of fields  $m$ . Let  $[m]$  denote the set of positive integers that are less than or equal to  $m$ . The  $h$ -th feature map at the first layer, denoted as  $\\mathbf{x}_h^1 \\in \\mathbb{R}^D$ , is calculated via:\n\n$$\n\\mathbf {x} _ {h} ^ {1} = \\sum_ {\\substack {i \\in [ m ] \\\\ j \\in [ m ]}} \\mathbf {W} _ {i, j} ^ {1, h} \\left(\\mathbf {x} _ {i} ^ {0} \\circ \\mathbf {x} _ {j} ^ {0}\\right) \\tag{10}\n$$\n\nTherefore, each feature map at the first layer models pair-wise interactions with  $O(m^2)$  coefficients. Similarly, the  $h$ -th feature map at the second layer is:\n\n$$\n\\begin{array}{l} \\mathbf{x}_{h}^{2} = \\sum_{\\substack{i\\in [m]\\\\ j\\in [m]}}\\mathbf{W}_{i,j}^{2,h}(\\mathbf{x}_{i}^{1}\\circ \\mathbf{x}_{j}^{0}) \\\\ = \\sum_ {\\substack {i \\in [ m ] \\\\ j \\in [ m ]}} \\sum_ {\\substack {l \\in [ m ] \\\\ k \\in [ m ]}} \\mathbf {W} _ {i, j} ^ {2, h} \\mathbf {W} _ {l, k} ^ {1, i} \\left(\\mathbf {x} _ {j} ^ {0} \\circ \\mathbf {x} _ {k} ^ {0} \\circ \\mathbf {x} _ {l} ^ {0}\\right) \\tag{11} \\\\ \\end{array}\n$$\n\nNote that all calculations related to the subscript  $l$  and  $k$  is already finished at the previous hidden layer. We expand the factors in Equation 11 just for clarity. We can observe that each feature map at the second layer models 3-way interactions with  $O(m^2)$  new parameters.\n\nA classical  $k$ -order polynomial has  $O(m^{k})$  coefficients. We show that CIN approximate this class of polynomial with only  $O(km^3)$  parameters in terms of a chain of feature maps. By induction hypothesis, we can prove that the  $h$ -th feature map at the  $k$ -th layer is:\n\n$$\n\\begin{array}{l} \\mathbf{x}_{h}^{k} = \\sum_{\\substack{i\\in [m]\\\\ j\\in [m]}}\\mathbf{W}_{i,j}^{k,h}(\\mathbf{x}_{i}^{k - 1}\\circ \\mathbf{x}_{j}^{0}) \\\\ = \\sum_ {\\substack {i \\in [ m ] \\\\ j \\in [ m ]}} \\dots \\sum_ {\\substack {r \\in [ m ] \\\\ t \\in [ m ]}} \\sum_ {\\substack {l \\in [ m ] \\\\ s \\in [ m ]}} \\mathbf {W} _ {i, j} ^ {k, h} \\dots \\mathbf {W} _ {l, s} ^ {1, r} (\\underbrace {\\mathbf {x} _ {j} ^ {0} \\circ \\dots \\circ \\mathbf {x} _ {s} ^ {0} \\circ \\mathbf {x} _ {l} ^ {0}} _ {k v e c t o r s}) \\tag{12} \\\\ \\end{array}\n$$\n\nFor better illustration, here we borrow the notations from [40]. Let  $\\pmb{\\alpha} = [\\alpha_{1},\\dots,\\alpha_{m}]\\in \\mathbb{N}^{d}$  denote a multi-index, and  $|\\pmb {\\alpha}| = \\sum_{i = 1}^{m}\\alpha_{i}$ . We omit the original superscript from  $\\mathbf{x}_i^0$ , and use  $\\mathbf{x}_i$  to denote it since we only use the feature maps from the 0-th layer (which is exactly the field embeddings) for the final expanded expression (refer to Eq. 12). Now a superscript is used to denote the vector operation, such as  $\\mathbf{x}_i^3 = \\mathbf{x}_i\\circ \\mathbf{x}_i\\circ \\mathbf{x}_i$ . Let  $VP_{k}(\\mathbf{X})$  denote a multi-vector polynomial of degree  $k$ :\n\n$$\nV P _ {k} (\\mathbf {X}) = \\left\\{\\sum_ {\\boldsymbol {\\alpha}} w _ {\\boldsymbol {\\alpha}} \\mathbf {x} _ {1} ^ {\\alpha_ {1}} \\circ \\mathbf {x} _ {2} ^ {\\alpha_ {2}} \\circ \\dots \\circ \\mathbf {x} _ {m} ^ {\\alpha_ {m}} \\mid 2 \\leqslant | \\boldsymbol {\\alpha} | \\leqslant k \\right\\} \\tag {13}\n$$\n\nEach vector polynomials in this class has  $O(m^{k})$  coefficients. Then, our CIN approaches the coefficient  $w_{\\alpha}$  with:\n\n$$\n\\hat {w} _ {\\alpha} = \\sum_ {i = 1} ^ {m} \\sum_ {j = 1} ^ {m} \\sum_ {B \\in P _ {\\alpha}} \\prod_ {t = 2} ^ {| \\alpha |} \\mathbf {W} _ {i, B _ {t}} ^ {t, j} \\tag {14}\n$$\n\nwhere,  $B = [B_{1}, B_{2}, \\dots, B_{|\\pmb{\\alpha}|}]$  is a multi-index, and  $P_{\\pmb{\\alpha}}$  is the set of all the permutations of the indices (1,...1, ..., m,..., m).\n\n$$\n\\alpha_ {1} \\text {t i m e s} \\quad \\alpha_ {m} \\text {t i m e s}\n$$\n\n# 3.3 Combination with Implicit Networks\n\nAs discussed in Section 2.2, plain DNNs learn implicit high-order feature interactions. Since CIN and plain DNNs can complement each other, an intuitive way to make the model stronger is to combine these two structures. The resulting model is very similar to the Wide&Deep or DeepFM model. The architecture is shown in Figure 5. We name the new model eXtreme Deep Factorization Machine (xDeepFM), considering that on one hand, it includes both low-order and high-order feature interactions; on the other hand, it includes both implicit feature interactions and explicit feature interactions. Its resulting output unit becomes:\n\n$$\n\\hat {y} = \\sigma \\left(\\mathbf {w} _ {\\text {l i n e a r}} ^ {T} \\mathbf {a} + \\mathbf {w} _ {d n n} ^ {T} \\mathbf {x} _ {d n n} ^ {k} + \\mathbf {w} _ {c i n} ^ {T} \\mathbf {p} ^ {+} + b\\right) \\tag {15}\n$$\n\nwhere  $\\sigma$  is the sigmoid function,  $\\mathbf{a}$  is the raw features.  $\\mathbf{x}_{dnn}^{k},\\mathbf{p}^{+}$  are the outputs of the plain DNN and CIN, respectively.  $\\mathbf{w}_{*}$  and  $b$  are learnable parameters. For binary classifications, the loss function is the log loss:\n\n$$\n\\mathcal {L} = - \\frac {1}{N} \\sum_ {i = 1} ^ {N} y _ {i} \\log \\hat {y} _ {i} + (1 - y _ {i}) \\log \\left(1 - \\hat {y} _ {i}\\right) \\tag {16}\n$$\n\nwhere  $N$  is the total number of training instances. The optimization process is to minimize the following objective function:\n\n$$\n\\mathcal {J} = \\mathcal {L} + \\lambda_ {*} | | \\boldsymbol {\\Theta} | | \\tag {17}\n$$\n\nwhere  $\\lambda_{*}$  denotes the regularization term and  $\\Theta$  denotes the set of parameters, including these in linear part, CIN part, and DNN part.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/198e1eda9e76c2a442acef4eb6059bb56c4f6d2994f37f2ea937fb0159aef6af.jpg)  \nFigure 5: The architecture of xDeepFM.\n\n3.3.1 Relationship with FM and DeepFM. Suppose all fields are univalent. It's not hard to observe from Figure 5 that, when the depth and feature maps of the CIN part are both set to 1, xDeepFM is a generalization of DeepFM by learning the linear regression weights for the FM layer (note that in DeepFM, units of FM layer are directly linked to the output unit without any coefficients). When we further remove the DNN part, and at the same time use a constant sum filter (which simply takes the sum of inputs without any parameter learning) for the feature map, then xDeepFM is downgraded to the traditional FM model.",
  "experiments": "# 4 EXPERIMENTS\n\nIn this section, we conduct extensive experiments to answer the following questions:\n\n- (Q1) How does our proposed CIN perform in high-order feature interactions learning?  \n- (Q2) Is it necessary to combine explicit and implicit high-order feature interactions for recommender systems?  \n- (Q3) How does the settings of networks influence the performance of xDeepFM?\n\nWe will answer these questions after presenting some fundamental experimental settings.\n\n# 4.1 Experiment Setup\n\n4.1.1 Datasets. We evaluate our proposed models on the following three datasets:\n\n1. Criteo Dataset. It is a famous industry benchmarking dataset for developing models predicting ad click-through rate, and is publicly accessible<sup>1</sup>. Given a user and the page he is visiting, the goal is to predict the probability that he will click on a given ad.\n\n2. Dianping Dataset. Dianping.com is the largest consumer review site in China. It provides diverse functions such as reviews, check-ins, and shops' meta information (including geographical messages and shop attributes). We collect 6 months' users check-in activities for restaurant recommendation experiments. Given a user's profile, a restaurant's attributes and the user's last three visited POIs (point of interest), we want to predict the probability that he will visit the restaurant. For each restaurant in a user's check-in instance, we sample four restaurants which are within 3 kilometers as negative instances by POI popularity.\n\n3. Bing News Dataset. Bing  $\\mathrm{News}^2$  is part of Microsoft's Bing search engine. In order to evaluate the performance of our model in a real commercial dataset, we collect five consecutive days' impression logs on news reading service. We use the first three days' data for training and validation, and the next two days for testing.\n\nFor the Criteo dataset and the Dianping dataset, we randomly split instances by 8:1:1 for training, validation and test. The characteristics of the three datasets are summarized in Table 1.\n\nTable 1: Statistics of the evaluation datasets. M indicates million and K indicates thousand.  \n\n<table><tr><td>Dataset</td><td>#instances</td><td>#fields</td><td>#features (sparse)</td></tr><tr><td>Criteo</td><td>45M</td><td>39</td><td>2.3M</td></tr><tr><td>Dianping</td><td>1.2M</td><td>18</td><td>230K</td></tr><tr><td>Bing News</td><td>5M</td><td>45</td><td>17K</td></tr></table>\n\n4.1.2 Evaluation Metrics. We use two metrics for model evaluation: AUC (Area Under the ROC curve) and Logloss (cross entropy). These two metrics evaluate the performance from two different angels: AUC measures the probability that a positive instance will be ranked higher than a randomly chosen negative one. It only takes into account the order of predicted instances and is insensitive to class imbalance problem. Logloss, in contrast, measures the\n\ndistance between the predicted score and the true label for each instance. Sometimes we rely more on Logloss because we need to use the predicted probability to estimate the benefit of a ranking strategy (which is usually adjusted as  $\\mathrm{CTR} \\times \\mathrm{bid}$ ).\n\n4.1.3 Baselines. We compare our xDeepFM with LR(logistic regression), FM, DNN (plain deep neural network), PNN (choose the better one from iPNN and oPNN) [31], Wide & Deep [5], DCN (Deep & Cross Network) [40] and DeepFM [9]. As introduced and discussed in Section 2, these models are highly related to our xDeepFM and some of them are state-of-the-art models for recommender systems. Note that the focus of this paper is to learn feature interactions automatically, so we do not include any hand-crafted cross features.  \n4.1.4 Reproducibility. We implement our method using Tensorflow<sup>3</sup>. Hyper-parameters of each model are tuned by grid-searching on the validation set, and the best settings for each model will be shown in corresponding sections. Learning rate is set to 0.001. For optimization method, we use the Adam [16] with a mini-batch size of 4096. We use a L2 regularization with  $\\lambda = 0.0001$  for DNN, DCN, Wide&Deep, DeepFM and xDeepFM, and use dropout 0.5 for PNN. The default setting for number of neurons per layer is: (1) 400 for DNN layers; (2) 200 for CIN layers on Criteo dataset, and 100 for CIN layers on Dianping and Bing News datasets. Since we focus on neural networks structures in this paper, we make the dimension of field embedding for all models be a fixed value of 10. We conduct experiments of different settings in parallel with 5 Tesla K80 GPUs. The source code is available at https://github.com/Leavingseason/xDeepFM.\n\nTable 2: Performance of individual models on the Criteo, Di-anping, and Bing News datasets. Column Depth indicates the best network depth for each model.  \n\n<table><tr><td>Model name</td><td>AUC</td><td>Logloss</td><td>Depth</td></tr><tr><td colspan=\"4\">Criteo</td></tr><tr><td>FM</td><td>0.7900</td><td>0.4592</td><td>-</td></tr><tr><td>DNN</td><td>0.7993</td><td>0.4491</td><td>2</td></tr><tr><td>CrossNet</td><td>0.7961</td><td>0.4508</td><td>3</td></tr><tr><td>CIN</td><td>0.8012</td><td>0.4493</td><td>3</td></tr><tr><td colspan=\"4\">Dianping</td></tr><tr><td>FM</td><td>0.8165</td><td>0.3558</td><td>-</td></tr><tr><td>DNN</td><td>0.8318</td><td>0.3382</td><td>3</td></tr><tr><td>CrossNet</td><td>0.8283</td><td>0.3404</td><td>2</td></tr><tr><td>CIN</td><td>0.8576</td><td>0.3225</td><td>2</td></tr><tr><td colspan=\"4\">Bing News</td></tr><tr><td>FM</td><td>0.8223</td><td>0.2779</td><td>-</td></tr><tr><td>DNN</td><td>0.8366</td><td>0.273</td><td>2</td></tr><tr><td>CrossNet</td><td>0.8304</td><td>0.2765</td><td>6</td></tr><tr><td>CIN</td><td>0.8377</td><td>0.2662</td><td>5</td></tr></table>\n\nTable 3: Overall performance of different models on Criteo, Dianping and Bing News datasets. The column Depth presents the best setting for network depth with a format of (cross layers, DNN layers).  \n\n<table><tr><td></td><td colspan=\"3\">Criteo</td><td colspan=\"3\">Dianping</td><td colspan=\"3\">Bing News</td></tr><tr><td>Model name</td><td>AUC</td><td>Logloss</td><td>Depth</td><td>AUC</td><td>Logloss</td><td>Depth</td><td>AUC</td><td>Logloss</td><td>Depth</td></tr><tr><td>LR</td><td>0.7577</td><td>0.4854</td><td>-, -</td><td>0.8018</td><td>0.3608</td><td>-, -</td><td>0.7988</td><td>0.2950</td><td>-, -</td></tr><tr><td>FM</td><td>0.7900</td><td>0.4592</td><td>-, -</td><td>0.8165</td><td>0.3558</td><td>-, -</td><td>0.8223</td><td>0.2779</td><td>-, -</td></tr><tr><td>DNN</td><td>0.7993</td><td>0.4491</td><td>-,2</td><td>0.8318</td><td>0.3382</td><td>-,3</td><td>0.8366</td><td>0.2730</td><td>-,2</td></tr><tr><td>DCN</td><td>0.8026</td><td>0.4467</td><td>2,2</td><td>0.8391</td><td>0.3379</td><td>4,3</td><td>0.8379</td><td>0.2677</td><td>2,2</td></tr><tr><td>Wide&amp;Deep</td><td>0.8000</td><td>0.4490</td><td>-,3</td><td>0.8361</td><td>0.3364</td><td>-,2</td><td>0.8377</td><td>0.2668</td><td>-,2</td></tr><tr><td>PNN</td><td>0.8038</td><td>0.4927</td><td>-,2</td><td>0.8445</td><td>0.3424</td><td>-,3</td><td>0.8321</td><td>0.2775</td><td>-,3</td></tr><tr><td>DeepFM</td><td>0.8025</td><td>0.4468</td><td>-,2</td><td>0.8481</td><td>0.3333</td><td>-,2</td><td>0.8376</td><td>0.2671</td><td>-,3</td></tr><tr><td>xDeepFM</td><td>0.8052</td><td>0.4418</td><td>3,2</td><td>0.8639</td><td>0.3156</td><td>3,3</td><td>0.8400</td><td>0.2649</td><td>3,2</td></tr></table>\n\n# 4.2 Performance Comparison among Individual Neural Components (Q1)\n\nWe want to know how CIN performs individually. Note that FM measures 2-order feature interactions explicitly, DNN model high-order feature interactions implicitly, CrossNet tries to model high-order feature interactions with a small number of parameters (which is proven not effective in Section 2.3), and CIN models high-order feature interactions explicitly. There is no theoretic guarantee of the superiority of one individual model over the others, due to that it really depends on the dataset. For example, if the practical dataset does not require high-order feature interactions, FM may be the best individual model. Thus we do not have any expectation for which model will perform the best in this experiment.\n\nTable 2 shows the results of individual models on the three practical datasets. Surprisingly, our CIN outperform the other models consistently. On one hand, the results indicate that for practical datasets, higher-order interactions over sparse features are necessary, and this can be verified through the fact that DNN, CrossNet and CIN outperform FM significantly on all the three datasets. On the other hand, CIN is the best individual model, which demonstrates the effectiveness of CIN on modeling explicit high-order feature interactions. Note that a  $k$ -layer CIN can model  $k$ -degree feature interactions. It is also interesting to see that it take 5 layers for CIN to yield the best result ON the Bing News dataset.\n\n# 4.3 Performance of Integrated Models (Q2)\n\nxDeepFM integrates CIN and DNN into an end-to-end model. While CIN and DNN covers two distinct properties in learning feature interactions, we are interested to know whether it is indeed necessary and effective to combine them together for jointly explicit and implicit learning. Here we compare several strong baselines which are not limited to individual models, and the results are shown in Table 3. We observe that LR is far worse than all the rest models, which demonstrates that factorization-based models are essential for measuring sparse features. Wide&Deep, DCN, DeepFM and xDeepFM are significantly better than DNN, which directly reflects that, despite their simplicity, incorporating hybrid components are important for boosting the accuracy of predictive systems. Our proposed xDeepFM achieves the best performance on all datasets, which demonstrates that combining explicit and implicit high-order\n\nfeature interaction is necessary, and xDeepFM is effective in learning this class of combination. Another interesting observation is that, all the neural-based models do not require a very deep network structure for the best performance. Typical settings for the depth hyper-parameter are 2 and 3, and the best depth setting for xDeepFM is 3, which indicates that the interactions we learned are at most 4-order.\n\n# 4.4 Hyper-Parameter Study (Q3)\n\nWe study the impact of hyper-parameters on xDeepFM in this section, including (1) the number of hidden layers; (2) the number of neurons per layer; and (3) activation functions. We conduct experiments via holding the best settings for the DNN part while varying the settings for the CIN part.\n\nDepth of Network. Figure 6a and 7a demonstrate the impact of number of hidden layers. We can observe that the performance of xDeepFM increases with the depth of network at the beginning. However, model performance degrades when the depth of network is set greater than 3. It is caused by overfitting evidenced by that we notice that the loss of training data still keeps decreasing when we add more hidden layers.\n\nNumber of Neurons per Layer. Adding the number of neurons per layer indicates increasing the number of feature maps in CIN. As shown in Figure 6b and 7b, model performance on Bing News dataset increases steadily when we increase the number of neurons from 20 to 200, while on Dianping dataset, 100 is a more suitable setting for the number of neurons per layer. In this experiment we fix the depth of network at 3.\n\nActivation Function. Note that we exploit the identity as activation function on neurons of CIN, as shown in Eq. 6. A common practice in deep learning literature is to employ non-linear activation functions on hidden neurons. We thus compare the results of different activation functions on CIN (for neurons in DNN, we keep the activation function with  $relu$ ). As shown in Figure 6c and 7c, identify function is indeed the most suitable one for neurons in CIN.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/38643cfb60ee38b08c9cab33c8f3be38b8ab1b4093fa11b0cc0c3698467dccae.jpg)  \n(a) Number of layers.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/856b4799237ff4d8d62626ac76236b485320a966949fd6df782bc8f2ab636c1a.jpg)  \n(b) Number of neurons per layer.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/20b3e4053f5c3c435857892c02778b7688aa6f02f7b9577e196db18117cfd89f.jpg)  \n(c) Activation functions\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/bcaf99a335e90360fe80f2b1ac6fdaf49d48d0d787bd10908bea68923c4de77f.jpg)  \n(a) Number of layers.  \nFigure 7: Impact of network hyper-parameters on Logloss performance.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/e1e962a5d654e2f83291bbcea5cf9c4e94df6f778942cbc16135854cf4ed682c.jpg)  \nFigure 6: Impact of network hyper-parameters on AUC performance.  \n(b) Number of neurons per layer.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-15/e2d0f4f7-331a-4d16-802e-4afdb0287b56/d621853cca70f939fee0ec60ab171fa64ffd2ed823dd5409de783b8b89696f5d.jpg)  \n(c) Activation functions",
  "hyperparameter": "Learning rate: 0.001; Optimizer: Adam; Mini-batch size: 4096; L2 regularization λ: 0.0001 (for DNN, DCN, Wide&Deep, DeepFM, xDeepFM); Dropout: 0.5 (for PNN); Field embedding dimension: 10; DNN neurons per layer: 400; CIN neurons per layer: 200 (Criteo dataset), 100 (Dianping and Bing News datasets); Best network depth: typically 2-3 layers for both CIN and DNN components (e.g., 3 CIN layers + 2 DNN layers for Criteo, 3+3 for Dianping, 3+2 for Bing News); Activation function: identity function for CIN neurons, ReLU for DNN neurons"
}