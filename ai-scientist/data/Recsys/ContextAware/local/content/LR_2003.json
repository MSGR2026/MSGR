{
  "id": "LR_2003",
  "paper_title": "Logistic Regression for Click-Through Rate Prediction",
  "alias": "LR",
  "year": 2003,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "Deep Interest Network (DIN) introduces a local activation unit with attention mechanism to replace traditional pooling for user behavior features in CTR prediction. Instead of compressing all historical behaviors into a fixed-length vector, DIN adaptively calculates activation weights between each behavior and the candidate ad, enabling the model to focus on relevant user interests. The attention weights are computed without softmax normalization to preserve interest intensity, and the model uses mini-batch aware regularization and Data Adaptive Activation Function (Dice) for efficient large-scale training.",
  "introduction": "# Introduction\nClick-Through Rate (CTR) prediction is critical for industrial applications like online advertising, as it directly impacts eCPM (effective cost per mille) and revenue. Deep learning-based models following the Embedding&MLP paradigm have become popular for CTR prediction, reducing manual feature engineering and enhancing model capability.\n\nHowever, these models face a key bottleneck: user interests are diverse, but they compress user behavior embeddings into a **fixed-length vector** (regardless of candidate ads). This limits the model’s expressive ability—expanding the vector dimension to capture diverse interests leads to excessive parameters, overfitting, and high computation/storage costs.\n\nMoreover, only part of a user’s interests influences their click decision for a specific ad (e.g., a swimmer’s click on goggles is driven by past bathing suit purchases, not shoes). Motivated by this, we propose the **Deep Interest Network (DIN)**, which introduces a local activation unit to adaptively learn user interest representations tailored to each candidate ad.\n\nAdditional challenges in training industrial deep networks (hundreds of millions of parameters, large-scale sparse features) are addressed with two techniques:\n1. **Mini-batch Aware Regularization**: Reduces computation by only regularizing parameters of features in the current mini-batch.\n2. **Data Adaptive Activation Function (Dice)**: Generalizes PReLU by adapting the rectified point to input data distribution.\n\nDIN has been successfully deployed in Alibaba’s online display advertising system, achieving significant business improvements. Contributions include:\n- Identifying fixed-length vector limitations and designing DIN to capture diverse user interests via adaptive activation.\n- Proposing two industrial-friendly training techniques for large-scale sparse feature networks.\n- Validating effectiveness on public datasets (Amazon, MovieLens) and Alibaba’s 2.14-billion-sample dataset.",
  "method": "# Method\n## 1. Feature Representation\nIndustrial CTR prediction data is multi-group categorical, encoded as high-dimensional sparse binary vectors (one-hot for single-valued features, multi-hot for user behaviors like visited item lists). An instance is represented as \\( x = [t_1^T, t_2^T, ..., t_M^T]^T \\), where \\( t_i \\) is the encoded vector of the i-th feature group, and \\( M \\) is the number of feature groups.\n\nKey feature groups (Table 1) include:\n- User Profile Features (gender, age_level): One-hot encoding.\n- User Behavior Features (visited_goods_ids, visited_cate_ids): Multi-hot encoding (core for capturing user interests).\n- Ad Features (goods_id, shop_id, cate_id): One-hot encoding.\n- Context Features (pid, time): One-hot encoding.\n\n## 2. Base Model (Embedding&MLP)\nThe baseline follows the standard Embedding&MLP paradigm:\n### 2.1 Embedding Layer\nMaps high-dimensional sparse vectors to low-dimensional dense embeddings. For feature group \\( i \\) with encoding vector \\( t_i \\):\n- If \\( t_i \\) is one-hot: Embedding is \\( e_i = w_j^i \\) (lookup \\( w_j^i \\) from embedding dictionary \\( W^i \\in \\mathbb{R}^{D×K_i} \\), \\( D \\) = embedding dimension, \\( K_i \\) = feature group size).\n- If \\( t_i \\) is multi-hot: Embedding is a list of vectors \\( \\{e_{i_1}, e_{i_2}, ..., e_{i_k}\\} \\) (lookup for each non-zero position).\n\n### 2.2 Pooling & Concat Layers\nMulti-hot behavior embeddings (variable length) are transformed to fixed-length vectors via sum/average pooling: \\( e_i = pooling(e_{i_1}, ..., e_{i_k}) \\). All group embeddings are concatenated to form the final instance representation.\n\n### 2.3 MLP & Loss\nThe concatenated vector is fed into a multi-layer perceptron (MLP) to learn non-linear feature interactions. The loss is negative log-likelihood:\n\\[ L = -\\frac{1}{N} \\sum_{(x,y) \\in S} (y \\log p(x) + (1-y) \\log (1-p(x))) \\]\nwhere \\( S \\) is the training set, \\( y \\in \\{0,1\\} \\) is the click label, and \\( p(x) \\) is the predicted CTR.\n\n## 3. Deep Interest Network (DIN)\nDIN modifies the base model by replacing the pooling layer for user behavior features with a **local activation unit**, enabling adaptive interest representation.\n\n### 3.1 Local Activation Unit\nFor a user with behavior embeddings \\( \\{e_1, e_2, ..., e_H\\} \\) and candidate ad embedding \\( v_A \\), the adaptive user interest representation is:\n\\[ v_U(A) = \\sum_{j=1}^H a(e_j, v_A) e_j = \\sum_{j=1}^H w_j e_j \\]\n- \\( a(e_j, v_A) \\): Activation weight (output of a feed-forward network), measuring relevance between behavior \\( j \\) and ad \\( A \\).\n- Input to \\( a(\\cdot) \\): \\( e_j \\), \\( v_A \\), and their outer product (explicit relevance signal).\n- No softmax normalization: Preserves interest intensity (sum of weights reflects activated interest strength).\n\n### 3.2 Network Architecture\nDIN retains the embedding, concat, and MLP layers of the base model—only the behavior feature pooling is replaced with the local activation unit. This ensures compatibility with industrial deployment while enhancing expressive ability.\n\n## 4. Training Techniques\n### 4.1 Mini-batch Aware Regularization\nAddresses the computational cost of traditional \\( \\ell_2 \\) regularization for large-scale parameters:\n- Approximates \\( \\ell_2 \\) regularization by only regularizing parameters of features present in the current mini-batch.\n- Gradient update for embedding weight \\( w_j \\):\n\\[ w_j \\leftarrow w_j - \\eta \\left[ \\frac{1}{|\\mathcal{B}_m|} \\sum_{(x,y) \\in \\mathcal{B}_m} \\frac{\\partial L}{\\partial w_j} + \\lambda \\frac{\\alpha_{mj}}{n_j} w_j \\right] \\]\n- \\( \\alpha_{mj} \\): 1 if feature \\( j \\) is in mini-batch \\( \\mathcal{B}_m \\), 0 otherwise; \\( n_j \\): feature \\( j \\)’s occurrence count in the training set.\n\n### 4.2 Data Adaptive Activation Function (Dice)\nGeneralizes PReLU by adapting to input distribution:\n\\[ f(s) = p(s) \\cdot s + (1-p(s)) \\cdot \\alpha s \\]\n\\[ p(s) = \\frac{1}{1 + e^{-\\frac{s-E[s]}{\\sqrt{Var[s]+\\epsilon}}}} \\]\n- \\( E[s] \\), \\( Var[s] \\): Mini-batch mean/variance of input \\( s \\) (moving average in testing).\n- Smoothly switches between two channels, adapting the rectified point to input data.",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nThree datasets (public + industrial) with user behavior data:\n| Dataset          | #Users    | #Goods/Movies | #Categories | #Samples       | Task                                  |\n|------------------|-----------|---------------|-------------|----------------|---------------------------------------|\n| Amazon (Electro) | 192,403   | 63,001        | 801         | 1,689,188      | Predict next reviewed product          |\n| MovieLens        | 138,493   | 27,278        | 21          | 20,000,263     | Predict high ratings (4-5 stars = 1)  |\n| Alibaba          | 60 million| 0.6 billion   | 100,000     | 2.14 billion   | Industrial CTR prediction (display ad) |\n\n### 1.2 Evaluation Metrics\n- **AUC**: User-weighted AUC (averages AUC per user, weighted by impressions) to reflect intra-user ranking quality.\n- **RelaImpr**: Relative improvement over BaseModel:\n\\[ RelaImpr = \\left( \\frac{AUC(\\text{model}) - 0.5}{AUC(\\text{BaseModel}) - 0.5} - 1 \\right) \\times 100\\% \\]\n\n### 1.3 Competitors\n- LR: Logistic regression (shallow baseline).\n- BaseModel: Embedding&MLP paradigm (strong baseline).\n- Wide&Deep: Combines \"wide\" (cross-product features) and \"deep\" (BaseModel) modules.\n- PNN: Adds product layer after embedding to capture high-order interactions.\n- DeepFM: Replaces Wide&Deep’s \"wide\" module with Factorization Machines (no feature engineering).\n\n### 1.4 Hyperparameters\n- Embedding dimension: 12 (Alibaba), adaptive (public datasets).\n- MLP layers: 192×200×80×2 (Alibaba), adaptive (public datasets).\n- Optimizer: SGD (public datasets), Adam (Alibaba).\n- Mini-batch size: 32 (public datasets), 5000 (Alibaba).\n\n## 2. Main Results\n### 2.1 Public Datasets (Amazon + MovieLens)\nTable 3 shows DIN outperforms all baselines, especially on Amazon (rich user behaviors):\n- DIN achieves 5.35% RelaImpr (Amazon) and 1.61% RelaImpr (MovieLens) over BaseModel.\n- DIN with Dice further improves performance (6.82% RelaImpr on Amazon), validating the data-adaptive activation.\n- DeepFM and PNN perform slightly better than Wide&Deep but are outperformed by DIN’s adaptive interest representation.\n\n### 2.2 Industrial Dataset (Alibaba)\n#### 2.2.1 Regularization Effectiveness\nTable 4 and Figure 4 show Mini-batch Aware (MBA) Regularization outperforms other methods (Dropout, Filter, DiFacto):\n- Without regularization, models overfit severely after the first epoch.\n- MBA Regularization achieves 9.68% RelaImpr, preserving fine-grained features (e.g., goods_ids) without losing information.\n\n#### 2.2.2 Model Comparison\nTable 5 confirms DIN’s superiority on industrial data:\n- DIN achieves 6.08% RelaImpr over BaseModel (absolute AUC gain 0.0059).\n- DIN + MBA Regularization + Dice achieves 11.65% RelaImpr (absolute AUC gain 0.0113), outperforming DeepFM (2.37% RelaImpr).\n\n### 2.3 Online A/B Testing\nDIN deployed in Alibaba’s advertising system achieves:\n- 10.0% CTR lift and 3.8% RPM (Revenue Per Mille) lift compared to BaseModel.\n- Meets industrial requirements: <10ms latency per request, 1M+ QPS per machine (after optimization).\n\n### 2.4 Visualization\n- **Activation Weight**: Behaviors relevant to the candidate ad get higher weights (Figure 5), verifying DIN’s adaptive focus.\n- **Embedding Clustering**: Goods of the same category form clusters (Figure 6), and DIN captures multimodal interest distributions for users with diverse interests.",
  "hyperparameter": "Embedding dimension: 12 (Alibaba dataset), adaptive for public datasets; MLP architecture: 192×200×80×2 (Alibaba), adaptive for public datasets; Optimizer: Adam (Alibaba), SGD (public datasets); Mini-batch size: 5000 (Alibaba), 32 (public datasets); Regularization parameter λ in mini-batch aware regularization; Dice activation parameter α (channel switching coefficient); Moving average coefficient for Dice statistics (mean and variance)"
}