{
  "id": "DCN_2017",
  "paper_title": "Deep & Cross Network for Ad Click Predictions",
  "alias": "DCN",
  "year": 2017,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "Deep & Cross Network (DCN) introduces a novel cross network that explicitly learns bounded-degree feature interactions in parallel with a deep network. The cross network uses a special layer structure (x_{l+1} = x_0 * x_l^T * w_l + b_l + x_l) that efficiently generates feature crosses with only O(d) parameters per layer while achieving polynomial degree up to l+1 for l layers, making it significantly more memory-efficient than DNNs. This design extends Factorization Machines to deeper structures with parameter sharing across multiple layers, enabling high-degree feature interactions crucial for CTR prediction.",
  "introduction": "# Introduction\nClick-through rate (CTR) prediction is a large-scale problem that is essential to multi-billion dollar online advertising industry. In the advertising industry, advertisers pay publishers to display their ads on publishers’ sites. One popular payment model is the cost-per-click (CPC) model, where advertisers are charged only when a click occurs. As a consequence, a publisher’s revenue relies heavily on the ability to predict CTR accurately.\n\nIdentifying frequently predictive features and at the same time exploring unseen or rare cross features is the key to making good predictions. However, data for Web-scale recommender systems is mostly discrete and categorical, leading to a large and sparse feature space that is challenging for feature exploration. This has limited most large-scale systems to linear models such as logistic regression.\n\nLinear models [3] are simple, interpretable and easy to scale; however, they are limited in their expressive power. Cross features, on the other hand, have been shown to be significant in improving the models’ expressiveness. Unfortunately, it often requires manual feature engineering or exhaustive search to identify such features; moreover, generalizing to unseen feature interactions is difficult.\n\nIn this paper, we aim to avoid task-specific feature engineering by introducing a novel neural network structure – a cross network – that explicitly applies feature crossing in an automatic fashion. The cross network consists of multiple layers, where the highest-degree of interactions are provably determined by layer depth. Each layer produces higher-order interactions based on existing ones, and keeps the interactions from previous layers. We train the cross network jointly with a deep neural network (DNN) [10, 14]. DNN has the promise to capture very complex interactions across features; however, compared to our cross network it requires nearly an order of magnitude more parameters, is unable to form cross features explicitly, and may fail to efficiently learn some types of feature interactions. Jointly training the cross and DNN components together, however, efficiently captures predictive feature interactions, and delivers state-of-the-art performance on the Criteo CTR dataset.\n\n## 1.1 Related Work\nDue to the dramatic increase in size and dimensionality of datasets, a number of methods have been proposed to avoid extensive task-specific feature engineering, mostly based on embedding techniques and neural networks.\n\nFactorization machines (FMs) [11, 12] project sparse features onto low-dimensional dense vectors and learn feature interactions from vector inner products. Field-aware factorization machines (FFMs) [7, 8] further allow each feature to learn several vectors where each vector is associated with a field. Regretably, the shallow structures of FMs and FFMs limit their representative power. There have been work extending FMs to higher orders [1, 18], but one downside lies in their large number of parameters which yields undesirable computational cost. Deep neural networks (DNN) are able to learn non-trivial high-degree feature interactions due to embedding vectors and nonlinear activation functions. The recent success of the Residual Network [5] has enabled training of very deep networks. Deep Crossing [15] extends residual networks and achieves automatic feature learning by stacking all types of inputs.\n\nThe remarkable success of deep learning has elicited theoretical analyses on its representative power. There has been research [16, 17] showing that DNNs are able to approximate an arbitrary function under certain smoothness assumptions to an arbitrary accuracy, given sufficiently many hidden units or hidden layers. Moreover, in practice, it has been found that DNNs work well with a feasible number of parameters. One key reason is that most functions of practical interest are not arbitrary.\n\nYet one remaining question is whether DNNs are indeed the most efficient ones in representing such functions of practical interest. In the Kaggle competition, the manually crafted features in many winning solutions are low-degree, in an explicit format and effective. The features learned by DNNs, on the other hand, are implicit and highly nonlinear. This has shed light on designing a model that is able to learn bounded-degree feature interactions more efficiently and explicitly than a universal DNN.\n\nThe wide-and-deep [4] is a model in this spirit. It takes cross features as inputs to a linear model, and jointly trains the linear model with a DNN model. However, the success of wide-and-deep hinges on a proper choice of cross features, an exponential problem for which there is yet no clear efficient method.\n\n## 1.2 Main Contributions\nIn this paper, we propose the Deep & Cross Network (DCN) model that enables Web-scale automatic feature learning with both sparse and dense inputs. DCN efficiently captures effective feature interactions of bounded degrees, learns highly nonlinear interactions, requires no manual feature engineering or exhaustive searching, and has low computational cost.\n\nThe main contributions of the paper include:\n1. We propose a novel cross network that explicitly applies feature crossing at each layer, efficiently learns predictive cross features of bounded degrees, and requires no manual feature engineering or exhaustive searching.\n2. The cross network is simple yet effective. By design, the highest polynomial degree increases at each layer and is determined by layer depth. The network consists of all the cross terms of degree up to the highest, with their coefficients all different.\n3. The cross network is memory efficient, and easy to implement. Our experimental results have demonstrated that with a cross network, DCN has lower logloss than a DNN with nearly an order of magnitude fewer number of parameters.",
  "method": "# Method\n# 2 DEEP & CROSS NETWORK (DCN)\nIn this section we describe the architecture of Deep & Cross Network (DCN) models. A DCN model starts with an embedding and stacking layer, followed by a cross network and a deep network in parallel. These in turn are followed by a final combination layer which combines the outputs from the two networks. The complete DCN model is depicted in Figure 1.\n\n## 2.1 Embedding and Stacking Layer\nWe consider input data with sparse and dense features. In Web-scale recommender systems such as CTR prediction, the inputs are mostly categorical features, e.g. \"country=usa\". Such features are often encoded as one-hot vectors e.g. \"[0,1,0]\"; however, this often leads to excessively high-dimensional feature spaces for large vocabularies.\n\nTo reduce the dimensionality, we employ an embedding procedure to transform these binary features into dense vectors of real values (commonly called embedding vectors):\n\\[x_{embed, i}=W_{embed, i} x_{i}, \\quad (1)\\]\nwhere \\(x_{embed, i}\\) is the embedding vector, \\(x_{i}\\) is the binary input in the i-th category, and \\(W_{embed, i} \\in \\mathbb{R}^{n_{e} ×n_{v}}\\) is the corresponding embedding matrix that will be optimized together with other parameters in the network, and \\(n_{e}\\), \\(n_{v}\\) are the embedding size and vocabulary size, respectively.\n\nIn the end, we stack the embedding vectors, along with the normalized dense features \\(x_{dense }\\), into one vector:\n\\[x_{0}=\\left[x_{embed, 1}^{T}, ..., x_{embed, k}^{T}, x_{dense }^{T}\\right], \\quad(2)\\]\nand feed \\(x_{0}\\) to the network.\n\n## 2.2 Cross Network\nThe key idea of our novel cross network is to apply explicit feature crossing in an efficient way. The cross network is composed of cross layers, with each layer having the following formula:\n\\[x_{l+1}=x_{0} x_{l}^{T} w_{l}+b_{l}+x_{l}=f\\left(x_{l}, w_{l}, b_{l}\\right)+x_{l}, (3)\\]\nwhere \\(x_{l}\\), \\(x_{l+1} \\in \\mathbb{R}^{d}\\) are column vectors denoting the outputs from the l-th and (l+1)-th cross layers, respectively; \\(w_{l}\\), \\(b_{l} \\in \\mathbb{R}^{d}\\) are the weight and bias parameters of the l-th layer. Each cross layer adds back its input after a feature crossing f, and the mapping function \\(f: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{d}\\) fits the residual of \\(x_{l+1}-x_{l}\\).\n\n### High-degree Interaction Across Features\nThe special structure of the cross network causes the degree of cross features to grow with layer depth. The highest polynomial degree (in terms of input \\(x_{0}\\)) for an l-layer cross network is \\(l+1\\). In fact, the cross network comprises all the cross terms \\(x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} ... x_{d}^{\\alpha_{d}}\\) of degree from 1 to \\(l+1\\).\n\n### Complexity Analysis\nLet \\(L_{c}\\) denote the number of cross layers, and d denote the input dimension. Then, the number of parameters involved in the cross network is:\n\\[d × L_{c} × 2.\\]\nThe time and space complexity of a cross network are linear in input dimension. Therefore, a cross network introduces negligible complexity compared to its deep counterpart, keeping the overall complexity for DCN at the same level as that of a traditional DNN. This efficiency benefits from the rank-one property of \\(x_{0} x_{l}^{T}\\), which enables us to generate all cross terms without computing or storing the entire matrix.\n\n## 2.3 Deep Network\nThe deep network is a fully-connected feed-forward neural network, with each deep layer having the following formula:\n\\[h_{l+1}=f\\left(W_{l} h_{l}+b_{l}\\right), (4)\\]\nwhere \\(h_{l} \\in \\mathbb{R}^{n_{l}}\\), \\(h_{l+1} \\in \\mathbb{R}^{n_{l+1}}\\) are the l-th and (l+1)-th hidden layer, respectively; \\(W_{l} \\in \\mathbb{R}^{n_{l+1} ×n_{l}}\\), \\(b_{l} \\in \\mathbb{R}^{n_{l+1}}\\) are parameters for the l-th deep layer; and \\(f(\\cdot)\\) is the ReLU function.\n\n### Complexity Analysis\nFor simplicity, we assume all the deep layers are of equal size. Let \\(L_{d}\\) denote the number of deep layers and m denote the deep layer size. Then, the number of parameters in the deep network is:\n\\[d × m+m+\\left(m^{2}+m\\right) ×\\left(L_{d}-1\\right) .\\]\n\n## 2.4 Combination Layer\nThe combination layer concatenates the outputs from two networks and feed the concatenated vector into a standard logits layer. The following is the formula for a two-class classification problem:\n\\[p=\\sigma\\left(\\left[x_{L_{1}}^{T}, h_{L_{2}}^{T}\\right] w_{logits }\\right), (5)\\]\nwhere \\(x_{L_{1}} \\in \\mathbb{R}^{d}\\), \\(h_{L_{2}} \\in \\mathbb{R}^{m}\\) are the outputs from the cross network and deep network, respectively, \\(w_{logits } \\in \\mathbb{R}^{(d+m)}\\) is the weight vector for the combination layer, and \\(\\sigma(x)=1 /(1+\\exp (-x))\\).\n\nThe loss function is the log loss along with a regularization term:\n\\[loss=-\\frac{1}{N} \\sum_{i=1}^{N} y_{i} log \\left(p_{i}\\right)+\\left(1-y_{i}\\right) log \\left(1-p_{i}\\right)+\\lambda \\sum_{l}\\left\\| w_{l}\\right\\| ^{2},(6)\\]\nwhere \\(p_{i}\\) are the probabilities computed from Equation 5, \\(y_{i}\\) are the true labels, N is the total number of inputs, and λ is the \\(L_{2}\\) regularization parameter.\n\nWe jointly train both networks, as this allows each individual network to be aware of the others during the training.\n\n## 3 CROSS NETWORK ANALYSIS\nIn this section, we analyze the cross network of DCN for the purpose of understanding its effectiveness. We offer three perspectives: polynomial approximation, generalization to FMs, and efficient projection. For simplicity, we assume \\(b_{i}=0\\).\n\n### Notations\nLet the i-th element in \\(w_{j}\\) be \\(w_{j}^{(i)}\\). For multi-index \\(\\alpha=[\\alpha_{1}, \\cdots, \\alpha_{d}] \\in \\mathbb{N}^{d}\\) and \\(x=[x_{1}, \\cdots, x_{d}] \\in \\mathbb{R}^{d}\\), we define \\(|\\alpha|=\\sum_{i=1}^{d} \\alpha_{i}\\).\n\n### Terminology\nThe degree of a cross term (monomial) \\(x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{d}^{\\alpha_{d}}\\) is defined by \\(|\\alpha|\\). The degree of a polynomial is defined by the highest degree of its terms.\n\n### 3.1 Polynomial Approximation\nBy the Weierstrass approximation theorem [13], any function under certain smoothness assumption can be approximated by a polynomial to an arbitrary accuracy. Therefore, we analyze the cross network from the perspective of polynomial approximation. In particular, the cross network approximates the polynomial class of the same degree in a way that is efficient, expressive and generalizes better to real-world datasets.\n\nWe study in detail the approximation of a cross network to the polynomial class of the same degree. Let us denote by \\(P_{n}(x)\\) the multivariate polynomial class of degree \\(n\\):\n\\[P_{n}(x)=\\left\\{\\sum_{\\alpha} w_{\\alpha} x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} ... x_{d}^{\\alpha_{d}} | 0 \\leq|\\alpha| \\leq n, \\alpha \\in \\mathbb{N}^{d}\\right\\} . (7)\\]\nEach polynomial in this class has \\(O(d^{n})\\) coefficients. We show that, with only \\(O(d)\\) parameters, the cross network contains all the cross terms occurring in the polynomial of the same degree, with each term’s coefficient distinct from each other.\n\n#### Theorem 3.1\nConsider an l-layer cross network with the (i+1)-th layer defined as \\(x_{i+1}=x_{0} x_{i}^{T} w_{i}+x_{i}\\). Let the input to the network be \\(x_{0}=[x_{1}, x_{2}, ..., x_{d}]^{T}\\), the output be \\(g_{l}(x_{0})=x_{l}^{T} w_{l}\\), and the parameters be \\(w_{i}\\), \\(b_{i} \\in \\mathbb{R}^{d}\\). Then, the multivariate polynomial \\(g_{l}(x_{0})\\) reproduces polynomials in the following class:\n\\[\\left\\{\\sum_{\\alpha} c_{\\alpha}\\left(w_{0}, ..., w_{l}\\right) x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} ... x_{d}^{\\alpha_{d}} | 0 \\leq|\\alpha| \\leq l+1, \\alpha \\in \\mathbb{N}^{d}\\right\\},\\]\nwhere \\(c_{\\alpha}=M_{\\alpha} \\sum_{i \\in B_{\\alpha}} \\sum_{j \\in P_{\\alpha}} \\prod_{k=1}^{|\\alpha|} w_{i_{k}}^{(j_{k})}\\), \\(M_{\\alpha}\\) is a constant independent of \\(w_{i}\\)’s, \\(i=[i_{1}, ..., i_{|\\alpha|}]\\) and \\(\\overrightarrow{j}=[j_{1}, ..., j_{|\\alpha|}]\\) are multi-indices, \\(B_{\\alpha}=\\{y \\in\\{0,1, \\cdots, l\\}^{|\\alpha|} | y_{i}<y_{j} \\Lambda y_{|\\alpha|}=l\\}\\), and \\(P_{\\alpha}\\) is α1 times αd times the set of all the permutations of the indices \\((1, \\cdots, 1 \\cdots d, \\cdots, d)\\).\n\n### 3.2 Generalization of FMs\nThe cross network shares the spirit of parameter sharing as the FM model and further extends it to a deeper structure.\n\nIn a FM model, feature \\(x_{i}\\) is associated with a weight vector \\(v_{i}\\), and the weight of cross term \\(x_{i} x_{j}\\) is computed by \\(<v_{i}, v_{j}>\\). In DCN, \\(x_{i}\\) is associated with scalars \\(\\{w_{k}^{(i)}\\}_{k=1}^{l}\\), and the weight of \\(x_{i} x_{j}\\) is the multiplications of parameters from the sets \\(\\{w_{k}^{(i)}\\}_{k=0}^{l}\\) and \\(\\{w_{k}^{(j)}\\}_{k=0}^{l}\\). Both models have each feature learned some parameters independent from other features, and the weight of a cross term is a certain combination of corresponding parameters.\n\nParameter sharing not only makes the model more efficient, but also enables the model to generalize to unseen feature interactions and be more robust to noise. The FM is a shallow structure and is limited to representing cross terms of degree 2. DCN, in contrast, is able to construct all the cross terms \\(x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} ... x_{d}^{\\alpha_{d}}\\) with degree \\(|\\alpha|\\) bounded by some constant determined by layer depth, as claimed in Theorem 3.1. Therefore, the cross network extends the idea of parameter sharing from a single layer to multiple layers and high-degree cross-terms. Note that different from the higher-order FMs, the number of parameters in a cross network only grows linearly with the input dimension.\n\n### 3.3 Efficient Projection\nEach cross layer projects all the pairwise interactions between \\(x_{0}\\) and \\(x_{l}\\), in an efficient manner, back to the input’s dimension.\n\nConsider \\(\\tilde{x} \\in \\mathbb{R}^{d}\\) as the input to a cross layer. The cross layer first implicitly constructs \\(d^{2}\\) pairwise interactions \\(x_{i} \\tilde{x}_{j}\\), and then implicitly projects them back to dimension d in a memory-efficient way. A direct approach, however, comes with a cubic cost.\n\nOur cross layer provides an efficient solution to reduce the cost to linear in dimension d. Consider \\(x_{p}=x_{0} \\tilde{x}^{T} w\\). This is in fact equivalent to:\n\\[x_{p}^{T}=\\left[\\begin{array}{llll} x_{1} \\overline{x}_{1} ... x_{1} \\overline{x}_{d} & ... & x_{d} \\overline{x}_{1} ... x_{d} \\overline{x}_{d} \\end{array}\\right]\\left[\\begin{array}{cccc} w & 0 & ... & 0 \\\\ | & & & \\\\ 0 & w & ... & 0 \\\\ & | & & \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & ... & w \\\\ 0 & 0 & ... & w \\\\ \\end{array}\\right]\\]\nwhere the row vector contains all \\(d^{2}\\) pairwise interactions \\(x_{i} \\bar{x}_{j}\\), the projection matrix has a block diagonal structure with \\(w \\in \\mathbb{R}^{d}\\) being a column vector.",
  "experiments": "# Experiment\n# 4 EXPERIMENTAL RESULTS\nIn this section, we evaluate the performance of DCN on some popular classification datasets.\n\n## 4.1 Criteo Display Ads Data\nThe Criteo Display Ads dataset is for the purpose of predicting ads click-through rate. It has 13 integer features and 26 categorical features where each category has a high cardinality. For this dataset, an improvement of 0.001 in logloss is considered as practically significant. When considering a large user base, a small improvement in prediction accuracy can potentially lead to a large increase in a company’s revenue. The data contains 11 GB user logs from a period of 7 days (∼41 million records). We used the data of the first 6 days for training, and randomly split day 7 data into validation and test sets of equal size.\n\n## 4.2 Implementation Details\nDCN is implemented on TensorFlow, we briefly discuss some implementation details for training with DCN.\n\n### Data processing and embedding\nReal-valued features are normalized by applying a log transform. For categorical features, we embed the features in dense vectors of dimension \\(6 ×( category cardinality )^{1 / 4}\\). Concatenating all embeddings results in a vector of dimension 1026.\n\n### Optimization\nWe applied mini-batch stochastic optimization with Adam optimizer [9]. The batch size is set at 512. Batch normalization [6] was applied to the deep network and gradient clip norm was set at 100.\n\n### Regularization\nWe used early stopping, as we did not find \\(L_{2}\\) regularization or dropout to be effective.\n\n### Hyperparameters\nWe report results based on a grid search over the number of hidden layers, hidden layer size, initial learning rate and number of cross layers. The number of hidden layers ranged from 2 to 5, with hidden layer sizes from 32 to 1024. For DCN, the number of cross layers is from 1 to 6. The initial learning rate was tuned from 0.0001 to 0.001 with increments of 0.0001. All experiments applied early stopping at training step 150,000, beyond which overfitting started to occur.\n\n## 4.3 Models for Comparisons\nWe compare DCN with five models: the DCN model with no cross network (DNN), logistic regression (LR), Factorization Machines (FMs), Wide and Deep Model (W&D), and Deep Crossing (DC).\n\n- **DNN**: The embedding layer, the output layer, and the hyperparameter tuning process are the same as DCN. The only change from the DCN model was that there are no cross layers.\n- **LR**: We used Sibyl [2]-a large-scale machine-learning system for distributed logistic regression. The integer features were discretized on a log scale. The cross features were selected by a sophisticated feature selection tool. All of the single features were used.\n- **FM**: We used an FM-based model with proprietary details.\n- **W&D**: Different than DCN, its wide component takes as input raw sparse features, and relies on exhaustive searching and domain knowledge to select predictive cross features. We skipped the comparison as no good method is known to select cross features.\n- **DC**: Compared to DCN, DC does not form explicit cross features. It mainly relies on stacking and residual units to create implicit crossings. We applied the same embedding (stacking) layer as DCN, followed by another ReLu layer to generate input to a sequence of residual units. The number of residual units was tuned form 1 to 5, with input dimension and cross dimension from 100 to 1026.\n\n## 4.4 Model Performance\nIn this section, we first list the best performance of different models in logloss, then we compare DCN with DNN in detail, that is, we investigate further into the effects introduced by the cross network.\n\n### Performance of different models\nThe best test logloss of different models are listed in Table 1. The optimal hyperparameter settings were 2 deep layers of size 1024 and 6 cross layers for the DCN model, 5 deep layers of size 1024 for the DNN, 5 residual units with input dimension 424 and cross dimension 537 for the DC, and 42 cross features for the LR model. That the best performance was found with the deepest cross architecture suggests that the higher-order feature interactions from the cross network are valuable. As we can see, DCN outperforms all the other models by a large amount. In particular, it outperforms the state-of-art DNN model but uses only 40% of the memory consumed in DNN.\n\nTable 1: Best test logloss from different models. “DC” is deep crossing, “DNN” is DCN with no cross layer, “FM” is Factorization Machine based model, “LR” is logistic regression.\n\n| Model | DCN | DC | DNN | FM | LR |\n|-------|-----|----|-----|----|----|\n| Logloss | 0.4419 | 0.4425 | 0.4428 | 0.4464 | 0.4474 |\n\nFor the optimal hyperparameter setting of each model, we also report the mean and standard deviation of the test logloss out of 10 independent runs: DCN: \\(0.4422 \\pm 9 ×10^{-5}\\), DNN: \\(0.4430 \\pm 3.7 × 10^{-4}\\), DC: \\(0.4430 \\pm 4.3 ×10^{-4}\\). As can be seen, DCN consistently outperforms other models by a large amount.\n\n### Comparisons Between DCN and DNN\nConsidering that the cross network only introduces \\(O(d)\\) extra parameters, we compare DCN to its deep network-a traditional DNN, and present the experimental results while varying memory budget and loss tolerance.\n\nIn the following, the loss for a certain number of parameters is reported as the best validation loss among all the learning rates and model structures. The number of parameters in the embedding layer was omitted in our calculation as it is identical to both models.\n\nTable 2 reports the minimal number of parameters needed to achieve a desired logloss threshold. From Table 2, we see that DCN is nearly an order of magnitude more memory efficient than a single DNN, thanks to the cross network which is able to learn bounded-degree feature interactions more efficiently.\n\nTable 2: #parameters needed to achieve a desired logloss.\n\n| Logloss | 0.4430 | 0.4460 | 0.4470 | 0.4480 |\n|---------|--------|--------|--------|--------|\n| DNN | 3.2 × 10⁶ | 1.5 × 10⁵ | 1.5 × 10⁵ | 7.8 × 10⁴ |\n| DCN | 7.9 × 10⁵ | 7.3 × 10⁴ | 3.7 × 10⁴ | 3.7 × 10⁴ |\n\nTable 3 compares performance of the neural models subject to fixed memory budgets. As we can see, DCN consistently outperforms DNN. In the small-parameter regime, the number of parameters in the cross network is comparable to that in the deep network, and the clear improvement indicates that the cross network is more efficient in learning effective feature interactions. In the large-parameter regime, the DNN closes some of the gap; however, DCN still outperforms DNN by a large amount, suggesting that it can efficiently learn some types of meaningful feature interactions that even a huge DNN model cannot.\n\nTable 3: Best logloss achieved with various memory budgets.\n\n| #Params | 5 × 10⁴ | 1 × 10⁵ | 4 × 10⁵ | 1.1 × 10⁶ | 2.5 × 10⁶ |\n|---------|----------|----------|----------|-----------|-----------|\n| DNN | 0.4480 | 0.4471 | 0.4439 | 0.4433 | 0.4431 |\n| DCN | 0.4465 | 0.4453 | 0.4432 | 0.4426 | 0.4423 |\n\nWe analyze DCN in finer detail by illustrating the effect from introducing a cross network to a given DNN model. We first compare the best performance of DNN with that of DCN under the same number of layers and layer size, and then for each setting, we show how the validation logloss changes as more cross layers are added. Table 4 shows the differences between the DCN and DNN model in logloss. Under the same experimental setting, the best logloss from the DCN model consistently outperforms that from a single DNN model of the same structure. That the improvement is consistent for all the hyperparameters has mitigated the randomness effect from the initialization and stochastic optimization.\n\nFigure 3 shows the improvement as we increase the number of cross layers on randomly selected settings. For the deep networks in Figure 3, there is a clear improvement when 1 cross layer is added to the model. As more cross layers are introduced, for some settings the logloss continues to decrease, indicating the introduced cross terms are effective in the prediction; whereas for others the logloss starts to fluctuate and even slightly increase, which indicates the higher-degree feature interactions introduced are not helpful.\n\nTable 4: Differences in the validation logloss (×10⁻²) between DCN and DNN. The DNN model is the DCN model with the number of cross layers set to 0. Negative values mean that the DCN outperforms DNN.\n\n| #Layers #Nodes | 32 | 64 | 128 | 256 | 512 | 1024 |\n|----------------|----|----|-----|-----|-----|------|\n| 2 | -0.28 | -0.10 | -0.16 | -0.06 | -0.05 | -0.08 |\n| 3 | -0.19 | -0.10 | -0.13 | -0.18 | -0.07 | -0.05 |\n| 4 | -0.12 | -0.10 | -0.06 | -0.09 | -0.09 | -0.21 |\n| 5 | -0.21 | -0.11 | -0.13 | -0.00 | -0.06 | -0.02 |\n\n## 4.5 Non-CTR datasets\nWe show that DCN performs well on non-CTR prediction problems. We used the forest covertype (581012 samples and 54 features) and Higgs (11M samples and 28 features) datasets from the UCI repository. The datasets were randomly split into training (90%) and testing (10%) set. A grid search over the hyperparameters was performed. The number of deep layers ranged from 1 to 10 with layer size from 50 to 300. The number of cross layers ranged from 4 to 10. The number of residual units ranged from 1 to 5 with their input dimension and cross dimension from 50 to 300. For DCN, the input vector was fed to the cross network directly.\n\nFor the forest covertype data, DCN achieved the best test accuracy 0.9740 with the least memory consumption. Both DNN and DC achieved 0.9737. The optimal hyperparameter settings were 8 cross layers of size 54 and 6 deep layers of size 292 for DCN, 7 deep layers of size 292 for DNN, and 4 residual units with input dimension 271 and cross dimension 287 for DC.\n\nFor the Higgs data, DCN achieved the best test logloss 0.4494, whereas DNN achieved 0.4506. The optimal hyperparameter settings were 4 cross layers of size 28 and 4 deep layers of size 209 for DCN, and 10 deep layers of size 196 for DNN. DCN outperforms DNN with half of the memory used in DNN.",
  "hyperparameter": "Embedding dimension: 6 × (category_cardinality)^(1/4), resulting in concatenated dimension of 1026; Batch size: 512; Optimizer: Adam; Initial learning rate: tuned from 0.0001 to 0.001 with 0.0001 increments; Number of hidden layers: 2-5 (optimal: 2 layers of size 1024); Hidden layer size: 32-1024; Number of cross layers: 1-6 (optimal: 6 layers); Gradient clip norm: 100; Early stopping: at 150,000 training steps; Regularization: early stopping only (L2 and dropout not effective); Activation function: ReLU for deep network; Batch normalization: applied to deep network"
}