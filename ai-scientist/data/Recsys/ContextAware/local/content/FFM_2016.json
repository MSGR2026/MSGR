{
  "id": "FFM_2016",
  "paper_title": "Field-aware Factorization Machines for CTR Prediction",
  "alias": "FFM",
  "year": 2016,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "Field-aware Factorization Machines (FFM) extends Factorization Machines by utilizing field information in CTR prediction. Instead of learning one latent vector per feature as in FM, FFM learns multiple latent vectors per feature—one for each field. When computing feature interactions, FFM selects the appropriate latent vector based on the field of the interacting feature, allowing the model to capture field-specific latent effects (e.g., w_{j1,f2} · w_{j2,f1} where f1 and f2 are fields). This results in more expressive modeling of feature interactions at the cost of increased parameters (nfk vs nk in FM) and complexity.",
  "introduction": "# INTRODUCTION\nClick-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.\n\nLogistic regression is probably the most widely used model for CTR prediction. Given a data set with m instances ((yᵢ, xᵢ), i=1, ..., m, where yᵢ is the label and xᵢ is an n-dimensional feature vector, the model w is obtained by solving a specific optimization problem. Learning the effect of feature conjunctions seems to be crucial for CTR prediction. We consider an artificial data set to better understand feature conjunctions: an ad from Gucci has a particularly high CTR on Vogue, which is difficult for linear models to learn as they learn the weights of Gucci and Vogue separately.\n\nTo address this problem, two models have been used to learn the effect of feature conjunction. The first model, degree-2 polynomial mappings (Poly2), learns a dedicated weight for each feature conjunction. The second model, factorization machines (FMs), learns the effect of feature conjunction by factorizing it into a product of two latent vectors.\n\nA variant of FM called pairwise interaction tensor factorization (PITF) was proposed for personalized tag recommendation. In KDD Cup 2012, a generalization of PITF called “factor model” was proposed by “Team Opera Solutions”. Because this term is too general and may easily be confused with factorization machines, we refer to it as “field-aware factorization machines” (FFMs) in this paper. The difference between PITF and FFM is that PITF considers three special fields including “user,”“item,” and “tag,” while FFM is more general. The limited discussion of FFM in the related work shows two key results: they use stochastic gradient method (SG) to solve the optimization problem with only one epoch to avoid over-fitting, and FFM performs the best among six models they tried.\n\nIn this paper, we aim to concretely establish FFM as an effective approach for CTR prediction. Our major results are as follows:\n1. We present the use of FFM as our major model to win two world-wide CTR competitions hosted by Criteo and Avazu, further demonstrating its effectiveness on CTR prediction.\n2. We compare FFMs with two related models, Poly2 and FMs, discussing conceptually why FFMs might be better and conducting experiments on accuracy and training time.\n3. We present techniques for training FFMs, including an effective parallel optimization algorithm and the use of early-stopping to avoid over-fitting.\n4. To make FFMs available for public use, we release an open source software.\n\nThis paper is organized as follows. Before presenting FFMs and its implementation in Section 3, we discuss the two existing models Poly2 and FMs in Section 2. Experiments comparing FFMs with other models are in Section 4. Finally, conclusions and future directions are in Section 5.\n\nCode used for experiments in this paper and the package LIBFFM are respectively available at:\nhttp://www.csie.ntu.edu.tw/˜cjlin/ffm/exps\nhttp://www.csie.ntu.edu.tw/˜cjlin/libffm",
  "method": "# FFM (Field-aware Factorization Machines)\n## 3.1 Core Concept of FFM\nThe idea of FFM originates from PITF proposed for recommender systems with personalized tags. In PITF, three available fields (User, Item, Tag) are assumed, and (User, Item), (User, Tag), and (Item, Tag) are factorized in separate latent spaces. FFM generalizes PITF for more fields (e.g., AdID, AdvertiserID, UserID, QueryID) and is effectively applied on CTR prediction.\n\nFor most CTR data sets, “features” can be grouped into “fields.” For example, features ESPN, Vogue, and NBC belong to the field Publisher, while Nike, Gucci, and Adidas belong to the field Advertiser. FFM is a variant of FM that utilizes this field information.\n\nIn FMs, every feature has only one latent vector to learn the latent effect with any other features. However, because different features may belong to different fields, the latent effects between feature pairs from different field combinations may be different. In FFMs, each feature has several latent vectors. Depending on the field of other features, one of them is used to compute the inner product.\n\nMathematically, the model of FFM is expressed as:\n\\[\n\\phi_{FFM}(w, x)=\\sum_{j_{1}=1}^{n} \\sum_{j_{2}=j_{1}+1}^{n}\\left(w_{j_{1}, f_{2}} \\cdot w_{j_{2}, f_{1}}\\right) x_{j_{1}} x_{j_{2}}\n\\]\nwhere \\(f_{1}\\) and \\(f_{2}\\) are respectively the fields of \\(j_{1}\\) and \\(j_{2}\\). If \\(f\\) is the number of fields, then the number of variables of FFMs is \\(n f k\\), and the complexity to compute the above formula is \\(O(\\bar{n}^{2} k)\\) (where \\(\\bar{n}\\) is the average number of non-zero elements per instance, \\(k\\) is the number of latent factors). It is worth noting that in FFMs, because each latent vector only needs to learn the effect with a specific field, usually \\(k_{FFM} \\ll k_{FM}\\).\n\n## 3.2 Solving the Optimization Problem\nThe optimization problem for FFM is the same as that for logistic regression, except that \\(\\phi_{LM}(w, x)\\) is replaced by \\(\\phi_{FFM}(w, x)\\). We use stochastic gradient methods (SG) for optimization, and adopt AdaGrad as the adaptive learning-rate schedule due to its effectiveness on matrix factorization (a special case of FFMs).\n\n### 3.2.1 Sub-gradient Calculation\nAt each step of SG, a data point (y, x) is sampled for updating \\(w_{j_{1}, f_{2}}\\) and \\(w_{j_{2}, f_{1}}\\) in the FFM model. Since x is highly sparse in CTR prediction applications, we only update dimensions with non-zero values. The sub-gradients are:\n\\[\ng_{j_{1},f_{2}}\\equiv \\nabla _{w_{j_{1},f_{2}}}f(w)=\\lambda \\cdot w_{j_{1},f_{2}}+\\kappa \\cdot w_{j_{2},f_{1}}x_{j_{1}}x_{j_{2}}\n\\]\n\\[\ng_{j_{2},f_{1}}\\equiv \\nabla _{w_{j_{2},f_{1}}}f(w)=\\lambda \\cdot w_{j_{2},f_{1}}+\\kappa \\cdot w_{j_{1},f_{2}}x_{j_{1}}x_{j_{2}}\n\\]\nwhere \\(\\kappa =\\frac {-y}{1+exp (y\\phi _{FFM}(w,x))}\\), and \\(\\lambda\\) is the regularization parameter.\n\n### 3.2.2 Model Update\nFirst, for each coordinate \\(d=1, ..., k\\), the sum of squared gradient is accumulated:\n\\[\n\\left(G_{j_{1}, f_{2}}\\right)_{d} \\leftarrow\\left(G_{j_{1}, f_{2}}\\right)_{d}+\\left(g_{j_{1}, f_{2}}\\right)_{d}^{2}\n\\]\n\\[\n\\left(G_{j_{2}, f_{1}}\\right)_{d} \\leftarrow\\left(G_{j_{2}, f_{1}}\\right)_{d}+\\left(g_{j_{2}, f_{1}}\\right)_{d}^{2}\n\\]\nThen, \\((w_{j_{1}, f_{2}})_{d}\\) and \\((w_{j_{2}, f_{1}})_{d}\\) are updated by:\n\\[\n\\left(w_{j_{1}, f_{2}}\\right)_{d} \\leftarrow\\left(w_{j_{1}, f_{2}}\\right)_{d}-\\frac{\\eta}{\\sqrt{\\left(G_{j_{1}, f_{2}}\\right)_{d}}}\\left(g_{j_{1}, f_{2}}\\right)_{d}\n\\]\n\\[\n\\left(w_{j_{2}, f_{1}}\\right)_{d} \\leftarrow\\left(w_{j_{2}, f_{1}}\\right)_{d}-\\frac{\\eta}{\\sqrt{\\left(G_{j_{2}, f_{1}}\\right)_{d}}}\\left(g_{j_{2}, f_{1}}\\right)_{d}\n\\]\nwhere \\(\\eta\\) is a user-specified learning rate. The initial values of \\(w\\) are randomly sampled from a uniform distribution between \\([0,1 / \\sqrt{k}]\\). The initial values of \\(G\\) are set to one to prevent a large value of \\((G_{j_{1}, f_{2}})_{d}^{-\\frac{1}{2}}\\).\n\n### 3.2.3 Algorithm Overview\nAlgorithm 1 Training FFM using SG\n1: Let \\(G \\in R^{n×f×k}\\) be a tensor of all ones\n2: Run the following loop for \\(t\\) epochs\n3: for \\(i \\in\\{1, · · · , m\\}\\) do\n4: Sample a data point (y, x)\n5: Calculate \\(\\kappa\\)\n6: for \\(j1 \\in\\) non-zero terms in \\(\\{1, · · · , n\\}\\) do\n7: for \\(j2 \\in\\) non-zero terms in \\(\\{j1 + 1, · · · , n\\}\\) do\n8: Calculate sub-gradient by the above sub-gradient formulas\n9: for \\(d \\in\\{1, · · · , k\\}\\) do\n10: Update the gradient sum by the above gradient sum accumulation formulas\n11: Update model by the above model update formulas\n\nEmpirically, normalizing each instance to have unit length makes the test accuracy slightly better and insensitive to parameters.\n\n## 3.3 Parallelization on Shared-memory Systems\nModern computers are widely equipped with multi-core CPUs. Fully utilizing these cores can significantly reduce training time. We apply Hogwild!, which allows each thread to run independently without any locking. Specifically, the for loop at line 3 of Algorithm 1 is parallelized.\n\n## 3.4 Adding Field Information\nWe extend the widely used LIBSVM data format to include field information:\nlabel field1:feat1:val1 field2:feat2:val2 · · ·\nThat is, each feature must be assigned a corresponding field. We discuss the field assignment for three typical classes of features:\n\n### 3.4.1 Categorical Features\nFor linear models, a categorical feature is commonly transformed to several binary features. For a data instance “Yes P:ESPN A:Nike G:Male”, the LIBSVM format is “Yes P-ESPN:1 A-Nike:1 G-Male:1”. To add field information, each category can be considered as a field, resulting in “Yes P:P-ESPN:1 A:A-Nike:1 G:G-Male:1”.\n\n### 3.4.2 Numerical Features\nThere are two possible ways to assign fields:\n1. Treat each feature as a dummy field. For example, the instance with numerical features “AR:45.73 Hidx:2 Cite:3” becomes “Yes AR:AR:45.73 Hidx:Hidx:2 Cite:Cite:3”. However, dummy fields may not be informative as they are merely duplicates of features.\n2. Discretize each numerical feature to a categorical one, then use the same setting for categorical features to add field information. For example, “AR:45.73” is rounded to “AR:45:1”. The main drawback is that determining the best discretization setting is usually not easy, and information may be lost after discretization.\n\n### 3.4.3 Single-field Features\nOn some data sets (e.g., NLP data sets), all features belong to a single field, making it meaningless to assign fields. If we assign this single field to all words, FFMs is reduced to FMs. Using dummy fields is impractical because \\(f=n\\) and \\(n\\) is often huge.",
  "experiments": "# EXPERIMENTS\nIn this section, we first provide details about the experimental setting. Then, we investigate the impact of parameters. We find that unlike LM or Poly2, FFM is sensitive to the number of epochs, so we discuss this issue in detail before proposing an early stopping trick. The speedup of parallelization is studied next. After checking various properties of FFMs, we compare FFMs with other models including Poly2 and FMs. We also include state-of-the-art packages LIBLINEAR and LIBFM for training LM/Poly2 and FMs, respectively, for fair comparison.\n\n## 4.1 Experiment Settings\n### 4.1.1 Data Sets\nWe mainly consider two CTR data sets (Criteo and Avazu) from Kaggle competitions, and more data sets are included in Section 4.6. For feature engineering, we apply our winning solution but remove complicated components. A hashing trick is applied to generate \\(10^6\\) features. The statistics of the two main data sets are:\n\n| Data Set | # instances | # features | # fields |\n| --- | --- | --- | --- |\n| Criteo | 45,840,617 | \\(10^7\\) | 39 |\n| Avazu | 40,428,967 | \\(10^7\\) | 33 |\n\nFor both data sets, the labels in the test sets are not publicly available, so we split the available data into training and validation sets:\n- Criteo: The last 6,040,618 lines are used as the validation set.\n- Avazu: The last 4,218,938 lines are selected as the validation set.\n\nWe use the following terms to represent different sets of a problem:\n- Va: The validation set mentioned above.\n- Tr: The new training set after excluding the validation set from the original training data.\n- TrVa: The original training set.\n- Te: The original test set (labels not released; split into “public set” and “private set” by competition organizers).\n\n### 4.1.2 Platform\nAll experiments are conducted on a Linux workstation with 12 physical cores on two Intel Xeon E5-2620 2.0GHz processors and 128 GB memory.\n\n### 4.1.3 Evaluation\nWe use logistic loss as the evaluation criterion:\n\\[\nlogloss =\\frac{1}{m} \\sum_{i=1}^{m} log \\left(1+exp \\left(-y_{i} \\phi\\left(w, x_{i}\\right)\\right)\\right)\n\\]\nwhere \\(m\\) is the number of test instances, and \\(\\phi(w, x)\\) is replaced by \\(\\phi_{LM}(w, x)\\), \\(\\phi_{Poly2}(w, x)\\), \\(\\phi_{FM}(w, x)\\), or \\(\\phi_{FFM}(w, x)\\) depending on the model.\n\n### 4.1.4 Implementation\nWe implement LMs, Poly2, FMs, and FFMs all in C++. For FMs and FFMs, we use SSE instructions to boost the efficiency of inner products. The parallelization discussed in Section 3.2 is implemented by OpenMP. Our implementations include linear terms and bias term as they improve performance in some data sets. For code extensibility, the field information is stored regardless of the model used.\n\n## 4.2 Impact of Parameters\nWe conduct experiments to investigate the impact of \\(k\\) (number of latent factors), \\(\\lambda\\) (regularization parameter), and \\(\\eta\\) (learning rate). The results are shown in Figure 1.\n\n- Regarding \\(k\\): Results show that it does not affect the logloss much. The average running time per epoch and the best logloss with different \\(k\\) values are shown in the following table:\n\n| k | time (seconds) | logloss |\n| --- | --- | --- |\n| 1 | 27.236 | 0.45773 |\n| 2 | 26.384 | 0.45715 |\n| 4 | 27.875 | 0.45696 |\n| 8 | 40.331 | 0.45690 |\n| 16 | 70.164 | 0.45725 |\n\n- Regarding \\(\\lambda\\): If \\(\\lambda\\) is too large, the model cannot achieve good performance. With a small \\(\\lambda\\), the model gets better results but easily overfits the data. The training logloss keeps decreasing as \\(\\lambda\\) decreases.\n- Regarding \\(\\eta\\): A small \\(\\eta\\) makes FFMs obtain the best performance slowly. A large \\(\\eta\\) allows FFMs to quickly reduce the logloss but leads to over-fitting.\n\nFrom the results, early-stopping is needed to address over-fitting.\n\n## 4.3 Early Stopping\nEarly stopping terminates the training process before reaching the best result on training data to avoid over-fitting. For FFM, the strategy is:\n1. Split the data set into a training set and a validation set.\n2. At the end of each epoch, use the validation set to calculate the loss.\n3. If the loss goes up, record the number of epochs. Stop or proceed to step 4.\n4. If needed, use the full data set to re-train a model with the number of epochs obtained in step 3.\n\nA difficulty in applying early stopping is that the logloss is sensitive to the number of epochs, so the best epoch on the validation set may not be the best one on the test set. Other approaches to avoid over-fitting (e.g., lazy update, ALS-based optimization methods) are not as successful as early stopping using a validation set.\n\n## 4.4 Speedup\nThe parallelization of SG may cause different convergence behavior. We experiment with different numbers of threads and find that our parallelization still leads to similar convergence behavior. We define speedup as:\n\\[\n\\frac{ Running time of one epoch with a single thread }{ Running time of one epoch with multiple threads }\n\\]\n\nResults show a good speedup when the number of threads is small. However, the speedup does not improve much with many threads because memory access conflicts occur more frequently.\n\n## 4.5 Comparison with LMs, Poly2, and FMs on Two CTR Competition Data Sets\nWe implement the same SG method for LMs, Poly2, FMs, and FFMs, and compare with two state-of-the-art packages:\n- LIBLINEAR: A widely used package for linear models. We modify its Poly2 extension to support the hashing trick (denoted as LIBLINEAR-Hash).\n- LIBFM: A widely used library for factorization machines. We use ALS (the best-performing optimization approach among SG, ALS, and MCMC).\n\nFor parameter selection, we select those leading to the best performance on the validation sets from a grid of points. Results on Criteo and Avazu are shown in the following tables.\n\n### Criteo Data Set Results\n| Model and implementation | parameters | training time (seconds) | public set logloss | public set rank | private set logloss | private set rank |\n| --- | --- | --- | --- | --- | --- | --- |\n| LM-SG | \\(\\eta=0.2, \\lambda=0, t=13\\) | 527 | 0.46262 | 93 | 0.46224 | 91 |\n| LM-LIBLINEAR-CD | \\(s=7, c=2\\) | 1,417 | 0.46239 | 91 | 0.46201 | 89 |\n| LM-LIBLINEAR-Newton | \\(s=0, c=2\\) | 7,164 | 0.46602 | 225 | 0.46581 | 222 |\n| Poly2-SG | \\(\\eta=0.2, \\lambda=0, B=10^{7}, t=10\\) | 12,064 | 0.44973 | 14 | 0.44956 | 14 |\n| Poly2-LIBLINEAR-Hash-CD | \\(s=7, c=2\\) | 24,771 | - | 13 | 0.44873 | 13 |\n| FM | \\(\\eta=0.05, \\lambda=2 ×10^{-5}, k=40, t=8\\) | 2,022 | 0.44930 | 14 | 0.44922 | 14 |\n| FM | \\(\\eta=0.05, \\lambda=2 ×10^{-5}, k=100, t=9\\) | 4,020 | 0.44867 | 11 | 0.44847 | 11 |\n| LIBFM | \\(\\lambda=40, k=40, t=20\\) | 23,700 | 0.45012 | 14 | 0.45000 | 15 |\n| LIBFM | \\(\\lambda=40, k=40, t=50\\) | 131,000 | 0.44904 | 14 | 0.44887 | 14 |\n| LIBFM | \\(\\lambda=40, k=100, t=20\\) | 54,320 | 0.44853 | 11 | 0.44834 | 11 |\n| LIBFM | \\(\\lambda=40, k=100, t=50\\) | 398,800 | 0.44794 | 9 | 0.44778 | 8 |\n| FFM | \\(\\eta=0.2, \\lambda=2 ×10^{-5}, k=4, t=9\\) | 6,587 | 0.44612 | 3 | 0.44603 | 3 |\n\n### Avazu Data Set Results\n| Model and implementation | parameters | training time (seconds) | public set logloss | public set rank | private set logloss | private set rank |\n| --- | --- | --- | --- | --- | --- | --- |\n| LM-SG | \\(\\eta=0.2, \\lambda=0, t=10\\) | 164 | 0.39018 | 57 | 0.38833 | 64 |\n| LM-LIBLINEAR-CD | \\(s=7, c=1\\) | 417 | 0.39131 | 115 | 0.38944 | 119 |\n| LM-LIBLINEAR-Newton | \\(s=0, c=1\\) | 650 | 0.39269 | 182 | 0.39079 | 183 |\n| Poly2-SG | \\(\\eta=0.2, \\lambda=0, B=10^{7}, t=10\\) | 911 | 0.38554 | 10 | 0.38347 | 10 |\n| Poly2-LIBLINEAR-Hash-CD | \\(s=7, c=1\\) | 1,756 | 0.38516 | 10 | 0.38303 | 9 |\n| Poly2-LIBLINEAR-Hash-Newton | \\(s=0, c=1\\) | 27,292 | 0.38598 | 11 | 0.38393 | 11 |\n| FM | \\(\\eta=0.05, \\lambda=2 ×10^{-5}, k=40, t=8\\) | 574 | 0.38621 | 11 | 0.38407 | 11 |\n| FM | \\(\\eta=0.05, \\lambda=2 ×10^{-5}, k=100, t=9\\) | 1,277 | 0.38740 | 17 | 0.38531 | 15 |\n| LIBFM | \\(\\lambda=40, k=40, t=20\\) | 18,712 | 0.39137 | 122 | 0.38963 | 127 |\n| LIBFM | \\(\\lambda=40, k=40, t=50\\) | 41,720 | 0.39786 | 935 | 0.39635 | 943 |\n| LIBFM | \\(\\lambda=40, k=100, t=20\\) | 39,719 | 0.39644 | 747 | 0.39470 | 755 |\n| LIBFM | \\(\\lambda=40, k=100, t=50\\) | 91,210 | 0.40740 | 1129 | 0.40585 | 1126 |\n| FFM | \\(\\eta=0.2, \\lambda=2 ×10^{-5}, k=4, t=4\\) | 340 | 0.38411 | 6 | 0.38223 | 6 |\n\nKey findings:\n- FFMs outperform other models in terms of logloss but require longer training time than LMs and FMs.\n- LMs have worse logloss but are significantly faster.\n- Poly2 is the slowest among all models, possibly due to the expensive computation of its model.\n- FM is a good balance between logloss and speed.\n- For LIBFM, its performance is close to our implementation of FMs in logloss on Criteo, but our implementation is significantly faster.\n\n## 4.6 Comparison on More Data Sets\nWe consider more data sets (most not CTR data) for comparison, excluding those with single-field features. The data sets are:\n- KDD2010-bridge: Includes both numerical and categorical features.\n- KDD2012: Contains both numerical and categorical features (target value transformed to binary “clicked or not”).\n- cod-rna: Contains only numerical features.\n- ijcnn: Contains only numerical features.\n- phishing: Contains only categorical features.\n- adult: Includes both numerical and categorical features.\n\nFor data preprocessing:\n- For KDD2010-bridge, KDD2012, and adult, all numerical features are discretized into 29, 13, and 94 bins respectively.\n- For cod-rna and ijcnn, two approaches are used to obtain field information: applying dummy fields and discretization.\n\nParameter selection follows the same procedure as Section 4.5. The statistics and experimental results are shown in the following table:\n\n| Data set | # instances | # features | # fields | LM logloss | Poly2 logloss | FM logloss | FFM logloss |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| KDD2010-bridge | 20,012,499 | 651,166 | 9 | 0.27947 | 0.26220 | 0.26372 | 0.25639 |\n| KDD2012 | 149,639,105 | 54,686,452 | 11 | 0.15069 | 0.15099 | 0.15004 | 0.14906 |\n| phishing | 11,055 | 100 | 30 | 0.14211 | 0.11512 | 0.09229 | 0.10650 |\n| adult | 48,842 | 308 | 14 | 0.30970 | 0.30655 | 0.30763 | 0.30565 |\n| cod-rna (dummy fields) | 331,152 | 8 | 8 | 0.13829 | 0.12874 | 0.12580 | 0.12914 |\n| cod-rna (discretization) | 331,152 | 2,296 | 8 | 0.16455 | 0.17576 | 0.16570 | 0.14993 |\n| ijcnn (dummy fields) | 141,691 | 22 | 22 | 0.20093 | 0.08981 | 0.07087 | 0.06920 |\n| ijcnn (discretization) | 141,691 | 69,867 | 22 | 0.21588 | 0.24578 | 0.20223 | 0.18608 |\n\nKey findings:\n- FFMs significantly outperform other models on KDD2010-bridge and KDD2012, which have mostly categorical features and are highly sparse after transformation to binary features.\n- On phishing and adult, FFM is not significantly better. For phishing, the data is not sparse so FFM, FM, and Poly2 have close performance. For adult, feature conjunction is not useful as all models perform similarly to the linear model.\n- For data sets with only numerical features, FFMs may not have an obvious advantage. Using dummy fields makes FFMs not outperform FMs; discretizing numerical features makes FFMs the best but with worse performance than using dummy fields.\n\nGuideline for applying FFMs:\n- FFMs are effective for data sets with categorical features transformed to binary features. If the transformed set is not sparse enough, FFMs bring less benefit.\n- Applying FFMs on numerical data sets is more difficult.",
  "hyperparameter": "Learning rate (η): 0.2 for FFM, 0.05 for FM; Regularization parameter (λ): 2×10^-5 (commonly used for both FFM and FM), 0 for some models; Number of latent factors (k): 4 for FFM (significantly smaller than FM), 40-100 for FM; Number of epochs (t): 4-9 for FFM, 8-9 for FM, determined by early stopping on validation set; Number of features after hashing: 10^7; Initial weight values: uniformly sampled from [0, 1/√k]; Initial gradient sum (G): set to 1 to prevent large values in AdaGrad; Instance normalization: normalize each instance to unit length"
}