{
  "id": "AFM_2017",
  "paper_title": "Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
  "alias": "AFM",
  "year": 2017,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "AFM enhances Factorization Machines by introducing an attention mechanism to learn the importance of different feature interactions, rather than treating all interactions equally. The core innovation is an attention network that assigns learnable weights (attention scores) to each pair-wise feature interaction based on the element-wise product of embedding vectors, enabling the model to discriminate between useful and noisy interactions. This approach achieves better performance than FM and deep learning models while maintaining interpretability and using significantly fewer parameters.",
  "introduction": "# Introduction\nSupervised learning often requires modeling feature interactions, especially for categorical variables (e.g., recommendation systems, online advertising). Linear regression fails to capture these interactions, while polynomial regression (PR) suffers from poor generalization on sparse data (unobserved cross features cannot be estimated).\n\nFactorization Machines (FMs) address PR’s limitation by parameterizing cross-feature weights as inner products of feature embeddings, enabling estimation of unobserved interactions. However, FM treats all feature interactions equally, which is suboptimal—some interactions (with irrelevant features) may introduce noise and degrade performance.\n\nThis paper proposes **Attentional Factorization Machines (AFM)**, which enhances FM by learning the importance of each feature interaction via an attention network. Key advantages:\n1. Automatically discriminates useful vs. useless feature interactions without domain knowledge.\n2. Improves interpretability by quantifying interaction importance.\n3. Outperforms FM and state-of-the-art deep learning methods (Wide&Deep, DeepCross) with a simpler structure and fewer parameters.\n\nAFM is validated on two real-world datasets, achieving 8.6% relative improvement over FM on regression tasks.",
  "method": "# Method\n## 1. Preliminaries: Factorization Machines (FM)\nFor a feature vector \\( x \\in \\mathbb{R}^n \\), FM models linear terms and second-order feature interactions:\n\\[\n\\hat{y}_{FM}(x) = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n (v_i^T v_j) x_i x_j\n\\]\n- \\( w_0 \\): Global bias, \\( w_i \\): Linear feature weight, \\( v_i \\in \\mathbb{R}^k \\): Feature embedding vector (k = latent dimension).\n- All interactions share the same weight (inner product), ignoring varying importance.\n\n## 2. Attentional Factorization Machines (AFM)\n### 2.1 Model Architecture\nAFM retains FM’s linear and embedding layers, adding two core components:\n1. **Pair-wise Interaction Layer**: Encodes interactions between non-zero feature embeddings as element-wise products:\n\\[\nf_{PI}(\\mathcal{E}) = \\{(v_i \\odot v_j) x_i x_j\\}_{(i,j) \\in \\mathcal{R}_x}\n\\]\n- \\( \\mathcal{E} = \\{v_i x_i\\}_{i \\in X} \\) (embedding outputs for non-zero features \\( X \\)), \\( \\odot \\): Element-wise product, \\( \\mathcal{R}_x = \\{(i,j) | i < j, i,j \\in X\\} \\).\n\n2. **Attention-based Pooling Layer**: Computes weighted sum of interaction vectors, where weights (attention scores) reflect interaction importance:\n\\[\nf_{Att}(f_{PI}(\\mathcal{E})) = \\sum_{(i,j) \\in \\mathcal{R}_x} a_{ij} (v_i \\odot v_j) x_i x_j\n\\]\n- \\( a_{ij} \\): Attention score for interaction (i,j), learned via an attention network.\n\n### 2.2 Attention Network\nThe attention network estimates \\( a_{ij} \\) using interaction vectors as input (ensures generalization to unobserved interactions):\n\\[\na_{ij}' = h^T \\text{ReLU}(W (v_i \\odot v_j) x_i x_j + b)\n\\]\n\\[\na_{ij} = \\text{softmax}(a_{ij}')\n\\]\n- \\( W \\in \\mathbb{R}^{t×k} \\), \\( b \\in \\mathbb{R}^t \\), \\( h \\in \\mathbb{R}^t \\): Attention network parameters (t = attention factor).\n- Softmax normalization ensures attention scores sum to 1.\n\n### 2.3 Final Prediction\nAFM’s prediction combines linear terms and weighted interaction features:\n\\[\n\\hat{y}_{AFM}(x) = w_0 + \\sum_{i=1}^n w_i x_i + p^T \\sum_{(i,j) \\in \\mathcal{R}_x} a_{ij} (v_i \\odot v_j) x_i x_j\n\\]\n- \\( p \\in \\mathbb{R}^k \\): Projection vector for interaction features.\n\n## 3. Learning & Regularization\n### 3.1 Objective Function\n- Regression task: Squared loss \\( L_r = \\sum_{x \\in \\mathcal{T}} (\\hat{y}_{AFM}(x) - y(x))^2 \\) ( \\( \\mathcal{T} \\): training set).\n- Classification/ranking: Log loss (adaptable for implicit feedback).\n\n### 3.2 Optimization\n- Optimizer: Stochastic Gradient Descent (SGD) or mini-batch Adagrad.\n- Overfitting Prevention:\n  1. Dropout on the pair-wise interaction layer (avoids neuron co-adaptation).\n  2. \\( L_2 \\) regularization on attention network weight \\( W \\) ( \\( L = L_r + \\lambda \\|W\\|^2 \\), \\( \\lambda \\): regularization strength).",
  "experiments": "# Experiment\n## 1. Experimental Settings\n### 1.1 Datasets\nTwo sparse datasets for supervised learning with categorical features:\n| Dataset    | Task                          | #Instances | #Features | #Negative Samples per Positive |\n|------------|-------------------------------|------------|-----------|--------------------------------|\n| Frappe     | Context-aware app recommendation | 288,609   | 5,382     | 2                              |\n| MovieLens  | Personalized tag recommendation | 2,006,859 | 90,445    | 2                              |\n- Target value: 1 (positive interaction), -1 (negative sample).\n- Split: 70% training, 20% validation, 10% test.\n\n### 1.2 Baselines\n- **LibFM**: Official FM implementation (SGD optimizer).\n- **HOFM**: Higher-order FM (order=3, models 3-way interactions).\n- **Wide&Deep**: Combines linear \"wide\" (cross features) and deep MLP (3 layers: 1024→512→256).\n- **DeepCross**: Deep residual network (5 residual units, hidden dimensions: 512→512→256→128→64).\n\n### 1.3 Evaluation Metric & Hyperparameters\n- Metric: RMSE (lower = better).\n- Common Settings: Embedding size = 256, attention factor = 256, mini-batch size = 128 (Frappe)/4096 (MovieLens).\n- Tuned Hyperparameters: Dropout ratio (0.2 for Frappe, 0.5 for MovieLens), \\( \\lambda \\in \\{0.5,1,...,16\\} \\) (attention network regularization).\n\n## 2. Main Results\n### 2.1 Hyperparameter Impact\n- **Dropout**: Optimal dropout ratios (0.2/0.5) improve AFM/FM generalization by preventing overfitting.\n- **Attention Factor**: AFM performance is stable across factors (1→256), validating the attention network’s robustness.\n- **Regularization**: \\( L_2 \\) regularization on \\( W \\) further reduces overfitting, with optimal \\( \\lambda = 4 \\) (Frappe) and \\( \\lambda = 8 \\) (MovieLens).\n\n### 2.2 Attention Network Effectiveness\n- AFM converges faster than FM and achieves lower test RMSE (Frappe: 0.4325 vs. FM’s 0.4512; MovieLens: 0.48 vs. FM’s 0.5130).\n- Micro-level analysis: AFM assigns higher attention scores to useful interactions (e.g., item-tag interactions in MovieLens), reducing prediction error vs. FM’s uniform weighting.\n\n### 2.3 Performance Comparison\nTable 2 (core result table) shows AFM outperforms all baselines:\n- **AFM vs. FM**: 8.6% relative RMSE improvement (Frappe), 6.4% (MovieLens).\n- **AFM vs. Deep Learning**: Outperforms Wide&Deep (4.3% better) and DeepCross (10%+ better) with 10x fewer parameters.\n- **HOFM**: Slight improvement over FM but doubles parameter count, less efficient than AFM.",
  "hyperparameter": "Embedding size (k): 256; Attention factor (t): 256; Mini-batch size: 128 (Frappe dataset), 4096 (MovieLens dataset); Dropout ratio: 0.2 (Frappe), 0.5 (MovieLens); L2 regularization strength (λ) for attention network: 4 (Frappe), 8 (MovieLens), tuned from {0.5, 1, ..., 16}; Optimizer: Mini-batch Adagrad or SGD"
}