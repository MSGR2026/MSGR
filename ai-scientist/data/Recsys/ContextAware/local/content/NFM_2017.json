{
  "id": "NFM_2017",
  "paper_title": "Neural Factorization Machines for Sparse Predictive Analytics",
  "alias": "NFM",
  "year": 2017,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "Neural Factorization Machines (NFM) introduces a Bi-Interaction pooling layer that efficiently encodes second-order feature interactions by element-wise product of embedding vectors, followed by shallow neural networks to model higher-order interactions in a non-linear way. This design combines the strengths of factorization machines for modeling low-level interactions and deep neural networks for learning non-linearity, achieving superior performance with fewer parameters than deep models like Wide&Deep. The Bi-Interaction layer has linear time complexity O(kN_x) and requires no additional parameters, making NFM more efficient and effective than concatenation-based deep models.",
  "introduction": "# INTRODUCTION\nPredictive analytics is crucial for many information retrieval (IR) and data mining (DM) tasks, including recommendation systems, targeted advertising, search ranking, visual analysis, and event detection. Typically, it involves estimating a function that maps predictor variables to a target (real-valued for regression, categorical for classification).\n\nUnlike continuous predictors in images and audios, predictors for web applications are mostly discrete and categorical. For example, in online advertising, predictors include user ID, occupation, and ad ID. These categorical variables are converted to binary feature vectors via one-hot encoding, resulting in high-dimensional but sparse vectors.\n\nTo build effective models with sparse data, capturing feature interactions is essential. Traditional solutions rely on manual crafting of combinatorial features (cross features), which requires heavy engineering efforts and domain knowledge, making generalization to new problems difficult.\n\nAn alternative is to design models that automatically learn feature interactions from raw data. Factorization Machines (FMs) are a popular approach, embedding features into a latent space and modeling pairwise (second-order) interactions via inner products of embedding vectors. FMs are general predictors applicable to any real-valued feature vector and have been successful in many tasks. However, their performance is limited by linearity and focus on second-order interactions, which may not capture the non-linear and complex structure of real-world data. Higher-order FMs exist but are still linear and hard to estimate, with marginal improvements over standard FMs.\n\nDeep Neural Networks (DNNs) have achieved great success in speech recognition, computer vision, and NLP but are less widely used in IR/DM due to the sparsity of data in these fields. Recent works have explored DNNs for sparse predictive analytics: Neural Collaborative Filtering (NCF) learns user-item interactions but is limited to two entities; FM-supported Neural Network (FNN) initializes DNNs with FM embeddings; Wide&Deep uses a multi-layer perceptron (MLP) on concatenated embeddings; DeepCross replaces MLP with residual networks. However, these DNN-based approaches have a key weakness: concatenating embeddings carries little interaction information, forcing deep layers to learn meaningful interactions, which is difficult due to vanishing/exploding gradients, overfitting, and degradation.\n\nIn this work, we propose Neural Factorization Machines (NFM) for sparse data prediction. NFM unifies FM’s strength in linear second-order interactions and neural networks’ ability to model non-linear higher-order interactions. We introduce a new Bi-Interaction pooling operation, subsuming FM under the neural network framework for the first time. Stacking non-linear layers above this pooling layer deepens FM, enabling effective modeling of higher-order and non-linear interactions. Compared to traditional DNN methods, Bi-Interaction pooling encodes more informative interactions, facilitating subsequent layers to learn meaningful information.\n\nExperiments on two public benchmarks (context-aware prediction and personalized tag recommendation) show that NFM with one hidden layer significantly outperforms FM (7.3% relative improvement) and state-of-the-art deep learning methods (Wide&Deep, DeepCross) with a simpler structure and fewer parameters, being easier to train and tune.\n\nMain contributions:\n1. Introduce Bi-Interaction pooling in neural network modeling, providing a new neural network view for FM.\n2. Develop the NFM model to deepen FM, enabling learning of higher-order and non-linear feature interactions.\n3. Conduct extensive experiments on real-world tasks, demonstrating NFM’s effectiveness and the promise of neural networks for sparse data prediction.",
  "method": "# NEURAL FACTORIZATION MACHINES (NFM)\n## 3.1 The NFM Model\nNFM is a general machine learner for real-valued feature vectors. Given a sparse input vector \\(x \\in \\mathbb{R}^n\\) ( \\(x_i=0\\) means the i-th feature is absent), NFM estimates the target as:\n\\[\n\\hat{y}_{NFM}(x) = w_0 + \\sum_{i=1}^n w_i x_i + f(x)\n\\]\n- The first two terms form the linear regression part (same as FM), modeling global bias and feature weights.\n- The third term \\(f(x)\\) is a multi-layer feedforward neural network that models feature interactions (core component of NFM).\n\n### 3.1.1 Layer-by-Layer Design of \\(f(x)\\)\n#### Embedding Layer\nA fully connected layer projecting each feature to a dense embedding vector. Let \\(v_i \\in \\mathbb{R}^k\\) be the embedding vector for feature i. After embedding, we obtain a set of scaled embedding vectors \\(\\mathcal{V}_x = \\{x_1 v_1, ..., x_n v_n\\}\\). For sparse \\(x\\), only non-zero feature embeddings are included (\\(\\mathcal{V}_x = \\{x_i v_i | x_i \\neq 0\\}\\)), as scaling by \\(x_i\\) accounts for real-valued features.\n\n#### Bi-Interaction Layer\nA pooling operation converting the set of embedding vectors to a single vector, encoding second-order feature interactions:\n\\[\nf_{BI}(\\mathcal{V}_x) = \\sum_{i=1}^n \\sum_{j=i+1}^n x_i v_i \\odot x_j v_j\n\\]\nwhere \\(\\odot\\) denotes element-wise product (\\((v_i \\odot v_j)_k = v_{ik} v_{jk}\\)). The output is a \\(k\\)-dimensional vector.\n\nKey properties:\n- No extra model parameters.\n- Linear time complexity: Reformulated as \\(f_{BI}(\\mathcal{V}_x) = \\frac{1}{2}\\left[(\\sum_{i=1}^n x_i v_i)^2 - \\sum_{i=1}^n (x_i v_i)^2\\right]\\), computed in \\(O(k N_x)\\) time (\\(N_x\\) is the number of non-zero entries in \\(x\\)).\n\n#### Hidden Layers\nStacked fully connected layers learning higher-order non-linear feature interactions:\n\\[\n\\begin{aligned}\nz_1 &= \\sigma_1(W_1 f_{BI}(\\mathcal{V}_x) + b_1) \\\\\nz_2 &= \\sigma_2(W_2 z_1 + b_2) \\\\\n&\\cdots \\\\\nz_L &= \\sigma_L(W_L z_{L-1} + b_L)\n\\end{aligned}\n\\]\n- \\(L\\): Number of hidden layers.\n- \\(W_l, b_l, \\sigma_l\\): Weight matrix, bias vector, and activation function (sigmoid, tanh, ReLU) for the l-th layer.\n- Non-linear activations enable modeling higher-order interactions in a non-linear way, superior to linear higher-order interaction methods (e.g., higher-order FM).\n\n#### Prediction Layer\nTransforms the output of the last hidden layer to the final prediction score:\n\\[\nf(x) = h^T z_L\n\\]\nwhere \\(h\\) is the weight vector of the prediction layer.\n\n### 3.1.2 Full NFM Formulation\n\\[\n\\hat{y}_{NFM}(x) = w_0 + \\sum_{i=1}^n w_i x_i + h^T \\sigma_L\\left(W_L\\left(...\\sigma_1\\left(W_1 f_{BI}(\\mathcal{V}_x) + b_1\\right)...\\right) + b_L\\right)\n\\]\nModel parameters: \\(\\Theta = \\{w_0, \\{w_i, v_i\\}, h, \\{W_l, b_l\\}\\}\\). Additional parameters compared to FM are \\(\\{W_l, b_l\\}\\) for higher-order interactions.\n\n### 3.1.3 Key Relationships and Complexity\n- **NFM Generalizes FM**: FM is a special case of NFM with no hidden layers (NFM-0). Fixing \\(h = (1,...,1)\\) recovers the FM model exactly.\n- **Relation to Wide&Deep/DeepCross**: Replacing Bi-Interaction pooling with embedding concatenation recovers Wide&Deep (MLP) or DeepCross (residual units). Bi-Interaction pooling captures more interaction information, easing training.\n- **Time Complexity**: \\(O(k N_x + \\sum_{l=1}^L d_{l-1} d_l)\\) (same as Wide&Deep/DeepCross), where \\(d_0 = k\\) and \\(d_l\\) is the dimension of the l-th hidden layer.\n\n## 3.2 Learning\nNFM is applicable to regression, classification, and ranking. Below focuses on regression (squared loss); other tasks use hinge loss (classification) or pairwise ranking loss (ranking).\n\n### 3.2.1 Objective Function\nSquared loss for regression:\n\\[\nL_{reg} = \\sum_{x \\in X} (\\hat{y}(x) - y(x))^2\n\\]\nwhere \\(X\\) is the training set and \\(y(x)\\) is the target of instance \\(x\\). Regularization is replaced by dropout (effective for preventing overfitting).\n\n### 3.2.2 Optimization\n- **Mini-batch Adagrad**: Used for optimization (adaptive learning rate, faster convergence than vanilla SGD).\n- **Gradient Calculation**: Chain rule for backpropagation. Key gradient for Bi-Interaction layer:\n  \\[\n  \\frac{d f_{BI}(\\mathcal{V}_x)}{d v_i} = \\sum_{j=1, j \\neq i}^n x_i x_j v_j\n  \\]\n- **Batch Size**: 128 for Frappe, 4096 for MovieLens (balances training speed and convergence).\n- **Early Stopping**: Stop training if validation RMSE increases for 4 consecutive epochs.\n\n### 3.2.3 Regularization and Stabilization\n#### Dropout\nPrevents overfitting by randomly dropping neurons during training. Applied to the Bi-Interaction layer (regularizes FM) and hidden layers. Disabled during testing.\n\n#### Batch Normalization (BN)\nAddresses internal covariate shift, speeding up training. Normalizes layer inputs to zero-mean unit-variance for each mini-batch:\n\\[\nBN(x_i) = \\gamma \\odot \\left(\\frac{x_i - \\mu_B}{\\sigma_B}\\right) + \\beta\n\\]\n- \\(\\mu_B\\), \\(\\sigma_B^2\\): Mini-batch mean and variance.\n- \\(\\gamma\\), \\(\\beta\\): Trainable scaling and shifting parameters.\n- Applied to outputs of Bi-Interaction layer and successive hidden layers.",
  "experiments": "# EXPERIMENTS\n## 4.1 Experimental Settings\n### 4.1.1 Datasets\nTwo public datasets for sparse predictive analytics:\n1. **Frappe**: Context-aware app discovery dataset (96,203 app usage logs). Features include user ID, app ID, and 8 context variables (weather, city, daytime). One-hot encoding results in 5,382 features. Target: 1 (app used), -1 (sampled unused app, 2 negatives per positive).\n2. **MovieLens**: Full MovieLens dataset (668,953 tag applications). Task: Personalized tag recommendation. Features: user ID, movie ID, tag. One-hot encoding results in 90,445 features. Target: 1 (tag assigned), -1 (sampled unassigned tag, 2 negatives per positive).\n\nDataset statistics:\n| Dataset | Instance# | Feature# | User# | Item# |\n| --- | --- | --- | --- | --- |\n| Frappe | 288,609 | 5,382 | 957 | 4,082 |\n| MovieLens | 2,006,859 | 90,445 | 17,045 | 23,743 |\n\n### 4.1.2 Evaluation Protocols\n- Data Split: Training (70%), validation (20%), test (10%).\n- Validation set: Tune hyperparameters.\n- Test set: Final performance comparison.\n- Metric: Root Mean Square Error (RMSE) (lower = better). Predictions out of [-1,1] are rounded to -1/1.\n- Statistical Significance: One-sample paired t-test (\\(p<0.05\\) = significant).\n\n### 4.1.3 Baselines\nImplemented in TensorFlow (NFM) or official/standard implementations:\n1. **LibFM**: Official FM implementation (SGD learner, baseline for sparse data).\n2. **HOFM**: Higher-order FM (order 3, TensorFlow implementation).\n3. **Wide&Deep**: 3-layer MLP (1024, 512, 256) on concatenated embeddings (raw features only, no manual cross features).\n4. **DeepCross**: 10-layer residual network (5 residual units, hidden dimensions 512, 512, 256, 128, 64) on concatenated embeddings.\n\n### 4.1.4 Parameter Settings\n- **Learning Rate**: Searched in [0.005, 0.01, 0.02, 0.05] for all methods.\n- **Regularization**:\n  - Linear models (LibFM, HOFM): \\(L_2\\) regularization tuned in \\([1e^{-6}, 5e^{-6}, ..., 1e^{-1}]\\).\n  - Neural models (Wide&Deep, DeepCross, NFM): Dropout ratio tuned in [0, 0.1, ..., 0.9].\n- **Optimizer**: Mini-batch Adagrad (all methods except LibFM, which uses vanilla SGD).\n- **Embedding Size**: Default 64; additional sizes (16, 32, 128, 256) tested.\n- **NFM Specific**: 1 hidden layer (ReLU activation), dropout ratio 0.5 (Bi-Interaction layer), tuned dropout for hidden layer.\n\n## 4.2 Study of Bi-Interaction Pooling (RQ1)\nAnalyze NFM-0 (no hidden layers, identical to FM) to evaluate Bi-Interaction pooling.\n\n### 4.2.1 Dropout vs. \\(L_2\\) Regularization\n- Dropout on Bi-Interaction layer outperforms \\(L_2\\) regularization. On Frappe, dropout ratio 0.3 achieves RMSE 0.3562 (vs. 0.3799 for \\(L_2\\)).\n- Dropout prevents overfitting by ensembling sub-models, while \\(L_2\\) only suppresses parameter values.\n\n### 4.2.2 Batch Normalization Speeds Up Training\n- BN accelerates convergence: On Frappe, BN reduces training error of epoch 20 to below epoch 60 without BN.\n- BN slightly improves generalization (not statistically significant) but may cause stability issues when combined with dropout (random neuron dropping changes normalized distribution).\n\n## 4.3 Impact of Hidden Layers (RQ2)\n### 4.3.1 Non-Linear Activations Improve Performance\n- NFM (1 hidden layer) outperforms NFM-0/LibFM: 11.3% (Frappe) and 5.2% (MovieLens) relative improvement.\n- Non-linear activations (sigmoid, ReLU, tanh) are essential; identity function (linear transformation) performs poorly.\n\n### 4.3.2 Deeper Layers Do Not Help\n- Best performance with 1 hidden layer; adding more layers (2-4) degrades performance.\n- Bi-Interaction pooling encodes sufficient second-order interactions, so a simple non-linear layer captures higher-order interactions effectively. Replacing Bi-Interaction with concatenation (Wide&Deep architecture) requires 3 layers to reach comparable performance (still inferior to NFM-1).\n\n### 4.3.3 Pre-Training Accelerates Convergence but Not Final Performance\n- Initializing NFM with FM embeddings leads to extremely fast convergence (5 epochs vs. 40 epochs from scratch).\n- Random initialization achieves slightly better final performance, demonstrating NFM’s robustness to parameter initialization (unlike Wide&Deep/DeepCross, which rely heavily on pre-training).\n\n## 4.4 Performance Comparison (RQ3)\n### 4.4.1 Key Results\nNFM achieves the best performance on both datasets with the fewest parameters (except LibFM):\n| Method | Frappe (RMSE, Factors=256) | MovieLens (RMSE, Factors=256) | Param# (Factors=256) |\n| --- | --- | --- | --- |\n| LibFM | 0.3385 | 0.4735 | 1.38M |\n| HOFM | 0.3331 | 0.4636 | 2.76M |\n| Wide&Deep (pre-train) | 0.3246 | 0.4512 | 4.66M |\n| DeepCross (pre-train) | 0.3548 | 0.5130 | 8.93M |\n| NFM | 0.3095** | 0.4443* | 1.45M |\n- **: \\(p<0.01\\), *: \\(p<0.05\\) (statistically significant vs. best baseline).\n\n### 4.4.2 Observations\n1. **NFM Outperforms FM/HOFM**: NFM’s non-linear higher-order interaction modeling is more expressive than linear FM/HOFM. HOFM only achieves marginal improvements over FM (1.45% Frappe, 1.04% MovieLens).\n2. **NFM Beats Wide&Deep/DeepCross**: Bi-Interaction pooling provides more informative low-level interactions, reducing the burden of deep layers. DeepCross (10-layer) underperforms due to optimization difficulties and overfitting.\n3. **Embedding Size Impact**: Performance improves with larger embedding sizes (up to 256) for all methods; NFM maintains superiority across all sizes.",
  "hyperparameter": "Embedding size: 64 (default), tested with {16, 32, 64, 128, 256}; Learning rate: searched in {0.005, 0.01, 0.02, 0.05}; Batch size: 128 (Frappe), 4096 (MovieLens); Number of hidden layers: 1 (optimal); Hidden layer activation: ReLU; Dropout ratio: 0.5 for Bi-Interaction layer, tuned in {0, 0.1, 0.2, ..., 0.9} for hidden layers; Optimizer: Mini-batch Adagrad; Early stopping: stop if validation RMSE increases for 4 consecutive epochs; Regularization for baselines (LibFM, HOFM): L2 regularization tuned in {1e-6, 5e-6, ..., 1e-1}"
}