{
  "id": "DCNV2_2020",
  "paper_title": "DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems",
  "alias": "DCNV2",
  "year": 2020,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "DCN V2 improves upon the original Deep & Cross Network by replacing the rank-1 weight matrix (w) with a full-rank weight matrix (W) in cross layers, significantly enhancing expressiveness while maintaining the elegant cross operation formula (x_{l+1} = x_0 ⊙ (W_l x_l + b_l) + x_l). The paper further proposes a mixture of low-rank DCN (DCN-Mix) that uses multiple low-rank experts with gating mechanisms to achieve cost-effective feature interaction learning, reducing computational cost by ~30% while maintaining accuracy. This enables explicit modeling of high-order feature crosses (up to order l+1 for l-layered network) combined with implicit interactions from deep networks.",
  "introduction": "# 1 Introduction\n\nLearning to rank (LTR) [4, 27] has remained to be one of the most important problems in modern-day machine learning and deep learning. It has a wide range of applications in search, recommendation systems [17, 39, 41], and computational advertising [2, 3]. Among the crucial components of LTR models, learning effective feature crosses continues to attract lots of attention from both academia [26, 35, 46] and industry [1, 6, 13, 34, 50].\n\nEffective feature crosses are crucial to the success of many models. They provide additional interaction information beyond individual features. For example, the combination of \"country\" and \"language\" is more informative than either one of them. In the era of linear models, ML practitioners rely on manually identifying such feature crosses [43] to increase model's expressiveness. Unfortunately, this involves a combinatorial search space, which is large and sparse in web-scale applications where the data is mostly categorical. Searching in such setting is exhaustive, often requires domain expertise, and makes the model harder to generalize.\n\nLater on, embedding techniques have been widely adopted to project features from high-dimensional sparse vectors to much lower-dimensional dense vectors. Factorization Machines (FMs) [36, 37] leverage the embedding techniques and construct pairwise feature interactions via the inner-product of two latent vectors.\n\nCompared to those traditional feature crosses in linear models, FM brings more generalization capabilities.\n\nIn the last decade, with more computing firepower and huge scale of data, LTR models in industry have gradually migrated from linear models and FM-based models to deep neural networks (DNN). This has significantly improved model performance for search and recommendation systems across the board [6, 13, 50]. People generally consider DNNs as universal function approximators, that could potentially learn all kinds of feature interactions [31, 47, 49]. However, recent studies [1, 50] found that DNNs are inefficient to even approximately model 2nd or 3rd-order feature crosses.\n\nTo capture effective feature crosses more accurately, a common remedy is to further increase model capacity through wider or deeper networks. This naturally crafts a double edged sword that we are improving model performance while making models much slower to serve. In many production settings, these models are handling extremely high QPS, thus have very strict latency requirements for real-time inference. Possibly, the serving systems are already pushed to a stretch that cannot afford even larger models. Furthermore, deeper models often introduce trainability issues, making models harder to train.\n\nThis has shed light on critical needs to design a model that can efficiently and effectively learn predictive feature interactions, especially in a resource-constraint environment that handles real-time traffic from billions of users. Many recent works [1, 6, 13, 26, 34, 35, 46, 50] tried to tackle this challenge. The common theme is to leverage those implicit high-order crosses learned from DNNs, with explicit and bounded-degree feature crosses which have been found to be effective in linear models. Implicit cross means the interaction is learned through an end-to-end function without any explicit formula modeling such cross. Explicit cross, on the other hand, is modeled by an explicit formula with controllable interaction order. We defer a detailed discussion of these models in Sec. 2.\n\nAmong these, Deep & Cross Network (DCN) [50] is effective and elegant, however, productionizing DCN in large-scale industry systems faces many challenges. The expressiveness of its cross network is limited. The polynomial class reproduced by the cross network is only characterized by  $O(\\mathrm{input~size})$  parameters, largely limiting its flexibility in modeling random cross patterns. Moreover, the allocated capacity between the cross network and DNN is unbalanced. This gap significantly increases when applying DCN to large-scale production data. An overwhelming portion of the parameters will be used to learn implicit crosses in the DNN.\n\nIn this paper, we propose a new model DCN-V2 that improves the original DCN model. We have already successfully deployed DCN-V2 in quite a few learning to rank systems across Google with significant gains in both offline model accuracy and online business metrics. DCN-V2 first learns explicit feature interactions of the inputs (typically the embedding layer) through cross layers, and then combines with a deep network to learn complementary implicit interactions. The core of DCN-V2 is the cross layers, which inherit the simple structure of the cross network from DCN, however significantly more expressive at learning explicit and bounded-degree cross features. The paper studies datasets with clicks as positive labels, however DCN-V2 is label agnostic and can be applied to any learning to rank systems. The main contributions of the paper are five-fold:\n\n- We propose a novel model—DCN-V2—to learn effective explicit and implicit feature crosses. Compared to existing methods, our model is more expressive yet remains efficient and simple.  \n- Observing the low-rank nature of the learned matrix in DCNV2, we propose to leverage low-rank techniques to approximate feature crosses in a subspace for better performance and latency trade-offs. In addition, we propose a technique based on the Mixture-of-Expert architecture [19, 45] to further decompose the matrix into multiple smaller sub-spaces. These sub-spaces are then aggregated through a gating mechanism.  \n- We conduct and provide an extensive study using synthetic datasets, which demonstrates the inefficiency of traditional ReLU-based neural nets to learn high-order feature crosses.  \n- Through comprehensive experimental analysis, we demonstrate that our proposed DCN-V2 models significantly outperform SOTA algorithms on Criteo and MovieLen-1M benchmark datasets.  \n- We provide a case study and share lessons in productionizing DCN-V2 in a large-scale industrial ranking system, which delivered significant offline and online gains.\n\nOur code and tutorial are open-sourced as part of TensorFlow Recommenders (TFRS) $^2$ .",
  "method": "# 3 Proposed Architecture: DCN-V2\n\nThis section describes a novel model architecture - DCN-V2 - to learn both explicit and implicit feature interactions. DCN-V2 starts with an embedding layer, followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions. The improvements made in DCN-V2 are critical for putting DCN into practice for highly-optimized production systems. DCN-V2 significantly improves the expressiveness of DCN [50] in modeling complex explicit cross terms in web-scale production data, while maintaining its elegant formula for easy deployment. The function class modeled by DCN-V2 is a strict superset of that modeled by DCN. The overall model architecture is depicted in Fig. 1, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel. In addition, observing the low-rank nature of the cross layers, we propose to leverage a mixture of low-rank cross layers to achieve healthier trade-off between model performance and efficiency.\n\n# 3.1 Embedding Layer\n\nThe embedding layer takes as input a combination of categorical (sparse) and dense features, and outputs  $\\mathbf{x}_0\\in \\mathbb{R}^d$ . It follows a similar setup as [50]. Unlike many related works [13, 16, 26, 34, 35, 46] which requires embedding sizes to match, our model accepts arbitrary embedding sizes. This is particularly important for industrial recommenders where the vocab size varies from  $O(10)$  to  $O(10^{8})$ .\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/38f311f58ea7d56010687d5bd41f0b91394c42538b5f5508f7081186714a9556.jpg)  \n(a) Stacked  \nFigure 1: Visualization of DCN-V2.  $\\otimes$  represents the cross operation in Eq. (1), i.e.,  $\\mathbf{x}_{l + 1} = \\mathbf{x}_0\\odot (W_l\\mathbf{x}_l + \\mathbf{b}_l) + \\mathbf{x}_l$\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/d0f7250b74bd0b165f6cf8dfd011929bf5e6bae0eb4c914610bc4284f3850e6c.jpg)  \n(b) Parallel\n\n# 3.2 Cross Network\n\nThe core of DCN-V2 lies in the cross layers that create explicit feature crosses. Eq. (1) shows the  $(l + 1)^{\\mathrm{th}}$  cross layer.\n\n$$\n\\mathbf {x} _ {l + 1} = \\mathbf {x} _ {0} \\odot \\left(W _ {l} \\mathbf {x} _ {l} + \\mathbf {b} _ {l}\\right) + \\mathbf {x} _ {l} \\tag {1}\n$$\n\nwhere  $\\mathbf{x}_0\\in \\mathbb{R}^d$  is the base layer that contains the original features of order 1, and is normally set as the embedding layer.  $\\mathbf{x}_{l},\\mathbf{x}_{l + 1}\\in \\mathbb{R}^{d}$ , respectively, represents the input and output of the  $(l + 1)^{\\mathrm{th}}$  cross layer.  $W_{l}\\in \\mathbb{R}^{d\\times d}$  and  $\\mathbf{b}_l\\in \\mathbb{R}^d$  are the learned weight matrix and bias vector. Fig. 2 visualizes an individual cross layer.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/7a5ddf1fce3ae689c729f504cbb9911c64a2de9cf9689b2a64335716d1da4c87.jpg)  \nFigure 2: Visualization of a cross layer.\n\nFor an  $l$ -layered cross network, the highest polynomial order is  $l + 1$  and the network contains all the feature crosses up to the highest order. Please see Sec. 4 for a detailed analysis, both from bitwise and feature-wise point of views. When  $W = 1 \\times \\mathbf{w}^{\\top}$ , where 1 represents a vector of ones, DCN-V2 falls back to DCN.\n\nThe cross layers could only reproduce polynomial classes of bounded degree; and any other complex function space could only be approximated<sup>3</sup>. Hence, we introduce a deep network next to complement the modeling of the inherent distribution in the data.\n\n# 3.3 Deep Network\n\nThe  $l^{\\mathrm{th}}$  deep layer's formula is given by  $\\mathbf{h}_{l + 1} = f(W_l\\mathbf{h}_l + \\mathbf{b}_l)$ , where  $\\mathbf{h}_l\\in \\mathbb{R}^{d_l},\\mathbf{h}_{l + 1}\\in \\mathbb{R}^{d_{l + 1}}$ , are the input and output of the  $l$ -th deep layer.  $W_{l}\\in \\mathbb{R}^{d_{l}\\times d_{l + 1}}$  is the weight matrix and  $\\mathbf{b}_l\\in \\mathbb{R}^{d_{l + 1}}$  is the bias vector.  $f(\\cdot)$  is an elementwise activation function and we set it to be ReLU, although any other activation functions are also suitable.\n\n# 3.4 Deep and Cross Combination\n\nWe seek structures to combine the cross network and deep network. Recent literature adopted two structures: stacked and parallel. In practice, we have found that which architecture works better is data dependent. Hence, we present both:\n\nStacked Structure (Fig. 1a): The input  $\\mathbf{x}_0$  is fed to the cross network followed by the deep network, and the final layer is given by  $\\mathbf{x}_{\\mathrm{final}} = \\mathbf{h}_{L_d}$ ,  $\\mathbf{h}_0 = \\mathbf{x}_{L_c}$ , which models the data as  $f_{\\mathrm{deep}} \\circ f_{\\mathrm{cross}}$ .\n\nParallel Structure (Fig. 1b): The input  $\\mathbf{x}_0$  is fed in parallel to both the cross and deep networks. The outputs  $\\mathbf{x}_{L_c}$  and  $\\mathbf{h}_{L_d}$  are concatenated to create the final output layer  $\\mathbf{x}_{\\mathrm{final}} = [\\mathbf{x}_{L_c};\\mathbf{h}_{L_d}]$ . This structure models the data as  $f_{\\mathrm{cross}} + f_{\\mathrm{deep}}$ .\n\nIn the end, the prediction is  $\\hat{y}_i\\coloneqq \\sigma (\\mathbf{w}_{\\mathrm{logit}}^\\top \\mathbf{x}_{\\mathrm{final}})$ , where  $\\mathbf{w}_{\\mathrm{logit}}$  is the weight vector for the logit, and  $\\sigma (x) = 1 / (1 + \\exp (-x))$ . For the final loss, we use the Log Loss (with a regularization term), which is commonly used for learning to rank systems especially with binary labels (e.g., clicks). Note that DCN-V2 itself is both prediction-task and loss-function agnostic.\n\n# 3.5 Cost-Effective Mixture of Low-Rank DCN\n\nDCN-V2 has successfully enabled significant model quality improvements for many highly-optimized production systems. For those production models, the model capacity is often constrained by limited serving resources and strict latency requirements. Hence, we seek methods to make DCN-v2 even more cost-efficient.\n\nThe simple structure of DCN-v2, where the computational bottleneck lies in matrix-vector multiplications, has allowed us to leverage matrix approximation techniques to reduce the cost. Low-rank techniques [12] are widely used [5, 9, 14, 20, 51, 53] to reduce the computational cost. It approximates a dense matrix  $M \\in \\mathbb{R}^{d \\times d}$  by two tall and skinny matrices  $U, V \\in \\mathbb{R}^{d \\times r}$ . When  $r \\leq d / 2$ , the cost will be reduced. However, they are most effective when the matrix shows a large gap in singular values or a fast spectrum decay. In many settings, we indeed observe that the learned matrix is numerically low-rank in practice.\n\nFig. 3a shows the singular decay pattern of the learned matrix  $W$  in DCN-V2 (see Eq. (1)) from a production model. Compared to the initial matrix, the learned matrix shows a much faster spectrum decay pattern. Let's define the numerical rank  $R_{T}$  with tolerance  $T$  to be  $\\mathrm{argmin}_k(\\sigma_k < T\\cdot \\sigma_1)$ , where  $\\sigma_{1}\\geq \\sigma_{2}\\geq ,\\ldots ,\\geq \\sigma_{n}$  are the singular values. Then,  $R_{T}$  means majority of the mass up to tolerance  $T$ , is preserved in the top  $k$  singular values. In the field of machine learning and deep learning, a model could still work surprisingly well with a reasonably high tolerance  $T^4$ .\n\nHence, it is well-motivated to impose a low-rank structure on  $W$ . Eq (2) shows the resulting  $(l + 1)$ -th low-rank cross layer\n\n$$\n\\mathbf {x} _ {l + 1} = \\mathbf {x} _ {0} \\odot \\left(U _ {l} \\left(V _ {l} ^ {\\top} \\mathbf {x} _ {i}\\right) + \\mathbf {b} _ {l}\\right) + \\mathbf {x} _ {i} \\tag {2}\n$$\n\nwhere  $U_{l},V_{l}\\in \\mathbb{R}^{d\\times r}$  and  $r\\ll d$  . Eq (2) has two interpretations: 1) we learn feature crosses in a subspace; 2) we project the input  $\\mathbf{x}$  to lower-dimensional  $\\mathbb{R}^r$  , and then project it back to  $\\mathbb{R}^d$  . The interpretations have inspired the following two model improvements.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/ff8ab429789b06257c39b4eeece91eff446240e97fcd323ce9d798bd48e3a279.jpg)  \n(a) Singular Values\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/ad4f3fe9eca0381f55c6f81f7e979429a4a6bee527e70b0151b57621b351de26.jpg)  \n(b) Mixture of Low-rank Experts  \nFigure 3: Left: Normalized singular values  $(1 = \\sigma_{1} \\geq \\sigma_{2} \\geq \\ldots \\geq \\sigma_{k})$  of the learned DCN-V2 weight matrix. Right: Visualization of mixture of low-rank cross layer.\n\nInterpretation 1 inspires us to adopt the idea from Mixture-of-Experts (MoE) [10, 19, 30, 45]. MoE-based models consist of two components: experts (typically a small network) and gating (a function of inputs). In our case, instead of relying on one single expert (Eq (2)) to learn feature crosses, we leverage multiple such experts, each learning feature interactions in a different subspaces, and adaptively combine the learned crosses using a gating mechanism that depends on input  $\\mathbf{x}$ . The resulting mixture of low-rank cross layer formulation is shown in Eq. (3) and depicted in Fig. 3b.\n\n$$\n\\mathbf {x} _ {l + 1} = \\sum_ {i = 1} ^ {K} G _ {i} \\left(\\mathbf {x} _ {l}\\right) E _ {i} \\left(\\mathbf {x} _ {l}\\right) + \\mathbf {x} _ {l} \\tag {3}\n$$\n\n$$\nE _ {i} (\\mathbf {x} _ {l}) = \\mathbf {x} _ {0} \\odot \\left(U _ {l} ^ {i} \\left(V _ {l} ^ {i \\top} \\mathbf {x} _ {l}\\right) + \\mathbf {b} _ {l}\\right)\n$$\n\nwhere  $K$  is the number of experts;  $G_{i}(\\cdot):\\mathbb{R}^{d}\\mapsto \\mathbb{R}$  is the gating function, common sigmoid or softmax;  $E_{i}(\\cdot):\\mathbb{R}^{d}\\mapsto \\mathbb{R}^{d}$  is the  $i^{\\mathrm{th}}$  expert in learning feature crosses.  $G(\\cdot)$  dynamically weights each expert for input  $\\mathbf{x}$ , and when  $G(\\cdot)\\equiv 1$ , Eq (3) falls back to Eq (2).\n\nInterpretation 2 inspires us to leverage the low-dimensional nature of the projected space. Instead of immediately projecting back from  $\\mathbb{R}^{d'}$  to  $\\mathbb{R}^d$  ( $d' \\ll d$ ), we further apply nonlinear transformations in the projected space to refine the representation [11].\n\n$$\nE _ {i} \\left(\\mathbf {x} _ {l}\\right) = \\mathbf {x} _ {0} \\odot \\left(U _ {l} ^ {i} \\cdot g \\left(C _ {l} ^ {i} \\cdot g \\left(V _ {l} ^ {i \\top} \\mathbf {x} _ {l}\\right)\\right) + \\mathbf {b} _ {l}\\right) \\tag {4}\n$$\n\nwhere  $g(\\cdot)$  represents any nonlinear activation function.\n\nDiscussions. This section aims to make effective use of the fixed memory/time budget to learn meaningful feature crosses. From Eqs (1)-(4), each formula represents a strictly larger function class assuming a fixed #params. Instead of post-training compression, our model imposes the structure prior to training and jointly learn the parameters. Due to that, the cross layer is an integral part of the nonlinear system  $f(\\mathbf{x}) = \\left(f_{k}(W_{k})\\circ \\dots \\circ f_{1}(W_{1})\\right)(\\mathbf{x})$ , where  $(f_{i + 1}\\circ f_i)(\\cdot)\\coloneqq f_{i + 1}(f_i(\\cdot))$ . Hence, the training dynamics of the overall system might be affected, and it would be interesting to study how the global statistics, such as Jacobian and Hessian matrices of  $f(\\mathbf{x})$ , are affected. We leave such investigations to future work.\n\nComplexity Analysis. Let  $d$  denote the embedding size,  $L_{c}$  denote the number of cross layers,  $K$  denote the number of low-rank DCN experts. Further, we assume each expert has the same smaller dimension  $r$  (upper bound on the rank). The time and space complexity for the cross network is  $O(d^{2}L_{c})$ , and for mixture of low-rank DCN (mixDCN) it's efficient when  $rK \\ll d$  with  $O(2drKL_{c})$ .",
  "experiments": "# 7 Experimental Results (RQ2 - RQ4)\n\nThis section empirically verifies the effectiveness of DCN-V2 in feature interaction learning across 3 datasets and 2 platforms, compared with SOTA. In light of recent concerns about poor reproducibility of published results [8, 33, 38], we conducted a fair and comprehensive experimental study with extensive hyper-parameter search to properly tune all the baselines and proposed approaches. In addition, for each optimal setup, we train 5 models with different random initialization, and report the mean and standard deviation.\n\nSec. 7.2 compares DCN-V2 with all the baselines comprehensively (RQ2). Sec. 7.3 evaluates the influence of hyper-parameters on the performance of DCN-V2 (RQ3). Sec. 7.4 focuses on model understanding (RQ4) of whether we are indeed discovering meaningful feature crosses with DCN-V2.\n\n# 7.1 Experiment Setup\n\nThis section describes training datasets, baseline approaches, and details of the hyper-parameter search and training process.\n\n7.1.1 Datasets We use 2 popular and standard public datasets.\n\nCriteo<sup>5</sup>. This click-through-rate (CTR) prediction benchmark dataset contains user logs over a period of 7 days. It has 45M examples and 39 features. We follow [46, 50] and use first 6 days for training, and randomly split the last day's data into validation and test set equally. We log-normalize (i.e.,  $\\log(x + 4)$  for feature-2 and  $\\log(x + 1)$  for others) the 13 dense features and embed the remaining 26 categorical features.\n\nMovieLen-1M<sup>6</sup>. The data contains 740k examples and 7 features. Each training example includes a <user-features>, movie-features, rating> triplet. Similar to [46], we formalize the task as a regression problem. All the ratings for 1s and 2s are normalized to be 0s; 4s and 5s to be 1s; and rating 3s are removed. 6 non-multivalent categorical features are used and embedded. The data is randomly split into  $80\\%$  for training,  $10\\%$  for validation and  $10\\%$  for testing.\n\n7.1.2 Baselines. We compare our proposed approaches with 6 SOTA feature interaction learning algorithms. A brief comparison between the approaches is highlighted in Tab. 2.\n\n7.1.3 Implementation Details. All the baselines and our approaches are implemented in TensorFlow v1. For a fair comparison, all the\n\nTable 2: High-level model comparisons. Assuming the input  $\\mathbf{x}_0 = [\\mathbf{v}_1;\\dots ;\\mathbf{v}_k]$  contains  $k$  feature embeddings.  $\\oplus \\coloneqq$  concatenation;  $\\otimes \\coloneqq$  outer-product;  $\\odot \\coloneqq$  Hadamard-product.  $f_{i}(\\cdot)$  represents implicit feature interactions, i.e., ReLU layers. In the last column, the  $^{\\prime} + ^{\\prime}$  sign is on the logit level  \n\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">Explicit Interactions (fe)</td><td rowspan=\"2\">Final Objective</td></tr><tr><td>Order</td><td>(Simplified) Key Formula</td></tr><tr><td rowspan=\"2\">PNN [35]</td><td rowspan=\"2\">2</td><td>xo=[viTvj|∀i, j] (IPNN)</td><td rowspan=\"2\">fi○fe</td></tr><tr><td>xo=[vec(vi⊗vj)|∀i, j] (OPNN)</td></tr><tr><td>DeepFM [13]</td><td>2</td><td>xo=[viTvj|∀i, j]</td><td>fi+fe</td></tr><tr><td>DLRM [34]</td><td>2</td><td>xo=[viTvj|∀i, j]</td><td>fi○fe</td></tr><tr><td>DCN [50]</td><td>≥2</td><td>xi+1= x0⊗ xiw</td><td>fi+fe</td></tr><tr><td>xDeepFM [26]</td><td>≥2</td><td>vhk= ∑i,j wijk(vik-1⊗vj)</td><td>fi+fe</td></tr><tr><td>AutoInt [46]</td><td>NA</td><td>v̅i=g(∑j exp((Wqvi,Wkvj))Wvvi)/∑j exp((Wqvi,Wkvj))</td><td>fi+fe</td></tr><tr><td rowspan=\"2\">DCN-V2 (ours)</td><td rowspan=\"2\">≥2</td><td rowspan=\"2\">xi=x0 ⊙ (Wi xi)</td><td>fi○fe</td></tr><tr><td>fi+fe</td></tr></table>\n\nimplementations were identical across all the models except for the feature interaction component<sup>7</sup>.\n\nEmbeddings. All the baselines require each feature's embedding size to be the same except for DNN and DCN models. Hence, we fixed it to be  $\\mathrm{Avg}_{\\mathrm{vocab}}(6 \\cdot (\\mathrm{vocab~cardinality})^{\\frac{1}{4}})$  (39 for Criteo and 30 for Movielen-1M) for all the models<sup>8</sup>.\n\nOptimization. We used Adam [22] with a batch size of 512 (128 for MovieLen). The kernels were initialized with He Normal [15], and biases to 0; the gradient clipping norm was 10; an exponential moving average with decay 0.9999 to parameters was applied.\n\nReproducibility and fair comparisons: hyper-parameters tuning and results reporting. For all the baselines, we conducted a coarse-level (larger-range) grid search over the hyper-parameters, followed by a finer-level (smaller-range) search. To ensure reproducibility and mitigate model variance, for each approach and dataset, we report the mean and stddev out of 5 independent runs for the best configuration. We describe detailed settings below for Criteo; and follow a similar process for MovieLens with different ranges.\n\nFor all the baselines on Criteo, the learning rate was tuned from  $10^{-4}$  to  $10^{-1}$  on a log scale and then narrowed down to  $10^{-4}$  to  $5 \\times 10^{-4}$  on a linear scale. The training steps were in  $\\{150\\mathrm{k}, 160\\mathrm{k}, 200\\mathrm{k}, 250\\mathrm{k}, 300\\mathrm{k}\\}$ . The hidden layer depth ranged in  $\\{1, 2, 3, 4\\}$  with their layer sizes in  $\\{562, 768, 1024\\}$ . And the regularization parameter  $\\lambda$  was in  $\\{0, 3 \\times 10^{-5}, 10^{-4}\\}$ .\n\nWe then describe each model's own hyper-parameters, where the search space is designed based on reported setting. For DCN, the number of cross layers ranged from 1 to 4. For AutoInt, the number of attention layers was from 2 to 4; the attention embedding size was in  $\\{20,32,40\\}$ ; the number of attention head was from 2 to 3; and the residual connection was either on or off. For xDeepFM, the CIN layer size was in  $\\{100,200\\}$ , depth in  $\\{2,3,4\\}$ , activation was identity, computation was either direct or indirect. For DLRM, the bottom MLP layer sizes and numbers were in  $\\{(512,256,64), (256,64)\\}$ . For PNN, we ran for IPNN, OPNN and PNN*, and for the latter two, the kernel type ranged in  $\\{\\text{full matrix}, \\text{vector}, \\text{number}\\}$ . For all the\n\nmodels, the total number of parameters was capped at  $1024^{2} \\times 5$  to limit the search space and avoid overly expensive computations.\n\n# 7.2 Model Performance (RQ2)\n\nThis section compares the performance between DCN-V2 approaches and the baselines. Note that the best setting reported for each model was searched over a wide-ranged model capacity and hyper-parameter space including the baselines. And if two settings performed on-par, we report the lower-cost one. Tab. 3 shows the best LogLoss and AUC (Area Under the ROC Curve) on testset for Criteo and MovieLen. For Criteo, a 0.001-level improvement is considered significant (see [13, 46, 50]).\n\nWe see that DCN-V2 consistently outperformed the baselines (including DNN) and achieved a healthy quality/cost trade-off. In our thorough hyper-parameter search for optimal settings for baseline models, we did explore wider and deeper models. However an even larger model could not yield more quality gains, clearly showing that many of the baselines were bottlenecked by quality rather than efficiency. It's also worth mentioning that the baselines' performances reported in Tab. 3 were improved over the numbers reported by previous papers (see Tab. 6a in Appendix); however, they were on-par and sometimes worse than the ReLU-based DNN with fine-granular model tuning.\n\nBest Settings. The optimal settings are in Tab. 3. For DCN-V2 models, both the 'stacked' and 'parallel' structures outperformed all the baselines, while 'stacked' worked better on Criteo and 'parallel' worked better on Movielen-1M. On Criteo, the setting was gate as constant, hard_tanh activation for DCN-Mix; gate as softmax and identity activation for CrossNet. The best training steps was  $150\\mathrm{k}$  for all the baselines; learning rate varies for all the models. Note that the best settings should depend on the data, model, and infrastructure. In practice, we found that 'stacked' structure improves the quality more while 'parallel' structure helps reduce the model variance.\n\nModel Quality - Comparisons among baselines. For 2nd-order methods, DLRM performed inferiorly to DeepFM although they are both derived from FM. This might be due to DLRM's omission of the 1st-order sparse features after the dot-product layer. For higher-order methods, xDeepFM, AutoInt and DCN behaved similarly on Criteo, while on MovieLens xDeepFm showed a high variance in Logloss.\n\nDCN-V2 achieved the best performance (0.001 considered to be significant on Criteo [26, 46, 50]) by explicitly modeling up to 3rd-order crosses beyond those implicit ones from DNN. DCN-Mix, the mixture of low-rank DCN, efficiently utilized the memory and reduced the cost by  $30\\%$  while maintaining the accuracy. Interestingly, CrossNet alone outperformed DNN on both datasets; we defer more discussions to end of this section.\n\nModel Quality - Comparisons with DNN. DNNs are universal approximators and tough-to-beat baselines when highly-optimized. Hence, we properly tuned DNN with all the baselines, and used a larger layer size than those used in literature (e.g., 200 - 400 in [26, 46]). To our surprise, DNN performed neck to neck with most baselines and even outperformed certain models.\n\nOur hypothesis is that those explicit feature crosses from baselines were not modeled in an expressive and easy-to-optimize\n\nmanner. The former makes its performance easy to be matched by a DNN with large capacity. The latter would easily lead to trainability issues, making the model unstable, hard to identify a good local optima or to generalize. Hence, when integrated with DNN, the overall performance is dominated by the DNN component. This becomes especially true with a large-capacity DNN, which could already approximate some simple cross patterns.\n\nIn terms of expressiveness, consider the 2nd-order methods. PNN models crosses more expressively than DeepFM and DLRM, which resulted in its superior performance on MovieLen-1M. This also explains the inferior performance of DCN compared to DCN-V2.\n\nIn terms of trainability, certain models might be inherently more difficult to train and resulted in unsatisfying performance. Consider PNN. On MoiveLen-1M, it outperformed DNN, suggesting the effectiveness of those 2nd-order crosses. On Criteo, however, PNN's advantage has diminished and the averaged performance was on-par with DNN. This was caused by the instability of PNN. Although its best run was better than DNN, its high stddev from multiple trials has driven up the mean loss. xDeepFM also suffers from trainability issue (see its high stddev on MovieLens). In xDeepFM, each feature map encodes all the pair-wise crosses while only relies on a single variable to learn the importance of each cross. In practice, a single variable is difficult to be learned when jointly trained with magnitudes more parameters. Then, an improperly learned variable would lead to noises.\n\nDCN-V2, on the other hand, consistently outperforms DNN. It successfully leveraged both the explicit and implicit feature interactions. We attribute this to the balanced number of parameters between the cross network and the deep network (expressive), as well as the simple structure of cross net which eased the optimization (easy-to-optimize). It's worth noting that the high-level structure of DCN-V2 shares a similar spirit of the self-attention mechanism adopted in AutoInt, where each feature embedding attends to a weighed combination of other features. The difference is that during the attention, higher-order interactions were modeled explicitly in DCN-V2 but implicitly in AutoInt.\n\nModel Efficiency. Tab. 3 also provides details for model size and FLOPS. FLOPS is a close estimation of run time, which is subjective to implementation details (See Tab. ??b in the Appendix for complexity analysis). The reported setting was properly tuned over the hyper-parameters of each model and the DNN component. For most models, the FLOPS is roughly  $2\\mathrm{x}$  of the #params; for xDeepFM, however, it is one magnitude higher, making it impractical in industrial-scale applications (also observed in [46]). Note that for DeepFM and DLRM, we've also searched over larger-capacity models; however, they didn't deliver better quality. Among all the methods, DCN-V2 delivered the best performance while remaining relatively efficient; DCN-Mix further reduced the cost, achieving a better trade-off between model efficiency and quality.\n\nCan Cross Layers Replace ReLU layers? ReLU layers are the backbone for various Neural Nets including DNN, RNN [18, 32, 40] and CNN [23, 24, 42]. Our experiments in Tab. 4 showed encouraging results that CrossNet consistently outperformed ReLU-based DNN with the same model capacity. The memory was controlled by varying the depth, width and rank (\\{128, 256\\}) of layers. This is a very interesting preliminary study and sheds light for our future explorations on cross layers and design of DNNs.\n\nTable 3: LogLoss and AUC (test) on Criteo and Movielen-1M. The metrics were averaged over 5 independent runs with their stddev in the parenthesis. In the 'Best Setting' column, the left reports DNN setting and the right reports model-specific setting.  $l$  denotes layer depth;  $n$  denotes CIN layer size;  $h$  and  $e$ , respectively, denotes #heads and att-embed-size;  $K$  denotes #experts and  $r$  denotes total rank.  \n\n<table><tr><td rowspan=\"2\">Baseline</td><td colspan=\"6\">Criteo</td><td colspan=\"4\">MovieLens-1M</td></tr><tr><td>Logloss</td><td>AUC</td><td>Params</td><td>FLOPS</td><td colspan=\"2\">Best Setting</td><td>Logloss</td><td>AUC</td><td>Params</td><td>FLOPS</td></tr><tr><td>PNN</td><td>0.4421 (5.8E-4)</td><td>0.8099 (6.1E-4)</td><td>3.1M</td><td>6.1M</td><td>(3, 1024)</td><td>OPNN</td><td>0.3182 (1.4E-3)</td><td>0.8955 (3.3E-4)</td><td>54K</td><td>110K</td></tr><tr><td>DeepFm</td><td>0.4420 (1.4E-4)</td><td>0.8099 (1.5E-4)</td><td>1.4M</td><td>2.8M</td><td>(2, 768)</td><td>-</td><td>0.3202 (1.0E-3)</td><td>0.8932 (7.7E-4)</td><td>46K</td><td>93K</td></tr><tr><td>DLRM</td><td>0.4427 (3.1E-4)</td><td>0.8092 (3.1E-4)</td><td>1.1M</td><td>2.2M</td><td>(2, 768)</td><td>[512,256,64]</td><td>0.3245 (1.1E-3)</td><td>0.8890 (1.1E-3)</td><td>7.7K</td><td>16K</td></tr><tr><td>xDeepFm</td><td>0.4421 (1.6E-4)</td><td>0.8099 (1.8E-4)</td><td>3.7M</td><td>32M</td><td>(3, 1024)</td><td>l=2, n=100</td><td>0.3251 (4.3E-3)</td><td>0.8923 (8.6E-4)</td><td>160K</td><td>990K</td></tr><tr><td>AutoInt+</td><td>0.4420 (5.7E-5)</td><td>0.8101 (2.6E-5)</td><td>4.2M</td><td>8.7M</td><td>(4, 1024)</td><td>l=2, h=2, e=40</td><td>0.3204 (4.4E-4)</td><td>0.8928 (3.9E-4)</td><td>260K</td><td>500K</td></tr><tr><td>DCN</td><td>0.4420 (1.6E-4)</td><td>0.8099 (1.7E-4)</td><td>2.1M</td><td>4.2M</td><td>(2, 1024)</td><td>l=4</td><td>0.3197 (1.9E-4)</td><td>0.8935 (2.1E-4)</td><td>110K</td><td>220K</td></tr><tr><td>DNN</td><td>0.4421 (6.5E-5)</td><td>0.8098 (5.9E-5)</td><td>3.2M</td><td>6.3M</td><td>(3, 1024)</td><td>-</td><td>0.3201 (4.1E-4)</td><td>0.8929 (2.3E-4)</td><td>46K</td><td>92K</td></tr><tr><td colspan=\"11\">Ours</td></tr><tr><td>DCN-V2</td><td>0.4406 (6.2E-5)</td><td>0.8115 (7.1E-5)</td><td>3.5M</td><td>7.0M</td><td>(2, 768)</td><td>l=2</td><td>0.3170 (3.6E-4)</td><td>0.8950 (2.7E-4)</td><td>110K</td><td>220K</td></tr><tr><td>DCN-Mix</td><td>0.4408 (1.0E-4)</td><td>0.8112 (9.8E-5)</td><td>2.4M</td><td>4.8M</td><td>(2, 512)</td><td>l=3, K=4, r=258</td><td>0.3160 (4.9E-4)</td><td>0.8964 (2.9E-4)</td><td>110K</td><td>210K</td></tr><tr><td>CrossNet</td><td>0.4413 (2.5E-4)</td><td>0.8107 (2.4E-4)</td><td>2.1M</td><td>4.2M</td><td>-</td><td>l=4, K=4, r=258</td><td>0.3185 (3.0E-4)</td><td>0.8937 (2.7E-4)</td><td>65K</td><td>130K</td></tr></table>\n\nTable 4: Logloss and AUC (test) with a fixed memory budget.  \n\n<table><tr><td colspan=\"2\">#Params</td><td>7.9E+05</td><td>1.3E+06</td><td>2.1E+06</td><td>2.6E+06</td></tr><tr><td rowspan=\"2\">LogLoss</td><td>CrossNet</td><td>0.4424</td><td>0.4417</td><td>0.4416</td><td>0.4415</td></tr><tr><td>DNN</td><td>0.4427</td><td>0.4426</td><td>0.4423</td><td>0.4423</td></tr><tr><td rowspan=\"2\">AUC</td><td>CrossNet</td><td>0.8096</td><td>0.8104</td><td>0.8105</td><td>0.8106</td></tr><tr><td>DNN</td><td>0.8091</td><td>0.8094</td><td>0.8096</td><td>0.80961</td></tr></table>\n\n# 7.3 How the Choice of Hyper-parameters Affect DCN-V2 Model Performance (RQ3)\n\nThis section examines the model performance as a function of hyper-parameters that include 1) depth of cross layers; 2) matrix rank of DCN-Mix; 3) number of experts in DCN-Mix.\n\nDepth of Cross Layer. By design, the highest feature cross order captured by the cross net increases with layer depth. Hence, we constrain ourselves to the full-rank cross layers, and evaluate the performance change with layer depth.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/244244f8f68cc59b7371e4496f6e3256972245bc88ba3a25b4de5fba9c70d5fc.jpg)  \n(a) Layer depth  \nFigure 5: Logloss and AUC (test) v.s. depth & matrix rank.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/86330e569f539f8e87633fc58325fc8fccdb1f4ced896aed33b5132a4dba4cd6.jpg)  \n(b) Matrix rank\n\nFig. 5a shows the test LogLoss and AUC while increasing layer depth on the Criteo dataset. We see a steady quality improvement with a deeper cross network, indicating that it's able to capture more meaningful crosses. The rate of improvement, however, slowed down when more layers were used. This suggests the contribution from that of higher-order crosses is less significant than those from\n\nlower-order crosses. We also used a same-sized DNN as a reference. When there were  $\\leq 2$  layers, DNN outperformed the cross network; when more layers became available, the cross network started to close the performance gap and even outperformed DNN. In the small-layer regime, the cross network could only approximate very low-order crosses (e.g.,  $1 \\sim 2$ ); in the large-layer regime, those low-order crosses were characterized with more parameters, and those high-order interactions were started to be captured.\n\nRank of Matrix. The rank of the weight matrix controls the # of params and the portion of low-frequency signals passing through the cross layers. Hence, we study its influence on model quality. The model based on a well-performed setting with 3 cross layers followed by 3 512-size ReLU layers. We approximate the matrix  $W$  in each cross layer by  $UV^{\\top}$  where  $U, V \\in \\mathbb{R}^{d \\times r}$ , and vary  $r$ .\n\nFig. 5b shows the test LogLoss v.s. matrix's rank  $r$  on Criteo. When  $r$  was as small as 4, the performance was on-par with other baselines. When  $r$  was increased from 4 to 64, the LogLoss decreased almost linearly with  $r$  (i.e., model's improving). When  $r$  was further increased from 64 to full, the improvement on LogLoss slowed down. We refer to 64 as the threshold rank. The significant slow down from 64 suggests that the important signals characterizing feature crosses could be captured in the top-64 singular values.\n\nOur hypothesis for the value of this threshold rank is  $O(k)$  where  $k$  represents # features (39 for Criteo). Consider the  $(i,j)$ -th block of matrix  $W$ , we can view  $W_{i,j} = W_{i,j}^{L} + W_{i,j}^{H}$ , where  $W_{i,j}^{L}$  stores the dominant signal (low-frequency) and  $W_{i,j}^{H}$  stores the rest (high-frequency). In the simplest case where  $W_{i,j}^{L} = c_{ij}11^{\\top}$ , the entire matrix  $W^{L}$  will be of rank  $k$ . The effectiveness of this hypothesis remains to be verified across multiple datasets.\n\nNumber of Experts. We've observed that 1) best-performed setting (#expert, gate, matrix activation type) was subjective to datasets and model architectures; 2) the best-performed model of each setting yielded similar results. For example, for a 2-layered cross net with total rank 256 on Criteo, the LogLoss for 1, 4, 8, 16, and 32 experts, respectively, was 0.4418, 0.4416, 0.4416, 0.4422, and 0.4420. The fact that more lower-rank experts weren't performing better than a single higher-rank expert might be caused by the naive gating functions and optimizations adopted. We believe more\n\nsophisticated gating [21, 28, 29] and optimizations (e.g., alternative training, special initialization, temperature adjustment) would yield better results with a mixture of experts architecture. This, however, is beyond the scope of this paper and we leave it to future work.\n\n# 7.4 Model Understanding (RQ4)\n\nA good understanding of the learned feature crosses is crucial to fields like ML fairness and ML for health. Fortunately, the weight matrix  $W$  in DCN-V2 exactly reveals what feature crosses the model has learned to be important. Assuming the  $i$ -th feature is embedded as  $\\mathbf{x}_i$ , then, the block-wise view shows that the importance of interaction between  $\\mathbf{x}_i$  and  $\\mathbf{x}_j$  is characterized by block  $W_{i,j}$ :\n\n$$\n\\mathbf {x} \\odot W \\mathbf {x} = \\left[ \\begin{array}{c} \\mathbf {x} _ {1} \\\\ \\mathbf {x} _ {2} \\\\ \\vdots \\\\ \\mathbf {x} _ {k} \\end{array} \\right] \\odot \\left[ \\begin{array}{c c c c} W _ {1, 1} & W _ {1, 2} & \\dots & W _ {1, k} \\\\ W _ {2, 1} & W _ {2, 2} & \\dots & W _ {2, k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ W _ {k, 1} & W _ {k, 2} & \\dots & W _ {k, k} \\end{array} \\right] \\left[ \\begin{array}{c} \\mathbf {x} _ {1} \\\\ \\mathbf {x} _ {2} \\\\ \\vdots \\\\ \\mathbf {x} _ {k} \\end{array} \\right]\n$$\n\nFig. 6 shows the learned weight matrix  $W$  in the first cross layer. Subplot (a) shows the entire matrix with orange boxes highlighting some notable feature crosses. The off-diagonal block corresponds to crosses that are known to be important, suggesting the effectiveness of DCN-V2. The diagonal block represents self-interaction ( $x^{2}$ 's). Subplot (b) indicates some strong interactions learned, e.g., Gender ×UserID, MovieId ×UserID.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/88ef199747c8a19834f93ef17612f12c3c6015e6e0993b66c820511b43287732.jpg)  \n(a) Production data  \nFigure 6: Learned weight matrix in DCN-V2.\n\n![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/f120ff87-2512-4d21-8210-a9078d0f3c6f/f389b61d46de52724c3151060193c708bbaec6d9658632ba496e7940174597e2.jpg)  \n(b) Movielen-1M\n\nRows/cols represent features. For (a), feature names were omitted for proprietary reasons; darker pixel represents larger (abs) weight. For (b), each block represents its Frobenius norm.\n",
  "hyperparameter": "Embedding size: Avg_vocab(6·(vocab_cardinality)^(1/4)), which equals 39 for Criteo and 30 for MovieLens-1M. Optimizer: Adam with batch size 512 (128 for MovieLens). Learning rate: searched from 10^-4 to 10^-1 on log scale, then narrowed to 10^-4 to 5×10^-4 on linear scale. Training steps: {150k, 160k, 200k, 250k, 300k}. Hidden layer depth: {1, 2, 3, 4} with layer sizes in {562, 768, 1024}. Regularization λ: {0, 3×10^-5, 10^-4}. Cross layer depth: 1-4 layers. For DCN-Mix: number of experts K=4, total rank r=258 for best performance. Gradient clipping norm: 10. Exponential moving average decay: 0.9999. Weight initialization: He Normal, bias initialization: 0. Best setting on Criteo: 2 cross layers + (2, 768) DNN layers for DCN-V2; 3 cross layers + (2, 512) DNN layers with K=4, r=258 for DCN-Mix."
}