{
  "id": "DeepFM_2017",
  "paper_title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
  "alias": "DeepFM",
  "year": 2017,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "DeepFM combines a Factorization Machine (FM) component and a deep neural network (DNN) component that share the same input embedding layer to jointly learn low-order (order-1 and order-2) and high-order feature interactions for CTR prediction. Unlike previous models (FNN, PNN, Wide & Deep), DeepFM requires no pre-training or manual feature engineering, as both components share the same feature embeddings and are trained end-to-end, with the FM component capturing explicit low-order interactions while the DNN component models implicit high-order interactions.",
  "introduction": "# Introduction\nThe prediction of click-through rate (CTR) is critical in recommender system, where the task is to estimate the probability a user will click on a recommended item. In many recommender systems the goal is to maximize the number of clicks, and so the items returned to a user can be ranked by estimated CTR; while in other application scenarios such as online advertising it is also important to improve revenue, and so the ranking strategy can be adjusted as CTR×bid across all candidates, where “bid” is the benefit the system receives if the item is clicked by a user. In either case, it is clear that the key is in estimating CTR correctly.\n\nIt is important for CTR prediction to learn implicit feature interactions behind user click behaviors. By our study in a mainstream apps market, we found that people often download apps for food delivery at meal-time, suggesting that the (order-2) interaction between app category and time-stamp can be used as a signal for CTR. As a second observation, male teenagers like shooting games and RPG games, which means that the (order-3) interaction of app category, user gender and age is another signal for CTR. In general, such interactions of features behind user click behaviors can be highly sophisticated, where both low- and high-order feature interactions should play important roles. According to the insights of the Wide & Deep model [Cheng et al., 2016] from google, considering low- and high-order feature interactions simultaneously brings additional improvement over the cases of considering either alone.\n\nThe key challenge is in effectively modeling feature interactions. Some feature interactions can be easily understood, thus can be designed by experts (like the instances above). However, most other feature interactions are hidden in data and difficult to identify a priori (for instance, the classic association rule “diaper and beer” is mined from data, instead of discovering by experts), which can only be captured automatically by machine learning. Even for easy-to-understand interactions, it seems unlikely for experts to model them exhaustively, especially when the number of features is large.\n\nDespite their simplicity, generalized linear models, such as FTRL [McMahan et al., 2013], have shown decent performance in practice. However, a linear model lacks the ability to learn feature interactions, and a common practice is to manually include pairwise feature interactions in its feature vector. Such a method is hard to generalize to model high-order feature interactions or those never or rarely appear in the training data [Rendle, 2010]. Factorization Machines (FM) [Rendle, 2010] model pairwise feature interactions as inner product of latent vectors between features and show very promising results. While in principle FM can model high-order feature interaction, in practice usually only order-2 feature interactions are considered due to high complexity.\n\nAs a powerful approach to learning feature representation, deep neural networks have the potential to learn sophisticated feature interactions. Some ideas extend CNN and RNN for CTR prediction [Liu et al., 2015; Zhang et al., 2014], but CNN-based models are biased to the interactions between neighboring features while RNN-based models are more suitable for click data with sequential dependency. [Zhang et al., 2016] studies feature representations and proposes Factorization-machine supported Neural Network (FNN). This model pre-trains FM before applying DNN, thus limited by the capability of FM. Feature interaction is studied in [Qu et al., 2016], by introducing a product layer between embedding layer and fully-connected layer, and proposing the Product-based Neural Network (PNN). As noted in [Cheng et al., 2016], PNN and FNN, like other deep models, capture little low-order feature interactions, which are also essential for CTR prediction. To model both low- and high-order feature interactions, [Cheng et al., 2016] proposes an interesting hybrid network structure (Wide & Deep) that combines a linear (“wide”) model and a deep model. In this model, two different inputs are required for the “wide part” and “deep part”, respectively, and the input of “wide part” still relies on expertise feature engineering.\n\nOne can see that existing models are biased to low- or high-order feature interaction, or rely on feature engineering. In this paper, we show it is possible to derive a learning model that is able to learn feature interactions of all orders in an end-to-end manner, without any feature engineering besides raw features. Our main contributions are summarized as follows:\n1. We propose a new neural network model DeepFM that integrates the architectures of FM and deep neural networks (DNN). It models low-order feature interactions like FM and models high-order feature interactions like DNN. Unlike the wide & deep model [Cheng et al., 2016], DeepFM can be trained end-to-end without any feature engineering.\n2. DeepFM can be trained efficiently because its wide part and deep part, unlike [Cheng et al., 2016], share the same input and also the embedding vector. In [Cheng et al., 2016], the input vector can be of huge size as it includes manually designed pairwise feature interactions in the input vector of its wide part, which also greatly increases its complexity.\n3. We evaluate DeepFM on both benchmark data and commercial data, which shows consistent improvement over existing models for CTR prediction.",
  "method": "# Method\n## 2 Our Approach\nSuppose the data set for training consists of n instances \\((\\chi, y)\\), where x is an m-fields data record usually involving a pair of user and item, and \\(y \\in \\{0,1\\}\\) is the associated label indicating user click behaviors ( \\(y=1\\) means the user clicked the item, and \\(y=0\\) otherwise). x may include categorical fields (e.g., gender, location) and continuous fields (e.g., age). Each categorical field is represented as a vector of one-hot encoding, and each continuous field is represented as the value itself, or a vector of one-hot encoding after discretization. Then, each instance is converted to \\((x, y)\\) where \\(x=[x_{field _{1}}, x_{field _{2}}, ..., x_{filed _{j}}, ..., x_{field _{m}}]\\) is a d dimensional vector, with \\(x_{field}\\) being the vector representation of the j-th field of x. Normally, x is high-dimensional and extremely sparse. The task of CTR prediction is to build a prediction model \\(\\hat{y}=CTR\\_model(x)\\) to estimate the probability of a user clicking a specific app in a given context.\n\n### 2.1 DeepFM\nWe aim to learn both low- and high-order feature interactions. To this end, we propose a Factorization-Machine based neural network (DeepFM). DeepFM consists of two components, FM component and deep component, that share the same input. For feature i, a scalar \\(w_{i}\\) is used to weigh its order-1 importance, a latent vector \\(V_{i}\\) is used to measure its impact of interactions with other features. \\(V_{i}\\) is fed in FM component to model order-2 feature interactions, and fed in deep component to model high-order feature interactions. All parameters, including \\(w_{i}\\), \\(V_{i}\\), and the network parameters (\\((W^{(l)}, b^{(l)}\\) below) are trained jointly for the combined prediction model:\n\\[\n\\hat{y}=sigmoid\\left(y_{FM}+y_{DNN}\\right), \\tag{1}\n\\]\nwhere \\(\\hat{y} \\in(0,1)\\) is the predicted CTR, \\(y_{FM}\\) is the output of FM component, and \\(y_{DNN}\\) is the output of deep component.\n\n#### FM Component\nThe FM component is a factorization machine, which is proposed in [Rendle, 2010] to learn feature interactions for recommendation. Besides a linear (order-1) interactions among features, FM models pairwise (order-2) feature interactions as inner product of respective feature latent vectors.\n\\[\ny_{FM}=<w, x>+\\sum_{j_{1}=1}^{d} \\sum_{j_{2}=j_{1}+1}^{d}\\left<V_{i}, V_{j}\\right> x_{j_{1}} \\cdot x_{j_{2}}, \\tag{2}\n\\]\nwhere \\(<w, x>\\) denotes the linear term (order-1 feature interactions), and the second term models the pairwise (order-2) feature interactions.\n\n#### Deep Component\nThe deep component is a standard multi-layer perceptron (MLP) that models high-order feature interactions. The structure of the deep component is as follows:\n1. **Embedding Layer**: Each field’s one-hot vector is mapped to a low-dimensional dense embedding vector. For a field with k categories, the embedding vector is learned as part of the model parameters. The output of the embedding layer is a concatenation of all fields’ embedding vectors, denoted as \\(a^{(0)}=[e_{1}, e_{2}, ..., e_{m}]\\) (Eq. (3)), where \\(e_{j}\\) is the embedding vector of the j-th field.\n2. **Hidden Layers**: The embedding vector \\(a^{(0)}\\) is fed into successive hidden layers. Each hidden layer computes the output as:\n\\[\na^{(l+1)}=\\sigma\\left(W^{(l)} a^{(l)}+b^{(l)}\\right),\n\\]\nwhere \\(W^{(l)}\\) and \\(b^{(l)}\\) are the weight matrix and bias vector of the l-th hidden layer, respectively, and \\(\\sigma(\\cdot)\\) is the activation function (e.g., ReLU, tanh).\n3. **Output Layer**: The output of the last hidden layer is fed into a fully-connected output layer to get the high-order interaction score \\(y_{DNN}\\).\n\n#### Comparison with Related Models\n- **FNN**: FNN pre-trains FM to initialize the embedding layer, while DeepFM trains the embedding layer jointly with both FM and deep components, avoiding reliance on pre-training.\n- **PNN**: PNN introduces a product layer to model feature interactions but ignores low-order interactions; DeepFM explicitly models low-order interactions via the FM component.\n- **Wide & Deep**: Wide & Deep requires separate inputs for the wide (linear) and deep components, with the wide component relying on manual feature engineering. DeepFM shares the same input and embedding vectors for both components, enabling end-to-end training without feature engineering.",
  "experiments": "# Experiment\n## 3 Experiments\nIn this section, we compare our proposed DeepFM and the other state-of-the-art models empirically. The evaluation result indicates that our proposed DeepFM is more effective than any other state-of-the-art model and the efficiency of DeepFM is comparable to the best ones among the others.\n\n### 3.1 Experiment Setup\n#### Datasets\nWe evaluate the effectiveness and efficiency of our proposed DeepFM on the following two datasets:\n1. **Criteo Dataset**: Includes 45 million users’ click records. There are 13 continuous features and 26 categorical ones. We split the dataset randomly into two parts: 90% is for training, while the rest 10% is for testing.\n2. **Company∗Dataset**: Collected 7 consecutive days of users’ click records from the game center of the Company∗ App Store for training, and the next 1 day for testing. There are around 1 billion records in the whole collected dataset. The dataset includes app features (e.g., identification, category), user features (e.g., user’s downloaded apps), and context features (e.g., operation time).\n\n#### Evaluation Metrics\nWe use two evaluation metrics in our experiments: AUC (Area Under ROC) and Logloss (cross entropy).\n\n#### Model Comparison\nWe compare 9 models in our experiments: LR, FM, FNN, PNN (three variants), Wide & Deep, and DeepFM. In the Wide & Deep model, for the purpose of eliminating feature engineering effort, we also adapt the original Wide & Deep model by replacing LR by FM as the wide part. We name them LR & DNN and FM & DNN, respectively.\n\n#### Parameter Settings\n- **Criteo Dataset**: Follow the parameter settings in [Qu et al., 2016] for FNN and PNN: dropout = 0.5; network structure = 400-400-400; optimizer = Adam; activation function = tanh for IPNN, relu for other deep models. DeepFM uses the same setting. Optimizers of LR and FM are FTRL and Adam respectively, and the latent dimension of FM is 10.\n- **Company∗Dataset**: Conducted careful parameter study to achieve the best performance for each individual model, discussed in Section 3.3.\n\n### 3.2 Performance Evaluation\n#### Efficiency Comparison\nThe efficiency of deep learning models is important to real-world applications. We compare the efficiency of different models on Criteo dataset by the formula: \\(\\frac{\\text{training time of deep CTR model}}{\\text{training time of LR}}\\). The results include tests on CPU and GPU, with the following observations:\n1. Pre-training of FNN makes it less efficient.\n2. Although the speed up of IPNN and PNN∗on GPU is higher than the other models, they are still computationally expensive because of the inefficient inner product operations.\n3. DeepFM achieves almost the most efficient in both tests.\n\n#### Effectiveness Comparison\nThe performance for CTR prediction of different models on Criteo dataset and Company∗dataset is shown in Table 2, with the following observations:\n- Learning feature interactions improves the performance of CTR prediction model. LR (which does not consider feature interactions) performs worse than the other models. DeepFM outperforms LR by 0.86% and 4.18% in terms of AUC (1.15% and 5.60% in terms of Logloss) on Company∗and Criteo datasets.\n- Learning high- and low-order feature interactions simultaneously and properly improves performance. DeepFM outperforms models that learn only low-order (FM) or high-order (FNN, IPNN, OPNN, PNN∗) feature interactions. Compared to the second best model, DeepFM achieves more than 0.37% and 0.25% in terms of AUC (0.42% and 0.29% in terms of Logloss) on Company∗and Criteo datasets.\n- Sharing feature embedding for high- and low-order interaction learning improves performance. DeepFM outperforms LR & DNN and FM & DNN (which use separate embeddings) by more than 0.48% and 0.33% in terms of AUC (0.61% and 0.66% in terms of Logloss) on Company∗and Criteo datasets.\n\nTable 2: Performance on CTR prediction.\n\n| Model | Company∗ - AUC | Company∗ - LogLoss | Criteo - AUC | Criteo - LogLoss |\n|-------|----------------|--------------------|--------------|------------------|\n| LR | 0.8640 | 0.02648 | 0.7686 | 0.47762 |\n| FM | 0.8678 | 0.02633 | 0.7892 | 0.46077 |\n| FNN | 0.8683 | 0.02629 | 0.7963 | 0.45738 |\n| IPNN | 0.8664 | 0.02637 | 0.7972 | 0.45323 |\n| OPNN | 0.8658 | 0.02641 | 0.7982 | 0.45256 |\n| PNN∗ | 0.8672 | 0.02636 | 0.7987 | 0.45214 |\n| LR & DNN | 0.8673 | 0.02634 | 0.7981 | 0.46772 |\n| FM & DNN | 0.8661 | 0.02640 | 0.7850 | 0.45382 |\n| DeepFM | 0.8715 | 0.02618 | 0.8007 | 0.45083 |\n\n### 3.3 Hyper-Parameter Study\nWe study the impact of different hyper-parameters of deep models on Company∗dataset, including:\n1. **Activation Function**: Relu is more appropriate than tanh for all deep models except IPNN, as relu induces sparsity.\n2. **Dropout Rate**: All models reach best performance when dropout is set between 0.6 and 0.9, indicating reasonable randomness strengthens model robustness.\n3. **Number of Neurons per Layer**: 200 or 400 neurons per layer is optimal; increasing beyond this may lead to overfitting.\n4. **Number of Hidden Layers**: Performance improves initially with more layers but degrades with excessive layers due to overfitting.\n5. **Network Shape**: The “constant” shape (e.g., 200-200-200) outperforms increasing, decreasing, and diamond shapes, consistent with previous studies [Larochelle et al., 2009].",
  "hyperparameter": "Criteo dataset: dropout=0.5, network structure=400-400-400 (3 hidden layers with 400 neurons each), optimizer=Adam, activation function=tanh for IPNN and relu for other deep models, FM latent dimension=10, LR optimizer=FTRL. Company dataset (after parameter study): dropout=0.6-0.9 (optimal range), number of neurons per layer=200 or 400, activation function=relu (except tanh for IPNN), network shape=constant (e.g., 200-200-200), number of hidden layers should be moderate to avoid overfitting."
}