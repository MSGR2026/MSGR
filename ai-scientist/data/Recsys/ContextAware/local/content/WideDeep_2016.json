{
  "id": "WideDeep_2016",
  "paper_title": "Wide & Deep Learning for Recommender Systems",
  "alias": "WideDeep",
  "year": 2016,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "The paper proposes a Wide & Deep learning framework that jointly trains a wide linear model (for memorization via cross-product feature transformations) and a deep neural network (for generalization via low-dimensional embeddings). The wide component captures feature interactions using cross-product transformations, while the deep component learns abstract representations through embedding layers and hidden layers. Joint training combines both components through a weighted sum fed into a common logistic loss, enabling the model to simultaneously memorize specific feature interactions and generalize to unseen feature combinations.",
  "introduction": "# INTRODUCTION\nA recommender system can be viewed as a search ranking system, where the input query includes user and contextual information, and the output is a ranked list of items. The core challenge is to balance **memorization** and **generalization**:\n- Memorization: Learns frequent co-occurrences of items/features and exploits historical correlation, resulting in topical and directly relevant recommendations.\n- Generalization: Explores new feature combinations via correlation transitivity, improving recommendation diversity.\n\nFor large-scale industrial recommendation systems, generalized linear models (e.g., logistic regression) are widely used due to simplicity, scalability, and interpretability. They rely on one-hot encoded sparse features and cross-product transformations (e.g., \"AND(user_installed_app=netflix, impression_app=pandora)\") to achieve memorization. However, cross-product transformations cannot generalize to unseen feature pairs, requiring heavy manual feature engineering for generalization.\n\nEmbedding-based models (e.g., factorization machines, deep neural networks) learn low-dimensional dense embeddings for sparse features, enabling generalization to unseen feature pairs with less feature engineering. However, when the query-item interaction matrix is sparse and high-rank (e.g., niche user preferences or niche items), dense embeddings may over-generalize, leading to irrelevant recommendations. Linear models with cross-product transformations can better memorize such \"exception rules\" with fewer parameters.\n\nThis paper presents the **Wide & Deep learning framework**, which jointly trains a linear model (wide component) and a deep neural network (deep component) to combine the benefits of memorization and generalization. Key contributions:\n1. A unified framework for joint training of linear models (with feature transformations) and deep neural networks (with embeddings) for sparse-input recommender systems.\n2. Productionization and evaluation on Google Play (1 billion+ active users, 1 million+ apps), verifying significant improvements in app acquisitions.\n3. Open-sourced implementation in TensorFlow with a high-level API.",
  "method": "# WIDE & DEEP LEARNING FRAMEWORK\n## 3.1 The Wide Component\nThe wide component is a generalized linear model that captures memorization via cross-product feature transformations:\n\\[\ny = w^T x + b\n\\]\n- \\(y\\): Prediction (log odds for logistic regression).\n- \\(x = [x_1, x_2, ..., x_d]\\): Vector of raw features and transformed features.\n- \\(w = [w_1, w_2, ..., w_d]\\): Model parameters; \\(b\\): Bias.\n\n### Key Transformation: Cross-Product Transformation\nCaptures feature interactions for memorization, defined as:\n\\[\n\\phi_k(x) = \\prod_{i=1}^d x_i^{c_{ki}}, \\quad c_{ki} \\in \\{0,1\\}\n\\]\n- \\(c_{ki}\\): 1 if the \\(i\\)-th feature is part of the \\(k\\)-th transformation, 0 otherwise.\n- For binary features, the transformation is 1 only if all constituent features are 1 (e.g., \"AND(gender=female, language=en)\"), adding nonlinearity to the linear model.\n\n## 3.2 The Deep Component\nThe deep component is a feed-forward neural network that captures generalization via low-dimensional embeddings:\n\n### Step 1: Embedding Layer\nConverts high-dimensional sparse categorical features into low-dimensional dense vectors (embedding size: \\(O(10)\\) to \\(O(100)\\)):\n- Input: Categorical feature strings (e.g., \"language=en\").\n- Initialization: Randomly initialized; updated during training to minimize loss.\n\n### Step 2: Hidden Layers\nThe concatenated embeddings (with continuous features) are fed into ReLU-activated hidden layers:\n\\[\na^{(l+1)} = f(W^{(l)}a^{(l)} + b^{(l)})\n\\]\n- \\(l\\): Layer index; \\(f\\): ReLU activation function.\n- \\(a^{(l)}\\): Activations of the \\(l\\)-th layer; \\(W^{(l)}\\): Weight matrix; \\(b^{(l)}\\): Bias vector.\n\n## 3.3 Joint Training\nThe wide and deep components are combined via a weighted sum of their output log odds, fed into a common logistic loss function for joint training (distinct from ensemble, where models are trained separately):\n\n### Prediction Formula (Logistic Regression Task)\n\\[\nP(Y=1|x) = \\sigma(w_{wide}^T[x, \\phi(x)] + w_{deep}^T a^{(l_f)} + b)\n\\]\n- \\(Y\\): Binary label (1 for positive action, e.g., app installation; 0 otherwise).\n- \\(\\sigma(\\cdot)\\): Sigmoid function (\\(\\sigma(z) = 1/(1+e^{-z})\\)).\n- \\(\\phi(x)\\): Cross-product transformations of raw features \\(x\\).\n- \\(w_{wide}\\): Weights of the wide component; \\(w_{deep}\\): Weights of the deep component’s final activations \\(a^{(l_f)}\\); \\(b\\): Bias.\n\n### Optimization\n- Wide component: Trained with Follow-the-Regularized-Leader (FTRL) algorithm + \\(L_1\\) regularization (suitable for sparse features).\n- Deep component: Trained with AdaGrad (adaptive learning rate for dense embeddings).\n- Joint training: Mini-batch stochastic optimization, with gradients backpropagated to both components simultaneously.",
  "experiments": "# EXPERIMENT RESULTS\n## 4. System Implementation\nThe recommendation pipeline consists of three stages: data generation, model training, and model serving.\n\n### 4.1 Data Generation\n- Training Data: Each example corresponds to an app impression, with label 1 (app installed) or 0 (not installed).\n- Feature Processing:\n  - Categorical features: Mapped to integer IDs via vocabularies (filtering low-frequency features).\n  - Continuous features: Normalized to [0,1] using quantile-based cumulative distribution function (CDF) mapping.\n\n### 4.2 Model Training\n- Model Structure (Figure 4):\n  - Wide component: Cross-product transformations of \"user installed apps\" and \"impression apps\".\n  - Deep component: 32-dimensional embeddings for categorical features → concatenated with dense features (≈1200 dimensions) → 3 ReLU layers (1024, 512, 256) → logistic output.\n- Training Scale: Over 500 billion examples.\n- Warm-Starting: New models are initialized with embeddings and linear weights from the previous model to avoid full retraining.\n- Validation: Dry run and quality check against the previous model before serving.\n\n### 4.3 Model Serving\n- Latency Optimization: Multithreading parallelism (split batches into smaller sizes) to meet 10ms serving latency requirements.\n- Inference: For each request, score app candidates from the retrieval system, rank by model scores, and return to users.\n\n## 5. Experiment Results\n### 5.1 App Acquisitions (Online A/B Test)\n- Duration: 3 weeks.\n- Control Group (1% users): Previous wide-only logistic regression model (rich cross-product features).\n- Experiment Groups:\n  1. Wide & Deep model (1% users): Same features as control.\n  2. Deep-only model (1% users): Same neural network structure as the deep component.\n\n| Model | Offline AUC | Online Acquisition Gain (vs. Control) |\n| --- | --- | --- |\n| Wide-only (Control) | - | Baseline |\n| Deep-only | - | +2.9% |\n| Wide & Deep | Slightly higher than control | +3.9% (statistically significant) |\n\nKey Findings:\n- Wide & Deep outperforms both wide-only (+3.9%) and deep-only (+1%) models.\n- Offline AUC improvement is modest, but online performance is more significant—generalization enables exploratory recommendations that learn from new user responses.\n\n### 5.2 Serving Performance\n- Peak Traffic: 10 million+ app scores per second.\n- Latency Optimization:\n  - Single-threaded batch inference: 31ms.\n  - Multithreaded parallel inference: 14ms (including serving overhead), meeting latency requirements.",
  "hyperparameter": "Embedding dimension: O(10) to O(100), specifically 32-dimensional embeddings used in experiments; Hidden layers: 3 ReLU layers with sizes [1024, 512, 256]; Input dimension after concatenation: approximately 1200 dimensions; Optimization: FTRL with L1 regularization for wide component, AdaGrad for deep component; Training uses mini-batch stochastic optimization; Serving latency target: 10ms with multithreaded parallelism achieving 14ms"
}