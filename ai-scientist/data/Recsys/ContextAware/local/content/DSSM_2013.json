{
  "id": "DSSM_2013",
  "paper_title": "Deep Structured Semantic Models for Web Search using Clickthrough Data",
  "alias": "DSSM",
  "year": 2013,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "The paper proposes Deep Structured Semantic Models (DSSM) for web search that maps queries and documents into low-dimensional semantic vectors using deep neural networks trained on clickthrough data. The key innovation is the word hashing technique based on letter n-grams (e.g., letter trigrams) that reduces input dimensionality from vocabulary size (500K words) to ~30K features, enabling scalable DNN training while handling out-of-vocabulary words and morphological variations. The model is trained in a supervised discriminative manner using clickthrough logs to maximize the likelihood of clicked documents given queries, computing relevance as cosine similarity between semantic vectors.",
  "introduction": "# INTRODUCTION\nModern search engines retrieve Web documents mainly by matching keywords in documents with those in search queries. However, lexical matching can be inaccurate due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries.\n\nLatent semantic models such as latent semantic analysis (LSA) are able to map a query to its relevant documents at the semantic level where lexical matching often fails (e.g., [6][15][2][8][21]). These latent semantic models address the language discrepancy between Web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. Thus, a query and a document, represented as two vectors in the lower-dimensional semantic space, can still have a high similarity score even if they do not share any term. Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA) and Latent Dirichlet Allocation (LDA) have also been proposed for semantic matching [15][2]. However, these models are often trained in an unsupervised manner using an objective function that is only loosely coupled with the evaluation metric for the retrieval task. Thus the performance of these models on Web search tasks is not as good as originally expected.\n\nRecently, two lines of research have been conducted to extend the aforementioned latent semantic models, which will be briefly reviewed below.\n\nFirst, clickthrough data, which consists of a list of queries and their clicked documents, is exploited for semantic modeling so as to bridge the language discrepancy between search queries and Web documents [9][10]. For example, Gao et al. [10] propose the use of Bi-Lingual Topic Models (BLTMs) and linear Discriminative Projection Models (DPMs) for query-document matching at the semantic level. These models are trained on clickthrough data using objectives that tailor to the document ranking task. More specifically, BLTM is a generative model that requires that a query and its clicked documents not only share the same distribution over topics but also contain similar factions of words assigned to each topic. In contrast, the DPM is learned using the S2Net algorithm [26] that follows the pairwise learning-to-rank paradigm outlined in [3]. After projecting term vectors of queries and documents into concept vectors in a low-dimensional semantic space, the concept vectors of the query and its clicked documents have a smaller distance than that of the query and its unclicked documents. Gao et al. [10] report that both BLTM and DPM outperform significantly the unsupervised latent semantic models, including LSA and PLSA, in the document ranking task. However, the training of BLTM, though using clickthrough data, is to maximize a log-likelihood criterion which is sub-optimal for the evaluation metric for document ranking. On the other hand, the training of DPM involves large-scale matrix multiplications. The sizes of these matrices often grow quickly with the vocabulary size, which could be of an order of millions in Web search tasks. In order to make the training time tolerable, the vocabulary was pruned aggressively. Although a small vocabulary makes the models trainable, it leads to suboptimal performance.\n\nIn the second line of research, Salakhutdinov and Hinton extended the semantic modeling using deep auto-encoders [22]. They demonstrated that hierarchical semantic structure embedded in the query and the document can be extracted via deep learning. Superior performance to the conventional LSA is reported [22]. However, the deep learning approach they used still adopts an unsupervised learning method where the model parameters are optimized for the reconstruction of the documents rather than for differentiating the relevant documents from the irrelevant ones for a given query. As a result, the deep learning models do not significantly outperform the baseline retrieval models based on keyword matching. Moreover, the semantic hashing model also faces the scalability challenge regarding large-scale matrix multiplication. We will show in this paper that the capability of learning semantic models with large vocabularies is crucial to obtain good results in real-world Web search tasks.\n\nIn this study, extending from both research lines discussed above, we propose a series of Deep Structured Semantic Models (DSSM) for Web search. More specifically, our best model uses a deep neural network (DNN) to rank a set of documents for a given query as follows. First, a non-linear projection is performed to map the query and the documents to a common semantic space. Then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. The neural network models are discriminatively trained using the clickthrough data such that the conditional likelihood of the clicked document given the query is maximized. Different from the previous latent semantic models that are learned in an unsupervised fashion, our models are optimized directly for Web document ranking, and thus give superior performance, as we will show shortly. Furthermore, to deal with large vocabularies, we propose the so-called word hashing method, through which the high-dimensional term vectors of queries or documents are projected to low-dimensional letter based n-gram vectors with little information loss. In our experiments, we show that, by adding this extra layer of representation in semantic models, word hashing enables us to learn discriminatively the semantic models with large vocabularies, which are essential for Web search. We evaluated the proposed DSSMs on a Web document ranking task using a real-world data set. The results show that our best model outperforms all the competing methods with a significant margin of 2.5-4.3% in NDCG@1.\n\nIn the rest of the paper, Section 2 reviews related work. Section 3 describes our DSSM for Web search. Section 4 presents the experiments, and Section 5 concludes the paper.",
  "method": "# DEEP STRUCTURED SEMANTIC MODELS FOR WEB SEARCH\n## 3.1 DNN for Computing Semantic Features\nThe typical DNN architecture we have developed for mapping the raw text features into the features in a semantic space is shown in Fig. 1. The input (raw text features) to the DNN is a high-dimensional term vector, e.g., raw counts of terms in a query or a document without normalization, and the output of the DNN is a concept vector in a low-dimensional semantic feature space. This DNN model is used for Web document ranking as follows: 1) to map term vectors to their corresponding semantic concept vectors; 2) to compute the relevance score between a document and a query as cosine similarity of their corresponding semantic concept vectors; rf. Eq. (3) to (5).\n\nMore formally, if we denote x as the input term vector, y as the output vector, \\(l_{i}\\) (i=1, ..., N-1) as the intermediate hidden layers, \\(W_{i}\\) as the i-th weight matrix, and \\(b_{i}\\) as the i-th bias term, we have:\n\\[\n\\begin{gathered}\nl_{1}=W_{1} x \\\\\nl_{i}=f\\left(W_{i} l_{i-1}+b_{i}\\right), i=2, ..., N-1 \\\\\ny=f\\left(W_{N} l_{N-1}+b_{N}\\right)\n\\end{gathered}\n\\]\nwhere we use the tanh as the activation function at the output layer and the hidden layers \\(l_{i}, i=2, ..., N-1\\):\n\\[\nf(x)=\\frac{1-e^{-2 x}}{1+e^{-2 x}}\n\\]\n\nThe semantic relevance score between a query Q and a document D is then measured as:\n\\[\nR(Q, D)=cosine\\left(y_{Q}, y_{D}\\right)=\\frac{y_{Q}^{T} y_{D}}{\\left\\| y_{Q}\\right\\| \\left\\| y_{D}\\right\\| }\n\\]\nwhere \\(y_{Q}\\) and \\(y_{D}\\) are the concept vectors of the query and the document, respectively. In Web search, given the query, the documents are sorted by their semantic relevance scores.\n\nConventionally, the size of the term vector, which can be viewed as the raw bag-of-words features in IR, is identical to that of the vocabulary that is used for indexing the Web document collection. The vocabulary size is usually very large in real-world Web search tasks. Therefore, when using term vector as the input, the size of the input layer of the neural network would be unmanageable for inference and model training. To address this problem, we have developed a method called \"word hashing\" for the first layer of the DNN, as indicated in the lower portion of Figure 1. This layer consists of only linear hidden units in which the weight matrix of a very large size is not learned. In the following section, we describe the word hashing method in detail.\n\n## 3.2 Word Hashing\nThe word hashing method described here aim to reduce the dimensionality of the bag-of-words term vectors. It is based on letter n-gram, and is a new method developed especially for our task. Given a word (e.g. good), we first add word starting and ending marks to the word (e.g. #good#). Then, we break the word into letter n-grams (e.g. letter trigrams: #go, goo, ood, od#). Finally, the word is represented using a vector of letter n-grams.\n\nOne problem of this method is collision, i.e., two different words could have the same letter n-gram vector representation. Table 1 shows some statistics of word hashing on two vocabularies. Compared with the original size of the one-hot vector, word hashing allows us to represent a query or a document using a vector with much lower dimensionality. Take the 40K-word vocabulary as an example. Each word can be represented by a 10,306-dimensional vector using letter trigrams, giving a fourfold dimensionality reduction with few collisions. The reduction of dimensionality is even more significant when the technique is applied to a larger vocabulary. As shown in Table 1, each word in the 500K-word vocabulary can be represented by a 30,621 dimensional vector using letter trigrams, a reduction of 16-fold in dimensionality with a negligible collision rate of 0.0044% (22/500,000).\n\nWhile the number of English words can be unlimited, the number of letter n-grams in English (or other similar languages) is often limited. Moreover, word hashing is able to map the morphological variations of the same word to the points that are close to each other in the letter n-gram space. More importantly, while a word unseen in the training set always cause difficulties in word-based representations, it is not the case where the letter n-gram based representation is used. The only risk is the minor representation collision as quantified in Table 1. Thus, letter n-gram based word hashing is robust to the out-of-vocabulary problem, allowing us to scale up the DNN solution to the Web search tasks where extremely large vocabularies are desirable. We will demonstrate the benefit of the technique in Section 4.\n\nIn our implementation, the letter n-gram based word hashing can be viewed as a fixed (i.e., non-adaptive) linear transformation, through which an term vector in the input layer is projected to a letter n-gram vector in the next layer higher up, as shown in Figure 1. Since the letter n-gram vector is of a much lower dimensionality, DNN learning can be carried out effectively.\n\n## 3.3 Learning the DSSM\nThe clickthrough logs consist of a list of queries and their clicked documents. We assume that a query is relevant, at least partially, to the documents that are clicked on for that query. Inspired by the discriminative training approaches in speech and language processing, we thus propose a supervised training method to learn our model parameters, i.e., the weight matrices \\(w_{i}\\) and bias vectors \\(b_{i}\\) in our neural network as the essential part of the DSSM, so as to maximize the conditional likelihood of the clicked documents given the queries.\n\nFirst, we compute the posterior probability of a document given a query from the semantic relevance score between them through a softmax function:\n\\[\nP(D | Q)=\\frac{exp (\\gamma R(Q, D))}{\\sum_{D' \\in D} exp \\left(\\gamma R\\left(Q, D'\\right)\\right)}\n\\]\nwhere γ is a smoothing factor in the softmax function, which is set empirically on a held-out data set in our experiment. D denotes the set of candidate documents to be ranked. Ideally, D should contain all possible documents. In practice, for each (query, clicked-document) pair, denoted by \\((Q, D^{+})\\) where Q is a query and \\(D^{+}\\) is the clicked document, we approximate D by including \\(D^{+}\\) and four randomly selected unclicked documents, denote by \\(\\{D_{j}^{-} ; j=1, ..., 4\\}\\). In our pilot study, we do not observe any significant difference when different sampling strategies were used to select the unclicked documents.\n\nIn training, the model parameters are estimated to maximize the likelihood of the clicked documents given the queries across the training set. Equivalently, we need to minimize the following loss function:\n\\[\nL(\\Lambda)=-log \\prod_{\\left(Q, D^{+}\\right)} P\\left(D^{+} | Q\\right)\n\\]\nwhere Λ denotes the parameter set of the neural networks \\(\\{W_{i}, b_{i}\\}\\). Since \\(L(\\Lambda)\\) is differentiable w.r.t. Λ, the model is trained readily using gradient-based numerical optimization algorithms. The detailed derivation is omitted due to the space limitation.\n\n## 3.4 Implementation Details\nTo determine the training parameters and to avoid over-fitting, we divided the clickthrough data into two factions that do not overlap, called training and validation datasets, respectively. In our experiments, the models are trained on the training set and the training parameters are optimized on the validation dataset. For the DNN experiments, we used the architecture with three hidden layers as shown in Figure 1. The first hidden layer is the word hashing layer containing about 30k nodes (e.g., the size of the letter-trigrams as shown in Table 1). The next two hidden layers have 300 hidden nodes each, and the output layer has 128 nodes. Word hashing is based on a fixed projection matrix. The similarity measure is based on the output layer with the dimensionality of 128. Following [20], we initialize the network weights with uniform distribution in the range between \\(-\\sqrt{6 /( fanin + fanout )}\\) and \\(\\sqrt{6 /( fanin + fanout )}\\) where fanin and fanout are the number of input and output units, respectively. Empirically, we have not observed better performance by doing layer-wise pre-training. In the training stage, we optimize the model using mini-batch based stochastic gradient descent (SGD). Each mini-batch consists of 1024 training samples. We observed that the DNN training usually converges within 20 epochs (passes) over the entire training data.\n\nTable 1: Word hashing token size and collision numbers as a function of the vocabulary size and the type of letter n-grams.\n\n| Word Size | Letter-Bigram |  | Letter-Trigram |  |\n| --- | --- | --- | --- | --- |\n| Token Size | Collision | Token Size | Collision |\n| 40k | 1107 | 18 | 10306 | 2 |\n| 500k | 1607 | 1192 | 30621 | 22 |",
  "experiments": "# EXPERIMENTS\nWe evaluated the DSSM, proposed in Section 3, on the Web document ranking task using a real-world data set. In this section, we first describe the data set on which the models are evaluated. Then, we compare the performances of our best model against other state of the art ranking models. We also investigate the break-down impact of the techniques proposed in Section 3.\n\n## 4.1 Data Sets and Evaluation Methodology\nWe have evaluated the retrieval models on a large-scale real world data set, called the evaluation data set henceforth. The evaluation data set contains 16,510 English queries sampled from one-year query log files of a commercial search engine. On average, each query is associated with 15 Web documents (URLs). Each query-title pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, where level 4 means that the document is the most relevant to query Q and 0 means D is not relevant to Q. All the queries and documents are preprocessed such that the text is white-space tokenized and lowercased, numbers are retained, and no stemming/inflection is performed.\n\nAll ranking models used in this study (i.e., DSSM, topic models, and linear projection models) contain many free hyperparameters that must be estimated empirically. In all experiments, we have used 2-fold cross validation: A set of results on one half of the data is obtained using the parameter settings optimized on the other half, and the global retrieval results are combined from the two sets.\n\nThe performance of all ranking models we have evaluated has been measured by mean Normalized Discounted Cumulative Gain (NDCG) [17], and we will report NDCG scores at truncation levels 1, 3, and 10 in this section. We have also performed a significance test using the paired t-test. Differences are considered statistically significant when the p-value is less than 0.05.\n\nIn our experiments, we assume that a query is parallel to the titles of the documents clicked on for that query. We extracted large amounts of the query-title pairs for model training from one year query log files using a procedure similar to [11]. Some previous studies, e.g., [24][11], showed that the query click field, when it is valid, is the most effective piece of information for Web search, seconded by the title field. However, click information is unavailable for many URLs, especially new URLs and tail URLs, leaving their click fields invalid (i.e., the field is either empty or unreliable because of sparseness). In this study, we assume that each document contained in the evaluation data set is either a new URL or a tail URL, thus has no click information (i.e., its click field is invalid). Our research goal is to investigate how to learn the latent semantic models from the popular URLs that have rich click information, and apply the models to improve the retrieval of those tail or new URLs. To this end, in our experiments only the title fields of the Web documents are used for ranking. For training latent semantic models, we use a randomly sampled subset of approximately 100 million pairs whose documents are popular and have rich click information. We then test trained models in ranking the documents in the evaluation data set containing no click information. The query-title pairs are pre-processed in the same way as the evaluation data to ensure uniformity.\n\n## 4.2 Results\nThe main results of our experiments are summarized in Table 2, where we compared our best version of the DSSM (Row 12) with three sets of baseline models. The first set of baselines includes a couple of widely used lexical matching methods such as TF-IDF (Row 1) and BM25 (Row 2). The second is a word translation model (WTM in Row 3) which is intended to directly address the query-document language discrepancy problem by learning a lexical mapping between query words and document words [9][10]. The third includes a set of state-of-the-art latent semantic models which are learned either on documents only in an unsupervised manner (LSA, PLSA, DAE as in Rows 4 to 6) or on clickthrough data in a supervised way (BLTM-PR, DPM, as in Rows 7 and 8). In order to make the results comparable, we reimplement these models following the descriptions in [10], e.g., models of LSA and DPM are trained using a 40k-word vocabulary due to the model complexity constraint, and the other models are trained using a 500K-word vocabulary. Details are elaborated in the following paragraphs.\n\nTF-IDF (Row 1) is the baseline model, where both documents and queries represented as term vectors with TF-IDF term weighting. The documents are ranked by the cosine similarity between the query and document vectors. We also use BM25 (Row 2) ranking model as one of our baselines. Both TF-IDF and BM25 are state-of-the-art document ranking models based on term matching. They have been widely used as baselines in related studies.\n\nWTM (Rows 3) is our implementation of the word translation model described in [9], listed here for comparison. We see that WTM outperforms both baselines (TF-IDF and BM25) significantly, confirming the conclusion reached in [9]. LSA (Row 4) is our implementation of latent semantic analysis model. We used PCA instead of SVD to compute the linear projection matrix. Queries and titles are treated as separate documents, the pair information from the clickthrough data was not used in this model. PLSA (Rows 5) is our implementation of the model proposed in [15], and was trained on documents only (i.e., the title side of the query-title pairs). Different from [15], our version of PLSA was learned using MAP estimation as in [10]. DAE (Row 6) is our implementation of the deep auto-encoder based semantic hashing model proposed by Salakhutdinov and Hinton in [22]. Due to the model training complexity, the input term vector is based on a 40k-word vocabulary. The DAE architecture contains four hidden layers, each of which has 300 nodes, and a bottleneck layer in the middle which has 128 nodes. The model is trained on documents only in an unsupervised manner. In the fine-tuning stage, we used cross-entropy error as training criteria. The central layer activations are used as features for the computation of cosine similarity between query and document. Our results are consistent with previous results reported in [22]. The DNN based latent semantic model outperforms the linear projection model (e.g., LSA). However, both LSA and DAE are trained in an unsupervised fashion on document collection only, thus cannot outperform the state-of-the-art lexical matching ranking models.\n\nBLTM-PR (Rows 7) is the best performer among different versions of the bilingual topic models described in [10]. BLTM with posterior regularization (BLTM-PR) is trained on query-title pairs using the EM algorithm with a constraint enforcing the paired query and title to have same fractions of terms assigned to each hidden topic. DPM (Row 8) is the linear discriminative projection model proposed in [10], where the projection matrix is discriminatively learned using the S2Net algorithm [26] on relevant and irrelevant pairs of queries and titles. Similar to that BLTM is an extension to PLSA, DPM can also be viewed as an extension of LSA, where the linear projection matrix is learned in a supervised manner using clickthrough data, optimized for document ranking. We see that using clickthrough data for model training leads to some significant improvement. Both BLTM-PR and DPM outperform the baseline models (TF-IDF and BM25).\n\nRows 9 to 12 present results of different settings of the DSSM. DNN (Row 9) is a DSSM without using word hashing. It uses the same structure as DAE (Row 6), but is trained in a supervised fashion on the clickthrough data. The input term vector is based on a 40k-word vocabulary, as used by DAE. L-WH linear (Row 10) is the model built using letter trigram based word hashing and supervised training. It differs from the L-WH non-linear model (Row 11) in that we do not apply any non-linear activation function, such as tanh, to its output layer. L-WH DNN (Row 12) is our best DNN-based semantic model, which uses three hidden layers, including the layer with the Letter-trigram-based Word Hashing (L-WH), and an output layer, and is discriminatively trained on query-title pairs, as described in Section 3. Although the letter n-gram based word hashing method can be applied to arbitrarily large vocabularies, in order to perform a fair comparison with other competing methods, the model uses a 500K-word vocabulary.\n\nThe results in Table 2 show that the deep structured semantic model is the best performer, beating other methods by a statistically significant margin in NDCG and demonstrating the empirical effectiveness of using DNNs for semantic matching.\n\nFrom the results in Table 2, it is also clear that supervised learning on clickthrough data, coupled with an IR-centric optimization criterion tailoring to ranking, is essential for obtaining superior document ranking performance. For example, both DNN and DAE (Row 9 and 6) use the same 40k-word vocabulary and adopt the same deep architecture. The former outperforms the latter by 3.2 points in NDCG@1.\n\nWord hashing allows us to use very large vocabularies for modeling. For instance, the models in Rows 12, which use a 500k-word vocabulary (with word hashing), significantly outperform the model in Row 9, which uses a 40k-word vocabulary, although the former has slightly fewer free parameters than the later since the word hashing layer containing about only 30k nodes.\n\nWe also evaluated the impact of using a deep architecture versus a shallow one in modeling semantic information embedded in a query and a document. Results in Table 2 show that DAE (Row 3) is better than LSA (Row 2), while both LSA and DAE are unsupervised models. We also have observed similar results when comparing the shallow vs. deep architecture in the case of supervised models. Comparing models in Rows 11 and 12 respectively, we observe that increasing the number of non-linear layers from one to three raises the NDCG scores by 0.4-0.5 point which are statistically significant, while there is no significant difference between linear and non-linear models if both are one-layer shallow models (Row 10 vs. Row 11).\n\nTable 2: Comparative results with the previous state of the art approaches and various settings of DSSM.\n\n| Model | NDCG@1 | NDCG@3 | NDCG@10 | References |\n| --- | --- | --- | --- | --- |\n| TF-IDF | - | - | - | - |\n| BM25 | - | - | - | - |\n| WTM | - | - | - | [9] |\n| LSA | 0.287 | 0.365 | 0.452 | [6] |\n| PLSA | 0.295 | 0.371 | 0.456 | [15] |\n| DAE | 0.310 | 0.377 | 0.459 | [22] |\n| BLTM-PR | 0.337 | 0.403 | 0.480 | [10] |\n| DPM | 0.329 | 0.401 | 0.479 | [10] |\n| DNN | 0.342 | 0.410 | 0.486 | - |\n| L-WH linear | 0.357 | 0.422 | 0.495 | - |\n| L-WH non-linear | 0.357 | 0.421 | 0.494 | - |\n| L-WH DNN | 0.362 | 0.425 | 0.498 | - |\n\n## Appendix 1. Gradient Computation and Gradient Descent\nSince \\(L(\\Lambda)\\) is differentiable w.r.t. to Λ, the model is trained readily using gradient-based numerical optimization algorithms. The update rule is:\n\\[\n\\Lambda_{t}=\\Lambda_{t-1}-\\left.\\epsilon_{t} \\frac{\\partial L(\\Lambda)}{\\partial \\Lambda}\\right|_{\\Lambda=\\Lambda_{t-1}}\n\\]\nwhere \\(\\epsilon_{t}\\) is the learning rate at the \\(t^{th}\\) iteration, \\(\\Lambda_{t}\\) and \\(\\Lambda_{t-1}\\) are the models at the \\(t^{th}\\) and the \\((t-1)^{th}\\) iteration, respectively.\n\nIn what follows, we derive the gradient of the loss function w.r.t. the parameters of the neural networks. Assuming that there are in total R (query, clicked-document) pairs, we denote \\((Q_{r}, D_{r}^{+})\\) as the r-th (query, clicked-document) pair. Then, if we denote:\n\\[\nL_{r}(\\Lambda)=-log P\\left(D_{r}^{+} | Q_{r}\\right)\n\\]\nwe have:\n\\[\n\\frac{\\partial L(\\Lambda)}{\\partial \\Lambda}=\\sum_{r=1}^{R} \\frac{\\partial L_{r}(\\Lambda)}{\\partial \\Lambda}\n\\]\n\nIn the following, we will show the derivation of \\(\\frac{\\partial L_{r}(\\Lambda)}{\\partial \\Lambda}\\)\n\nFor a query Q and a document D, we denote \\(l_{i, Q}\\) and \\(l_{i, D}\\) be the activation in the hidden layer i, and \\(y_{Q}\\) and \\(y_{D}\\) be the output activation for Q and D, respectively. They are computed according to Eq.(3).\n\nWe then derive \\(\\frac{\\partial L_{r}(\\Lambda)}{\\partial \\Lambda}\\) as follows. For simplification, the subscript of r will be omitted hereafter.\n\nFirst, the loss function in Eq.(9) can be written as:\n\\[\nL(\\Lambda)=log \\left(1+\\sum_{j} exp \\left(-\\gamma \\Delta_{j}\\right)\\right)\n\\]\nwhere \\(\\Delta_{j}=R(Q, D^{+})-R(Q, D_{j}^{-})\\). The gradient of the loss function w.r.t. the N th weight matrix \\(W_{N}\\) is:\n\\[\n\\frac{\\partial L(\\Lambda)}{\\partial W_{N}}=\\sum_{j} \\alpha_{j} \\frac{\\partial \\Delta_{j}}{\\partial W_{N}}\n\\]\nwhere:\n\\[\n\\frac{\\partial \\Delta_{j}}{\\partial W_{N}}=\\frac{\\partial R\\left(Q, D^{+}\\right)}{\\partial W_{N}}-\\frac{\\partial R\\left(Q, D_{j}^{-}\\right)}{\\partial W_{N}}\n\\]\nand:\n\\[\n\\alpha_{j}=\\frac{-\\gamma exp \\left(-\\gamma \\Delta_{j}\\right)}{1+\\sum_{j'} exp \\left(-\\gamma \\Delta_{j'}\\right)}\n\\]\n\nTo simplify the notation, let a, b, c be \\(y_{Q}^{T} y_{D}\\), \\(1 /\\left\\|y_{Q}\\right\\|\\), and \\(1 /\\left\\|y_{D}\\right\\|\\), respectively. With tanh as the activation function in our model, each term in the right-hand side of Eq.(13) can be calculated using the following formula:\n\\[\n\\frac{\\partial R(Q, D)}{\\partial W_{N}}=\\frac{\\partial}{\\partial W_{N}} \\frac{y_{Q}^{T} y_{D}}{\\left\\| y_{Q}\\right\\| \\left\\| y_{D}\\right\\| }=\\delta_{y_{Q}}^{(Q, D)} l_{N-1, Q}^{T}+\\delta_{y_{D}}^{(Q, D)} l_{N-1, D}^{T}\n\\]\nwhere \\(\\delta_{y_{Q}}^{(Q, D)}\\) and \\(\\delta_{y_{D}}^{(Q, D)}\\) for a pair of \\((Q, D)\\) are computed as:\n\\[\n\\begin{aligned}\n& \\delta_{y_{Q}}^{(Q, D)}=\\left(1-y_{Q}\\right) \\circ\\left(1+y_{Q}\\right) \\circ\\left(b c y_{D}-a c b^{3} y_{Q}\\right) \\\\\n& \\delta_{y_{D}}^{(Q, D)}=\\left(1-y_{D}\\right) \\circ\\left(1+y_{D}\\right) \\circ\\left(b c y_{Q}-a b c^{3} y_{D}\\right)\n\\end{aligned}\n\\]\nwhere the operator ∘ is the element-wise multiplication (Hadamard product).\n\nFor hidden layers, we also need to calculate δ for each \\(\\Delta_{j}\\). For example, each δ in the hidden layer i can be calculated through back propagation as:\n\\[\n\\begin{array} {r}\n{\\delta _{i,Q}^{(Q,D)}=\\left( 1+l_{i,Q}\\right) \\circ \\left( 1-l_{i,Q}\\right) \\circ W_{i+1}^{T}\\delta _{i+1,Q}^{(Q,D)}} \\\\\n{\\delta _{i,D}^{(Q,D)}=\\left( 1+l_{i,D}\\right) \\circ \\left( 1-l_{i,D}\\right) \\circ W_{i+1}^{T}\\delta _{i+1,D}^{(Q,D)}}\n\\end{array}\n\\]\nand eventually we have \\(\\delta_{N, Q}^{(Q, D)}=\\delta_{y_{Q}}^{(Q, D)}\\) and \\(\\delta_{N, D}^{(Q, D)}=\\delta_{y_{D}}^{(Q, D)}\\)\n\nCorrespondingly, the gradient of the loss function w.r.t. the intermediate weight matrix, \\(W_{i}, i=2, ..., N-1\\), can be computed as:\n\\[\n\\frac{\\partial L(\\Lambda)}{\\partial W_{i}}=\\sum_{j} \\alpha_{j} \\frac{\\partial \\Delta_{j}}{\\partial W_{i}}\n\\]\nwhere:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\Delta_{j}}{\\partial W_{i}}=\\left(\\delta_{i, Q}^{\\left(Q, D^{+}\\right)} l_{i-1, Q}^{T}\\right. & \\left.+\\delta_{i, D^{+}}^{\\left(Q, D^{+}\\right)} l_{i-1, D^{+}}^{T}\\right) \\\\\n& -\\left(\\delta_{i, Q}^{\\left(Q, D_{j}^{-}\\right)} l_{i-1, Q}^{T}+\\delta_{i, D_{j}^{-}}^{\\left(Q, D_{j}^{-}\\right)} l_{i-1, D_{j}^{-}}^{T}\\right)\n\\end{aligned}\n\\]\n\n### II. Analysis on Document Ranking Errors\nIn the test data, among 16,412 unique queries, we compare each query's NDCG@1 values using TF-IDF and our best model, letter trigram based word hashing with supervised DNN (L-WH DNN). There are in total 1,985 queries on which L-WH DNN performs better than TF-IDF (the sum of NDCG@1 differences is 1332.3). On the other hand, TF-IDF outperforms L-WH DNN on 1077 queries (the sum of NDCG@1 differences is 630.61). For both cases, we sample several concrete examples. They are shown in Tables 5 and 6, respectively. We observe in Table 5 that the NDCG improvement is largely due to the better match between queries and titles at the semantic level than at the lexical level.\n\nTable 5: Examples that our deep semantic model performs better than TF-IDF.\n\n| | Query | Title |\n| --- | --- | --- |\n| 1 | bfpo | postcodes in the united kingdom wikipedia the free encyclopedia |\n| 2 | univ of penn | university of pennsylvania wikipedia the free encyclopedia |\n| 3 | citibank | citi com |\n| 4 | ccra | canada revenue agency website |\n| 5 | search galleries | photography community including forums reviews net and galleries from photo |\n| 6 | met art | metropolitan museum of encyclopedia art wikipedia the free |\n| 7 | new york brides | long island bride and groom wedding magazine website |\n| 8 | motocycle loans | blank check auto financing is easy with the capital one |\n| 9 | boat | new and used yachts for sale yachtingworld com |\n| 10 | bbc games | bbc sport |\n\nTable 6: Examples that our deep semantic model performs worse than TF-IDF.\n\n| | Query | Title |\n| --- | --- | --- |\n| 1 | hey arnold | hey arnold the movie |\n| 2 | internet by dell | dell hyperconnect mobile internet solutions dell |\n| 3 | www mcdonalds com | mcdonald s’ |\n| 4 | m t | m t bank |\n| 5 | board of directors | board of directors west s encyclopedia of american answers com law full article from |\n| 6 | puppet skits | skits |\n| 7 | montreal canada attractions | go montreal tourist information |\n| 8 | how to address a cover letter | letter how to write a cover |\n| 9 | bbc television | bbc academy |\n| 10 | rock com | from answers com rock music information |\n\nTo make our method more intuitive, we have also visualized the learned hidden representations of the words in the queries and documents. We do so by treating each word as a unique document and passing it as an input to the trained DNN. At each output node, we group all the words with high activation levels and cluster them accordingly. Table 7 shows some example clusters, each corresponding to an output node of the DNN model. It is interesting to see that words with the same or related semantic meanings do stay in the same cluster.\n\nTable 7: Examples of the clustered words on five different output nodes of the trained DNN. The clustering criterion is high activation levels at the output nodes of the DNN.\n\n| automotive | chevrolet | youtube | bear | systems |\n| --- | --- | --- | --- | --- |\n| wheels | fuel | videos | hunting | protect |\n| cars | motorcycle | dvd | texas | platform |\n| auto | toyota | downloads | colorado | efficiency |\n| car | chevy | movie | hunter | oems |\n| vehicle | motorcycles | cd | tucson | systems32 |\n\nNotes:\n1. We present only the derivation for the weight matrices. The derivation for the bias vector is similar and is omitted.\n2. Note that \\(W_{1}\\) is the matrix of word hashing. It is fixed and needs no training.",
  "hyperparameter": "Architecture: 3 hidden layers - first layer (word hashing) with ~30K nodes (letter trigrams), two intermediate hidden layers with 300 nodes each, output layer with 128 nodes; Activation function: tanh for all hidden and output layers; Weight initialization: uniform distribution in range [-√(6/(fanin+fanout)), √(6/(fanin+fanout))]; Training: mini-batch SGD with batch size of 1024 samples, converges within 20 epochs; Vocabulary: 500K words; Letter n-grams: trigrams for word hashing; Negative sampling: 4 randomly selected unclicked documents per clicked document; Smoothing factor γ in softmax: set empirically on held-out data; Model selection: 2-fold cross validation"
}