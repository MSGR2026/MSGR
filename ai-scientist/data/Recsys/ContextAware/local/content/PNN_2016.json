{
  "id": "PNN_2016",
  "paper_title": "Product-based Neural Networks for User Response Prediction",
  "alias": "PNN",
  "year": 2016,
  "domain": "Recsys",
  "task": "ContextAwareRecommendation",
  "idea": "PNN introduces a product layer between the embedding layer and fully connected layers to explicitly model feature interactions in multi-field categorical data for CTR prediction. The core innovation is using product operations (inner product for IPNN, outer product for OPNN) to capture second-order feature interactions, mimicking logical 'AND' operations, while maintaining computational efficiency through mathematical optimizations that reduce complexity from O(D₁N²) to O(D₁MN) for IPNN and from O(D₁M²N²) to O(D₁M(M+N)) for OPNN. This design enables learning complex interaction patterns more effectively than simple concatenation or addition operations used in prior neural CTR models.",
  "introduction": "# INTRODUCTION\nLearning and predicting user response plays a crucial role in many personalization tasks in information retrieval (IR), such as recommender systems, web search, and online advertising. The goal is to estimate the probability that a user will provide a predefined positive response (e.g., clicks, purchases) in a given context. This predicted probability indicates the user’s interest in specific items (e.g., news articles, commercial products, ads) and influences subsequent decisions like document ranking and ad bidding.\n\nData in these IR tasks is mostly in multi-field categorical form (e.g., [Weekday=Tuesday, Gender=Male, City=London]), typically transformed into high-dimensional sparse binary features via one-hot encoding. For example, the three-field vectors with one-hot encoding are concatenated into a single sparse vector.\n\nMany machine learning models have been proposed for such high-dimensional sparse data, including linear logistic regression (LR), non-linear gradient boosting decision trees (GBDT), and factorization machines (FM). However, these models highly depend on feature engineering to capture high-order latent patterns.\n\nRecently, deep neural networks (DNNs) have shown great capability in classification and regression tasks (e.g., computer vision, speech recognition, natural language processing). DNNs can automatically learn more expressive feature representations, making them promising for user response prediction. To improve multi-field categorical data interaction, an embedding methodology based on pre-trained factorization machines has been proposed, with multi-layer perceptrons (MLPs) built on concatenated embedding vectors to explore feature interactions. However, the quality of embedding initialization is limited by the factorization machine, and the \"add\" operations of the perceptron layer may not effectively explore interactions of categorical data across multiple fields. Previous work has shown that local dependencies between features from different fields can be better explored by feature vector \"product\" operations instead of \"add\" operations.\n\nTo leverage DNNs’ learning ability and mine latent data patterns more effectively than MLPs, this paper proposes Product-based Neural Networks (PNN): (i) starting with an embedding layer without pre-training; (ii) building a product layer based on embedded feature vectors to model inter-field feature interactions; (iii) distilling high-order feature patterns with fully connected MLPs. Two types of PNNs are presented (with inner and outer product operations in the product layer) to efficiently model interactive patterns.\n\nTaking CTR estimation in online advertising as a working example, extensive experiments on two large-scale real-world datasets demonstrate that PNN consistently outperforms state-of-the-art user response prediction models across various metrics.",
  "method": "# PRODUCT-BASED NEURAL NETWORKS (PNN)\n## 3.1 Core Framework of PNN\nPNN is designed for user response prediction (taking CTR estimation as an example) on multi-field categorical data. The model architecture includes an embedding layer, a product layer, fully connected hidden layers, and an output layer.\n\n### 3.1.1 Overall Model Formulation\nThe output of PNN is a real number \\(\\hat{y} \\in (0,1)\\) representing the predicted CTR:\n\\[\n\\hat{y} = \\sigma(W_3 l_2 + b_3)\n\\]\n- \\(\\sigma(x) = 1/(1+e^{-x})\\) (sigmoid activation function).\n- \\(W_3 \\in \\mathbb{R}^{1×D_2}\\), \\(b_3 \\in \\mathbb{R}\\): Parameters of the output layer.\n- \\(l_2 \\in \\mathbb{R}^{D_2}\\): Output of the second hidden layer, computed as:\n  \\[\n  l_2 = relu(W_2 l_1 + b_2)\n  \\]\n  - \\(relu(x) = max(0, x)\\) (activation function for hidden layers).\n  - \\(W_2 \\in \\mathbb{R}^{D_2×D_1}\\), \\(b_2 \\in \\mathbb{R}^{D_2}\\): Parameters of the second hidden layer.\n- \\(l_1 \\in \\mathbb{R}^{D_1}\\): Output of the first hidden layer, combining linear signals \\(l_z\\) and quadratic signals \\(l_p\\):\n  \\[\n  l_1 = relu(l_z + l_p + b_1)\n  \\]\n  - \\(b_1 \\in \\mathbb{R}^{D_1}\\): Bias vector of the first hidden layer.\n  - \\(l_z\\) (linear signals) and \\(l_p\\) (quadratic signals) are derived from the product layer:\n    \\[\n    l_z = (l_z^1, l_z^2, ..., l_z^{D_1}), \\quad l_z^n = W_z^n \\odot z\n    \\]\n    \\[\n    l_p = (l_p^1, l_p^2, ..., l_p^{D_1}), \\quad l_p^n = W_p^n \\odot p\n    \\]\n    - \\(W_z^n\\), \\(W_p^n\\): Weights in the product layer (shapes determined by \\(z\\) and \\(p\\)).\n    - \\(\\odot\\): Tensor inner product (element-wise multiplication followed by summation to a scalar).\n\n### 3.1.2 Key Layers\n#### Embedding Layer\nConverts sparse one-hot encoded field vectors into dense embedding vectors. For field \\(i\\), the embedding vector \\(f_i \\in \\mathbb{R}^M\\) is computed as:\n\\[\nf_i = W_0^i x[start_i:end_i]\n\\]\n- \\(x\\): Input feature vector (multi-field one-hot encoded).\n- \\(x[start_i:end_i]\\): One-hot encoded sub-vector for field \\(i\\).\n- \\(W_0^i \\in \\mathbb{R}^{M×(end_i - start_i + 1)}\\): Embedding weights for field \\(i\\) (fully connected within the field).\n\n#### Product Layer\nModels inter-field feature interactions, outputting linear signals \\(z\\) and quadratic signals \\(p\\):\n- Linear signals \\(z\\): Preserves linear information of embeddings:\n  \\[\n  z = (z_1, z_2, ..., z_N) \\triangleq (f_1, f_2, ..., f_N)\n  \\]\n  - \\(N\\): Number of input fields.\n- Quadratic signals \\(p\\): Captures pairwise feature interactions, defined as \\(p = \\{p_{i,j}\\}\\) (\\(i,j=1,...,N\\)), where \\(p_{i,j} = g(f_i, f_j)\\) ( \\(g\\) is the interaction operation, varying by PNN variant).\n\n#### Objective Function\nSupervised training minimizes log loss (captures divergence between predicted and true distributions):\n\\[\nL(y, \\hat{y}) = -y log\\hat{y} - (1-y)log(1-\\hat{y})\n\\]\n- \\(y\\): Ground truth (1 for click, 0 for non-click).\n- \\(\\hat{y}\\): Predicted CTR.\n\n## 3.2 PNN Variants\n### 3.2.1 Inner Product-based Neural Network (IPNN)\nDefines feature interaction as vector inner product: \\(g(f_i, f_j) = <f_i, f_j>\\).\n\n#### Complexity Reduction\nThe original formulation has \\(O(D_1 N^2)\\) complexity. By assuming \\(W_p^n = \\theta^n \\theta^{nT}\\) (\\(\\theta^n \\in \\mathbb{R}^N\\)), complexity is reduced to \\(O(D_1 MN)\\):\n\\[\nW_p^n \\odot p = \\left<\\sum_{i=1}^N \\delta_i^n, \\sum_{i=1}^N \\delta_i^n\\right>\n\\]\n- \\(\\delta_i^n = \\theta_i^n f_i\\) (weighted embedding vector for field \\(i\\)).\n- Final \\(l_p\\) formulation:\n  \\[\n  l_p = \\left(\\left\\|\\sum_i \\delta_i^1\\right\\|, ..., \\left\\|\\sum_i \\delta_i^{D_1}\\right\\|\\right)\n  \\]\n\n### 3.2.2 Outer Product-based Neural Network (OPNN)\nDefines feature interaction as vector outer product: \\(g(f_i, f_j) = f_i f_j^T\\) (outputs an \\(M×M\\) matrix).\n\n#### Complexity Reduction\nBy element-wise superposition, \\(p\\) is redefined to reduce complexity from \\(O(D_1 M^2 N^2)\\) to \\(O(D_1 M(M+N))\\):\n\\[\np = \\sum_{i=1}^N \\sum_{j=1}^N f_i f_j^T = f_{\\sum} (f_{\\sum})^T, \\quad f_{\\sum} = \\sum_{i=1}^N f_i\n\\]\n- \\(p \\in \\mathbb{R}^{M×M}\\) (symmetric matrix), so \\(W_p^n\\) is also symmetric.\n\n## 3.3 Key Discussions\n- **Relationship to Existing Models**: Removing \\(l_p\\) from PNN makes it identical to FNN; without hidden layers and with uniform output weights, IPNN is identical to FM.\n- **Generalization**: Product layers can be extended to more complex interaction operations beyond inner/outer products.\n- **Intuition**: Product operations mimic \"AND\" gates (vs. \"OR\" gates for addition), enabling PNN to learn logical rules for multi-field categorical data.",
  "experiments": "# EXPERIMENTS\n## 4.1 Experimental Settings\n### 4.1.1 Datasets\nTwo large-scale real-world ad click datasets:\n1. **Criteo**: 7 consecutive days of training data + 1 day of evaluation data. After negative down-sampling and feature mapping, it contains 79.38M instances with 1.64M feature dimensions. Recalibrated CTR: \\(q = p/(p + (1-p)/w)\\) ( \\(w\\) is down-sampling ratio).\n2. **iPinYou**: 10-day ad click logs (seasons 2 and 3). After one-hot encoding, it has 19.50M instances with 937.67K feature dimensions. Train/test split: Last 3 days as test set, rest as training set.\n\n### 4.1.2 Compared Models\nAll models implemented with TensorFlow, trained with Stochastic Gradient Descent (SGD):\n- **LR**: Linear logistic regression (baseline, no non-linear capture).\n- **FM**: Factorization machines (captures feature interactions, effective on sparse data).\n- **FNN**: FM-supported neural network (captures high-order latent patterns).\n- **CCPM**: Convolutional Click Prediction Model (learns local-global features via convolutions).\n- **IPNN**: PNN with inner product layer.\n- **OPNN**: PNN with outer product layer.\n- **PNN***: PNN combining inner and outer product layers.\n\n### 4.1.3 Regularization\n- LR/FM: L2 regularization added to loss function.\n- Neural networks (FNN/CCPM/PNNs): Dropout (default rate 0.5) to prevent overfitting.\n\n### 4.1.4 Evaluation Metrics\nFour metrics for comprehensive evaluation:\n1. **AUC**: Area under ROC curve (standard for classification/CTR estimation).\n2. **RIG**: Relative Information Gain ( \\(RIG = 1 - NE\\), \\(NE\\) is Normalized Cross Entropy).\n3. **Log Loss**: Objective function (Eq. (9)).\n4. **RMSE**: Root mean square error.\n\n### 4.1.5 Model Configurations\n- FM: 10-order factorization.\n- Neural networks: 10-order embedding.\n- CCPM: 1 embedding layer + 2 convolution layers (max pooling) + 1 hidden layer (5 layers total).\n- FNN: 1 embedding layer + 3 hidden layers (4 layers total).\n- PNNs: 1 embedding layer + 1 product layer + 3 hidden layers (5 layers total).\n\n## 4.2 Performance Comparison\n### 4.2.1 Overall Results\n#### Criteo Dataset\n| Model | AUC | Log Loss | RMSE | RIG |\n| --- | --- | --- | --- | --- |\n| LR | 71.48% | 0.1334 | 9.362e-4 | 6.680e-2 |\n| FM | 72.20% | 0.1324 | 9.284e-4 | 7.436e-2 |\n| FNN | 75.66% | 0.1283 | 9.030e-4 | 1.024e-1 |\n| CCPM | 76.71% | 0.1269 | 8.938e-4 | 1.124e-1 |\n| IPNN | 77.79% | 0.1252 | 8.803e-4 | 1.243e-1 |\n| OPNN | 77.54% | 0.1257 | 8.846e-4 | 1.211e-1 |\n| PNN* | 77.00% | 0.1270 | 8.988e-4 | 1.118e-1 |\n\n#### iPinYou Dataset\n| Model | AUC | Log Loss | RMSE | RIG |\n| --- | --- | --- | --- | --- |\n| LR | 73.43% | 5.581e-3 | 5.350e-07 | 7.353e-2 |\n| FM | 75.52% | 5.504e-3 | 5.343e-07 | 8.635e-2 |\n| FNN | 76.19% | 5.443e-3 | 5.285e-07 | 9.635e-2 |\n| CCPM | 76.38% | 5.522e-3 | 5.343e-07 | 8.335e-2 |\n| IPNN | 79.14% | 5.195e-3 | 4.851e-07 | 1.376e-1 |\n| OPNN | 81.74% | 5.211e-3 | 5.293e-07 | 1.349e-1 |\n| PNN* | 76.61% | 4.975e-3 | 4.819e-07 | 1.740e-1 |\n\n### 4.2.2 Key Observations\n1. FM outperforms LR, verifying the effectiveness of feature interactions.\n2. Neural networks (FNN/CCPM/PNNs) outperform LR/FM, highlighting the value of high-order latent patterns.\n3. PNNs (IPNN/OPNN) achieve the best performance on both datasets across metrics.\n4. PNN* (combination of IPNN/OPNN) has no obvious advantage, indicating IPNN/OPNN alone sufficiently capture feature interactions.\n5. T-test results (Table III) show PNNs significantly outperform baselines (p-values < 10⁻⁵ under log loss).\n\n### 4.2.3 Learning Curves\nOn the iPinYou dataset, neural networks converge faster than LR/FM. IPNN/OPNN show better convergence than other neural networks, demonstrating the efficiency of the product layer in capturing interactions.\n\n## 4.3 Ablation Study on Network Architecture\n### 4.3.1 Embedding Layer Size\nTested embedding orders (2, 10, 50, 100). Larger orders increase memory usage and over-fitting risk. 10-order embedding is chosen for all neural networks (consistent with FM’s factorization order).\n\n### 4.3.2 Network Depth\nCompared 1, 3, 5, 7 hidden layers for FNN/IPNN/OPNN:\n- Networks with 3 hidden layers achieve the best generalization on test sets.\n- Representation layers (product/convolution layers) capture complex patterns with fewer parameters, improving training efficiency and generalization.\n\n### 4.3.3 Activation Function\nCompared sigmoid, tanh, and relu:\n- Tanh outperforms sigmoid (consistent with prior work).\n- Relu also performs well, benefiting from sparse activation, efficient gradient propagation, and low computation cost.",
  "hyperparameter": "Embedding dimension: 10 (consistent with FM's factorization order); Network depth: 3 hidden layers (best generalization); Activation function: ReLU for hidden layers, sigmoid for output layer; Regularization: Dropout rate 0.5; Optimization: Stochastic Gradient Descent (SGD); Model configuration: 1 embedding layer + 1 product layer + 3 hidden layers (5 layers total for PNN); FM baseline: 10-order factorization; FNN baseline: 1 embedding layer + 3 hidden layers; CCPM baseline: 1 embedding layer + 2 convolution layers + 1 hidden layer"
}