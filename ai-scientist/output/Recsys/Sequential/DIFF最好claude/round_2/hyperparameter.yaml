n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 256                # (int) The number of features in the hidden state.
inner_size: 1024                # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability.
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
selected_features: ['categories', 'brand']    # (list of str) The list of selected item features.
pooling_mode: 'mean'            # (str) The intra-feature pooling mode.
freq_ratio: 0.06                # (float) Ratio for low-frequency cutoff (c/L, e.g., 3/50=0.06).
fusion_type: 'gate'             # (str) Fusion type for attention scores: sum, concat, or gate.
alpha: 0.5                      # (float) Balance between ID-centric and attribute-enriched paths.
align_lambda: 20.0              # (float) Weight for representation alignment loss.
loss_type: 'CE'                 # (str) The type of loss function.
train_neg_sample_args: ~        # Disable negative sampling for CE loss.