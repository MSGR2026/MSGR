embedding_size: 100             # (int) The embedding size of items, paper suggests 100-120 for stable performance.
hidden_size: 100                # (int) The hidden size, should match embedding_size for this architecture.
step: 1                         # (int) The number of GNN propagation steps.
n_layers: 3                     # (int) The number of self-attention blocks, paper suggests 3-4.
n_heads: 2                      # (int) The number of attention heads, hidden_size must be divisible by n_heads.
inner_size: 400                 # (int) The inner hidden size in feed-forward network, typically 4x hidden_size.
hidden_dropout_prob: 0.2        # (float) The dropout probability for hidden layers.
attn_dropout_prob: 0.2          # (float) The dropout probability for attention layers.
layer_norm_eps: 1e-12           # (float) The epsilon for layer normalization.
weight: 0.6                     # (float) The weight factor omega to balance global and local embeddings, paper suggests 0.4-0.8.
reg_weight: 1e-5                # (float) The L2 regularization weight for embeddings.
loss_type: 'CE'                 # (str) The type of loss function. Range in ['BPR', 'CE'].
train_neg_sample_args: ~        # Disable negative sampling for CE loss.