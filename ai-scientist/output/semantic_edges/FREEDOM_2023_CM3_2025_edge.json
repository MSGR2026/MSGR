{
  "source": "FREEDOM_2023",
  "target": "CM3_2025",
  "type": "in-domain",
  "similarities": "1. Both methods utilize a dual-graph learning paradigm, incorporating both user-item bipartite graphs and item-item graphs constructed from multimodal features to capture collaborative signals and item semantic relationships.\n2. Both methods employ LightGCN-style graph convolutions for information propagation on the user-item graph, using normalized adjacency matrices and layer-wise aggregation with a READOUT function (mean/sum) to derive final representations.\n3. Both methods construct item-item graphs based on multimodal feature similarities using k-NN sparsification, where top-k similar items are connected based on cosine similarity of their multimodal embeddings.\n4. Both methods use Deep Neural Networks (MLPs/DNNs) to project high-dimensional raw multimodal features (visual and textual) into lower-dimensional latent spaces that align with the ID embedding dimension.\n5. Both methods combine representations from multiple graphs - the user-item graph provides collaborative filtering signals while the item-item graph provides multimodal semantic information, with final item representations aggregating information from both sources.\n6. Both methods consider visual and textual modalities (M = {v, t}) and use pre-trained feature extractors to obtain initial multimodal features.\n7. Both methods use inner product (dot product) between user and item representations to compute preference scores for top-K recommendation.",
  "differences": "1. Loss Function Design: FREEDOM uses BPR loss with auxiliary multimodal reconstruction losses, while CM3 adopts alignment and uniformity losses from contrastive learning. CM3 introduces a novel 'calibrated uniformity loss' that differentiates item relationships based on their multimodal similarity scores, amplifying repulsion between dissimilar items by factor e^{2t(1-φ)}.\n2. Multimodal Fusion Strategy: FREEDOM uses a simple weighted sum (α_v * visual + α_t * textual) to combine modality-specific item-item graphs. CM3 introduces 'Spherical Bézier Multimodal Fusion' using De Casteljau's algorithm for iterative spherical interpolation on hyperspherical manifold, enabling infinite multimodal fusion while preserving geometric properties.\n3. User-Item Graph Processing: FREEDOM implements degree-sensitive edge pruning (denoising) by sampling edges based on inverse degree probability during training. CM3 does not mention any graph denoising or edge dropout mechanism on the user-item graph.\n4. Item-Item Graph Usage: FREEDOM freezes the item-item graph before training (pre-computed and fixed), eliminating O(N²d_m) computational cost during training. CM3 does not explicitly mention freezing the item-item graph, suggesting it may be dynamically computed or updated.\n5. User Preference Modeling: CM3 introduces a learnable weight matrix W_3 to model user-specific preferences across different modalities by partitioning user embeddings into |M|+1 segments. FREEDOM does not have this explicit user preference differentiation mechanism.\n6. Feature Concatenation: CM3 concatenates all modality features plus the mixed feature (|M|+1 vectors) to form item representations before graph learning. FREEDOM keeps modality features separate and only combines item-item graph output with user-item graph output at the final stage.\n7. Representation Space: CM3 explicitly constrains representations on a unit hypersphere for alignment and uniformity optimization. FREEDOM does not impose such geometric constraints on the embedding space.\n8. Graph Convolution on Item-Item Graph: FREEDOM uses the item-item graph output directly as the multimodal view representation (h̃_i = h̃_i^{L_ii}). CM3 adds a residual connection between the final layer representation and initial representation (X̂ = X̄^{L_ii} + X̃^0).\n9. Similarity Score Computation for Calibrated Loss: CM3 pre-computes clamped similarity scores s(ī, ī') between items using mixed multimodal features for the calibrated uniformity loss, which is a unique component not present in FREEDOM."
}