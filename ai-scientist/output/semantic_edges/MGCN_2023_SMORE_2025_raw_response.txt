```json
{
  "similarities": "1. Both methods adopt a multi-view graph learning paradigm that combines User-Item View (behavioral/collaborative signals) and Item-Item View (semantic/modality-specific signals) for multimodal recommendation.\n2. Both use LightGCN-style message propagation on the user-item interaction graph with symmetric normalization (D^{-1/2}AD^{-1/2}) and layer-wise aggregation via mean pooling to capture high-order collaborative signals.\n3. Both construct item-item affinity graphs based on cosine similarity of raw modality features and apply KNN sparsification to retain top-K edges, followed by symmetric normalization (D^{-1/2}SD^{-1/2}).\n4. Both employ a behavior-guided gating mechanism to filter preference-relevant modality features: E_id ⊙ σ(W·H + b), using ID embeddings to guide modality feature purification.\n5. Both use shallow (single-layer) graph convolution on item-item modality graphs to avoid over-smoothing and noise amplification from deep propagation.\n6. Both aggregate user modality features by weighted-sum aggregation from interacted items: h_u = Σ(1/√|N_u||N_i|)·h_i.\n7. Both use attention mechanisms to extract modality-shared or weighted features: softmax(q^T·tanh(W·H + b)) pattern for computing attention weights.\n8. Both employ InfoNCE contrastive loss to maximize mutual information between behavioral embeddings and multimodal side embeddings for users and items.\n9. Both use BPR loss as the primary optimization objective combined with contrastive auxiliary loss and L2 regularization.\n10. Both produce final user/item representations by adding behavioral embeddings and multimodal side embeddings: e = e_id + e_mul.\n11. Both use inner product for prediction: ŷ = e_u^T · e_i.",
  "differences": "1. **Modality Fusion Approach**: MGCN separates modality-shared and modality-specific features via subtraction (E_specific = E_m - E_shared) and fuses them with preference gates. SMORE introduces Spectrum Modality Fusion using FFT to convert features to frequency domain, applies learnable complex filters for denoising, and performs point-wise product fusion (equivalent to circular convolution) before IFFT conversion back - this is a novel frequency-domain fusion approach requiring implementation of FFT/IFFT operations with complex-valued learnable filters W^c ∈ C^{n×d}.\n2. **Fusion Graph Construction**: MGCN only has modal-specific item-item graphs. SMORE additionally constructs a Fusion Graph using max-pooling strategy across modality graphs: S^f_{a,b} = max(S^m_a, S^{m'}_b) to capture cross-modality complementary features, requiring separate propagation on this fusion graph.\n3. **Feature Propagation Targets**: MGCN propagates behavior-guided modal features on item-item graphs. SMORE propagates both denoised uni-modal features (Ḧ_{i,m}) AND fused features (Ḧ_{i,f}) through their respective graphs, requiring dual propagation paths.\n4. **Modality Preference Extraction**: MGCN uses behavioral embeddings directly through gate functions to get preferences. SMORE uses fusion embeddings (H̄_f) to compute attention weights (α_m) for weighting uni-modal features, then extracts BOTH uni-modal preferences (Q_m) AND fusion preferences (Q_f) from behavioral embeddings - requiring separate preference gates for each.\n5. **Final Side Feature Computation**: MGCN: E_mul = E_s + (1/|M|)Σ(Ẽ_m ⊙ P_m). SMORE: H_s = (1/|M|)(Σ(H*_m ⊙ Q_m)) + (H_f ⊙ Q_f), explicitly incorporating fusion preference-weighted fusion features as an additive term.\n6. **Denoising Mechanism**: MGCN relies on behavior-guided gating for implicit denoising. SMORE implements explicit frequency-domain filtering with learnable complex weights that act as frequency selectors to suppress noise-related information.\n7. **Attention Weight Computation**: MGCN uses shared attention parameters across modalities for extracting common features. SMORE uses modality-specific attention parameters (W_{5,m}, b_{5,m}, p_m) and fusion embeddings as the query source rather than the modality features themselves.\n8. **Implementation Complexity**: SMORE requires: (a) FFT/IFFT operations on modality features, (b) complex-valued neural network components for filters, (c) separate processing pipeline for fusion features throughout the model, (d) max-pooling graph construction, (e) dual contrastive signals from both uni-modal and fusion paths."
}
```