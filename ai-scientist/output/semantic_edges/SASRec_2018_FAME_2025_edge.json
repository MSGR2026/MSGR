{
  "source": "SASRec_2018",
  "target": "FAME_2025",
  "type": "in-domain",
  "similarities": "1. Both methods are built upon the self-attentive sequential recommendation framework, using Transformer-based architecture to model user behavior sequences for next-item prediction.\n2. Both use item embedding layers to convert item IDs into dense vector representations, with learnable embedding matrices of dimension d.\n3. Both employ positional embeddings to inject position information into the sequence, as the self-attention mechanism itself is position-agnostic.\n4. Both utilize the scaled dot-product attention mechanism with queries, keys, and values, following the standard Transformer attention formulation.\n5. Both apply Layer Normalization and Dropout for regularization and training stability, using residual connections to facilitate gradient flow.\n6. Both use point-wise feed-forward networks (FFN) with ReLU activation to introduce non-linearity after the attention layer.\n7. Both generate the final prediction by computing dot product between the sequence representation and item embeddings.\n8. Both use cross-entropy loss as the training objective for next-item prediction.\n9. The target paper (FAME) explicitly states it is built upon SASRec and can be integrated into any attention-based recommendation model, maintaining the core SASRec structure.",
  "differences": "1. **Multi-Head Prediction Mechanism**: SASRec concatenates all attention heads and uses a single FFN for prediction, while FAME treats each attention head independently for prediction. Each head generates its own preference score using sub-embeddings of dimension d'=d/H, requiring implementation of separate prediction paths per head.\n2. **Head-Specific Item Sub-Embeddings**: FAME introduces learnable projection matrices W_f^(h) to transform item embeddings into head-specific sub-embeddings x_v^(h) for each head h. This requires adding H projection matrices of size d×d' and computing separate item representations per head.\n3. **Shared Head-Specific FFN**: FAME uses a shared FFN' across all heads but with reduced dimension d' instead of d. The FFN' parameters (W_1', W_2', b_1', b_2') operate on d'-dimensional inputs, requiring modification of the FFN layer dimensions.\n4. **Gate Mechanism for Head Aggregation**: FAME introduces a gate mechanism (Equation 10-11) with parameters W_g and b_g to compute importance weights for each head's prediction. The final preference score is a weighted sum of all heads' predictions, requiring implementation of softmax-based gating.\n5. **Mixture-of-Experts (MoE) Self-Attention**: FAME replaces the standard query generation with N expert networks per head. Each expert has its own query projection matrix W_Q(n)^(h), generating N different query vectors per item per head. This requires implementing N parallel query transformations.\n6. **Expert Router Network**: FAME adds a router network with parameters W_exp^(h) to dynamically weight the importance of each expert's output within each head. The router computes importance scores β via softmax over concatenated expert representations.\n7. **Integrated Item Representation**: The final item representation in each head is computed as a weighted sum of all experts' outputs (Equation 17), aggregating multiple preference perspectives within each facet.\n8. **Training Pipeline**: FAME employs a two-stage training approach: first pre-training the base SASRec model, then replacing the final layer's query matrix with MoE network and fine-tuning end-to-end. The new components (FFN', gate, router) are randomly initialized.\n9. **Architecture Modification Location**: FAME only modifies the final Transformer layer, keeping earlier layers unchanged. The MoE attention replaces only the query generation while retaining original key and value matrices.\n10. **Computational Structure**: While SASRec has a single attention pathway, FAME requires computing N×H attention patterns (N experts × H heads) in the final layer, significantly increasing the computational graph complexity for the last layer."
}