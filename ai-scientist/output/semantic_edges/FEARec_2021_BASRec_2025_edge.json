{
  "source": "FEARec_2021",
  "target": "BASRec_2025",
  "type": "in-domain",
  "similarities": "1. **Problem Formulation**: Both papers address the Sequential Recommendation (SR) task with identical problem setup - given a user's chronological interaction sequence, predict the next item the user will interact with. Both use the same notation conventions for user set U, item set V/I, and sequence representation.\n\n2. **Embedding Layer Design**: Both methods maintain an item embedding matrix to project item IDs to dense representations. FEARec uses `nn.Embedding(n_items, hidden_size)` and BASRec follows the same standard SR paradigm with embedding matrix M_V ∈ R^{|V|×D}. Both apply Look-up operations to convert item sequences to embedding sequences.\n\n3. **Position Encoding**: Both methods incorporate positional information. FEARec explicitly uses positional embedding (`self.position_embedding`) added to item embeddings, and BASRec follows standard SR paradigm (referencing SASRec) which also uses positional embeddings.\n\n4. **Encoder-based Architecture**: Both methods use an encoder to process the sequence of item embeddings. FEARec uses FEAEncoder (stacked transformer-like blocks), while BASRec mentions using an Encoder module that takes sequence representations as input and outputs sequence/user representations.\n\n5. **Binary Cross-Entropy Loss**: Both methods use BCE loss for training. FEARec implements `nn.CrossEntropyLoss()` for CE loss type, and BASRec explicitly uses BCE loss (Eq. 13) as the main recommendation objective.\n\n6. **Contrastive/Augmentation Learning**: Both methods employ data augmentation strategies to enhance training. FEARec uses contrastive learning with dropout-based augmentation and supervised contrastive samples (sequences with same target item). BASRec uses mixup-based augmentation (M-Reorder, M-Substitute) for similar purposes of generating augmented training samples.\n\n7. **Multi-task Training Strategy**: Both methods combine multiple loss terms. FEARec combines L_Rec + λ₁L_CLReg + λ₂L_FReg. BASRec combines L_main + L_ssa + L_csa. Both use weighted combinations of different objectives.\n\n8. **Two-stage or Progressive Training**: Both methods consider training stability. FEARec applies dropout-based augmentation progressively, while BASRec explicitly uses a two-stage training strategy where augmentation is only applied in the second stage after initial representation learning.",
  "differences": "1. **Core Innovation Focus**: FEARec focuses on frequency domain analysis using FFT to capture periodic behavior patterns and different frequency bands in user sequences. BASRec focuses on balanced data augmentation using mixup techniques to generate diverse yet relevant training samples. For implementation, BASRec does NOT require any FFT operations or frequency domain processing.\n\n2. **Attention Mechanism**: FEARec introduces a novel Hybrid Attention combining time-domain self-attention and frequency-domain auto-correlation attention with frequency ramp sampling. BASRec is model-agnostic and can work with any backbone encoder (e.g., standard SASRec self-attention). For implementation, BASRec should use standard transformer attention without the complex frequency sampling and auto-correlation mechanisms.\n\n3. **Augmentation Strategy - Single Sequence**: FEARec uses dropout-based augmentation where the same sequence passes through the encoder twice with different dropout masks. BASRec introduces M-Reorder (shuffle sub-sequence) and M-Substitute (replace items with similar ones) operators, then mixes original and augmented representations using mixup (Eq. 6-7). Implementation requires: (a) sampling rate from Uniform(a,b), (b) applying reorder/substitute operations, (c) mixing embeddings with Beta-distributed λ.\n\n4. **Augmentation Strategy - Cross Sequence**: FEARec uses supervised contrastive learning with sequences sharing the same target item as positive pairs. BASRec introduces Cross-sequence Augmentation that shuffles batch sequences and performs Item-wise or Feature-wise nonlinear mixup (Eq. 10-12) to discover collaborative signals. Implementation requires: (a) batch shuffling, (b) sampling Λ_I ∈ R^{B×N} or Λ_F ∈ R^{B×D} from Beta distribution, (c) Hadamard product mixing.\n\n5. **Adaptive Loss Weighting**: FEARec uses fixed hyperparameters λ₁, λ₂ for loss weighting. BASRec introduces adaptive loss weighting ω based on augmentation rate and mixup coefficient (Eq. 9), normalizing weights to [0,1] range. Implementation requires computing ω = 1/(rate·λ) and min-max normalization.\n\n6. **Frequency Domain Regularization**: FEARec includes L1 regularization in frequency domain (Eq. 26) to align augmented views' spectrums. BASRec has NO frequency domain components - all operations are in the representation space. No FFT-related loss terms needed.\n\n7. **Frequency Ramp Structure**: FEARec implements layer-specific frequency sampling where different layers focus on different frequency bands (high-frequency in bottom layers, low-frequency in top layers) with formulas for p^l, q^l indices. BASRec has NO such structure - the encoder processes full representations without frequency decomposition.\n\n8. **Auto-correlation Mechanism**: FEARec computes auto-correlation using Wiener-Khinchin theorem to find time-delay patterns and aggregate top-k most correlated sequences (Eq. 14-17). BASRec does NOT use auto-correlation - it uses direct mixup operations instead.\n\n9. **Positive Sample Definition**: FEARec defines supervised positive samples as sequences with the same target item (requires `same_item_index` lookup). BASRec defines positive samples through mixup with shuffled batch sequences - no explicit same-target lookup needed, but requires batch-level operations.\n\n10. **Model Parameter Overhead**: FEARec's HybridAttention introduces additional parameters for frequency processing. BASRec explicitly states NO additional learnable parameters are introduced - only operational changes during training. Implementation should not add new nn.Parameter or nn.Module beyond the base encoder.\n\n11. **Inference Behavior**: Both methods disable augmentation during inference, but FEARec still uses its hybrid attention mechanism. BASRec completely deactivates all augmentation modules, reverting to standard encoder inference.\n\n12. **Mixup Coefficient Sampling**: FEARec does not use Beta distribution for augmentation mixing. BASRec samples λ from Beta(α,α) distribution for all mixup operations. Implementation requires `torch.distributions.Beta(alpha, alpha).sample()` or equivalent."
}