```json
{
  "similarities": "1. Both methods operate in the same domain (Multimedia Recommendation) and address the same task of leveraging multimodal features (visual and textual) to improve recommendation performance.\n2. Both methods construct item-item similarity graphs based on multimodal features using cosine similarity and kNN sparsification to capture item relationships.\n3. Both methods apply Laplacian normalization (symmetric normalization) to the constructed adjacency matrices for stable graph convolution operations.\n4. Both methods use message passing/graph convolution on item-item graphs to propagate and aggregate information, capturing high-order item relationships.\n5. Both methods fuse multiple modality-specific graphs through weighted combination to obtain a unified item-item relationship graph.\n6. Both methods combine the learned item representations from multimodal graphs with collaborative filtering embeddings (user-item interaction graph) for final prediction.\n7. Both methods use BPR (Bayesian Personalized Ranking) loss as the primary optimization objective for pairwise ranking.\n8. Both methods compute final prediction scores using inner product between user and item embeddings.\n9. Both methods use ID embeddings for users and transform/use multimodal features for items in their respective graph learning components.",
  "differences": "1. **Embedding Initialization**: LATTICE uses separate ID embeddings for items in the CF component, while PGL maps item content features through MLP and concatenates them as item raw representations, arguing content features are more informative for model convergence.\n2. **Principal Graph Learning (Novel Component)**: PGL introduces principal subgraph extraction from the user-item interaction graph, which LATTICE does not have. This includes two extraction operators:\n   - Global-Aware Extraction: Uses randomized SVD decomposition followed by truncated reconstruction with ratio γ to identify principal subgraphs with highest information content.\n   - Local-Aware Extraction: Uses multinomial sampling based on node degree (edges connected to lower-exposure nodes have higher sampling probability), with sampling ratio p (typically 0.3).\n3. **Training vs Inference Strategy**: PGL performs message passing on the principal subgraph during training but uses the complete graph during inference to avoid subgraph fragmentation issues. LATTICE uses the same graph structure for both training and inference.\n4. **Latent Graph Learning Simplification**: PGL pre-constructs latent item-item graphs based on raw modality features and keeps them fixed, avoiding the dynamic graph structure learning in LATTICE. LATTICE learns latent structures by transforming features through linear layers and combining learned graphs with initial graphs via skip connection (λ coefficient).\n5. **Feature Transformation**: LATTICE transforms raw modality features to high-level features using linear transformation (W_m, b_m) for dynamic graph learning. PGL simplifies this by using pre-computed similarity matrices without learnable transformation during latent graph construction.\n6. **Self-Supervised Learning**: PGL incorporates an auxiliary self-supervised task using feature masking and InfoNCE loss to enhance representation distinguishability. LATTICE does not include self-supervised learning components.\n7. **Loss Function**: PGL uses combined loss: L = L_BPR + λ_SSL * L_SSL, while LATTICE only uses BPR loss with embedding regularization.\n8. **Final Representation Combination**: PGL explicitly combines representations from principal graph learning and latent graph learning: ê_u = e_u^principal + e_u^latent. LATTICE adds normalized item graph embeddings to CF item embeddings.\n9. **Graph Convolution Target**: PGL applies principal graph learning on the user-item bipartite graph (extracting subgraphs from interaction matrix), while LATTICE focuses graph convolution primarily on item-item graphs constructed from multimodal features.\n10. **Modality Weight Learning**: LATTICE uses learnable modal weights with softmax normalization. PGL mentions weighted addition of modality matrices but the specific mechanism for weight learning is not detailed in the same way."
}
```