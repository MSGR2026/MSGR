{
  "source": "SASRec_2018",
  "target": "DIFF_2025",
  "type": "in-domain",
  "similarities": "1. Both methods are designed for sequential recommendation, taking a user's historical item interaction sequence as input to predict the next item.\n2. Both utilize the Transformer architecture with self-attention mechanisms as the core sequence encoder, including multi-head attention and feed-forward networks (FFN).\n3. Both employ embedding layers to convert item IDs into dense vector representations, with learnable embedding matrices.\n4. Both use causal (unidirectional) attention masking to ensure the model only attends to previous items when predicting the next item, preventing information leakage from future positions.\n5. Both apply layer normalization and dropout for training stabilization and regularization, following standard Transformer practices.\n6. Both use the final position's hidden representation as the user representation for next-item prediction.\n7. Both compute prediction scores via dot-product between the user representation and item embeddings, followed by softmax for probability distribution.\n8. Both support cross-entropy loss as the training objective for next-item prediction.\n9. Both handle variable-length sequences by padding shorter sequences and truncating longer ones to a maximum length.\n10. The stacking of multiple self-attention blocks (layers) is common to both, enabling hierarchical feature extraction from the sequence.",
  "differences": "1. **Side-Information Integration**: SASRec uses only item IDs, while DIFF incorporates multiple item attributes (e.g., brand, category) alongside item IDs. For implementation, DIFF requires separate embedding layers for each attribute type (Equation 5) and a fusion mechanism to combine them.\n2. **Frequency-based Noise Filtering**: DIFF introduces Discrete Fourier Transform (DFT) to decompose embeddings into low-frequency (long-term interests) and high-frequency (short-term/noisy) components (Equations 1-8). This is a novel preprocessing step before attention, requiring FFT/IFFT operations and learnable scaling parameters (β) to control high-frequency component influence. SASRec has no such frequency-domain processing.\n3. **Dual Fusion Strategy**: DIFF employs both early fusion (fusing ID and attributes before attention via Equation 4) and intermediate fusion (ID-centric fusion combining attention scores from separate ID/attribute sequences in Equations 9-11). SASRec only processes a single item ID sequence without any fusion mechanism.\n4. **Dual Multi-sequence Attention**: DIFF computes separate attention scores for item ID sequence and each attribute sequence, then fuses these attention matrices before aggregating values (Equations 9-11). Additionally, it applies self-attention on the early-fused sequence (Equations 12-14). The final user representation combines both paths (Equation 15). SASRec applies standard self-attention on a single sequence.\n5. **Representation Alignment Loss**: DIFF adds a contrastive alignment loss (Equations 16-18) to align item ID embeddings with fused attribute embeddings, ensuring semantic consistency. SASRec uses only the recommendation loss (BCE or CE) without any auxiliary alignment objective.\n6. **Multi-task Learning**: DIFF's total loss combines recommendation loss and alignment loss with a hyperparameter λ (Equation 21). SASRec uses a single-task objective.\n7. **Position Embedding**: SASRec explicitly adds learnable position embeddings to item embeddings (Equation 1). DIFF's method section does not explicitly mention position embeddings, suggesting they may be omitted or handled differently (possibly implicitly through the frequency-based filtering which captures sequential patterns).\n8. **Embedding Sharing**: SASRec explicitly discusses using shared item embeddings for both input and prediction (Equation 6). DIFF uses the item ID embedding matrix for prediction (Equation 19) but maintains separate embedding spaces for IDs and attributes.\n9. **Hyperparameter for Fusion Balance**: DIFF introduces α (Equation 15) to balance ID-centric and attribute-enriched representations. SASRec has no such balancing mechanism.\n10. **Temperature Scaling**: DIFF uses a learnable temperature τ in the alignment loss (Equation 16). SASRec does not have temperature-scaled softmax operations beyond standard attention scaling.\n11. **Implementation Flow for Replication**: To implement DIFF, one should: (a) create separate embedding layers for item IDs and each attribute type; (b) implement early fusion to combine all embeddings; (c) apply FFT to each embedding sequence, split into LFC/HFC, apply IFFT, and recombine with learnable β weights; (d) implement ID-centric fusion with separate Q/K projections for ID and attributes, fuse attention scores, and aggregate with ID values; (e) implement attribute-enriched fusion with self-attention on fused embeddings; (f) combine both representations with α weighting; (g) add alignment loss computation between ID and fused attribute embeddings."
}