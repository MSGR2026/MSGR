{
  "source": "SGL_2021",
  "target": "WeightedGCL_2025",
  "type": "in-domain",
  "similarities": "1. **Core Framework**: Both methods are built upon LightGCN as the backbone GCN encoder, using the same message aggregation mechanism across L layers and averaging embeddings from all layers (Equation 2 in WeightedGCL corresponds to the LightGCN aggregation in SGL).\n\n2. **Contrastive Learning Paradigm**: Both adopt the self-supervised contrastive learning framework with InfoNCE loss function. They create two augmented views and maximize agreement between the same node's representations across views while minimizing agreement between different nodes (Equation 10 in WeightedGCL mirrors Equation 10 in SGL).\n\n3. **Multi-task Training Strategy**: Both combine the main recommendation task (BPR loss) with the self-supervised contrastive loss using a weighted sum. The final loss follows the same pattern: L = L_rec + λ * L_cl (Equation 12 in WeightedGCL corresponds to Equation 11 in SGL).\n\n4. **Temperature Hyperparameter**: Both use a temperature parameter τ in the softmax of the InfoNCE loss to control the sharpness of the contrastive distribution.\n\n5. **User-Item Bipartite Graph Structure**: Both operate on the same user-item interaction graph structure and compute separate contrastive losses for users and items, then sum them together.\n\n6. **Embedding Initialization and Regularization**: Both use L2 regularization on model parameters and similar embedding initialization strategies for user and item embeddings.",
  "differences": "1. **Perturbation Strategy Location**: SGL applies perturbations (Node Dropout, Edge Dropout, or Random Walk) to the graph structure before/during GCN propagation, affecting all layers. WeightedGCL only applies random noise perturbation to the final GCN layer embeddings (E^(L)), keeping earlier layer representations stable. For implementation, instead of modifying the adjacency matrix like SGL, add uniform random noise matrices Δ̄ and Δ̃ ~ U(0,1) directly to the L-th layer embeddings.\n\n2. **Perturbation Type**: SGL uses structural perturbations (dropping nodes/edges from the graph). WeightedGCL uses feature-level perturbations by adding random noise to embedding representations. Implementation requires generating random noise matrices of shape (d × |N|) rather than masking graph edges/nodes.\n\n3. **Dynamic Feature Weighting via SENet**: WeightedGCL introduces a novel Squeeze-and-Excitation Network that SGL lacks entirely:\n   - **Squeeze Network**: Implement average pooling across the feature dimension d to compress each node's d-dimensional perturbed representation into a scalar, producing S ∈ R^(1×|N|).\n   - **Excitation Network**: Implement a multi-layer feedforward network that expands S back to T ∈ R^(d×|N|) using K layers with gradually increasing dimensions (scale s = d^(1/K)), with sigmoid activation.\n   - **Recalibration**: Element-wise multiply the weight matrix T with the perturbed view F to get the final weighted representation R.\n\n4. **Contrastive View Construction**: SGL creates views by graph augmentation (subgraph sampling), while WeightedGCL creates views by: (a) adding noise to final layer embeddings, then (b) applying learned feature weights. The contrastive loss in WeightedGCL operates on the recalibrated representations R̄ and R̃, not directly on the perturbed embeddings.\n\n5. **No Graph Reconstruction Per Epoch**: SGL requires regenerating augmented subgraphs at each training epoch (calling graph_construction()). WeightedGCL only needs to sample new noise matrices per forward pass, which is computationally simpler - no need to recompute normalized adjacency matrices.\n\n6. **Learnable Parameters in Augmentation**: SGL's augmentation is parameter-free (only dropout operations). WeightedGCL introduces trainable parameters in the excitation network (W_1, b_1, ..., W_K, b_K) that need to be optimized jointly with the recommendation model.\n\n7. **Implementation Flow for WeightedGCL**:\n   - Step 1: Run standard LightGCN forward pass to get E^(0) through E^(L)\n   - Step 2: Generate two random noise matrices and add to E^(L) to get Ē^(L) and Ẽ^(L)\n   - Step 3: Compute aggregated representations F̄ and F̃ by averaging all layer embeddings\n   - Step 4: Apply Squeeze operation (average pooling) to get S̄ and S̃\n   - Step 5: Apply Excitation network (multi-layer MLP with sigmoid) to get T̄ and T̃\n   - Step 6: Recalibrate: R̄ = T̄ ⊙ F̄, R̃ = T̃ ⊙ F̃\n   - Step 7: Compute InfoNCE loss using R̄ and R̃\n   - Step 8: Combine with BPR loss for final optimization"
}