{
  "source": "SASRec_2018",
  "target": "BASRec_2025",
  "type": "in-domain",
  "similarities": "1. Both methods address the sequential recommendation problem with the same formulation: given a user's interaction sequence, predict the next item the user will interact with.\n2. Both methods use an item embedding matrix to transform item IDs into dense vector representations, with the same notation M_V ∈ R^{|V|×D}.\n3. Both methods adopt the self-attention based Transformer encoder as the backbone architecture for sequence modeling, following the SASRec paradigm.\n4. Both methods use positional embeddings to inject position information into the input sequence representations.\n5. Both methods apply the same preprocessing strategy: padding short sequences and truncating long sequences to a fixed maximum length.\n6. Both methods use Binary Cross-Entropy (BCE) loss as the primary training objective for the recommendation task.\n7. Both methods share the same space complexity analysis - the model parameters do not grow with the number of users, only with items and embedding dimensions.\n8. Both methods use the final hidden state of the sequence encoder to represent user preferences for prediction.\n9. Both methods compute relevance scores through dot product between sequence output representation and item embeddings.",
  "differences": "1. **Data Augmentation Framework**: BASRec introduces two novel augmentation modules (Single-sequence Augmentation and Cross-sequence Augmentation) that operate in the representation space, while SASRec has no data augmentation mechanism. For implementation, create separate augmentation modules that can be plugged into the training pipeline.\n2. **M-Reorder Operator**: BASRec proposes M-Reorder which (a) selects a sub-sequence with length c = rate × |s_u| where rate ~ Uniform(a,b), (b) shuffles this sub-sequence, and (c) performs mixup between original and reordered sequence embeddings using λ ~ Beta(α,α). Implementation requires: sampling rate from uniform distribution, performing reorder operation, and mixing embeddings via E_u^In = λ·E_u + (1-λ)·E_u'.\n3. **M-Substitute Operator**: BASRec introduces M-Substitute which replaces items at randomly selected indices with correlated items (selected via cosine similarity), then performs the same mixup process. Implementation requires: computing item-item similarity matrix, sampling indices, substituting items, and mixing embeddings.\n4. **Adaptive Loss Weighting**: BASRec computes adaptive weights ω = (1/(rate·λ) - ω_min)/(ω_max - ω_min) for single-sequence augmented samples, allowing the model to learn based on augmentation intensity. Implement this by computing weights for each augmented sample and applying them to the loss.\n5. **Cross-sequence Augmentation**: BASRec mixes representations across different users in a batch through Item-wise and Feature-wise Nonlinear Mixup. Implementation: (a) shuffle batch representations, (b) sample Λ_I ∈ R^{B×N} or Λ_F ∈ R^{B×D} from Beta distribution, (c) perform Hadamard product mixing: H_u^Out = Λ ∘ H_u + (1-Λ) ∘ H_u'.\n6. **Two-stage Training Strategy**: BASRec uses a two-stage training approach - first stage trains with standard BCE loss only (L_main), second stage adds augmentation losses (L_ssa and L_csa). Implementation requires: a training phase flag, and conditional application of augmentation modules.\n7. **Multi-component Loss Function**: BASRec's final loss combines three components: L = L_main + L_ssa + L_csa, where L_ssa is weighted by ω. SASRec uses only the standard BCE/BPR loss. Implement separate loss computation for each component.\n8. **Inference Mode**: BASRec explicitly disables all augmentation modules during inference, returning to standard SASRec-like prediction. Add a training/inference mode switch that bypasses augmentation during evaluation.\n9. **Mixup Coefficient Sampling**: BASRec extensively uses Beta(α,α) distribution for sampling mixup coefficients throughout both augmentation modules. Implement a utility function for beta distribution sampling with configurable α parameter.\n10. **Batch-level Operations**: Cross-sequence augmentation requires batch-level shuffle and mixing operations, which differs from SASRec's sample-independent processing. Implement batch shuffle function and ensure positive/negative item representations are mixed consistently with sequence representations."
}