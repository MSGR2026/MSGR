{
  "source": "LightGCN_2020",
  "target": "NLGCL_2025",
  "type": "in-domain",
  "similarities": "1. Both methods use LightGCN as the backbone for graph convolution, employing the same normalized adjacency matrix formulation: A_hat = D^{-0.5} × A × D^{-0.5}, where A is the user-item bipartite graph adjacency matrix.\n2. Both methods maintain the same graph convolution operation that aggregates neighbor embeddings without feature transformation or nonlinear activation, following the LGC (Light Graph Convolution) design.\n3. Both methods use the same user-item interaction matrix structure to construct the bipartite graph, with the adjacency matrix defined as [[0, R], [R^T, 0]].\n4. Both methods employ BPR (Bayesian Personalized Ranking) loss as part of the optimization objective for the recommendation task.\n5. Both methods use L2 regularization on model parameters to prevent overfitting.\n6. Both methods generate layer-wise embeddings e^{(0)}, e^{(1)}, ..., e^{(L)} through iterative graph convolution propagation.\n7. The trainable parameters in both methods are primarily the initial embeddings at layer 0 (E^{(0)}), maintaining the same model complexity as standard matrix factorization.\n8. Both methods use inner product between user and item embeddings for final prediction/ranking scores.",
  "differences": "1. **Embedding Aggregation Strategy**: LightGCN combines all layer embeddings using weighted sum (mean) to form final representations for prediction, while NLGCL uses individual layer embeddings separately for contrastive learning purposes - the (l-1)-th layer embedding is contrasted with the l-th layer neighbor embeddings.\n2. **Contrastive Learning Framework**: NLGCL introduces a novel contrastive learning component that does not exist in LightGCN. It constructs contrastive views between adjacent layers naturally without additional data augmentation (e.g., node/edge dropout), whereas LightGCN has no contrastive learning mechanism.\n3. **Positive Pair Definition**: In NLGCL, for a user u at layer (l-1), positive pairs are formed with ALL neighbor items' embeddings at layer l (i.e., e_i^{(l)} for all i ∈ N_u), allowing multiple positive samples per anchor. This differs fundamentally from traditional CL methods that use single positive pairs.\n4. **Negative Pair Definition**: NLGCL defines two scopes - (a) Heterogeneous scope: negatives are all non-neighbor items/users; (b) Entire scope: negatives include all nodes except positive neighbors. Implementation needs to handle both scopes.\n5. **Loss Function**: NLGCL adds a neighbor layer contrastive loss L_nl = L_nl_u + L_nl_i to the BPR loss, with the CL loss computed using InfoNCE-style formulation with temperature parameter τ and product over multiple positives in the numerator.\n6. **Layer Selection for CL**: NLGCL uses only the first G layers (G ≤ L) for contrastive learning based on Theorem 1, which states earlier layers provide more effective contrastive views. Implementation should include a hyperparameter G for selecting contrastive view groups.\n7. **Multi-task Training**: NLGCL requires joint optimization of L = L_bpr + λ_1 * L_nl + λ_2 * ||Θ||^2, introducing additional hyperparameter λ_1 to balance the CL loss.\n8. **Computational Consideration**: While NLGCL adds computational overhead proportional to G for CL loss, it avoids the expensive view construction cost of traditional CL methods by using naturally existing layer embeddings. Implementation should efficiently compute the CL loss using batch matrix operations.\n9. **Temperature Parameter**: NLGCL introduces temperature τ in the contrastive loss softmax normalization, which is not present in LightGCN and needs to be tuned.\n10. **Normalization in CL Loss**: The CL loss in NLGCL averages over G groups and normalizes by |N_u| for each user's positive set, requiring careful implementation of the aggregation logic."
}