{
  "source": "BM3_2023",
  "target": "CM3_2025",
  "type": "in-domain",
  "similarities": "1. Both methods utilize LightGCN as the backbone for graph-based representation learning on the user-item interaction graph, employing the same simplified graph convolution without feature transformation and non-linear activation layers.\n2. Both methods project high-dimensional multimodal features (visual and textual) into a lower-dimensional latent space using linear transformations or MLPs, ensuring multimodal features share the same embedding dimension as ID embeddings.\n3. Both methods use a READOUT function (mean in BM3, sum in CM3) to aggregate representations across multiple GCN layers to obtain final user and item embeddings.\n4. Both methods incorporate residual connections for item embeddings, adding initial item representations to the aggregated GCN outputs to mitigate over-smoothing issues.\n5. Both methods compute recommendation scores using inner products between user and item representations for top-K recommendation.\n6. Both methods handle two modalities (vision and text) with modality-specific projection layers, and the architecture can be extended to more modalities.\n7. Both methods operate on the same type of datasets (Amazon Baby, Sports, Electronics) with similar preprocessing and evaluation protocols.",
  "differences": "1. **Multimodal Fusion Strategy**: BM3 does not explicitly fuse multimodal features but aligns them separately with ID embeddings through contrastive losses. CM3 introduces Spherical Bézier Multimodal Fusion using De Casteljau's algorithm to iteratively interpolate multimodal features on a hyperspherical manifold, creating mixed features that lie on the unit hypersphere.\n2. **Feature Concatenation**: CM3 concatenates individual modality features with the mixed multimodal features to form item representations (Equation 4), while BM3 keeps modality features separate and uses them only in contrastive alignment losses.\n3. **Loss Function Design**: BM3 uses a self-supervised contrastive learning framework with three losses: graph reconstruction loss (cosine similarity between user-item pairs), inter-modality alignment loss, and intra-modality masked loss, all based on dropout-based augmentation and stop-gradient. CM3 uses alignment loss (L2 distance for positive pairs) and uniformity losses, with a novel calibrated uniformity loss that differentiates item relationships based on similarity scores.\n4. **Uniformity Loss Innovation**: CM3 introduces calibrated uniformity loss (Equation 10) that amplifies repulsion between dissimilar items by a factor of e^{2t(1-φ)}, where φ is the pre-computed similarity score. This replaces the standard uniformity loss for items while keeping standard uniformity for users. BM3 has no such uniformity constraint.\n5. **User Preference Mining**: CM3 partitions user embeddings into |M|+1 segments corresponding to different modalities and uses a learnable weight matrix W3 to compute differentiated user preferences (Equation 7). BM3 does not have this modality-specific user preference mechanism.\n6. **Item-Item Graph**: CM3 constructs and performs graph convolutions on an item-item graph S based on multimodal features for higher-order item relationships (Section 3.4.3), while BM3 only uses the user-item interaction graph.\n7. **Contrastive View Generation**: BM3 generates contrastive views through dropout-based augmentation with Bernoulli masking and uses a predictor MLP with stop-gradient on target views. CM3 does not use such augmentation-based contrastive learning.\n8. **Projection Architecture**: BM3 uses single-layer linear projections for multimodal features, while CM3 uses two-layer DNNs with non-linear activation (Leaky_relu) for feature projection (Equation 1).\n9. **Embedding Dimension Structure**: CM3 uses concatenated features with dimension ℓ = |M|d + d, while BM3 maintains separate embeddings of dimension d for ID and each modality.\n10. **Regularization**: BM3 explicitly includes L2 regularization on online embeddings in the loss function. CM3's regularization is implicit through the uniformity losses that encourage uniform distribution on the hypersphere."
}