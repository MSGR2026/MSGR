```json
{
  "similarities": "1. Both papers address the sequential recommendation task, where the goal is to predict the next item a user will interact with based on their historical interaction sequence.\n2. Both methods use GRU (Gated Recurrent Units) as a core component to capture sequential dependencies in user behavior. The GRU formulation with update gate and reset gate is identical in both papers.\n3. Both methods use item embedding layers to project discrete item IDs into continuous dense vectors before processing by the sequential model.\n4. Both methods generate a session/sequence representation that is then used to compute similarity scores with candidate item embeddings for prediction.\n5. Both methods use a bilinear/dot-product matching scheme between the learned sequence representation and item embeddings to compute recommendation scores, followed by softmax for probability distribution.\n6. Both methods employ cross-entropy loss for training the recommendation model.\n7. Both methods aim to capture both local and global information from the sequence - NARM uses global encoder (last hidden state) and local encoder (attention-weighted sum), while GLINT-RU uses dense selective GRU and linear attention experts.",
  "differences": "1. **Attention Mechanism Type**: NARM uses a custom attention mechanism computed via Eq.(8) with learnable matrices A1, A2 and vector v to compute attention weights between the final hidden state and all previous hidden states. GLINT-RU uses linear attention (Eq.2) with ELU activation and L2 normalization, which has O(Nd²) complexity instead of O(N²d).\n2. **Dense Selective GRU Module**: GLINT-RU introduces temporal convolution layers (TemporalConv1d) before and after the GRU to aggregate local temporal features from adjacent items (Eq.5, 8). This is absent in NARM which directly feeds item embeddings to GRU.\n3. **Selective Gate Mechanism**: GLINT-RU employs a selective gate (Eq.9) that uses the GRU input to filter/weight the GRU hidden states via element-wise multiplication with SiLU activation. NARM has no such gating on GRU outputs.\n4. **Expert Mixing Architecture**: GLINT-RU uses parallel expert modules (dense selective GRU expert + linear attention expert) with learnable mixing weights α1, α2 (Eq.10) that are softmax-normalized. NARM concatenates global and local representations sequentially rather than mixing parallel experts.\n5. **Gated MLP Block**: GLINT-RU includes a gated MLP block (Eq.12) with GeLU activation for further filtering and nonlinear transformation. NARM uses a simple bilinear transformation (matrix B) without gating.\n6. **Positional Encoding**: NARM does not explicitly handle positional information. GLINT-RU argues that GRU inherently generates fine-grained positional representations (Eq.6) through the recurrent update mechanism, eliminating the need for explicit positional embeddings.\n7. **Representation Combination**: NARM concatenates c_t^g and c_t^l to form the final representation (Eq.9). GLINT-RU uses weighted addition of attention and GRU outputs followed by element-wise gating with the original input.\n8. **Channel Crossing Layer**: GLINT-RU includes a channel crossing layer Φ(H) (Eq.7) to project hidden states into a latent space before the second temporal convolution. NARM does not have this intermediate projection.\n9. **Data-Aware Gating**: GLINT-RU applies multiple data-aware gates (δ1, δ2, δ3) throughout the network that condition on input features to filter intermediate representations. NARM only uses dropout for regularization without input-conditioned gating.\n10. **Architecture Depth**: GLINT-RU is designed as a single-layer architecture with parallel experts for efficiency. NARM uses a single GRU layer but the attention computation is sequential with the global encoder."
}
```